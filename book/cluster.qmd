# Кластеризация и метод главных компонент

## Виды кластерного анализа

Все методы машинного обучения делятся на методы обучения с учителем и методы обучения без учителя. В первом случае у нас есть некоторое количество признаков X, измеренных у n объектов, и некоторый отклик Y. Задача заключается в предсказании Y по X. Например, мы измерили вес и пушистость у сотни котов известных пород, и хотим предсказать породу других котов, зная их вес и пушистость.

_Обучение без учителя_ предназначено для случаев, когда у нас есть только некоторый набор признаков X, но нет значения отклика. Например, есть группа котов, для которых мы измерили вес и пушистость, но мы не знаем, на какие породы они делятся. 

_Кластеризация_ относится к числу методов для обнаружения неизвестных групп (кластеров) в данных. Точнее, это целый набор методов. Мы рассмотрим два из них: 

- кластеризация по методу K средних
- иерархическая кластеризация

В случае с кластеризацией по методу K средних мы пытаемся разбить наблюдения на некоторое заранее заданное число кластеров. Иерархическая кластеризация возвращает результат в виде дерева (_дендрограммы_), которая позволяет увидеть все возможные кластеры.

## Кластеризация по методу K средних 

Алгоритм кластеризации:

1. Каждому наблюдению присваивается случайно выбранное число из интервала от 1 до K (число кластеров). Это исходные метки.

![](https://delladata.fr/wp-content/uploads/2020/05/algo_kmeans.jpg.webp)

2. Вычисляется _центроид_ для каждого из кластеров. Центроид k-го класса -- вектор из p средних значений признаков, описывающих наблюдения из этого кластера.

3. Каждому наблюдению присваивается метка того кластера, чей центроид находится ближе всего к этому наблюдению (удаленность выражается обычно в виде евклидова расстояния).

4. Шаги 2-3 до тех пор, пока метки классов не станут изменяться. 

Это дает возможность минимизировать _внутрикластерный разброс_: хорошей считается такая кластеризация, при которой такой разброс минимален. 

Когда центроиды двигаются, кластеры приобретают и теряют документы.


![](https://www.tidymodels.org/learn/statistics/k-means/kmeans.gif)


:::{.callout-info icon=false}
Внутрикластерный разброс в кластере k -- это сумма квадратов евклидовых расстояний между всеми парами наблюдений в этом кластере, разделенная на общее число входящих в него наблюдений.
:::

### K-means в R

Рассмотрим это сначала на симулированных, а затем на реальных данных. 

```{r}
set.seed(07092024)
x = matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1:2] = x[1:25, 1:2] + 3
x[26:50, 1:2] = x[1:25, 1:2] - 4
```

```{r}
km.out <- kmeans(x, centers = 2, nstart = 20)

km.out$cluster
```
Наблюдения разделились идеально. Вот так выглядят наши центроиды:

```{r message=FALSE, warning=FALSE}
library(tidyverse)

as_tibble(x)  |> 
  ggplot(aes(V1, V2, color = as.factor(km.out$cluster))) +
  geom_point(show.legend = F) +
  geom_point(data = as.data.frame(km.out$centers), color = "grey40", size = 3, alpha = 0.7) +
  theme_light()
```
Аргумент `nstart` позволяет запустить алгоритм функции несколько раз с разными начальными метками кластеров; функция вернет наилучший результат. 

### Кластеризация текстов

Я воспользуюсь датасетом из пакета `stylo`, в котором хранятся частотности 3000 наиболее частотных слов для 26 книг 5 авторов. Один из этих авторов -- таинственный Роберт Гэлбрейт, как выяснилось -- псевдоним Джоан Роулинг. 

```{r message=FALSE}
library(stylo)
data("galbraith")

galbraith <- as.data.frame.matrix(galbraith) |> 
  select(1:150)

galbraith[1:10, 1:10]
```

Если одни признаки имеют больший разброс значений, чем другие, то при вычислении расстояний будут преобладать элементы с более широкими диапазонами.  Поэтому перед применением алгоритма в некоторых случаях рекомендуется нормализовать данные по Z-оценке: из значения признака Х вычитается среднее арифметическое, а результат разделить на стандартное отклонение Х. Это делает функция `scale()`.

$$ X_{new} = \frac{X - Mean(X)}{StDev(X)}$$


```{r}
set.seed(07092024)
km.out <- kmeans(scale(galbraith), centers = 5, nstart = 20)

km.out$cluster
```

```{r}
expected <- str_remove_all(names(km.out$cluster), "_.*")

tibble(expected = expected, 
       predicted = km.out$cluster)  |>  
  group_by(expected) |> 
  count(predicted)
```

Почти все авторы разошлись по разным кластерам (кроме Роулинг), при этом Гэлбрейт в одном кластере с Роулинг. Результат кластеризации по методу k-средних можно визуализировать в двумерном пространстве, прибегнув к методу главных компонент.

## Метод главных компонент

### PCA: общий смысл 

Метод главных компонент (англ. principal component analysis, PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. Этот метод привлекается, в частности, когда надо визуализировать многомерные данные. 

Общий принцип хорошо объясняет Гаральд Баайен [@Baayen2008, 119].

![](./images/pca.png) 


Серый цвет верхнего левого куба означает, что точки распределены равномерно -- нужны все три измерения для того, чтобы описать положение точки в кубе. Куб справа сверху по-прежнему имеет три измерения, но нам достаточно только двух, вдоль которых рассеяны данные. Куб слева снизу тоже имеет два измерения, но вдоль оси y разброс данных меньше, чем вдоль x. Наконец, для куба справа снизу достаточно только одного измерения.

Метод главных компонент ищет такие измерения, вдоль которых наблюдается наибольший разброс данных, причем каждая следующая компонента будет объяснять меньше разброса. 

### PCA в базовом R

```{r}
pca_fit <- prcomp(galbraith, scale = T, center = T)

names(pca_fit)
```
Первый элемент хранит данные о стандартном отклонении, соответствующем каждой компоненте. 

```{r}
round(pca_fit$sdev, 3)
```
Это можно узнать также, вызвав функцию summary.

```{r}
summary(pca_fit)
```

Таким образом, первые две компоненты объясняют почти половину дисперсии, а последняя почти не имеет объяснительной ценности.

```{r}
plot(pca_fit)
```

Координаты текстов в новом двумерном пространстве, определяемом первыми двумя компонентами, хранятся в элементе под названием `x`.

```{r}
pca_fit$x[,1:2]
```

### PCA и кластеры K-means

Функция `augment()` из пакета `broom` позволяет соединить результат анализа с исходными данными. 

```{r}
library(broom)

pca_fit  |> 
  augment(galbraith) |> 
  mutate(expected = str_remove_all(.rownames, "_.+")) |> 
  ggplot(aes(.fittedPC1, .fittedPC2,
             color = expected, 
             shape = as.factor(km.out$cluster))) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_discrete(name = "автор") +
  scale_shape_discrete(name = "кластер") +
  theme_minimal()
```
Еще один способ представить наблюдения.

```{r message=FALSE, warning=FALSE}
# install.packages("FactoMineR")
# install.packages("factoextra")
library(FactoMineR)
library(factoextra)

fviz_pca_ind(pca_fit, geom = c("text"),
             habillage = as.factor(km.out$cluster),
             addEllipses = TRUE) +
  theme(legend.position = "none")
```

Аналогично можно представить и нагрузки компонент. 

```{r warning=FALSE, message=FALSE}

fviz_pca_var(pca_fit, col.var="contrib",
             select.var = list(contrib = 40),
             repel = TRUE)+
  theme_minimal() +
  theme(legend.position = "none")
```

При интерпретации этого графика следует учитывать, что положительно коррелированные переменные находятся рядом, а отрицательно коррелированные переменные находятся в противоположных квадрантах. Например, для первого измерения "his" и "as" коррелированы отрицательно. Это можно проверить, достав матрицу c нагрузками компонент из объекта `pca_fit` (в качестве координат используются коэффициенты корреляции между переменными и компонентами):

```{r}
pca_fit$rotation[c("his", "as"),1:2]
```

Теперь - наблюдения и переменные на одном графике. 

```{r}
fviz_pca_biplot(pca_fit,  geom = c("text"),
                select.var = list(cos2 = 40),
                habillage = as.factor(km.out$cluster),
                col.var = "steelblue",
                alpha.var = 0.3,
                repel = TRUE,
                ggtheme = theme_minimal()) +
  theme(legend.position = "none")

```

Поработать над оформлением такого графика вы сможете в домашнем задании.

## Иерархическая кластеризация 

### Интерпретация дендрограммы

Одним из недостатков кластеризации по методу k-средних является то, что она требует предварительно указать число кластеров. Этого недостатка лишена иерархическая кластеризация. Если такая кластеризация происходит "снизу вверх", она называется _агломеративной_. При этом построение дендрограммы начинается с "листьев" и продолжается вплоть до самого "ствола".

![](./images/clusters1.png) 

При интерпретации дерева надо иметь в виду, что существует $2^{n-1}$ способов упорядочения ветвей дендрограммы, где n -- это число листьев. В каждой из точек слияния можно поменять местами наблюдения, не изменяя смысла дендрограммы. Поэтому выводы  о сходстве двух наблюдений нельзя делать на основе из близости по горизонтальной оси. См. рис. из книги [@хасти2017, 423)]. На рисунке видно, что наблюдение 9 похоже на наблюдение 2 не больше, чем оно похоже на наблюдения 8, 5 и 7. Выводы делаются, исходя из положения на вертикальной оси той точки, где происходит слияние наблюдений. 

Количество кластеров определяется высотой, на которой мы разрезаем дендрограмму. Из этого следует, что одну и ту же дендрограмму можно использовать для получения разного числа кластеров.

### Алгоритм кластеризации

1. Вычислить меру различия для всех пар наблюдений. На первом шаге все наблюдения рассматриваются как отдельный кластер. 

2. Найти пару наиболее похожих кластеров и объединить их. Различие между кластерами соответствует высоте, на которой происходит их слияние в дендрограмме. 

3. Повторить шаги 1-2, пока не останется 1 кластер.

![](./images/clusters2.png)

### Тип присоединения

Вид дерева будет зависеть от типа присоединения. На рисунке ниже представлено три способа: полное, одиночное, среднее. 

![](https://www.researchgate.net/publication/329208978/figure/fig5/AS:755481513562120@1557132237914/Different-linkage-methods-for-hierarchical-clustering.png)

Обычно предпочитают среднее и полное, т.к. они приводят к более сбалансированным дендрограммам. 

![](./images/clusters3.png)

Для функции `hclust()` в R по умолчанию выставлено значение аргумента `method = "complete"`.

### Иерархическая кластеризация в R

Применим алгоритм к симулированным данным, которые мы создали выше. Функция `dist()` по умолчанию считает евклидово расстояние. 

```{r}
hc.complete <- hclust(dist(x), method = "complete")
plot(hc.complete)
```

На картинке видно, что наблюдения из верхих и нижних рядов расходятся на два больших кластера. 

### Иерархическая кластеризация текстов

Для вычисления расстояния между текстами лучше подойдет не евклидово, а косинусное расстояние на нормализованных данных. В базовой `dist()` его нет, поэтому воспользуемся пакетом `philentropy`.

```{r}
dist_mx <- galbraith  |> 
  scale() |> 
  philentropy::distance(method = "cosine", use.row.names = T) 
```

Преобразуем меру сходства в меру расстояния и передадим на кластеризацию. 

```{r}
dist_mx <- as.dist(1 - dist_mx)
hc <- hclust(dist_mx)

plot(hc)
```

Для получения меток кластеров, возникающих в результате рассечения дендрограммы на той или иной высоте, можно воспользоваться функцией `cutree()`.

```{r}
cutree(hc, 5)
```

Этим меткам можно назначить свой цвет.

```{r message=FALSE}
library(dendextend)
hcd <- as.dendrogram(hc)
par(mar=c(2,2,2,7))
hcd |> 
  set("branches_k_color", k = 5) |> 
  set("labels_col", k=5) |> 
  plot(horiz = TRUE)
abline(v=0.8, col="pink4",lty=2)
```


### PCA и иерархическая кластеризация

Код почти как выше, но надо указать, на сколько кластеров мы разрезаем дерево. 


```{r}
pca_fit |> 
  augment(galbraith) |> 
  ggplot(aes(.fittedPC1, .fittedPC2, 
             color = expected, shape = as.factor(cutree(hc, 5)))) +
  geom_point(size = 3, alpha = 0.7)
```

## Многомерное шкалирование

Кроме этого, для визуализации многомерных данных применяют многомерное шкалирование (cmd = classical multidimensional scaling). Функция получает на входе матрицу расстояний. 

```{r}
cmd_fit <- cmdscale(dist_mx)  |> 
  as_tibble()

cmd_fit
```


```{r}
cmd_fit  |> 
  ggplot(aes(V1, V2, 
             color = expected, 
             shape = as.factor(cutree(hc, 5)))) +
  geom_point(size = 3, alpha = 0.7)
```

Многомерное шкалирование стремится отразить _расстояния_ между наблюдениями.
