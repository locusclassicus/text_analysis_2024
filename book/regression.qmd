# Регрессионный анализ

В этом уроке мы познакомимся с методами _простой_ и _множественой линейной регрессии_. 

## Данные: Оксфордская керамика

Данные для этого урока основаны на нескольких публикациях Яна Ходдера, который проанализировал пространственное распределение поздней романо-британской керамики, произведенной в Оксфорде, в статьях 1974 г. "[The Distribution of Two Types of Romano-British coarse pottery](https://www.jstor.org/stable/525737)"  и "[A Regression Analysis of Some Trade and Marketing Patterns](https://www.jstor.org/stable/124001)". Датасет доступен в пакете `archdata`, содержащем и другие наборы данных для археологов.


```{r message=FALSE}
library(archdata)
library(tidyverse)
library(easystats)
library(equatiomatic)
library(broom)
library(gt)
data("OxfordPots")
OxfordPots
```

Датафрейм содержит 30 наблюдений по следующим 7 переменным:

- место;
- процент оксфордской керамики;
- расстояние до Оксфорда в милях;
- процент гончарных изделий из Нью-Фореста;
- расстояние до Нью-Фореста;
- площадь обнесенного стеной города;
- наличие водного транспортного сообщения.


## Простая линейная регрессия

Простая линейная регрессия -- метод, который позволяет предсказывать количественный отклик переменной `y` на основе единственной независимой переменной `x`. Случайная величина, которая используется для целей предсказания, называется _предиктором_. Величина, значения которой предсказываются, называется _переменной отклика_. Здесь мы освоим лишь самые азы, подробнее стоит посмотреть соответствующие уроки курса "[Introduction to Modern Statistics (2e)](https://openintro-ims.netlify.app/)".


### Линейная функция 

Чтобы разобраться с регрессией, надо вспомнить, что такое линейная зависимость:

$$y\approx\beta_o + \beta_1x$$ 

В этом уравнении $\beta_o$ и $\beta_1$ - это константы, известные как _свободный член_ и _угол наклона_ линейной модели. Совокупно их называют _коэффициентами_, или параметрами,  модели. Геометрически первый из них определяет точку пересечения оси y (`intercept`), а второй -- угол наклона (`slope`). 

Посмотрите внимательно на линии на примере и подумайте, чем они отличаются.

![](images/linear.png)

### Ошибка прогноза

На практике линейная зависимость в чистом виде почти не встречается: всегда есть небольшая ошибка прогноза ($\epsilon$):

$$y\approx\beta_o + \beta_1x + \epsilon$$ 

```{r echo=FALSE, message=FALSE}
library(paletteer)
cols <- paletteer_d("ggthemes::wsj_rgby")

set.seed(03102024)
tibble(x = rnorm(50),
       e = rnorm(50),
       y = 10 + 3*x + e) |> 
  ggplot(aes(x, y)) +
  geom_point(color = cols[3], size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", color = cols[2], se = FALSE) +
  theme_minimal()
```
При создании регрессионной модели наша задача заключается в том, чтобы на основе доступных наблюдений подобрать коэффициенты $\beta_0$ и $\beta_1$ таким образом, чтобы минимизировать ошибку. 


$$\sum(y_i- \hat y)^2 = \sum\epsilon^2$$


Чтобы подчеркнуть, что речь идет лишь об оценке, над бетой ставится "крышечка": 

$$\hat y \approx \hat\beta_o + \hat\beta_1x$$

### Простая регрессия с `lm()`

Посмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.

```{r message=FALSE}
library(paletteer)
cols <- paletteer_d("ggthemes::wsj_rgby")

OxfordPots |> 
  ggplot(aes(OxfordDst, OxfordPct)) +
  geom_smooth(method = "lm", color = cols[2], se = FALSE) +
  geom_point(color = cols[3], 
              size = 3, 
              alpha = 0.6
              ) +
  theme_minimal() 
```

Чем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон. 

И наклон, и точку пересечения с осью y определяет функция `lm`. 

```{r}
fit <- lm(OxfordPct ~ OxfordDst, data = OxfordPots)
parameters(fit)
```


```{r echo=FALSE}
parameters(fit) |> 
  gt()  |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 2                
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),  # Bold column labels
    locations = cells_column_labels()   # This targets all column headers
  )
```
## Параметры модели

### Коэффициенты модели

Первый столбец в таблице с параметрами содержит коэффициенты модели. 

```{r echo=FALSE}
parameters(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 2                  
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = Coefficient)
  ) |>
  tab_style(
    style = cell_text(weight = "bold"),  # Bold column labels
    locations = cells_column_labels()   # This targets all column headers
  )
```


Это значит, что наши данные описываются функцией:

```{r}
extract_eq(fit, use_coefs = TRUE)
```


Интуитивно понятно, что коэффициент $\beta_1$ связан с ковариацией (мерой совместной изменчивости двух величин). Действительно, он рассчитывается по формуле: 

$$\beta_1=\frac{Cov(x,y)}{Var(x)}$$
<details>
<summary>Проверить.</summary>


```{r}
x <- OxfordPots$OxfordDst
y <- OxfordPots$OxfordPct

beta_1<- cov(x, y) / var(x)
beta_1
```

</details>


Зная $\beta_1$, можно вычислить $\beta_0$ по формуле:

$$\beta_0=\bar y - \beta_1 \bar x$$
<details>
<summary>Снова проверим.</summary>

```{r}
beta_0 = mean(y) - beta_1 * mean(x)
beta_0
```

</details>



### Стандартные ошибки коэффициентов

Для обоих коэффициентов приведена стандартная ошибка и t-статистика. Столбец `t`, как легко убедиться, содержит результат деления коэффицентов на стандартную ошибку.

```{r echo=FALSE}
parameters(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 2                  
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(SE, t))
    ) |>
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  )
```

<details>
<summary>Как рассчитываются стандартные ошибки.</summary>

Стандартная ошибка для $\beta_0$ рассчитывается по формуле:

$$SE(\beta_0)=\sqrt{\frac{\sum_{i=1}^n\epsilon^2}{n-2}} \times \sqrt{\frac{1}{n}+\frac{\bar x^2}{\sum_{i=1}^n(x_i-\bar x)^2}}$$

Первый множитель в этой формуле -- это дисперсия остатков модели. Чем она больше, тем больше неопределенность.  На второй множитель влияет как размер выборки, так и разброс независимой переменной `x`: чем больше размер выборки n, тем меньше $\frac{1}{n}$ и чем больше $Σ(x - \bar x)^2$, тем меньше второй множитель. Посчитаем вручую и сравним с результатом, который возвращает команда `summary(fit)`.

```{r}
x_bar <- mean(x)

mult1 <- sqrt(sum(fit$residuals^2) / 28)
mult2 <- sqrt(1/30 + ( x_bar^2 / sum((x - x_bar)^2)))

mult1 * mult2
```

Стандартная ошибка для $\beta_1$ рассчитывается по формуле: 

$$SE(b_1)=\sqrt{\frac{\frac{\sum_{i=1}^n\epsilon^2}{n-2}}{\sum_{i=1}^n(x_i-\bar x)^2}}$$

Большая дисперсия остатков (в числителе) будет приводить к увеличению ошибки, а размах $x_i$ --  к уменьшению; интуитивно это объясняется тем, что в таком случае у нас больше информации для оценивания угла наклона. Снова перепроверим.

```{r}
mult1 / sqrt(sum((x - x_bar)^2))
```

</details>

Функция `geom_smooth` добавляет стандартную ошибку коэффициента наклона на график в виде серой полосы, которая означает, что с вероятностью 95% (значение по умолчанию, которое можно поменять) истинное значение отклика находится в этой зоне (`predicted ± 1.95 * se`). В статистике это называется _доверительный интервал_.

```{r message=FALSE}
OxfordPots |> 
  ggplot(aes(OxfordDst, OxfordPct)) +
  geom_smooth(method = "lm", color = cols[2], 
              se = TRUE, level = 0.95) +
  geom_point(color = cols[3], 
              size = 3, 
              alpha = 0.6
              ) +
  theme_minimal() 
```

### P-значения

Столбец `p.value` указывает, какова вероятность случайно получить такое значение. В нашем случае -- почти 0, что говорит о том, что доля оксфордской керамики на участке действительно зависит от расстояния. 


```{r echo=FALSE}
parameters(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 2                  
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(p))
    ) |>
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  )
```

<details>
<summary>Как считается p-value.</summary>

```{r}
tidy(fit) |> 
  transmute(t_stat = estimate / std.error) |> 
  mutate(p_val = 2*pt(abs(t_stat), 28, lower.tail = FALSE)) |> 
  export_table()
```

Результат, возвращаемый функцией `pt()`, умножается на два, т.к. используется двусторонний t-test. Буква `p` в названии означает функцию распределения вероятностей (probability), а `t` -- распределение Стьюдента для заданного числа степеней свободы (28 в нашем случае).

</details>


## Оценка модели

Общая оценка модели проводится при помощи функции `performance()` из пакета `{easystats}` или базовой `summary()`.

```{r}
performance(fit)
```

```{r echo=FALSE}
performance(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3                  
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  )
```

### RSE, MSE, RMSE

```{r echo=FALSE}
performance(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3                  
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(Sigma, RMSE))
    ) 
```

Поскольку наши оценки могут быть как завышенными, так и заниженными, значения ошибок возводятся в квадрат и суммируются по всем точкам данных. Узнаем сумму квадратов остатков (RSS = Residual sum of squares), которая считается по формуле:

$$RSS = \sum_{i=n}^n(y_i- \hat y_i)^2$$

```{r}
rss <- sum(fit$residuals^2)
rss
```

Зная это число, определяем _среднеквадратичную ошибку_ (MSE = Mean square error), _корень  из среднеквадратичной ошибки_  (RMSE), а также _стандартную ошибку остатков_ (RSE = Residual standard error). 

```{r}
mse <- rss / length(fit$residuals)
mse

rmse <- sqrt(mse)
rmse
```

```{r}
rse <-  sqrt(rss / fit$df.residual)
rse
```


### $R^2$

RSE -- это мера несоответствия модели данным. Но поскольку она выражается в тех же единицах измерения, что и `y`, то не всегда бывает ясно, какая RSE является хорошей. Коэффициент детерминации $R^2$ представляет собой альтернативную меру соответствия. Этот показатель принимает форму доли -- доли объясненной дисперсии, в связи с чем он всегда изменяется от 0 до 1 и не зависит от шкалы измерения.

$$R^2 = \frac{TSS-RSS}{TSS} = 1 - \frac{RSS}{TSS}$$
Здесь $TSS = \sum(y_i - \bar y)^2$, то есть общая сумма квадратов. 

`TSS` является мерой общей дисперсии отклика Y, и о ней можно думать как о степени изменчивости, присущей отклику до выполнения регрессионного анализа. В то же время `RSS` выражает степень изменчивости, которая осталась необъясненной после построения регрессионной модели. Следовательно, `TSS - RSS` выражает количество дисперсии отклика, объясненное ("изъятое") после выполнения регрессионного анализа, а $R^2$ -- долю дисперсии Y, объясненную при помощи X. Статистика $R^2$, близкая к 1, означает, что значительная доля изменчивости отклика была объяснена регрессионной моделью [@хасти2017, 82].

```{r}
tss <- sum((y - mean(y))^2)
tss

1 - rss / tss
```

Снова сравним с результатом, который нам вернула модель.

```{r echo=FALSE}
performance(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3                  
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(R2))
    ) 
```

Для простой линейной регрессии статистика $R^2$ совпадает с квадратом коэффициента корреляции.

```{r}
cor(x, y)^2
```

### Анализ остатков

Как правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые _невязки_) должны быть минимальны. 

Невязка -- это разница между прогнозируемым и фактическим значениями отклика: $(y_i- \hat y)$. На графике ниже невязки обозначены пунктиром. 

```{r}
OxfordPots |> 
  ggplot(aes(OxfordDst, OxfordPct)) +
  geom_smooth(method = "lm", color = cols[2], se = FALSE) +
  geom_point(color = cols[3], 
              size = 3, 
              alpha = 0.6
              ) +
  geom_segment(aes(xend = OxfordDst,
                   yend = predict(fit)), 
               linetype = 2, 
               color = cols[1]) +
  theme_minimal() 
```

<details>
<summary>Перепроверим.</summary>

Мы можем убедиться в том, что невязки (`fit$residuals`) представляют собой разницу между фактическим (`OxfordPots$OxfordPct`) и предсказанным значением (`fit$fitted.value`). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями. 

```{r}
all.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)
```

</details>

Если модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.

```{r}
mean(fit$residuals) |> 
  round(2)
cov(fit$residuals, OxfordPots$OxfordDst) |> 
  round(2)
```

Кроме того, полезно проверить остатки на нормальность и гомоскедастичность (равномерность дисперсии остатков). Это можно сделать при помощи специального теста или визуально. 

```{r}
shapiro.test(residuals(fit))
``` 

Высокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально. 

Также проведем визуальные тесты. 


```{r}
#| fig-width: 9
#| fig-height: 9

check_model(fit)
```


:::{.callout-warning icon=false}
Установите курс `swirl::install_course("Regression_Models")`, запустите `swirl()` и пройдите уроки №1 "Introduction", №2 "Residuals", №3 "Least Squares Estimation", №4 "Residual Variation". 
:::


## Сравнение моделей

### Нулевая модель

Важно знать, что следующие два вызова возвращают одинаковые модели.

```{r}
fit1 <- lm(OxfordPct ~ OxfordDst, data = OxfordPots)
fit2 <- lm(OxfordPct ~ 1 + OxfordDst, data = OxfordPots)
```

```{r}
fit1$coef == fit2$coef
```
Единица в вызове функции означает пересечение оси y, то есть свободный член. Это значит, что мы можем построить нулевую модель, где любому значению `x` будет соответствовать одно и то же (среднее) значение `y`.

```{r}
fit_null <- lm(OxfordPct ~ 1, data = OxfordPots)
parameters(fit_null)
```

```{r echo=FALSE}
parameters(fit_null) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3                  
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  ) 
```

Единственный коэффициент в таком случае совпадает со средним значением `y`. 

```{r}
mean(OxfordPots$OxfordPct)
```

На графике это будет выглядеть вот так.

```{r}
OxfordPots |> 
  ggplot(aes(OxfordDst, OxfordPct)) +
  # обратите внимание на формулу!
  geom_smooth(method = "lm", formula = y ~ 1,
              color = cols[2], se = FALSE) +
  geom_point(color = cols[3], 
              size = 3, 
              alpha = 0.6
              ) +
  theme_minimal()
```

Такая модель может быть использована для сравнения, чтобы понять,  насколько мы выиграли, добавив предикторы. 

### ANOVA

Функция `anova()` сравнивает две вложенные линейные регрессионные модели с помощью анализа дисперсии. Цель — выяснить, добавляет ли переменная OxfordDst значительное улучшение модели по сравнению с моделью без предикторов.

```{r}
anova(fit_null, fit)
```

```{r echo=FALSE}
anova(fit_null, fit) |> 
  export_table()
```



| Столбец         | Значение                                                    |
|-----------------|-------------------------------------------------------------|
| Res.Df      | Остаточные степени свободы: число наблюдений минус число параметров модели. |
| RSS        | Residual Sum of Squares — сумма квадратов остатков. Чем меньше, тем лучше. |
| Df          | Разница в степени свободы между моделями (число добавленных предикторов). |
| Sum of Sq   | Улучшение, достигнутое за счёт добавленного предиктора (OxfordDst), то есть разница в RSS. |
| F          | F-статистика для оценки значимости улучшения модели.        |
| Pr(>F)      | p-значение: насколько вероятно наблюдать такую F-статистику случайно. |

RSS уменьшилась с 1408.88 до 878.44 после добавления переменной OxfordDst, значит модель улучшилась. _F-статистика_ = 16.908, а _p-value_ значительно ниже уровня значимости 0.05. Три звездочки (***) означают статистически значимую разницу между моделями.

:::{.callout-note icon=FALSE}
F-статистика — это статистика, которая используется для оценки качества модели в анализе дисперсии (ANOVA) и в регрессионном анализе. Она показывает, насколько хорошо модель с предикторами объясняет данные по сравнению с моделью без предикторов (или с меньшим их числом). Чем больше значение F, тем сильнее улучшение модели при добавлении переменных. Если получить такую большую F при случайных данных маловероятно (что отражает малое p-значение), то мы делаем вывод, что переменная значимо улучшает модель.
:::


### Сравнение с `{easystats}`

Для сравнения моделей полезны следующие функции:

```{r}
compare_performance(fit_null, fit, rank = TRUE)
```

```{r echo=FALSE}
compare_performance(fit_null, fit, rank = TRUE) |> 
  export_table()
```


```{r}
compare_performance(fit_null, fit) |> 
  plot()
```

```{r}
library(report)
compare_performance(fit_null, fit) |> 
  report()
```

## Предсказания с `predict()`

Предсказанные значения можно извлечь при помощи `predict()`. Это почти то же самое, что `fit$fitted.values`. Разница в том, что функции `predict()` можно передать новые данные. Узнаем, какую долю оксфордской керамики наша модель ожидает обнаружить на расстоянии ровно 100 миль от Оксфорда.

```{r}
newdata <- data.frame(OxfordDst = 100)
predict(fit, newdata)
```

Под капотом функция `predict()` подставляет подогнанные значения коэффициентов:

```{r}
fit$coefficients[[1]] + fit$coefficients[[2]] * 100
```


## Множественная регрессия

Множественная регрессия подходит для тех случаев, где на переменную отклика могут влиять несколько предикторов. Допустим, что в случае с долей оксфордской керамики это не только расстояние от Оксфорда, но и близость крупных городских центров, вокруг которых выстраивались торговые взаимодействия.

В общем виде множественная регрессионная модель имеет форму:

$$y = \beta_0 + \beta_1x_1 + \beta_2x_2+ ... \beta_px_p + \epsilon$$

### Модель с двумя предикторами

Подгоним вторую модель и посмотрим, дает ли нам что-то добавление второго предиктора.

```{r}
fit2 <- lm(OxfordPct ~ OxfordDst + WalledArea, data = OxfordPots)
```

```{r echo=FALSE}
# для сравнения
performance(fit) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3                  
  ) |> 
  tab_header(
    title = md("*Model with 1 Predictor*")  
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")),
    locations = cells_body(columns = c(R2, Sigma))
    )
```
```{r echo=FALSE}
performance(fit2) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3                  
  ) |> 
  tab_style(
    style = cell_text(weight = "bold"),  
    locations = cells_column_labels()   
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(R2, Sigma))
    ) |> 
  tab_header(
    title = md("*Model with 2 Predictors*")  # 👈 Bold title using markdown
  ) 
```
На первый взгляд, все хорошо: RSE уменьшилась, а доля объясненной дисперсии увеличилась. 

```{r message=FALSE}
compare_performance(fit_null, fit, fit2) |> 
  plot()
```


Однако p-value для второго предиктора (0.69) указывает на то, что он не является статистически значимым. 

```{r}
parameters(fit2)
```

```{r echo=FALSE}
parameters(fit2) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3    
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(p))
    )
```

Это может означать, что связи между площадью обнесенного стеной города и числом оксфордских горшков на самом деле нет.

:::{.callout-info icon=false}
Кстати, к похожему выводу пришел и Ян Ходдер в упомянутых исследованиях: торговля грубой керамикой, данные о которой содержит наш датасет, меньше зависит от городов, чем торговля более изысканными товарами. Одним словом, горшки везде нужны, и в городе, и в деревне.
:::

Почему же мы видим увеличение $R^2$? Дело в том, что этот показатель _всегда возрастает_ при добавлении в модель дополнительных переменных, даже если эти переменные очень слабо связаны с откликом. Поэтому важнейшая задача при обучении модели связана с отбором информативных переменных. В противном случае велик риск _переобучить_ модель.  

### Мнимые переменные

Для построения модели можно использовать не только количественные, но и качественные предикторы. Если качественный предиктор имеет только два уровня (например, мужской и женский пол), то он превращается в фиктивную переменную, принимающую значения 1 или 0. В нашем датасете в таком виде хранятся сведения о наличии водного сообщения между Оксфордом и местом обнаружения керамических осколков. 

```{r}
fit3 <- lm(OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)
parameters(fit3)
```

```{r echo=FALSE}
parameters(fit3) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 3    
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(Coefficient))
    )
```

Обратите внимание, что угловой коэффициент для WaterTrans представляет собой положительное число: если водный путь есть, линия регрессии не так резко уходит вниз по мере удаления от Оксфорда. 

Очевидно, что наличие водного пути -- важный предиктор, что можно подтвердить графически. 

```{r message=FALSE}
library(paletteer)
cols <- paletteer_d("ggthemes::wsj_rgby")

OxfordPots |> 
  ggplot(aes(OxfordDst, OxfordPct, 
             color = as.factor(WaterTrans), 
             group = as.factor(WaterTrans))) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_color_manual("WaterTrans", values = cols[4:3]) +
  theme_minimal()
```



Сравним эффективность моделей при помощи функции `compare_performance()`. Добавим и вторую модель тоже, хотя мы помним, что она содержит статистически незначимый предиктор.


```{r echo=FALSE, message=FALSE}
compare_performance(fit_null, fit, fit2, fit3, rank = TRUE) |> 
  gt() |> 
  fmt_number(
    columns = where(is.numeric),  
    decimals = 2
  ) |> 
  tab_style(
    style = list(
      cell_fill(color = cols[1]),
      cell_text(weight = "bold")
    ),
    locations = cells_body(columns = c(R2, Sigma))
    )
```
```{r message=FALSE}
compare_performance(fit_null, fit, fit2, fit3) |> 
  plot()
```

## Что осталось за кадром

В этом уроке мы не рассмотрели множество аспектов регрессионного анализа: необходимость трансформации данных, учет эффектов взаимодействия переменных, использование полиномиальных моделей и др.  



:::{.callout-warning icon=false}
Установите курс `swirl::install_course("Regression_Models")`, запустите `swirl()` и пройдите уроки №5 "Introduction to Multivariable Regression", №2 "MultiVar Examples", №3 "MultiVar Examples2", №4 "MultiVar Examples3", "Residuals Diagnostics and Variation", "Variance Inflation Factors", "Overfitting and Underfitting", "Binary Outcomes", "Count Outcomes"
:::

