# Веб-скрапинг

Файлы html, как и XML, хранят данные в структурированном виде. Извлечь их позволяет пакет `rvest`. С его помощью мы добудем архив телеграм-канала Antibarbari HSE. Канал публичный, и Telegram дает возможность скачать архив в формате html при помощи кнопки export (эта функция может быть недоступна на MacOS, в этом случае стоит попробовать [Telegram Lite](https://apps.apple.com/us/app/telegram-lite/id946399090)). 

Эта глава опирается в основом на [второе издание](https://r4ds.hadley.nz/webscraping.html) книги R for Data Science Хадли Викхема.

## Структура html 

Документы html (HyperText Markup Language) имеют ирархическую структуру, состоящую из **элементов**.  В каждом элементе есть **открывающий тег** (\<tag\>), опциональные **атрибуты** (id=\'first\') и **закрывающий тег** (\</tag\>). Все, что находится между открывающим и закрывающим тегом, называется **содержанием** элемента. 

Важнейшие теги, о которых стоит знать:

- \<html\> (есть всегда), с двумя детьми (дочерними элементами): \<head\> и \<body\>
- элементы, отвечающие за структуру: \<h1\> (заголовок), \<section\>, \<p\> (параграф), \<ol\> (упорядоченный список)
- элементы, отвечающие за оформление: \<b\> (bold), \<i\> (italics), \<a\> (ссылка)

Чтобы увидеть структуру веб-страницы, надо нажать правую кнопку мыши и выбрать `View Source` (это работает и для тех html, которые хранятся у вас на компьютере).


## Каскадные таблицы стилей

У тегов могут быть именованные атрибуты; важнейшие из них -- это `id` и `class`, которые в сочетании с CSS контролируют внешний вид страницы.

:::{.callout-note icon=false}
CSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML).
:::

Пример css-правила (такие инфобоксы использованы в [предыдущей](https://locusclassicus.github.io/text_analysis_2023/%D0%B2%D0%B5%D0%B1-%D1%81%D0%BA%D1%80%D0%B0%D0%BF%D0%B8%D0%BD%D0%B3.html#%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%82%D1%83%D1%80%D0%B0-html) версии курса):

```{css}
.infobox {
  padding: 1em 1em 1em 4em;
  background: aliceblue 5px center/3em no-repeat;
  color: black;
}
```

Проще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — _селектор_ и _блок объявлений_. Селектор, расположенный в левой части правила до знака `{`, определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;».

Селекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так:

- `p` выберет все элементы \<p\>
- `.title` выберет элементы с классом "title"
- `#title` выберет все элементы с атрибутом id='title'

Важно: если изменится структура страницы, откуда вы скрапили информацию, то и код придется переписывать.

## Чтение html

Чтобы прочесть файл html, используем одноименную функцию.

```{r message=FALSE}
library(rvest)
antibarbari_files <- list.files("../files/antibarbari_2024-08-18", pattern = "html", full.names = TRUE)
```

Используем пакет `purrr`, чтобы прочитать сразу три файла из архива. 

```{r message=FALSE}
library(tidyverse)
antibarbari_archive <- map(antibarbari_files, read_html)
```

## Парсинг html: отдельные элементы

На следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельный html; `rvest` позволяет это сделать (но внутри двойных кавычек должны быть только одинарные):

```{r}
example_html <-  minimal_html("
<div class='message default clearfix' id='message83'>
      <div class='pull_left userpic_wrap'>
       <div class='userpic userpic2' style='width: 42px; height: 42px'>
        <div class='initials' style='line-height: 42px'>
A
        </div>
       </div>
      </div>
      <div class='body'>
       <div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'>
11:18
       </div>
       <div class='from_name'>
Antibarbari HSE 
       </div>
       <div class='text'>
Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. <br><br>«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).<br><a href='http://antibarbari.ru/2022/05/19/digest_1/'>http://antibarbari.ru/2022/05/19/digest_1/</a>
       </div>
       <div class='signature details'>
Olga Alieva
       </div>
      </div>
     </div>
")
```

Из всего этого мне может быть интересно id сообщения (`\<div class='message default clearfix' id='message83'\>`), текст сообщения (`\<div class='text'\>`), дата публикации (`\<div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'\>`), а также, если указан, автор сообщения (`\<div class='signature details'\>`). Извлекаем текст (для этого [рекомендуется](https://r4ds.hadley.nz/webscraping.html#fn6) использовать функцию `html_text2()`):

```{r}
example_html |>
  html_element(".text") |> 
  html_text2()
```

В классе `signature details` есть пробел, достаточно на его месте поставить точку:

```{r}
example_html |>
  html_element(".signature.details") |> 
  html_text2()
```

Осталось добыть дату и message id:

```{r}
example_html |> 
  html_element(".pull_right.date.details") |> 
  html_attr("title")
```

```{r}
example_html |>
  html_element("div") |> 
  html_attr("id")
```

Теперь мы можем сохранить все нужные нам данные в таблицу.

```{r message=FALSE}

tibble(id = example_html |> 
         html_element("div") |> 
         html_attr("id"),
       date = example_html |> 
         html_element(".pull_right.date.details") |> 
         html_attr("title"),
       signature = example_html |>
         html_element(".signature.details") |> 
         html_text2(),
       text = example_html |> 
         html_element(".text") |>
         html_text2()
)
```

## Парсинг html: вложенные элементы

До сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, и нам надо извлечь только такие div, которым соответствует определенный класс. Также не будем забывать, что архив выгрузился в виде трех html-файлов, так что понадобится наше знание итераций в `purrr`. Пока пробуем на одном из них:

```{r}
archive_1 <- antibarbari_archive[[1]]

archive_1 |>
  html_elements("div.message.default") |> 
  head()
```

Уже из этого набора узлов можем доставать все остальное. 

```{r}
archive_1_tbl <- tibble(id = archive_1 |> 
         html_elements("div.message.default") |> 
         html_attr("id"),
       date = archive_1 |> 
         html_elements("div.message.default") |> 
         html_element(".pull_right.date.details") |> 
         html_attr("title"),
       signature = archive_1 |>
         html_elements("div.message.default") |> 
         html_element(".signature.details") |> 
         html_text2(),
       text = archive_1 |> 
         html_elements("div.message.default") |> 
         html_element(".text") |>
         html_text2()
)

archive_1_tbl
```

Обратите внимание, что мы сначала извлекаем нужные элементы при помощи `html_elements()`, а потом применяем к каждому из них `html_element()`. Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений, т.к. функция `html_element()`, если она не может найти, например, подпись, возвращает NA.

Как вы уже поняли, теперь нам надо проделать то же самое для двух других файлов из архива антиварваров, а значит пришло время превратить наш код в функцию.

```{r eval=FALSE}
scrape_antibarbari <- function(html_file){
  messages_tbl <- tibble(
    id = html_file |>
      html_elements("div.message.default") |>
      html_attr("id"),
    date = html_file |>
      html_elements("div.message.default") |>
      html_element(".pull_right.date.details") |>
      html_attr("title"),
    signature = html_file |>
      html_elements("div.message.default") |>
      html_element(".signature.details") |>
      html_text2(),
    text = html_file |>
      html_elements("div.message.default") |>
      html_element(".text") |>
      html_text2()
  )
  messages_tbl
}


messages_tbl <- map_df(antibarbari_archive, scrape_antibarbari)
```

```{r echo=FALSE}
#save(messages_tbl, file = "../data/messages_tbl.RData")
load("../data/messages_tbl.RData")
```

Вот что у нас получилось.

```{r}
messages_tbl
```

## Разведывательный анализ

Создатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. Кроме того, в некоторых постах только фото, для них в столбце text -- NA, их можно сразу отсеять.

```{r}
messages_tbl <- messages_tbl |>
  filter(!is.na(text))

messages_tbl
```

Также преобразуем столбец, в котором хранится дата и время. Разделим его на два и выясним, в какое время и день недели чаще всего публикуются сообщения.

:::{.callout-warning icon=false}
Из курса `Getting and Cleaning Data` в swirl будет полезно пройти урок `Dates and Times with lubridate`. 
:::


```{r}
messages_tbl2 <- messages_tbl |> 
  separate(date, into = c("date", "time", NA), sep = " ") |> 
  mutate(date = dmy(date), 
         time = hms(time)) |> 
  mutate(year = year(date), 
        month = month(date, label = TRUE),
        wday = wday(date, label = TRUE),
        hour = hour(time),
        length = str_count(text, " ") + 1)

messages_tbl2
```

```{r message=FALSE}
summary1 <- messages_tbl2 |> 
  group_by(year, month) |> 
  summarise(n = n()) 

summary1

summary2 <- messages_tbl2 |> 
  group_by(year, hour) |> 
  summarise(n = n())

summary2

summary3 <- messages_tbl2 |> 
   group_by(wday) |> 
   summarise(n = n())

summary3
```
```{r message=FALSE, warning=FALSE}
library(gridExtra)

p1 <- summary1 |> 
  ggplot(aes(month, n, color = as.factor(year), group = year)) +
  geom_line(show.legend = FALSE) +
  labs(title = "Число постов в месяц") +
  theme(legend.title = element_blank(), 
        legend.position = c(0.8, 0.3)) +
  labs(x = NULL, y = NULL)


p2 <- summary2 |> 
  ggplot(aes(hour, n, color = as.factor(year), group = year)) + 
  geom_line() +
  scale_x_continuous(breaks = seq(1,23,2)) +
  labs(x = NULL, title = "Время публикации поста") + 
  theme(legend.title = element_blank(), 
        legend.position = c(0.8, 0.8))


p3 <- summary3 |> 
  ggplot(aes(wday, n, fill = wday)) + 
  geom_bar(stat = "identity", 
           show.legend = FALSE) + 
  coord_flip() + 
  labs(x = NULL, y = NULL, title  = "Публикации по дням недели")


p4 <- messages_tbl2 |> 
  ggplot(aes(as.factor(year), length, fill = as.factor(year))) +
  geom_boxplot(show.legend = FALSE) +
  labs(title = "Длина поста по годам") + 
  labs(x = NULL, y = NULL)


grid.arrange(p1, p2, p3, p4, nrow = 2)
```

## Html таблицы

Если вам повезет, то ваши данные уже будут храниться в HTML-таблице, и их можно будет просто считать из этой таблицы^[https://r4ds.hadley.nz/webscraping#tables]. Распознать таблицу в браузере обычно несложно: она имеет прямоугольную структуру из строк и столбцов, и ее можно скопировать и вставить в такой инструмент, как Excel.

Таблицы HTML строятся из четырех основных элементов: \<table\>, \<tr\> (строка таблицы), \<th\> (заголовок таблицы) и \<td\> (данные таблицы).  Мы соберем информацию о [проектных группах](https://hum.hse.ru/proj/project2022_2024) ФГН в 2022-2024 г.

```{r}
#| cache: true

html <- read_html("https://hum.hse.ru/proj/project2022_2024")
my_table <- html |>  
  html_element(".bordered") |> 
  html_table()

my_table
```


:::{.callout-warning icon=false}
С сайта [Новой философской энциклопедии](https://iphlib.ru/library/collection/newphilenc/browse/CL1/21) извлеките список слов на букву П. Используйте `map_df()` для объединения таблиц.
:::


:::{.callout-tip icon=false}
Сколько всего слов на букву П в НФЭ?
:::

```{r echo=FALSE, results='asis'}
library(checkdown)
check_question("267", right = "ПППравильно ✅", wrong =  "ППодумайте еще ❌")
```


## Wikisource

Многие тексты доступны на сайте Wikisource.org. Попробуем извлечь все сказки Салтыкова-Щедрина. 

```{r}
url <- "https://ru.wikisource.org/wiki/%D0%9C%D0%B8%D1%85%D0%B0%D0%B8%D0%BB_%D0%95%D0%B2%D0%B3%D1%80%D0%B0%D1%84%D0%BE%D0%B2%D0%B8%D1%87_%D0%A1%D0%B0%D0%BB%D1%82%D1%8B%D0%BA%D0%BE%D0%B2-%D0%A9%D0%B5%D0%B4%D1%80%D0%B8%D0%BD"
html = read_html(url)
```

Для того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот [тут](https://youtu.be/oqNTfWrGdbk) можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни.

```{r warning=FALSE}
toc <- html |> 
  html_elements("ul:nth-child(22) a")

head(toc)
```

Теперь у нас есть список ссылок.

```{r}
tales <- tibble(
  title = toc |>
    html_attr("title"),
  href = toc |> 
    html_attr("href")
)
```

Данных о годе публикации под тегом <a> нет; надо подняться на уровень выше:

```{r}
toc2 <- html |> 
  html_elements("ul:nth-child(22) li")

head(toc2)
```

```{r}
toc2 |>
  html_text2()
```

Соединяем:

```{r}
tales <- tibble(
  title_year = toc2 |>
    html_text2(),
  href = toc |> 
    html_attr("href")
)

tales
```

Дальше можно достать текст для каждой сказки. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила.

```{r}
url_test <- tales |> 
  filter(row_number() == 1) |> 
  mutate(link = paste0("https://ru.wikisource.org", href)) |> 
  pull(link)

text <- read_html(url_test) |> 
  html_elements(".indent p") |> 
  html_text2() 

text[1]
text[length(text)]
```

Первый и последний параграф достали верно; можно обобщать.

```{r}
tales <- tales |> 
    mutate(href = paste0("https://ru.wikisource.org", href))
```


```{r}
urls <- tales |> 
  pull(href)
```

Функция для извлечения текстов.

```{r}
get_text <- function(url) {
  read_html(url) |> 
  html_elements(".indent p") |> 
  html_text2() |> 
  paste(collapse= " ")
}
```

```{r}
tales_text <- map(urls, get_text)
```

Несколько сказок не выловились: там другая структура html, но в целом все получилось.

```{r}
tales_text <- tales_text |>
  flatten_chr() |> 
  as_tibble()

tales <- tales |> 
  bind_cols(tales_text)
```

```{r}
tales
```

Дальше можно разделить столбец с названием и годом и, например, удалить ссылку, она больше не нужна. Разделить по запятой не получится, т.к. она есть в некоторых названиях.

```{r}
tales <- tales |> 
  select(-href) |> 
  separate(title_year, into = c("title", "year"), sep = -5) |> 
  mutate(title = str_remove(title, ",$"))
```

```{r}
tales
```

Недостающие две сказки я не буду пытаться извлечь, но логику вы поняли.



***



_Поздравляем, на этом закончился первый большой раздел нашего курса "Основы работы в R" `r emo::ji("celebration")`. За восемь уроков вы познакомились с основными структурами данных в R, научились собирать и трансформировать данные, строить графики, писать функции и циклы, а также готовить html-отчеты о своих исследованиях. Впереди нас ждут методы анализа текстовых данных._

![](https://24.media.tumblr.com/16f8aa95bbcd604292a6147b369e4048/tumblr_mttd1z5I1s1r3wl1po1_500.gif)
