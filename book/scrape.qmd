# Веб-скрапинг

Выше мы говорили о таком импорте html, когда все теги разом удаляются. Это не всегда удобно, поскольку файл хранит данные в структурированном виде, например, под разными тегами дату, автора и текст. И может быть желательно эту структуру сохранить. 

В R это позволяет делать пакет `rvest`. С его помощью мы подготовим для дальнейшего построения тематической модели архив телеграм-канала Antibarbari HSE. Канал публичный, и Telegram дает возможность скачать архив в формате html при помощи кнопки export (однако эта функция может быть недоступна на MacOS). 

Эта глава опирается в основом на [второе издание](https://r4ds.hadley.nz/webscraping.html) книги R for Data Science Хадли Викхема.

## Структура html 

Документы html (HyperText Markup Language) имеют ирархическую структуру, состоящую из **элементов**.  В каждом элементе есть **открывающий тег** (\<tag\>), опциональные **атрибуты** (id=\'first\') и **закрывающий тег** (\</tag\>). Все, что находится между открывающим и закрывающим тегом, называется **содержанием** элемента. 

Важнейшие теги, о которых стоит знать:

- \<html\> (есть всегда), с двумя детьми (дочерними элементами): \<head\> и \<body\>
- элементы, отвечающие за структуру: \<h1\> (заголовок), \<section\>, \<p\> (параграф), \<ol\> (упорядоченный список)
- элементы, отвечающие за оформление: \<b\> (bold), \<i\> (italics), \<a\> (ссылка)

Чтобы увидеть структуру веб-страницы, надо нажать правую кнопку мыши и выбрать View Source (это работает и для тех html, которые хранятся у вас на компьютере).


## Каскадные таблицы стилей

У тегов могут быть именованные атрибуты; важнейшие из них -- это id и class, которые в сочетании с CSS контролируют внешний вид страницы.

:::{.callout-note icon=false}
CSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML).
:::

Пример css-правила:

```{}
.infobox {
  padding: 1em 1em 1em 4em;
  background: aliceblue 5px center/3em no-repeat;
  color: black;
}
```

Проще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — **селектор** и **блок объявлений**. Селектор, расположенный в левой части правила до знака «{», определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;».

Селекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так:

- p выберет все элементы \<p\>
- .title выберет элементы с классом "title"
- #title выберет все элементы с атрибутом id='title'

Важно: если изменится структура страницы, откуда вы скрапили информацию, то и код, возможно, придется переписывать.

## Извлечение данных

Чтобы прочесть файл html, используем одноименную функцию.

```{r message=FALSE}
library(rvest)
messages <- read_html("../files/antibarbari.html")

messages
```

На следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельный html; `rvest` позволяет это сделать (но внутри двойных кавычек должны быть только одинарные):

```{r}
html <-  minimal_html("
<div class='message default clearfix' id='message83'>
      <div class='pull_left userpic_wrap'>
       <div class='userpic userpic2' style='width: 42px; height: 42px'>
        <div class='initials' style='line-height: 42px'>
A
        </div>
       </div>
      </div>
      <div class='body'>
       <div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'>
11:18
       </div>
       <div class='from_name'>
Antibarbari HSE 
       </div>
       <div class='text'>
Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. <br><br>«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).<br><a href='http://antibarbari.ru/2022/05/19/digest_1/'>http://antibarbari.ru/2022/05/19/digest_1/</a>
       </div>
       <div class='signature details'>
Olga Alieva
       </div>
      </div>
     </div>
")
```

Из всего этого мне может быть интересно id сообщения (\<div class='message default clearfix' id='message83'\>), текст сообщения (\<div class='text'\>), а также, если указан, автор сообщения (\<div class='signature details'\>). Извлекаем текст (для этого [рекомендуется](https://r4ds.hadley.nz/webscraping.html#fn6) использовать функцию `html_text2()`):

```{r}
html |>
  html_element(".text") |> 
  html_text2()
```


В классе signature details есть пробел, достаточно на его месте поставить точку:

```{r}
html |>
  html_element(".signature.details") |> 
  html_text2()
```

Важно помнить, что `html_element` всегда возвращает один элемент. Если их больше, надо использовать `html_elements`.

Осталось добыть message id:

```{r}
html |>
  html_element("div") |> 
  html_attr("id")
```

## Извлечение в тиббл

```{r message=FALSE}
library(tidyverse)

tibble(id = html |> 
         html_element("div") |> 
         html_attr("id"),
       signature = html |>
         html_element(".signature.details") |> 
         html_text2(),
       text = html |> 
         html_element(".text") |>
         html_text2()
)
```

## Скрапим телеграм-канал

До сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, нам надо извлечь только такие div, которым соответствует определенный класс:

```{r}
messages |>
  html_elements("div.message.default") |> 
  head()
```
Уже из этого списка можем доставать все остальное. 

```{r}
messages_tbl <- tibble(id = messages |> 
         html_elements("div.message.default") |> 
         html_attr("id"),
       signature = messages |>
         html_elements("div.message.default") |> 
         html_element(".signature.details") |> 
         html_text2(),
       text = messages |> 
         html_elements("div.message.default") |> 
         html_element(".text") |>
         html_text2()
)

```

Обратите внимание, что мы сначала извлекаем нужные элементы при помощи `html_elements()`, а потом применяем к каждому из них `html_element()`. Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений, т.к. функция `html_element()`, если она не может найти, например, подпись, возвращает NA.

Создатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. В некоторых постах только фото, для них в столбце text -- NA, их можно сразу отсеять.

```{r}
messages_tbl <- messages_tbl |>
  filter(!is.na(text))
```

</br>

:::{.callout-warning icon=false}
Извлеките [из архива антиварваров](https://github.com/locusclassicus/text_analysis_2024/raw/main/files/antibarbari.html) дату публикации для каждого поста.
:::

##  Эмотиконы

В постах довольно много эмотиконов. Я их удалю, но сначала скажу о полезном пакете, который позволяет их все извлечь и, например, посчитать.


```{r message=FALSE}
library(emoji)

messages_tbl |> 
  mutate(emoji = emoji_extract_all(text)) |> 
  pull(emoji) |> 
  unlist() |> 
  as_tibble() |>
  count(value) |> 
  arrange(-n) 
```

Заменяем их все на пробелы.

```{r}
messages_tbl <- messages_tbl |> 
  mutate(text = emoji_replace_all(text, " "))
```


## Рутинная уборка

Подготовка текста для анализа включает в себя удаление сносок, иногда хэштегов, чисел, имейлов, возможно имен и т.п. В нашем случае ситуация осложняется тем, что тексты включают цитаты на латыни и древнегреческом, некоторые технические сокращения, номера страниц и др. Вот так, например, выглядит типичный пост:

```{r}
example <- messages_tbl$text[340]

example
```
Вот так вылавливается гиперссылка.

```{r}
str_extract_all(example, "(http|https)(\\S+)")
```
Вот так вылавливается пагинация и номер семинара (и некоторые другие числа).

```{r}
str_extract_all(example, "#?\\d{2,3}\\w?\\d?-?")
```
Похожим образом можно выловить даты, имейлы и т.п. Все это удаляем из текста. 

```{r}
messages_clean <- messages_tbl |> 
  mutate(text = str_replace_all(text, "(http|https)(\\S+)", " ")) |> 
  mutate(text = str_replace_all(text, "\\d{2}\\.\\d{2}\\.\\d{4}", " ")) |> 
  mutate(text = str_replace_all(text, "\\W[-A-Za-z0-9_.%]+\\@[-A-Za-z0-9_.%]+\\.[A-Za-z]", " ")) |> 
  mutate(text = str_replace_all(text, "#?\\d{2,3}\\w?\\d?-?", " ")) |> 
  mutate(text = str_replace_all(text, "\\n+", " "))
```

Остались еще сокращения вроде "г.", но токены из одной буквы можно будет удалить после разделения на слова. Знаки пунктуации можно оставить или убрать -- иногда они бывают интересным стилистическим маркером. В любом случае лучше это делать после лемматизации, т.к. на тексте без знаков препинания анализатор покажет себя хуже.


```{r}
messages_clean |> 
  filter(row_number() == 340)
```

Число id и число текстов не совпадает, поскольку для некоторых постов текста нет (NA), а у других он совпадает ("Пост выходного дня"). Это надо сразу исправить, чтобы результат лемматизации можно было потом соединить с данными о подписи. Я просто уберу очень короткие посты, поскольку для анализа они неинтересны.

```{r}
messages_clean <- messages_clean |>
  filter(nchar(text) > 19)
```

```{r}
dim(messages_clean)
```

Переименуем первый столбец и переназначим id, чтобы можно было потом соединить с результатами лемматизации.

```{r}
messages_clean <- messages_clean |> 
  rename(doc_id = id) |> 
  mutate(doc_id = paste0("doc", row_number()))
```

## Html таблицы

Если вам повезет, то ваши данные уже будут храниться в HTML-таблице, и их можно будет просто считать из этой таблицы^[https://r4ds.hadley.nz/webscraping#tables]. Распознать таблицу в браузере обычно несложно: она имеет прямоугольную структуру из строк и столбцов, и ее можно скопировать и вставить в такой инструмент, как Excel.

Таблицы HTML строятся из четырех основных элементов: \<table\>, \<tr\> (строка таблицы), \<th\> (заголовок таблицы) и \<td\> (данные таблицы).  Мы соберем информацию о [проектных группах](https://hum.hse.ru/proj/project2022_2024) ФГН в 2022-2024 г.

```{r}
html <- read_html("https://hum.hse.ru/proj/project2022_2024")
my_table <- html |>  
  html_element(".bordered") |> 
  html_table()

DT::datatable(my_table)
```
</br>

:::{.callout-warning icon=false}
С сайта [Новой философской энциклопедии](https://iphlib.ru/library/collection/newphilenc/browse/CL1/21) извлеките список слов на букву П. Используйте `map_df()` для объединения таблиц.
:::

</br>

:::{.callout-tip icon=false}
Сколько всего слов на букву П в НФЭ?
:::

```{r echo=FALSE, results='asis'}
library(checkdown)
check_question("267", right = "ПППравильно ✅", wrong =  "ППодумайте еще ❌")
```


## Wikisource

Многие тексты доступны на сайте Wikisource.org. Попробуем извлечь все сказки Салтыкова-Щедрина. 

```{r}
url <- "https://ru.wikisource.org/wiki/%D0%9C%D0%B8%D1%85%D0%B0%D0%B8%D0%BB_%D0%95%D0%B2%D0%B3%D1%80%D0%B0%D1%84%D0%BE%D0%B2%D0%B8%D1%87_%D0%A1%D0%B0%D0%BB%D1%82%D1%8B%D0%BA%D0%BE%D0%B2-%D0%A9%D0%B5%D0%B4%D1%80%D0%B8%D0%BD"
html = read_html(url)
```

Для того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот [тут](https://youtu.be/oqNTfWrGdbk) можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни.

```{r warning=FALSE}
toc <- html |> 
  html_elements("ul:nth-child(22) a")

head(toc)
```

Теперь у нас есть список ссылок.

```{r}
tales <- tibble(
  title = toc |>
    html_attr("title"),
  href = toc |> 
    html_attr("href")
)
```

Данных о годе публикации под тегом <a> нет; надо подняться на уровень выше:

```{r}
toc2 <- html |> 
  html_elements("ul:nth-child(22) li")

head(toc2)
```

```{r}
toc2 |>
  html_text2()
```

Соединяем:

```{r}
tales <- tibble(
  title_year = toc2 |>
    html_text2(),
  href = toc |> 
    html_attr("href")
)

tales
```

Дальше можно достать текст для каждой сказки. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила.

```{r}
url_test <- tales |> 
  filter(row_number() == 1) |> 
  pull(href) |> 
  paste0("https://ru.wikisource.org", .)

text <- read_html(url_test) |> 
  html_elements(".indent p") |> 
  html_text2() 

text[1]
text[length(text)]
```

Первый и последний параграф достали верно; можно обобщать.

```{r}
tales <- tales |> 
    mutate(href = paste0("https://ru.wikisource.org", href))
```


```{r}
urls <- tales |> 
  pull(href)
```

Функция для извлечения текстов.

```{r}
get_text <- function(url) {
  read_html(url) |> 
  html_elements(".indent p") |> 
  html_text2() |> 
  paste(collapse= " ")
}
```

```{r}
tales_text <- map(urls, get_text)
```

Несколько сказок не выловились: там другая структура html, но в целом все получилось.

```{r}
tales_text <- tales_text |>
  flatten_chr() |> 
  as_tibble()

tales <- tales |> 
  bind_cols(tales_text)
```

```{r}
tales
```

Дальше можно разделить столбец с названием и годом и, например, удалить ссылку, она больше не нужна. Разделить по запятой не получится, т.к. она есть в некоторых названиях.

```{r}
tales <- tales |> 
  select(-href) |> 
  separate(title_year, into = c("title", "year"), sep = -5) |> 
  mutate(title = str_remove(title, ",$"))
```

```{r}
tales
```
Недостающие две сказки я не буду пытаться извлечь, но логику вы поняли. 
