# Многоклассовая классификация

Многоклассовая классификация может использоваться для определения
автора, жанра, тематики или эмоциональной тональности текста. В этом уроке мы научимся классифицировать тексты по автору, воспользовавшись [учебным датасетом русской прозы](https://github.com/JoannaBy/RussianNovels).В формате zip можно забрать [здесь](https://github.com/locusclassicus/text_analysis_2024/raw/refs/heads/main/files/russian_corpus.zip).

Основные задачи этого урока:

- Ознакомиться с методом *логистической регрессии* и понятием *регуляризации*, а также научиться применять для решения задач классификации методы SVM, Random Forest и другие.
- Научиться использовать объект *workflow_set* из экосистемы  `{tidymodels}` для организации и управления моделями и рецептами предобработки в рамках единого набора рабочих процессов (workflows).
- Понять, как могут применяться *методы снижения размерности* в разведывательном анализе данных и на этапе обучения. 

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(textrecipes)
library(tidymodels)
library(tidytext)
library(stylo)
```

## Подготовка данных


```{r eval=FALSE}
corpus <- load.corpus.and.parse(corpus.dir = "../files/russian_corpus")
```

Разделим тексты на отрывки длиной 2000 слов.

```{r eval=FALSE}
corpus_samples <- make.samples(corpus, 
                               sample.size = 2000, 
                               sampling = "normal.sampling",
                               sample.overlap = 0,
                               sampling.with.replacement = FALSE)
```

Перед созданием списка слов удалим еры, которые встречаются в некоторых
изданиях ("съ" и т.п.).

```{r eval=FALSE}
corpus_samples_clean <- map(corpus_samples, 
                              function(text) str_remove(text, "ъ$") 
                            )
```

## Подготовка датасета

Для построения модели берем 500 наиболее частотных слов (токенов). Как мы увидим ниже, в этот список попали некоторые имена героев. В настоящем исследовании от них лучше избавиться, однако пока мы оставим все, как есть.

```{r eval=FALSE}
mfw <- make.frequency.list(corpus_samples_clean)[1:500]
```

Составим матрицу с частотностями.

```{r eval=FALSE}
corpus_tf <- stylo::make.table.of.frequencies(corpus_samples_clean, mfw) |> 
  as.data.frame.matrix() |> 
  rownames_to_column("id") |> 
  as_tibble()
```

```{r eval=FALSE, echo=FALSE}
save(corpus_tf, file = "../data/corpus_tf.Rdata")
```

```{r echo=FALSE}
load("../data/corpus_tf.Rdata")
```

```{r}
corpus_tf
```

Мы будем определять автора, поэтому разделим первый столбец на два.

```{r warning=FALSE}
corpus_tf <- corpus_tf |> 
  separate(id, into = c("author", "title", NA), sep = "_") 
corpus_tf
```


Посмотрим, сколько отрывков для каждого автора в корпусе.

```{r}
corpus_tf |> 
  count(author) |> 
  ggplot(aes(reorder(author, n), n, fill = author)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_d() + 
  theme_light() +
  coord_flip()
```

```{r}
corpus_tf |> 
  count(author) |> 
  arrange(n)
```

Для ускорения вычислений пока удалим авторов, у которых не так много
отрывков.

```{r}
corpus_top <- corpus_tf |> 
  add_count(author) |> 
  filter(n > 120) |> 
  select(-n, -title) 
```

```{r}
corpus_top |> 
  count(author) |> 
  ggplot(aes(reorder(author, n), n, fill = author)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_d() + 
  theme_light() +
  coord_flip()
```

## Обучающая и тестовая выборки

```{r}
set.seed(06042025)
data_split <- corpus_top |> 
  mutate(author = as.factor(author)) |> 
  initial_split(strata = author)

data_train <- training(data_split) 
data_test <- testing(data_split)
```

```{r}
# folds
set.seed(06042025)
folds <- vfold_cv(data_train, strata = author, v = 5)
folds
```

## Снова о `prep()` и `bake()` 

Большую часть препроцессинга мы сделали в `stylo`, поэтому нам нужно
всего несколько шагов.

```{r}
base_rec <- recipe(author ~ ., data = data_train) |>
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

base_rec
```

Также создадим рецепт, в котором используем главные компоненты в
качестве предикторов. Позже число компонент можно настроить при помощи `tune()`.

```{r}
pca_rec <- base_rec |> 
  step_pca(all_predictors(), num_comp = 7)

pca_rec
```

::: {.callout-note icon="false"}
На очень большом числе признаков `step_pca()` может сильно замедлять вычисления, в этом случае можно попробовать `step_pca_truncated()` из пакета {[embed](https://embed.tidymodels.org/reference/step_pca_truncated.html)}. Также стоит помнить, что PCA выполняет линейное снижение размерности, что подходит не для всех данных. Для нелинейного подхода воспользуйтесь функцией `step_umap()` из того же пакета.
:::


Функция `prep()` обучает (подготавливает) рецепт на основе обучающего датасета. Она применяет операции, которые требуют "обучения" на данных, так что ее можно рассматривать как аналог функции `fit()`. Аргумент `retain = TRUE` в функции `prep()` управляет тем, будут ли сохранены предобработанные обучающие данные внутри подготовленного объекта рецепта.


```{r}
base_trained <- base_rec |>
  prep(data_train) 

base_trained
```

Что касается `bake()`, то это скорее аналог функции `predict()`: она применяет подготовленный рецепт к новым данным — например к обучающим или тестовым примерам. Она использует информацию, рассчитанную на этапе `prep()`. 

Если вы вызывали `prep(..., retain = TRUE)`, то можете использовать `juice()` вместо `bake()` для получения обработанных обучающих данных напрямую.


```{r}
base_trained |> 
  # или juice()
  bake(new_data = NULL)
```

## Методы снижения размерности

### PCA для разведывательного анализа

PCA (Principal Component Analysis) — это один из основных и наиболее понятных подходов к уменьшению размерности данных. Он относится к линейным методам обучения без учителя, что означает, что для его работы не требуется информация о целевых переменных (например, метках классов). Метод создаёт новые переменные (главные компоненты) — линейные комбинации исходных признаков, которые максимизируют дисперсию данных. Первые несколько компонентов содержат основную информацию (вариативность) из всего набора признаков. Подробнее о нем см. [урок 15](https://locusclassicus.github.io/text_analysis_2024/cluster.html#%D0%BC%D0%B5%D1%82%D0%BE%D0%B4-%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85-%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82).

```{r}
pca_trained <- pca_rec |>
  prep(data_train) 

pca_trained |> 
  juice()
```

PCA часто используется в *разведывательном анализе данных* (EDA — Exploratory Data Analysis), чтобы упростить структуру данных, выявить важные зависимости и визуализировать сложные многомерные данные. Вот как именно PCA применяется в EDA:

1. Уменьшение размерности для визуализации: диаграмма рассеяния помогает увидеть, есть ли скрытые кластеры, группы, выбросы или тенденции.

2. Обнаружение кластеров или структуры в данных: Если после проекции на первые главные компоненты наблюдаются чётко различимые группы, это может свидетельствовать о наличии скрытой структуры или категорий.

3. Обнаружение выбросов: объекты, которые лежат далеко от большинства других точек в новом пространстве, могут быть аномальными.

4. Оценка корреляции между признаками: в процессе анализа компонент (например, с помощью графиков нагрузок — loadings plot) можно понять, какие переменные сильно коррелируют между собой.

В нашем случае визуализация главных компонент PC1 и PC2 показывает, что распределение классов (авторов) частично перекрывается, хотя некоторые группы имеют тенденцию образовывать кластеры (например, Толстой и Достоевский). Однако большинство классов на плоскости пересекаются друг с другом, особенно в центральной части графика.

```{r}
pca_trained |> 
  juice() |> 
  ggplot(aes(PC1, PC2, color = author)) +
  geom_point() + 
  theme_light()
```
Пакет `{learntidymodels}` позволяет визуализировать нагрузки компонент.

```{r}
#devtools::install_github("tidymodels/learntidymodels")
library(learntidymodels)
pca_trained |> 
  plot_top_loadings(component_number <= 4, n = 10) +
  scale_fill_brewer(palette = "Paired") +
  theme_light()
```

Слова, имеющий наибольшую нагрузку в одной компоненте, являются коррелированными ("клим" и "самгин", "она" и "сказала"). Визуализируйте компоненты 3 и 4, чтобы убедиться, что они хорошо выделяют Горького.


### PLS 

Методы PLS и UMAP — это популярные техники понижения размерности в машинном обучении. Они используются для уменьшения количества признаков (переменных) в данных и извлечения наиболее важной информации, которая определяет закономерности в датасете. 

*Partial Least Squares* (PLS; метод частичных наименьших квадратов):

PLS — это метод, который находит линейные комбинации исходных признаков, называемые компонентами, с учётом зависимости от отклика (целевой переменной). В отличие от PCA (главных компонент), который полностью игнорирует зависимую переменную и ищет направления максимальной дисперсии, PLS является методом обучения с учителем. Это означает, что он учитывает целевой признак при поиске новых компонент. PLS такие ищет проекции в пространстве признаков, которые одновременно объясняют вариацию и в предикторах, и в ответе, что делает его особенно полезным при построении моделей классификации или регрессии.

В машинном обучении метод применяется, когда имеется большое количество сильно коррелированных признаков (что может мешать моделированию). В таком случае PLS позволяет уменьшить размерность, сохранив полезную информацию для предсказаний. 

Добавим еще один шаг к обученному рецепту выше.  

```{r}
# BiocManager::install('mixOmics')

pls_trained <- base_trained |> 
  step_pls(all_numeric_predictors(), outcome = "author", num_comp = 7) |> 
  # дообучение
  prep() 

pls_trained |> 
  juice() 
```

```{r}
pls_trained |> 
  juice() |> 
  ggplot(aes(PLS1, PLS2, color = author)) +
  geom_point() +
  theme_light()
```
Нагрузки компонент выводятся аналогично тому, как мы делали выше.

```{r}
pls_trained |> 
  plot_top_loadings(component_number <= 4, n = 10, type = "pls") +
  scale_fill_brewer(palette = "Paired") +
  theme_light()
```

### UMAP

Еще один способ улучшить точность и интерпретируемость моделей, а также ускорить их обучение называется UMAP (*Uniform Manifold Approximation and Projection*).  Это метод нелинейного понижения размерности, аналогичный t-SNE, но более быстрый. На первом этапе строится граф на основе расстояний между точками (обычно через k-ближайших соседей), который отражает топологию исходного пространства. Затем UMAP пытается разместить точки в пространстве меньшей размерности так, чтобы сохранить как можно больше свойств этого графа. Для этого используется оптимизационная функция на основе кросс-энтропии.

В машинном обучении используется для визуализации данных высокой размерности в 2D или 3D; может быть полезен как этап предварительной обработки перед моделированием, особенно в случаях, когда признаков много или они сильно нелинейно связаны. Важно: _UMAP может применяться как без учителя, так и с учителем_, но из-за стохастического характера может давать разную картину при каждом запуске и чувствителен к настройке гиперпараметров (число соседей).

```{r}
library(embed)

base_trained |> 
  step_umap(all_numeric_predictors(), outcome = "author", num_comp = 7) |> 
  prep() |> 
  juice() |> 
  ggplot(aes(UMAP1, UMAP2, color = author)) +
  geom_point(alpha = 0.5) +
  theme_light()
```

Создадим еще два рецепта, которые понадобятся нам при моделировании.

```{r}
pls_rec <- base_rec |> 
  step_pls(all_numeric_predictors(), outcome = "author", num_comp = tune())
```

```{r}
umap_rec <- base_rec |> 
  step_umap(all_numeric_predictors(), 
            outcome = "author",
            num_comp = tune(),
            neighbors = tune(),
            min_dist = tune()
  )
```


Как мы вскоре убедимся, снижение размерности (DR) не всегда улучшает качество модели, особенно в случае таких моделей, как Random forest или "наивный Байес", которые хорошо справляются с коллинеарными предикторами и разреженными данными.  


## Логистическая регрессия

Вы уже знаете, что линейные модели используются в задачах регрессии:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p$$

где:

- y — предсказание модели,
- $x_1, x_2, ..., x_p$ — признаки (например, частоты слов),
- $\beta_0$ — свободный член (intercept),
- $\beta_i$ — коэффициенты модели, отражающие вклад соответствующего признака.

В задачах классификации, таких как определение темы текста, используют не просто линейную регрессию, а _логистическую регрессию_. Логистическая регрессия применяется для задач, где исходная переменная y категориальна (например, "спорт" или "политика"). Она предсказывает вероятность принадлежности объекта к одному из классов на основании логистической функции ( (иногда также называемой _сигмоидой_ или логит-функцией):

$$p(y = 1 \mid x) = \frac{1}{1 + e^{-z}}$$

Здесь значение `p` -- вероятность принадлежности к положительному классу, а `z` -- это линейная комбинация признаков:

$$z = β_0 + β_1x_1 + β_2x_2 + … + β_nx_n$$

Проведя некоторые преобразования, получаем: 


$$\text{logit}(p) = \log\left( \frac{p}{1 - p} \right) = z = \beta_0 + \beta_1 x_1 + \dots + \beta_n x_n$$
Левая часть уравнения называется "логит" (он же логарифм риска). Само по себе значение `z` может принимать любые значения. Однако, когда вы подставляете `z` в сигмоиду:

  $$\sigma(z) = \frac{1}{1 + e^{-z}}$$

тогда результат всегда ограничен от 0 до 1.

Это значение интерпретируется как вероятность принадлежности к положительному классу. Если полученная вероятность ≥ 0.5 — модель предсказывает класс 1 ("положительный"). Если < 0.5 — класс 0 ("отрицательный"). Значение 0.5 является границей между классами.

![](images/logit.png)

Таким образом, даже если логит z может принимать любые значения от минус бесконечности до плюс бесконечности, благодаря сигмоиде результат всегда находится между 0 и 1. Это делает логистическую регрессию очень удобной для задач классификации. Однако при большом числе признаков эта модель склонна к переобучению (overfitting) — она приспосабливается слишком точно под обучающую выборку, что ухудшает её обобщающую способность.

##  Регуляризация: Lasso и Ridge

Когда мы анализируем текстовые данные (например, классифицируем статьи по жанру, определяем тональность отзывов и т.д.), мы сталкиваемся с задачей представления текстов в числовом виде. Один из распространённых способов — построение мешка слов (bag-of-words), в котором каждый уникальный термин (слово, биграмма и пр.) — это отдельный признак. В результате для небольшого корпуса текстов может получиться десятки тысяч признаков (столбцов), большинство из которых обнулены (то есть в документе конкретное слово отсутствует). Такие данные называются _разреженными_ (sparse), а количество признаков может значительно превышать количество наблюдений (документов).

Когда число признаков очень велико, далеко не все алгоритмы машинного обучения работают одинаково хорошо. Некоторые, как, например, метод k-ближайших соседей (k-NN), плохо справляются с высокоразмерными пространствами. Это связано с тем, что в таких пространствах наблюдения становятся «далёкими» друг от друга, и расстояния между точками плохо отражают истинные различия между текстами. Это называют _проклятием размерности_ (curse of dimensionality).

В таких случаях особенно полезны так называемые линейные модели с регуляризацией. 

Чтобы справиться с переобучением и улучшить интерпретируемость модели, используют регуляризацию — добавление штрафа за слишком большие коэффициенты β. За счет штрафа модель старается хорошо описывать данные, но при этом не сильно "разгоняться" в значениях коэффициентов.

Существуют два основных типа регуляризации:

- L1-регуляризация или Lasso (Least Absolute Shrinkage and Selection Operator),
- L2-регуляризация или Ridge.
  
При _L2-регуляризации_ (Ridge Regression, гребневая регрессия) штрафом является сумма квадратов весов (здесь $w$ - это вектор весов модели):

$$RSS + \lambda \sum_{j=1}^{p} w_j^2$$

Этот метод уменьшает величину весов, не зануляя их. Он хорошо  работает, когда все признаки важны.

_L1-регуляризация_ (Lasso Regression) использует как штраф сумму модулей весов:

$$RSS + \lambda \sum_{j=1}^{p} |w_j|$$
Этот метод может занулять отдельные коэффициенты, то есть по сути производит отбор признаков.

Объединение обеих регуляризаций называют Elastic Net. Этот метод позволяет достичь баланса между отбором признаков и сглаживанием коэффициентов.


$$RSS + \lambda_1 \| {w} \|_1 + \lambda_2 \| {w} \|_2^2$$


Поскольку в нашем датасете несколько классов, то мы применим _многоклассовую логистическую регрессию_. Пакет `{tidymodels}` предоставляет удобные инструменты для построения и настройки моделей с регуляризацией.

```{r}
lasso_spec <- multinom_reg(penalty = tune(), mixture = 1) |> 
  set_mode("classification") |> 
  set_engine("glmnet")
```

```{r}
ridge_spec <- multinom_reg(penalty = tune(), mixture = 0) |> 
  set_mode("classification") |> 
  set_engine("glmnet")
```


Обратите внимание на аргумент mixture:

- `mixture = 1` задает лассо-модель;
- `mixture = 0` - это гребневая регрессия;
- `0 < mixture < 1` соответствуют Elastic Net.


## Опорные векторы (SVM)

Метод опорных векторов (SVM) используется как в задачах регрессии, так и в задачах классификации. 

Во втором случае он пытается найти такую границу (гиперплоскость), которая максимально хорошо разделяет два класса объектов. Если упростить задачу до двух измерений, то метод ищет такую прямую, чтобы _расстояние от неё до ближайших точек с каждой стороны было максимальным_: классы должны быть как можно дальше от границы. Чем дальше граница от обучающих точек, тем устойчивее она к ошибкам на новых данных.

Для этого SVM строит разделяющую прямую, которая максимально "отодвинута" от крайних точек обоих классов. Эти крайние точки, которые "касаются" границы — называются _опорные векторы_ (support vectors). 

_Маржа_ (англ. margin) — это расстояние от разделяющей границы до ближайших точек каждого класса. Чем больше маржа, тем увереннее разделяются классы.

Это проще всего пояснить при помощи графика. Обычные точки — это просто обучающие примеры. Черными отмечены как раз опорные векторы — те точки, которые оказались на краю своих классов и определили положение границы.
Благодаря этим точкам SVM "знает", где должна проходить разделяющая граница. Все "внутренние" точки не влияют на её положение.

![](images/svm.png)

```{r}
svm_spec <- svm_linear(cost = tune()) |> 
  set_mode("classification") |> 
  set_engine("LiblineaR")

svm_spec
```

Пояснение параметров:

- `cost = tune()` — здесь мы указываем, что параметр `cost` будет подобран автоматически (в процессе переподбора гиперпараметров с помощью `tune()`).
- `set_mode("classification")` — устанавливает режим задачи как классификацию.
- `set_engine("LiblineaR")` — указывает, что используется движок `LiblineaR`, реализующий SVM с линейным ядром (в пакете `{tidymodels}`).

Параметр cost — это _коэффициент штрафа_ за ошибки классификации. Он контролирует компромисс между количеством ошибок на обучающем наборе (т.е. насколько сильно модель стремится избежать ошибок) и шириной "маржи" — расстояния между разделительной гиперплоскостью и ближайшими точками разных классов. 

Если cost большое, модель старается классифицировать обучающую выборку как можно точнее: допускается меньшая ширина маржи, но это может привести к переобучению (overfitting).

Если cost меньше, то модель допускает больше ошибок на обучении: маржа будет шире,  это может привести к недообучению (underfitting), но лучше обобщается на новых данных.

## Еще несколько моделей для сравнения


*Однослойная нейронная сеть* - простейшая форма нейронной сети, также известная как перцептрон или логистическая регрессия с несколькими выходами.  

```{r}
mlp_spec <- mlp(hidden_units = tune(),
                penalty = tune(),
                epochs = tune()) |> 
  set_engine("nnet") |> 
  set_mode("classification")
```

*Бэггинг деревьев решений* -- упомянутый ранее ансамблевый метод; строит множество решающих деревьев на бутстреп-выборках, а результат — среднее (для регрессии) или голосование (для классификации).  

```{r}
bagging_spec <- bag_tree() |> 
  set_engine("rpart") |> 
  set_mode("classification")
```

*Flexible Discriminant Analysis* (FDA) -- расширение линейного дискриминантного анализа (LDA), где границы между классами аппроксимируются при помощи нелинейных моделей (например, сплайнов).  

```{r}
fda_spec <- discrim_flexible(prod_degree = tune()) |> 
  set_engine("earth")
```

*Regularized Discriminant Analysis* (RDA) - rомпромисс между линейным (LDA) и квадратичным дискриминантным анализом (QDA) с добавлением регуляризации.  

```{r}
rda_spec <- discrim_regularized(frac_common_cov = tune(), 
                                frac_identity = tune())  |> 
  set_engine('klaR')
```

*Метод ближайших соседей* (K-Nearest Neighbors — KNN) - классификация (или регрессия) объекта производится на основе меток (или значений) K ближайших к нему объектов из обучающей выборки.

```{r}
#devtools::install_github("KlausVigo/kknn")

knn_mod <- nearest_neighbor(neighbors = 5) |> 
  set_engine("kknn") |> 
  set_mode("classification")
```


## Workflow_set

В пакете {tidymodels} для R объект workflow_set используется для организации и управления несколькими комбинациями моделей (model specifications) и рецептов предобработки (recipes) в рамках единого набора рабочих процессов (workflows).

Workflow_set — это объект, который содержит множество разных рабочих процессов (workflows), каждая из которых представляет собой:

- определённую модель (модельную спецификацию);
- определённый способ предобработки данных (recipe).

Иначе говоря: это способ систематически и удобно перебрать (или протестировать) различные комбинации моделей и рецептов на одном наборе данных.

```{r}
library(baguette)
library(discrim)

wflow_set <- workflow_set(  
  preproc = list(base = base_rec,
                 pca = pca_rec,
                 pls = pls_rec,
                 umap = umap_rec),  
  models = list(svm = svm_spec,
                lasso = lasso_spec,
                ridge = ridge_spec,
                mlp = mlp_spec,
                bagging = bagging_spec,
                fda = fda_spec,
                rda = rda_spec,
                knn = knn_mod),  
  cross = TRUE
)

wflow_set
```

Параллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями. Если не уверены, сколько у вас процессоров, выполните:

```{r}
parallel::detectCores()
```

Здесь придется немного (или много) подождать: мы обучаем сразу 32 модели на пяти фолдах, при этом некоторые -- с разными параметрами! По умолчанию количество задач (циклов обучения) на этапе кросс-валидации ограничено числом фолдов. Поэтому, сколько бы ни было ядер в системе, в каждый момент времени не может быть запущено больше v параллельных задач (это позднее можно перенастроить, но пока не будем). 

Аргументы `grid = 3` означает, что будет использоваться 3 различных набора/комбинации гиперпараметров для каждой модели, включенной в workflow set. Он применяется, когда мы запускаем grid search — метод перебора гиперпараметров.


```{r eval=FALSE}
library(future)
plan(multisession, workers = 5)

train_res <- wflow_set |> 
  workflow_map(
    verbose = TRUE,
    seed = 180525,
    resamples = folds,
    grid = 3,
    metrics = metric_set(f_meas, accuracy),
    control = control_resamples(save_pred = TRUE)
  )
# i  1 of 32 tuning:     base_svm
# ✔  1 of 32 tuning:     base_svm (33.2s)
# i  2 of 32 tuning:     base_lasso
# ...
# ✔  2 of 32 tuning:     base_lasso (23.1s)
# i  3 of 32 tuning:     base_ridge
# ✔  3 of 32 tuning:     base_ridge (27.2s)
# i  4 of 32 tuning:     base_mlp
# ...
# ✔  4 of 32 tuning:     base_mlp (26.7s)
# i	No tuning parameters. `fit_resamples()` will be attempted
# i  5 of 32 resampling: base_bagging
# ✔  5 of 32 resampling: base_bagging (27.6s)
# i  6 of 32 tuning:     base_fda
# ✔  6 of 32 tuning:     base_fda (24.2s)
# i  7 of 32 tuning:     base_rda
# ...
# ✔  7 of 32 tuning:     base_rda (1m 8.9s)
# i	No tuning parameters. `fit_resamples()` will be attempted
# i  8 of 32 resampling: base_knn
# ...
# ✔  8 of 32 resampling: base_knn (25.3s)
# i  9 of 32 tuning:     pca_svm
# ✔  9 of 32 tuning:     pca_svm (24.5s)
# i 10 of 32 tuning:     pca_lasso
# ...
# ✔ 10 of 32 tuning:     pca_lasso (23.5s)
# i 11 of 32 tuning:     pca_ridge
# ...
# ✔ 11 of 32 tuning:     pca_ridge (23.4s)
# i 12 of 32 tuning:     pca_mlp
# ...
# ✔ 12 of 32 tuning:     pca_mlp (23.7s)
# i	No tuning parameters. `fit_resamples()` will be attempted
# i 13 of 32 resampling: pca_bagging
# ✔ 13 of 32 resampling: pca_bagging (24s)
# i 14 of 32 tuning:     pca_fda
# ✔ 14 of 32 tuning:     pca_fda (25.2s)
# i 15 of 32 tuning:     pca_rda
# ✔ 15 of 32 tuning:     pca_rda (26.9s)
# i	No tuning parameters. `fit_resamples()` will be attempted
# i 16 of 32 resampling: pca_knn
# ...
# ✔ 16 of 32 resampling: pca_knn (25.2s)
# i 17 of 32 tuning:     pls_svm
# ...
# ✔ 17 of 32 tuning:     pls_svm (25.8s)
# i 18 of 32 tuning:     pls_lasso
# ...
# ✔ 18 of 32 tuning:     pls_lasso (24.6s)
# i 19 of 32 tuning:     pls_ridge
# ...
# ✔ 19 of 32 tuning:     pls_ridge (25.1s)
# i 20 of 32 tuning:     pls_mlp
# ...
# ✔ 20 of 32 tuning:     pls_mlp (26.7s)
# i 21 of 32 tuning:     pls_bagging
# ✔ 21 of 32 tuning:     pls_bagging (26.5s)
# i 22 of 32 tuning:     pls_fda
# ...
# ✔ 22 of 32 tuning:     pls_fda (23.7s)
# i 23 of 32 tuning:     pls_rda
# ...
# ✔ 23 of 32 tuning:     pls_rda (25.1s)
# i 24 of 32 tuning:     pls_knn
# ...
# ✔ 24 of 32 tuning:     pls_knn (26.6s)
# i 25 of 32 tuning:     umap_svm
# ...
# ✔ 25 of 32 tuning:     umap_svm (58.1s)
# i 26 of 32 tuning:     umap_lasso
# ...
# ✔ 26 of 32 tuning:     umap_lasso (54.7s)
# i 27 of 32 tuning:     umap_ridge
# ...
# ✔ 27 of 32 tuning:     umap_ridge (52.1s)
# i 28 of 32 tuning:     umap_mlp
# ...
# ✔ 28 of 32 tuning:     umap_mlp (53.7s)
# i 29 of 32 tuning:     umap_bagging
# ✔ 29 of 32 tuning:     umap_bagging (54.2s)
# i 30 of 32 tuning:     umap_fda
# ✔ 30 of 32 tuning:     umap_fda (52.7s)
# i 31 of 32 tuning:     umap_rda
# ...
# ✔ 31 of 32 tuning:     umap_rda (52.4s)
# i 32 of 32 tuning:     umap_knn
# ...
# ✔ 32 of 32 tuning:     umap_knn (57.3s)
```

```{r echo=FALSE}
library(future)
```

```{r}
plan(sequential)
```

В некоторых случаях вы можете получить сообщение о том, что невозможно вычилить метрику precision (точности) для мультиклассовой классификации: в некоторых фолдах кросс-валидации модель не предсказала ни одного примера для некоторых классов. Если модель не сделала вообще не сделала ни одного предсказания (даже ошибочного), то метрика `precision = TP / (TP + FP)` становится неопределённой, так как знаменатель равен нулю.


```{r eval=FALSE, echo=FALSE}
save(train_res, file = "../data/train_res.Rdata")
```

```{r echo=FALSE}
load("../data/train_res.Rdata")
```


## Оценка и выбор модели

Хорошо видно, что снижение размерности привело к существенному улучшению качества модели KNN, которая, однако, уступает регрессионным. Такое же улучшение можно зафиксировать для нейросети (mlp), а в случае с rda результат как минимум не хуже при заметном ускорении. 

```{r}
autoplot(train_res, metric = "f_meas") + 
  theme_light() +
  theme(legend.position = "none") +
  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),
            angle = 90, hjust = 1.5) +
  coord_cartesian(ylim = c(-0.3, NA))
```

Отберем наилучшие результаты.

```{r message=FALSE}
rank_results(train_res, select_best = TRUE) |> 
  print()
```

Взглянем на параметры наилучшей модели (в данном случае это штрафные коэффициенты).

```{r}
autoplot(train_res, id = "base_ridge") +
  theme_light()
```


Финализируем воркфлоу.


```{r}
best_results <- 
   train_res |> 
   extract_workflow_set_result("base_ridge") |> 
   select_best(metric = "accuracy")

print(best_results)
```

Функция `extract_workflow()` используется для извлечения конкретного workflow (модели) из набора `train_res`. Аргумент "base_ridge" — это имя модели (или ID), которую мы использовали при создании `workflow_set`. Таким образом, этот шаг извлекает сам workflow для модели "base_ridge", включая препроцессинг и модель (ещё с неуточнёнными гиперпараметрами). 

Функция `finalize_workflow()` подставляет наилучшие значения гиперпараметров (например, penalty) в workflow. 

Наконец, `last_fit()` имитирует реальный процесс разработки модели:  после настройки и выбора лучшей модели, мы обучаем её на всей обучающей выборке  и оцениваем на ранее отложенной тестовой выборке.


```{r}
ridge_res <- train_res |> 
  extract_workflow("base_ridge") |> 
  finalize_workflow(best_results) |> 
  last_fit(split = data_split, metrics = metric_set(f_meas, accuracy, roc_auc))
``` 

На тестовой выборке наша модель отработала идеально!

```{r}
collect_metrics(ridge_res) |> 
  print()
```


```{r message=FALSE}
collect_predictions(ridge_res) |> 
  conf_mat(truth = author, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  scale_fill_gradient(low = "white", high = "#233857") +
  theme(panel.grid.major = element_line(colour = "#233857"),
        axis.text = element_text(color = "#233857"),
        axis.title = element_text(color = "#233857"),
        plot.title = element_text(color = "#233857"),
        axis.text.x = element_text(angle = 90))
```

```{r}
collect_predictions(ridge_res) |>
  roc_curve(truth = author, .pred_Bulgakov:.pred_Turgenev) |>
  # или autoplot()
  ggplot(aes(1 - specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  theme_light()

```

## Интерпретация модели

```{r}
final_model <- extract_fit_parsnip(ridge_res)
```

```{r}
top_terms <- tidy(final_model) |>
  filter(term != "(Intercept)") |>
  group_by(class) |>                           
  slice_max(abs(estimate), n = 7)  |>             
  ungroup()  |> 
  mutate(term = fct_reorder(term, abs(estimate)))

print(top_terms)
```

```{r}
top_terms  |> 
  ggplot(aes(x = estimate, y = term, fill = class)) +
  geom_col(show.legend = FALSE, alpha = 0.85) +
  facet_wrap(~ class, scales = "free_y", nrow = 4) +
  scale_fill_brewer(palette = "Dark2") +
  labs(
    title = "Наиболее важные признаки для каждого автора",
    x = "Коэффициент",
    y = "Признак"
  ) +
  theme_minimal() 
```

У Горького "Самгин", у Шолохова -- "Григорий", вроде все логично. Или "совершенно" логично, как сказал бы Булгаков.  

Отличная работа `r emo::ji("award")` `r emo::ji("award")` `r emo::ji("award")`


