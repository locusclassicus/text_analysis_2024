---
editor: 
  markdown: 
    wrap: 72
---

# Многоклассовая классификация

Многоклассовая классификация может использоваться для определения
автора, жанра, тематики или эмоциональной тональности текста.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(textrecipes)
library(tidymodels)
library(tidytext)
library(stylo)
```

## Подготовка данных

Исходник: <https://github.com/JoannaBy/RussianNovels>.
В формате zip можно забрать [здесь](https://github.com/locusclassicus/text_analysis_2024/raw/refs/heads/main/files/russian_corpus.zip).

```{r eval=FALSE}
corpus <- load.corpus.and.parse(corpus.dir = "../files/russian_corpus")
```

Разделим тексты на отрывки длиной 2000 слов.

![](./images/tolstoy.jpg){width="50%"}

```{r eval=FALSE}
corpus_samples <- make.samples(corpus, 
                               sample.size = 2000, 
                               sampling = "normal.sampling",
                               sample.overlap = 0,
                               sampling.with.replacement = FALSE)
```

Перед созданием списка слов удалим еры, которые встречаются в некоторых
изданиях ("съ" и т.п.).

```{r eval=FALSE}
corpus_samples_clean <- map(corpus_samples, 
                              function(text) str_remove(text, "ъ$") 
                            )
```

## Отбор переменных

Для построения модели берем 500 наиболее частотных слов (токенов),
наименее связанных с тематикой.

```{r eval=FALSE}
mfw <- make.frequency.list(corpus_samples_clean)[1:500]
sample(mfw, 20)
```

Составим матрицу с частотностями.

```{r eval=FALSE}
corpus_tf <- stylo::make.table.of.frequencies(corpus_samples_clean, mfw) |> 
  as.data.frame.matrix() |> 
  rownames_to_column("id") |> 
  as_tibble()
```

```{r eval=FALSE, echo=FALSE}
save(corpus_tf, file = "../data/corpus_tf.Rdata")
```

```{r echo=FALSE}
load("../data/corpus_tf.Rdata")
```

```{r}
corpus_tf
```

Мы будем определять автора, поэтому разделим первый столбец на два.

```{r warning=FALSE}
corpus_tf <- corpus_tf |> 
  separate(id, into = c("author", "title", NA), sep = "_") 
corpus_tf
```

Посмотрим, сколько произведений для каждого автора в корпусе.

```{r}
corpus_tf |> 
  count(author) |> 
  ggplot(aes(reorder(author, n), n, fill = author)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_d() + 
  theme_light() +
  coord_flip()
```

```{r}
corpus_tf |> 
  count(author) |> 
  arrange(n)
```

Для ускорения вычислений пока удалим авторов, у которых не так много
текста. Также нам не нужен столбец с названием (кажется, что
предсказывать автора "Войны и мира" по названию -- не очень честно).

```{r}
corpus_tf <- corpus_tf |> 
  add_count(author) |> 
  filter(n > 50) |> 
  select(-n, -title) 
```

## Обучающая и тестовая выборки

```{r}
set.seed(06042025)
data_split <- corpus_tf |> 
  mutate(author = as.factor(author)) |> 
  initial_split(strata = author)

data_train <- training(data_split) 
data_test <- testing(data_split)
```

```{r}
# folds
set.seed(06042025)
folds <- vfold_cv(data_train, strata = author, v = 5)
folds
```

## Препроцессинг

Большую часть препроцессинга мы сделали в `stylo`, поэтому нам нужно
всего несколько шагов.

```{r}
tf_rec <- recipe(author ~ ., data = data_train) |>
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors())

tf_rec
```

Также создадим рецепт, в котором используем главные компоненты в
качестве предикторов. В PCA максимальное число компонент равно минимуму
из

-   числа переменных (признаков) в исходных данных;
-   числа наблюдений минус один.

В нашем случае классов `r data_train$author |> unique() |> length()`.

```{r}
pca_rec <- recipe(author ~ ., data = data_train) |>
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) |> 
  step_pca(all_predictors(), num_comp = 13)

pca_rec
```

::: {.callout-note icon="false"}
На очень большом числе признаков `step_pca()` может сильно замедлять вычисления, в этом случае можно попробовать `step_pca_truncated()` из пакета {[embed](https://embed.tidymodels.org/reference/step_pca_truncated.html)}. Также стоит помнить, что PCA выполняет линейное снижение размерности, что подходит не для всех данных. Для нелинейного подхода воспользуйтесь функцией `step_umap()` из того же пакета.
:::

## Результат препроцессинга

```{r}
prep_train_tf <- tf_rec |>
  prep(data_train) 

tidy(prep_train_tf)
```

```{r}
bake_train_tf <- prep_train_tf |> 
  bake(new_data = NULL)

bake_train_tf
```

```{r}
prep_train_pca <- pca_rec |>
  prep(data_train) 

tidy(prep_train_pca)
```

```{r}
bake_train_pca <- prep_train_pca |> 
  bake(new_data = NULL)

bake_train_pca
```

## Логистическая регрессия

Когда мы анализируем текстовые данные (например, классифицируем статьи по жанру, определяем тональность отзывов и т.д.), мы сталкиваемся с задачей представления текстов в числовом виде. Один из распространённых способов — построение мешка слов (bag-of-words), в котором каждый уникальный термин (слово, биграмма и пр.) — это отдельный признак. В результате для небольшого корпуса текстов может получиться десятки тысяч признаков (столбцов), большинство из которых обнулены (то есть в документе конкретное слово отсутствует). Такие данные называются _разреженными_ (sparse), а количество признаков может значительно превышать количество наблюдений (документов).

Когда число признаков очень велико, далеко не все алгоритмы машинного обучения работают одинаково хорошо. Некоторые, как, например, метод k-ближайших соседей (k-NN), плохо справляются с высокоразмерными пространствами. Это связано с тем, что в таких пространствах наблюдения становятся «далёкими» друг от друга, и расстояния между точками плохо отражают истинные различия между текстами. Это называют _проклятием размерности_ (curse of dimensionality).

В таких случаях особенно полезны так называемые линейные модели с регуляризацией. Вы уже знаете, что линейные модели используются в задачах регрессии:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p$$

где:

- y — предсказание модели,
- $x_1, x_2, ..., x_p$ — признаки (например, частоты слов),
- $\beta_0$ — свободный член (intercept),
- $\beta_i$ — коэффициенты модели, отражающие вклад соответствующего признака.

В задачах классификации, таких как определение темы текста, используют не просто линейную регрессию, а _логистическую регрессию_. Логистическая регрессия применяется для задач, где исходная переменная y категориальна (например, "спорт" или "политика"). Она предсказывает вероятность принадлежности объекта к одному из классов на основании логистической функции ( (иногда также называемой сигмоидой или логит-функцией):

$$P(y = 1 \mid X) = \frac{1}{1 + e^{-z}}$$

Здесь значение z -- это линейная комбинация признаков:

$$z = β_0 + β_1x_1 + β_2x_2 + … + β_nx_n$$

Его также называют "логит" (он же логарифм шансов). Применив к нему сигмоидальную (логистическую) функцию, мы  переводим любое значение z в диапазон от 0 до 1. Это интерпретируется как вероятность принадлежности к положительному классу. Если полученная вероятность ≥ 0.5 — модель предсказывает класс 1 ("положительный"). Если < 0.5 — класс 0 ("отрицательный"). Значение 0.5 является границей между классами.

```{r echo=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)

# сигмоида
sigmoid <- function(z) {
  1 / (1 + exp(-z))
}

# создаем значения логитов
logits <- seq(-10, 10, by = 0.1)
probabilities <- sigmoid(logits)

# датафрейм
data <- tibble(
  z = logits,
  p = probabilities
)

# построение графика
ggplot(data, aes(x = z, y = p)) +
  geom_line(color = "royalblue", linewidth = 1.2) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "tomato") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray40") +
  annotate("text", x = 6, y = 0.6, label = "p ≥ 0.5 → класс 1", size = 5, color = "darkolivegreen") +
  annotate("text", x = -6, y = 0.4, label = "p < 0.5 → класс 0", size = 5, color = "tomato") +
  labs(
    title = "Решение логистической регрессии на основе сигмоиды",
    x = "Логит (z)",
    y = "Вероятность положительного класса (p)"
  ) +
  theme_minimal(base_size = 14)
```

Таким образом, даже если логит z может принимать любые значения от минус бесконечности до плюс бесконечности, благодаря сигмоиде результат всегда находится между 0 и 1. Это делает логистическую регрессию очень удобной для задач классификации. Однако при большом числе признаков эта модель склонна к переобучению (overfitting) — она приспосабливается слишком точно под обучающую выборку, что ухудшает её обобщающую способность.

##  Регуляризация: Lasso и Ridge

Чтобы справиться с переобучением и улучшить интерпретируемость модели, используют регуляризацию — добавление штрафа за слишком большие коэффициенты β. За счет штрафа модель старается хорошо описывать данные, но при этом не сильно "разгоняться" в значениях коэффициентов.

Существуют два основных типа регуляризации:

- L1-регуляризация или Lasso (Least Absolute Shrinkage and Selection Operator),
- L2-регуляризация или Ridge. Она штрафует сумму квадратов коэффициентов:
  
При _L2-регуляризации_ (Ridge Regression, гребневая регрессия) штрафом является сумма квадратов весов (здесь $w$ - это вектор весов модели):

$$RSS + \lambda \sum_{j=1}^{p} w_j^2$$

Этот метод уменьшает величину весов, не зануляя их. Он хорошо  работает, когда все признаки важны.

_L1-регуляризация_ (Lasso Regression) использует как штраф сумму модулей весов:

$$RSS + \lambda \sum_{j=1}^{p} |w_j|$$
Этот метод может занулять отдельные коэффициенты, то есть по сути производит отбор признаков.

Объединение обеих регуляризаций называют Elastic Net. Этот метод позволяет достичь баланса между отбором признаков и сглаживанием коэффициентов.


$$RSS + \lambda_1 \| {w} \|_1 + \lambda_2 \| {w} \|_2^2$$

## Первая модель

Поскольку в нашем датасете несколько классов, то мы применим _многоклассовую логистическую регрессию_. Пакет `{tidymodels}` предоставляет удобные инструменты для построения и настройки моделей с регуляризацией.

```{r}
lasso_spec <- multinom_reg(penalty = tune(), mixture = 1) |> 
  set_mode("classification") |> 
  set_engine("glmnet")
```

Обратите внимание на аргумент mixture:

- `mixture = 1` задает лассо-модель;
- `mixture = 0` - это гребневая регрессия;
- `0 < mixture < 1` соответствуют Elastic Net.

Выбираем лассо, чтобы отобрать наиболее значимые переменные (признаки). Гиперпараметр подберем путем настройки. 

```{r}
lasso_param <- extract_parameter_set_dials(lasso_spec)
  
lasso_grid <- lasso_param |> 
  grid_regular(levels = 3)

lasso_grid
```

```{r}
lasso_wflow <- workflow() |> 
  add_model(lasso_spec) |> 
  add_recipe(tf_rec)

lasso_wflow
```

Здесь придется немного (или много) подождать. Параллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями.

```{r eval=FALSE}
library(tictoc)
library(future)

plan(multisession, workers = 5)

tic()
set.seed(06042025)
lasso_tune <- lasso_wflow |> 
  tune_grid(
    resamples = folds, 
    grid = lasso_grid,
    metrics = metric_set(accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)
  )

lasso_tune 

toc()
# 12.376 sec elapsed
plan(sequential)
```

```{r eval=FALSE, echo=FALSE}
save(lasso_tune, file = "../data/lasso_tune.Rdata")
```

```{r echo=FALSE}
load("../data/lasso_tune.Rdata")
```

```{r}
autoplot(lasso_tune)
```

Наша модель уже достигла достаточно высокой точности ~~расходимся~~.

```{r}
collect_predictions(lasso_tune) |> 
  roc_curve(truth = author, .pred_Bulgakov:.pred_Vovchok)  |> 
  ggplot(aes(1 - specificity, sensitivity, color = .level)) +
  geom_abline(slope = 1, color = "gray50", lty = 2, alpha = 0.8) +
  geom_path(linewidth = 1.5, alpha = 0.7) +
  labs(color = NULL) +
  coord_fixed() +
  theme_light()
```

Вспомним, что все это значит:

`Sensitivity` (Чувствительность) = True Positive Rate (TPR): 

- Формула: $TP/(TP+FN)$ 
- Это доля верно определенных положительных примеров
среди всех положительных примеров 
- Показывает, насколько хорошо модель находит нужные объекты из всех существующих 
- Другие названия: полнота (recall), истинноположительная доля 
- Ось Y на ROC-кривой

`1-Specificity` = False Positive Rate (FPR): 

- Формула: $FP/(FP+TN) = 1 - TN/(FP+TN) = 1 - Specificity$
- Это доля неверно определенных положительных примеров среди всех отрицательных примеров 
- Показывает, насколько часто модель ошибочно причисляет негативные примеры к
позитивным 
- Другие названия: ложноположительная доля 
- Ось X на ROC-кривой

В нашем контексте:

-   sensitivity (для автора А) -- это доля текстов автора А, которые
    правильно определены как тексты автора А,
-   1-specificity (для автора А) -- это доля текстов НЕ автора А,
    которые ошибочно определены как тексты автора А.

```{r}
collect_metrics(lasso_tune) |> 
  filter(.metric == "accuracy")
```

## SVM

Метод опорных векторов (SVM) используется как в задачах регрессии, так и в задачах классификации. 

Во втором случае он пытается найти такую границу (гиперплоскость), которая максимально хорошо разделяет два класса объектов. Если упростить задачу до двух измерений, то метод ищет такую прямую, чтобы _расстояние от неё до ближайших точек с каждой стороны было максимальным_: классы должны быть как можно дальше от границы. Чем дальше граница от обучающих точек, тем устойчивее она к ошибкам на новых данных.

Для этого SVM строит разделяющую прямую, которая максимально "отодвинута" от крайних точек обоих классов. Эти крайние точки, которые "касаются" границы — называются _опорные векторы_ (support vectors). 

_Маржа_ (англ. margin) — это расстояние от разделяющей границы до ближайших точек каждого класса. Чем больше маржа, тем увереннее разделяются классы.

Это проще всего пояснить при помощи графика. Обычные точки — это просто обучающие примеры. Черными отмечены как раз опорные векторы — те точки, которые оказались на краю своих классов и определили положение границы.
Благодаря этим точкам SVM "знает", где должна проходить разделяющая граница. Все "внутренние" точки не влияют на её положение.


```{r echo=FALSE}
# Подключение необходимых пакетов
library(e1071)
library(ggplot2)

# Генерация данных
set.seed(123)

# Два класса с нормальным распределением
class1 <- data.frame(
  x = rnorm(20, mean = 2),
  y = rnorm(20, mean = 2),
  label = factor("Class 1")
)

class2 <- data.frame(
  x = rnorm(20, mean = 4),
  y = rnorm(20, mean = 4),
  label = factor("Class 2")
)

# Объединяем в один набор
my_data <- rbind(class1, class2)

# Обучаем линейную SVM-модель
model <- svm(label ~ ., data = my_data, kernel = "linear", cost = 1, scale = FALSE)

# Получаем коэффициенты w и b
w <- t(model$coefs) %*% model$SV  # веса w для гиперплоскости
b <- -model$rho                   # смещение

# Функция для построения разделяющей границы и маржи
plot_margin <- function() {
  
  # Создаем сетку для прогнозов (зернистость)
  make_grid <- function(data, resolution = 100) {
    x_range <- seq(min(data$x) - 1, max(data$x) + 1, length.out = resolution)
    y_range <- seq(min(data$y) - 1, max(data$y) + 1, length.out = resolution)
    grid <- expand.grid(x = x_range, y = y_range)
    return(grid)
  }
  
  grid <- make_grid(my_data)
  grid$pred <- predict(model, grid)
  sv <- my_data[model$index, ]  # Опорные векторы

  # Основной график
  p <- ggplot() +
    geom_tile(data = grid, aes(x = x, y = y, fill = pred), alpha = 0.3) +  # область решений
    geom_point(data = my_data, aes(x = x, y = y, color = label), size = 2) +  # точки
    geom_point(data = sv, aes(x = x, y = y), shape = 1, size = 5, stroke = 1.5, color = "grey30") +  # опорные векторы
    ggtitle("SVM: Разделяющая гиперплоскость и маржа") +
    theme_minimal()
  
  # Добавим разделяющую прямую и маржи
  # Формула прямой: w1*x + w2*y + b = 0 => y = -(w1/w2)*x - b/w2
  slope <- -w[1] / w[2]
  intercept <- -b / w[2]
  margin <- 1 / sqrt(sum(w^2))

  # Коэффициенты для линий маржи
  margin_up <- intercept + margin
  margin_down <- intercept - margin

  # Добавим границу и маржи в график
  p <- p +
    geom_abline(intercept = intercept, slope = slope, color = "royalblue", size = 1.2) + # разделяющая гиперплоскость
    geom_abline(intercept = margin_up, slope = slope, linetype = "dashed", color = "royalblue") +  # верхняя маржа
    geom_abline(intercept = margin_down, slope = slope, linetype = "dashed", color = "royalblue")   # нижняя маржа
  
  return(p)
}

# Отображение графика
plot_margin()
```

## SVM в `{tidymodels}`

```{r}
svm_spec <- svm_linear(cost = tune()) |> 
  set_mode("classification") |> 
  set_engine("LiblineaR")

svm_spec
```
Пояснение параметров:

- `cost = tune()` — здесь мы указываем, что параметр `cost` будет подобран автоматически (в процессе переподбора гиперпараметров с помощью `tune()`).
- `set_mode("classification")` — устанавливает режим задачи как классификацию.
- `set_engine("LiblineaR")` — указывает, что используется движок `LiblineaR`, реализующий SVM с линейным ядром (в пакете `{tidymodels}`).

Параметр cost — это _коэффициент штрафа_ за ошибки классификации. Он контролирует компромисс между количеством ошибок на обучающем наборе (т.е. насколько сильно модель стремится избежать ошибок) и шириной "маржи" — расстояния между разделительной гиперплоскостью и ближайшими точками разных классов. 

Если cost большое, модель старается классифицировать обучающую выборку как можно точнее: допускается меньшая ширина маржи, но это может привести к переобучению (overfitting).

Если cost меньше, то модель допускает больше ошибок на обучении: маржа будет шире,  это может привести к недообучению (underfitting), но лучше обобщается на новых данных.



```{r}
svm_param <- extract_parameter_set_dials(svm_spec)

svm_grid <- svm_param |> 
  grid_regular(levels = 3)

svm_grid
```

## Random forest



```{r}
rf_spec <- rand_forest(
  trees = tune()) |>        
  set_mode("classification") |> 
  set_engine("ranger")

rf_spec
```

Для случайного леса создадим решетку вручную.

```{r}
rf_grid <- tibble(trees = c(100, 200, 300))

rf_grid
```

## Workflow_set

```{r}
wflow_set <- workflow_set(  
  preproc = list(tf = tf_rec,
                 pca = pca_rec),  
  models = list(svm = svm_spec,
                rf = rf_spec),  
  cross = TRUE
)


wflow_set
```

```{r}
wflow_set_final <- wflow_set |> 
  option_add(grid = svm_grid, id = "tf_svm") |> 
  option_add(grid = svm_grid, id = "pca_svm") |> 
  option_add(grid = rf_grid, id = "tf_rf") |> 
  option_add(grid = rf_grid, id = "pca_rf")
```

Снова немного подождем. Обратите внимание: обе модели подгоняются 5 раз
с тремя разными гиперпараметрами и двумя рецептами препроцессинга. Это
займет время, так как по сути мы обучаем
``` r 2*5*3*2`` моделей. Чтобы все сработало, должны быть установлены пакеты ```{LiblineaR}`и`{ranger}\`.

```{r eval=FALSE}
plan(multisession, workers = 5)

set.seed(06042025)
tic()
wflow_set_fit <- 
  workflow_map(
    wflow_set_final, 
    verbose = TRUE, 
    metrics = metric_set(accuracy),
    resamples = folds,
    control = control_resamples(save_pred = TRUE),
    fn = "tune_grid"
  )
toc()
plan(sequential)
# i 1 of 4 tuning:     tf_svm
# ✔ 1 of 4 tuning:     tf_svm (9.8s)
# i 2 of 4 tuning:     tf_rf
# ✔ 2 of 4 tuning:     tf_rf (4.5s)
# i 3 of 4 tuning:     pca_svm
# ✔ 3 of 4 tuning:     pca_svm (3.5s)
# i 4 of 4 tuning:     pca_rf
# ✔ 4 of 4 tuning:     pca_rf (4.1s)
# 24.538 sec elapsed
```

```{r echo=FALSE, eval=FALSE}
save(wflow_set_fit, file = "../data/wflow_set_fit.Rdata")
```

```{r echo=FALSE}
load("../data/wflow_set_fit.Rdata")
```

## Объединение воркфлоу

```{r}
wflow_set_final <- wflow_set_fit |> 
  bind_rows(as_workflow_set(lasso_tf = lasso_tune))

wflow_set_final
```

Лучшие результаты показывает SVM. Одна из моделей лассо (с очень высоким
штрафным коэффициентом) находится в самом низу.

```{r warning=FALSE}
autoplot(wflow_set_final, metric = "accuracy") + 
  theme_light() +
  theme(legend.position = "none") +
  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),
            angle = 90, hjust = 1.5) 
```

## Выбор модели и окончательная настройка

```{r warning=FALSE}
rank_results(wflow_set_final, rank_metric = "accuracy")
```

```{r}
autoplot(wflow_set_fit, id = "tf_svm") +
  theme_light()
```

```{r}
best_results <- 
   wflow_set_final |> 
   extract_workflow_set_result("tf_svm") |> 
   select_best(metric = "accuracy")

best_results
```

Обратите внимание: на этом этапе мы "распечатываем" тестовые данные!

```{r eval=FALSE}
svm_test_results <- 
   wflow_set_final |> 
   extract_workflow("tf_svm") |> 
   finalize_workflow(best_results) |> 
   last_fit(split = data_split,
            metrics = metric_set(accuracy, f_meas))

svm_test_results
```

```{r echo=FALSE, eval=FALSE}
save(svm_test_results, file = "../data/svm_test_results.Rdata")
```

```{r echo=FALSE}
load("../data/svm_test_results.Rdata")
```

## Оценка

Оцениваем эффективность на тестовых данных.

```{r}
collect_metrics(svm_test_results)
```

```{r message=FALSE, warning=FALSE}
svm_test_results |> 
  collect_predictions() |>
  conf_mat(truth = author, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  scale_fill_gradient(low = "white", high = "#233857") +
  theme(panel.grid.major = element_line(colour = "#233857"),
        axis.text = element_text(color = "#233857"),
        axis.title = element_text(color = "#233857"),
        plot.title = element_text(color = "#233857"),
        axis.text.x = element_text(angle = 90))
  
```

Отличная работа `r emo::ji("award")` `r emo::ji("award")` `r emo::ji("award")`
