# Импорт: JSON

```{r message=FALSE}
library(tidyverse)
library(jsonlite)
library(imager)
library(httr)
```

## Формат JSON

Формат JSON (JavaScript Object Notation) предназначен для представления структурированных данных. JSON имеет шесть основных [типов данных](https://r4ds.hadley.nz/rectangling#json). Четыре из них - скаляры:

-   cамый простой тип - `null`, который играет ту же роль, что и `NA` в R. Он представляет собой отсутствие данных;
-   cтрока (string) похожа на строку в R, но в ней всегда должны использоваться двойные кавычки;
-   число аналогично числам в R, при этом поддерживается целочисленная (например, 123), десятичная (например, 123.45) или научная (например, 1,23e3) нотация. JSON не поддерживает `Inf`, `-Inf` или `NaN`;
-   логическое значение аналогично `TRUE` и `FALSE` в R, но использует строчные буквы `true` и `false`.

Строки, числа и булевы значения в JSON очень похожи на символьные, числовые и логические векторы в R. Основное отличие заключается в том, что скаляры JSON могут представлять только одно значение. Для представления нескольких значений необходимо использовать один из двух оставшихся типов: массивы и объекты.

И массивы, и объекты похожи на списки в R, разница заключается в том, именованы они или нет. _Массив_ подобен безымянному списку и записывается через `[]`. Например, `[1, 2, 3]` - это массив, содержащий 3 числа, а `[null, 1, "string", false]` - массив, содержащий ноль, число, строку и булево значение. 

_Объект_ подобен именованному списку и записывается через `{}`. Имена (ключи в терминологии JSON) являются строками, поэтому должны быть заключены в кавычки. Например, `{"x": 1, "y": 2}` - это объект, который сопоставляет x с 1, а y -- с 2.

## От JSON к таблице

Загрузим небольшой файл `TBBT.json`, хранящий данные о сериале "Теория большого взрыва" ([источник](https://gist.github.com/sahithyandev/540c82170a19f97deef9e23796083f01)). Скачать лучше из репозитория курса [ссылка](https://github.com/locusclassicus/text_analysis_2024/blob/main/files/TBBT.json). 

```{r message=FALSE}
path <- "https://raw.githubusercontent.com/locusclassicus/text_analysis_2024/refs/heads/main/files/TBBT.json"
tbbt <- read_json(path)
```

Функция `read_json()` вернула нам список со следующими элементами:

```{r}
summary(tbbt)
```


Выборочно преобразуем список в тиббл, используя функцию `pluck()` из пакета `{purrr}`.

```{r}
episodes_count <- tibble(
  season = tbbt |>  
    pluck("episodes_count_per_season") |> 
    names(),
  n = tbbt |> 
    pluck("episodes_count_per_season") |> 
    as.integer()
)

episodes_count |> 
  print()
```


```{r message=FALSE}
tbbt |> 
  pluck("casting") |> 
  map_dfr(as_tibble) |> 
  print()
```

Еще один способ. 


```{r}
tibble(
  episode_id = map_chr(tbbt$episode_list, pluck, "episode_id"),
  title = map_chr(tbbt$episode_list, pluck, "title")
  ) |> 
  print()
```

![](https://media1.tenor.com/m/clpoDp_S7D0AAAAC/tumblr-gif.gif)

##  Кейс: Шедевры Пушкинского музея 

JSON -- популярный формат для публикации открытых данных. В таком виде часто публикуют данные органы государственной власти, культурные и некоммерческие организации и др. Например, [Пушкинский музей](https://pushkinmuseum.art/open_data/).

Взглянем на датасет "Шедевры из коллекции музея". JSON можно прочитать напрямую из Сети.

```{r}
doc <- read_json("https://pushkinmuseum.art/json/masterpieces.json")
```

Датасет содержит информацию о 97 единицах хранения.

```{r}
names(doc)
```
Для каждого предмета дано подробное описание. 

```{r}
summary(doc[[1]])
```

Заберем только то, что нам интересно. 

```{r}
masterpieces <- tibble(
  name = map_chr(doc, pluck, "name", "ru"),
  get_year = map_chr(doc, pluck, "get_year"),
  year = map_int(doc, pluck, "year"),
  period = map_chr(doc, pluck, "period", "name", "ru"),
  country = map_chr(doc, pluck, "country", "ru"),
  gallery = paste0("https://pushkinmuseum.art", map_chr(doc, pluck, "gallery", 1, 1)))
```

Библиотека `imager` позволяет работать с изображениями из датасета. Вот так мы могли бы забрать одно из них.

```{r eval=FALSE}
load.image(masterpieces$gallery[1]) |> 
  plot()
```

```{r echo=FALSE, eval=FALSE}
save(img, file = "../data/img.Rdata")
```

![](./images/pushkin1.png)

В пакете `imager` есть функция `map_il()`, которая похожа на свою родню из `{purrr}`, но возвращает список изображений. 

```{r eval=FALSE}
img_gallery <- map_il(masterpieces$gallery, load.image)
```

```{r echo=FALSE, eval=FALSE}
save(img_gallery, file = "../data/img_gallery.Rdata")
```

Функция `walk()` из пакета `purrr` -- это аналог `map()` для тех случаев, когда нас интересует только вывод, т.е.не надо ничего сохранять в окружение.

```{r eval=FALSE}
par(mfrow = c(10, 10), mar = rep(0,4))
walk(img_gallery, plot, axes = FALSE)
```

![](./images/pushkin2.png)

:::{.callout-warning icon=false}
Попробуйте самостоятельно узнать, когда приобретена большая часть шедевров и из каких регионов они происходят.
:::

## Кейс: Нобелевские лауреаты

Мы заберем данные о [нобелевских лауреатах](https://www.nobelprize.org/about/developer-zone-2/) по литературе. В данном случае API не требует ключа авторизации.

```{r}
# Базовый URL API Нобелевской премии
base_url <- "https://api.nobelprize.org/2.1/laureates"
```

```{r}

# Параметры запроса (фильтрация и ограничение результатов)
query_params <- list(
  #nobelPrizeCategory = "lit",  # Фильтр по категории
  limit = 1100                 # Ограничение количества результатов
)
```

Другие категории: 

- phy (физика), 
- che (химия), 
- med (медицина), 
- lit (литература), 
- pea (мир), 
- eco (экономика).


```{r eval=FALSE}
# Выполнение GET-запроса
response <- GET(url = base_url, query = query_params)
```

```{r echo=FALSE, eval=FALSE}
save(response, file = "../data/response.Rdata")
```

```{r echo=FALSE}
load("../data/response.Rdata")
```

Функция `content()` берет сырой ответ от API и возвращает готовые к анализу данные.

```{r}
nobel_data <- content(response, "text")
```
  
```{r}
laureates_data <- fromJSON(nobel_data) 
```
  
Нам осталось преобразовать данные в таблицу. 

```{r}
laureates_tbl <- laureates_data |> 
  pluck("laureates") 
```

Не все столбцы в этой таблице представляют собой вектор, это можно проверить.

```{r}
tibble(name = map_chr(laureates_tbl, class) |> 
         names(),
       type = map_chr(laureates_tbl, class) |> 
         as.character()
)
```


```{r}
list_cols <- laureates_tbl |> 
  select_if(is.list) |> 
  names()

list_cols
```
Теперь все распакуем. Заметьте, как увеличилось количество столбцов. 

```{r}
laureates_unnested <- laureates_tbl |> 
  unnest_wider(all_of(list_cols), names_sep = "_")

list_cols <- laureates_unnested |> 
  select_if(is.list) |> 
  names()


```

Уберем все столбцы, которые содержат информацию на шведском и норвежском. 

```{r}
laureates_en <- laureates_unnested |> 
  select(-ends_with("_se"), -ends_with("_no"))

laureates_en
```

Снова проделаем фокус с "распаковкой" целой серии столбцов.

```{r}
list_cols <- laureates_en |> 
  select_if(is.list) |> 
  names()

list_cols
```

```{r}
laureates_en_unnested <- laureates_en |> 
  unnest_wider(all_of(list_cols), names_sep = "_") |> 
  select(-contains("links"))
```

Уберем лишние столбцы. 

```{r}
laureates_en_unnested |> 
  colnames()
```

```{r}
list_cols <- laureates_en_unnested |> 
  select_if(is.list) |> 
  names()

list_cols
```

Некоторые из этих столбцов сразу уберем, другие распакуем.

```{r}
laureates_en_tidy <- laureates_en_unnested |> 
  select(-contains("_se"), -contains("_no"), -contains("locationString")) |> 
  unnest_wider(where(is.list), names_sep = "_") |> 
  # удаляю столбцы на свое усмотрение
  select(-contains("wikipedia"), -contains("_se"), -contains("_no"),
         -contains("longitude"), -contains("latitude"), -contains("sameAs"),
         -contains("wikidata"), -contains("portion"), -contains("Amount"), 
         -contains("locationString"), -givenName_en, -fullName_en, 
         -familyName_en, -contains("city"), -contains("sortOrder"),
         -contains("continent"), -contains("nativeName"), -contains("penName")
         )
```


```{r}
laureates_en_tidy |> 
  colnames()
```
В оставшихся данных нарушен принцип опрятного хранения: если на одного человека приходится несколько премий, они хранятся как отдельные столбцы, а не наблюдения. Это можно попробовать исправить (или же просто запросить данные, организованные по премиям, а не по людям -- см. документацию).

```{r}
laur_prize2 <- laureates_en_tidy |> 
  filter(!is.na(nobelPrizes_awardYear_2) & is.na(nobelPrizes_awardYear_3))

laur_prize3 <- laureates_en_tidy |> 
  filter(!is.na(nobelPrizes_awardYear_2) & !is.na(nobelPrizes_awardYear_3))
```

С тремя премиями (1917, 1944, 1963) -- только Красный Крест. Среди обладателей двух премий -- некто refugees (1954 и 1981), за этим стоит Служба Верховного комиссара ООН по делам беженцев. Для остальных пока для простоты возьмем только первую премию.

```{r}
laureates_final <- laureates_en_tidy |> 
  filter(is.na(nobelPrizes_awardYear_3)) |> 
  select(-contains("_2"), -contains("_3"), -fileName, -acronym) |> 
  select(-contains("countryNow"), -contains("cityNow"), -contains("nameNow"),
         -contains("residences"), -contains("topMotivation"), -contains("penName"))
```

Дальше там надо еще много чистить, но чтобы немного ускорить:

```{r}
col_names_old <- colnames(laureates_final)
col_names_old
```

```{r}
col_names_new <- str_remove_all(col_names_old, "_en") |> 
  str_remove_all("_1") |> 
  str_remove_all("nobelPrizes_")

colnames(laureates_final) <- col_names_new
```

```{r}
laureates_2025 <- laureates_final |> 
  mutate(awardYear = as.numeric(awardYear)) |> 
  filter(awardYear == 2025)

laureates_2025 |> 
  print()
```


В качестве упражнения посчитайте статистику слов в различных номинациях за все годы и визуализируйте результат.


Пример исследования, выполненного по итогам этого курса, можно посмотреть по [ссылке](https://sysblok.ru/visual/recept-nobelevskoj-premii-issleduem-otkrytye-dannye-o-laureatah/).


## Кейс: Google Books 

Подробные [инструкции](https://developers.google.com/books/docs/v1/using) для разработчиков. Значение `maxResults` не может превышать 40 за один запрос, в день не более 1000.  

```{r}
search_google_books <- function(query, max_results = 15, start_index = 0) {
  
  base_url <- "https://www.googleapis.com/books/v1/volumes"
  
  # Ограничиваем max_results 40
  if (max_results > 40) {
    warning("max_results не может быть больше 40. Установлено 40.")
    max_results <- 40
  }
  
  full_url <- paste0(
    base_url, 
    "?q=", URLencode(query), 
    "&maxResults=", max_results,
    "&startIndex=", start_index,
    "&printType=books"
  )
  
  response <- GET(full_url)
  
  if (status_code(response) != 200) {
    stop("Ошибка при запросе к API Google Books")
  }

  content <- content(response, "text", encoding = "UTF-8")
  data <- fromJSON(content)
  
  return(data)
}
```

Попробуем в действии.

```{r eval=FALSE}
res <- search_google_books('plato')
```


```{r echo=FALSE, eval=FALSE}
save(res, file = "../data/res.Rdata")
```

```{r echo=FALSE}
load("../data/res.Rdata")
```

```{r}
# Извлекаем метаданные
plato_data <- res$items$volumeInfo |> 
  mutate(authors = map_chr(authors, ~paste(.x, collapse = ", "))) |> 
  select(-industryIdentifiers, -readingModes, -printType, -maturityRating, -allowAnonLogging, -contentVersion, -panelizationSummary) |> 
  unnest_wider(imageLinks, names_sep = "_") 

plato_data |> 
  print()
```

Создадим галерею обложек. 

```{r eval=FALSE}
library(magick)

catch_cover <- function(url) {
   
   img <- image_read(url) |> 
        image_border("white", "10x10")  # Добавляем рамку
   return(img)
}

plato_gallery <- map(plato_data$imageLinks_smallThumbnail, catch_cover)
```

<details>
  <summary>Функция для создания сетки обложек.</summary>
  <p>
```{r eval=FALSE}
# Функция для создания сетки обложек
create_cover_grid <- function(images, cols = 4, target_width = 200) {
  if (length(images) == 0) return(NULL)
  
  # Ресайзим все изображения к одинаковой ширине
  images_resized <- map(images, ~ image_scale(.x, paste0(target_width, "x")))
  
  # Вычисляем количество строк
  rows <- ceiling(length(images_resized) / cols)
  
  # Создаем строки
  gallery_rows <- map(1:rows, function(row) {
    start_idx <- (row - 1) * cols + 1
    end_idx <- min(row * cols, length(images_resized))
    
    row_images <- images_resized[start_idx:end_idx]
    
    # Если в последней строке меньше изображений, добавляем пустые места
    if (length(row_images) < cols) {
      empty_count <- cols - length(row_images)
      empty_images <- map(1:empty_count, ~ image_blank(target_width, 300, "white"))
      row_images <- c(row_images, empty_images)
    }
    
    # Объединяем изображения в строку
    image_append(do.call(c, row_images), stack = FALSE)
  })
  
  # Объединяем все строки
  image_append(do.call(c, gallery_rows), stack = TRUE)
}
```
  </p>
</details>

```{r eval=FALSE}
# Создаем и отображаем галерею
gallery <- create_cover_grid(plato_gallery, cols = 5)
print(gallery)
```

![](./images/plato_covers.jpeg)

При работе важно учитывать лимиты API.  Множественные запросы отправляем, например, так (но можно придумать и другие решения).

```{r eval=FALSE}
start_idx <- seq(0, 200, 10)
google_data <- map(start_idx, ~search_google_books("plato", start_index = .x))
```

Извлекаем данные из списка.

```{r eval=FALSE}
google_tbl <-  map_dfr(1:length(google_data), ~pluck(google_data, .x, 3, "volumeInfo"))
```

```{r eval=FALSE, echo=FALSE}
save(google_tbl, file = "../data/google_tbl.Rdata")
```

```{r echo=FALSE}
load("../data/google_tbl.Rdata")
```

И дальше работаем с ними как обычно: приводим к опрятному виду, обобщаем, строим разведывательные графики.

```{r}
google_tbl |> 
  as_tibble() |> 
  print()
```

И дальше работаем с ними как обычно, приводим к опрятному виду т.д. 

## Видео

- [Видео](https://vk.com/video91786643_456239078) 2025 г.

## Домашнее задание

Для этого задания необходимо исследовать датасет Министерства Культуры о репертуарах российских театров ([источник](https://opendata.mkrf.ru/opendata/7705851331-stat_theatres_repertoire/)). 

Это ПОЛОВИНА задания на оценку 0-10, т.е. максимальная оценка за него = 5 баллов. Вторая половина будет следующий раз (результаты суммируются). Все подробности в GitHub Classroom по [ссылке](https://classroom.github.com/a/2BQ18GKA). Дедлайн 19 октября 21-00 мск.




