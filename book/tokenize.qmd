# Синтаксический парсинг

Основные этапы NLP включают в себя токенизацию, морфологический и синтаксический анализ, а также анализ семантики и прагматики. В этом уроке речь пойдет про первые три этапа. Мы научимся разбивать текст на токены (слова), определять морфологические характеристики слов и находить их начальные формы (леммы), а также анализировать структуру предложения с использованием синтаксических парсеров. 

## Токенизация

Токенизация — процесс разделения текста на составляющие (их называют «токенами»). Токенами могут быть слова, символьные или словесные _энграмы_ (n-grams), то есть сочетания символов или слов, даже предложения или параграфы. 

Токенизировать можно в базовом R с использованием регулярных выражений, и @jockers2014 прекрасно показывает, как это можно делать. Но мы воспользуемся двумя пакетами, которые предназначены специально для работы с текстовыми данными и разделяют идеологию `tidyverse`: `tidytext` [@textmining2017] и `tokenizers` [@hvitfeldt2022].

```{r message=FALSE}
library(tidyverse) 
library(tidytext)
library(tokenizers)
```

Для анализа воспользуемся датасетом c латинским текстом "Записок о Галльской войне", который мы подготовили в предыдущем уроке. Его можно забрать [отсюда](https://github.com/locusclassicus/text_analysis_2024/raw/main/data/caesar.Rdata).


```{r warning=FALSE}
load("../data/caesar.RData")
caesar <- caesar |> 
  rename(text = value) |> 
  select(-link)

caesar
```

Функция `unnest_tokens()` из пакета `tidytext` принимает на входе тиббл, название столбца, в котором хранится текст для токенизации, а также название нового столбца, куда будут "сложены" отдельные токены (зачастую это слова, но не обязательно). 

```{}
unnest_tokens(
  tbl,
  output,
  input,
  token = "words",
  format = c("text", "man", "latex", "html", "xml"),
  to_lower = TRUE,
  drop = TRUE,
  collapse = NULL,
  ...
)
```

Аргумент `token` принимает следующие значения:

- "words" (default), 
- "characters", 
- "character_shingles", 
- "ngrams", 
- "skip_ngrams", 
- "sentences", 
- "lines", 
- "paragraphs", 
- "regex", 
- "ptb" (Penn Treebank). 

Используя уже знакомую функцию `map`, можно запустить `unnest_tokens()` с разными аргументами:


```{r message=FALSE}
test <- tibble(text = "Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt.")
```

```{r}
params <- tribble(
  ~tbl, ~output, ~input, ~token,
  test, "word", "text", "words", 
  test, "sentence", "text", "sentences",
  test, "char", "text", "characters", 
)

params
```

```{r}
params |> 
  pmap(unnest_tokens) 
```

Следующие значения аргумента `token` требуют также аргумента `n`:

```{r message=FALSE}
params <- tribble(
  ~tbl, ~output, ~input, ~token, ~n,
  test, "ngram", "text", "ngrams", 3,
  test, "shingles", "text", "character_shingles", 3
)

params  |> 
  pmap(unnest_tokens)  |> 
  head()
```


Дальше мы будем работать со словами, поэтому сохраним токенизированный текст "Записок" в виде "опрятного" датасета (одно наблюдение - один ряд). 

```{r}
caesar_tokens <- caesar |> 
  unnest_tokens("word", "text")

caesar_tokens
```


При работе с данными в текстовом формате `unnest_tokens()` опирается на пакет `tokenizers`, из которого в нашем случае подтягивает функцию `tokenize_words`. У этой функции есть несколько полезных аргументов: `strip_non_alphanum` (удаляет пробельные символы и пунктуацию), `strip_punct` (удаляет пунктуацию), `strip_numeric` (удаляет числа).

Эти аргументы мы тоже можем задать через `unnest_tokens()`, поскольку у функции есть аргумент `...` (загляните в документацию, чтобы убедиться).


```{r}
caesar |> 
  unnest_tokens("word", "text", strip_punct = FALSE)
```

## Лемматизация и частеречная разметка

Лемматизация -- приведение слов к начальной форме (лемме). Как правило, она сопровождается частеречной разметкой слов (POS-tagging). В R это умеет делать, например, пакет `udpipe` (Universal Dependencies Pipeline). Он позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных данных.

Прежде всего нужно выбрать и загрузить модель   ([список](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html)); в нашем случае это модель [Perseus](https://universaldependencies.org/treebanks/la_perseus/index.html), но можно попробовать и другие доступные на сайте <https://universaldependencies.org/>.


```{r eval=FALSE}
library(udpipe)

#  скачиваем модель в рабочую директорию
udpipe_download_model(language = "latin-perseus")

# загружаем модель
latin_perseus <- udpipe_load_model(file = "latin-perseus-ud-2.5-191206.udpipe")

# аннотируем
caesar_annotate <- udpipe_annotate(latin_perseus, caesar$text)
```

Результат возвращается в формате [CONLL-U](https://universaldependencies.org/format.html); это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. Вот пример разбора предложения:

![](https://www.researchgate.net/publication/341522061/figure/fig1/AS:893293068046336@1589989072121/1-CONLL-U-format-example.ppm)

Cтроки слов содержат следующие поля:

1. `ID`: индекс слова, целое число, начиная с 1 для каждого нового
предложения; может быть диапазоном токенов с несколькими словами.
2. `FORM`: словоформа или знак препинания.
3. `LEMMA`: Лемма или основа словоформы.
4. `UPOSTAG`: универсальный тег части речи.
5. `XPOSTAG`: тег части речи для конкретного языка.
6. `FEATS`: список морфологических характеристик.
7. `HEAD`: заголовок текущего токена, который является либо
значением ID, либо нулем (0).
8. `DEPREL`: Universal Stanford dependency relation к (root iff HEAD
= 0) или определенному зависящему от языка подтипу.
9. `DEPS`: Список вторичных зависимостей.
10. `MISC`: любая другая аннотация.

Для работы данные удобнее трансформировать в прямоугольный формат.

```{r eval=FALSE}
caesar_pos <- as_tibble(caesar_annotate) |> 
  select(-paragraph_id)

caesar_pos
```

```{r echo=FALSE, eval=FALSE}
save(caesar_pos, file = "../data/caesar_pos.Rdata")
```

```{r echo=FALSE}
load("../data/caesar_pos.Rdata")
caesar_pos
```

## Обучение модели

Можно заметить, что модель Perseus 2.5 справилась не безупречно: все бельги оказались женского рода, а кельты и вовсе признаны глаголом. Есть ошибки в падежах и числах: например, "provinciae" в четвертом предложении, конечно, не именительный, а родительный падеж. Множество топонимов не опознано в качестве имен собственных. 

Здесь есть два пути. Первый: пробовать другие модели, доступные в пакете `udpipe`. Например, для латыни это [PROIEl](https://universaldependencies.org/treebanks/la_proiel/index.html), обученная не только на классических авторах, но и на Вульгате, или [ITTB](https://universaldependencies.org/treebanks/la_ittb/index.html), обученная на сочинениях Фомы. 

Второй путь - обучить модель самостоятельно. Например, для трибанка Perseus доступны более свежие версии (2.13 на момент написания этой главы) на [GitHub](https://github.com/UniversalDependencies/UD_Latin-Perseus). Вот некоторые изменения:

-  появилась метка `dep_rel` для _ablativus absolutus_ (`advcl:abs`);
- [исправлены аннотации](https://aclanthology.org/2021.udw-1.1.pdf) для супина (`VerbForm=Conv`, `Aspect=Prosp`), а также герундия и герундива  (`VerbForm=Part`, `Aspect=Prosp`);
- добавлен тип для местоимения (`PronType`) и вид для глагола (`Aspect`) и др. 

Инструкцию по обучению модели можно найти [здесь](https://www.bnosac.be/index.php/blog/102-udpipe-r-package-updated). По сути трибанк представляет собой коллекцию проверенных вручную CONLL-U файлов, которые передаются нейросети. Следуя этой инструкции и используя трибанк Perseus 2.13, мы обучили новую модель (это заняло около 8 часов на персональном компьютере), которую можно [загрузить](https://github.com/locusclassicus/text_analysis_2024/raw/main/latin_model/la_perseus-2.13-20231115.udpipe) и использовать для аннотации. 

Надо иметь в виду, что само по себе обновление трибанка еще не гарантирует того, что модель будет лучше справляться с парсингом: многое зависит от параметров обучения. В нашем случае, впрочем, некоторые улучшения есть: например, "provinciae" корректно опознано как родительный падеж. Но есть и потери: "fortissimi" в том же предложении выше - это nominativus pluralis, который ошибочно опознан как генитив единственного числа. 

```{r eval=FALSE}
latin_perseus_new <- udpipe_load_model("../latin_model/la_perseus-2.13-20231115.udpipe")

caesar_annotate2 <- udpipe_annotate(latin_perseus_new, caesar$text[1])

caesar_pos2 <- as_tibble(caesar_annotate2) |> 
  select(-paragraph_id)
```

```{r eval=FALSE, echo=FALSE}
save(caesar_pos2, file = "../data/caesar_pos2.Rdata")
```

```{r echo=FALSE}
load("../data/caesar_pos2.Rdata")
```

```{r}
caesar_pos2
```

Пока для наших задач достигнутой точности хватит, но можно попробовать построить нейросеть с более сложной архитектурой. Например, в 2024 г. такая архитектура была предложена [и для латинского языка](https://github.com/ufal/evalatin2024-latinpipe).

## Поле UPOS

Морфологическая аннотация, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, местоимения.

```{r}
caesar_pos2 |> 
  filter(upos == "PRON") |> 
  select(token, lemma, upos, xpos)
```

Посчитать [части речи](https://universaldependencies.org/u/pos/) можно так:

```{r}
upos_counts <- caesar_pos2 |> 
  group_by(upos) |> 
  count() |> 
  arrange(-n)

upos_counts
```

Столбиковая диаграмма позволяет наглядно представить результаты подсчетов:

```{r}
upos_counts |> 
  ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) +
  geom_bar(stat = "identity", show.legend = F) +
  coord_flip() +
  labs(x = NULL) +
  theme_bw() 
```

Отберем наиболее частотные имена и имена собственные.

```{r}
nouns <- caesar_pos2  |> 
  filter(upos %in% c("NOUN", "PROPN")) |> 
  count(lemma) |> 
  arrange(-n) 

nouns
```

```{r warning=FALSE, fig.width=9}
library(wordcloud)
library(RColorBrewer)

pal <- RColorBrewer::brewer.pal(8, "Dark2")

wordcloud(nouns$lemma, nouns$n, colors = pal, max.words = 130)
```

## Поле FEATS

Допустим, нам нужны не все местоимения, а лишь определенные их формы: например, относительные.

```{r}
rel_pron <- caesar_pos2  |> 
  filter(str_detect(feats, "PronType=Rel")) 

rel_pron
```

Посмотрим на некоторые местоимения в контексте. Для этого добавим html-теги:

```{r results='asis'}
highlight_string <- function(idx) str_replace_all(
  rel_pron$sentence[idx], 
  str_glue("(?<= ){rel_pron$token[idx]}(?=\\W)"),
  str_glue("<mark>{rel_pron$token[idx]}</mark>"))

highlight_string(1)

highlight_string(13)
```


## Поле XPOS

Чтение `xpos` требует сноровки: например причастие _sublata_ там описывается так: `v-srppfb-`, где 

- `v` = verbum;
- `-` на месте лица;
- `s` = singularis;
- `r` = perfectum (не `p`, потому что `p` = praesens);
- `p` = participium; 
- `p` = passivum;
- `f` = femininum; 
- `b` = ablativus (не `a`, потому что `a` = accusativus).

Сравним с описанием личной формы глагола _differunt_ `v3ppia---`:

- `v` = verbum; 
- `3` = 3. persona; 
- `p` = pluralis; 
- `p` = praesens; 
- `i` = indicativus; 
- `a` = activum; 
- `--` на месте рода и падежа, т.к. форма неличная. 

Последнее "место" (`Degree`) у глаголов всегда свободно; в первой книге там стоит `s` (superlativus) лишь у _florentissimis_, что явно ошибка, потому что это не глагол. 

:::{.callout-note icon=false}
Спецификацию всех xpos-тегов для латинского языка можно найти [по ссылке](https://git.informatik.uni-leipzig.de/celano/latinnlp/-/blob/master/guidelines/03_morphology.md?ref_type=heads). 
:::

Для удобства разобьем xpos на 9 столбцов. 

```{r}
caesar_pos2_sep <- caesar_pos2 |> 
  separate(xpos, into = c("POS", "xpos"), sep = 1) |> 
  separate(xpos, into = c("persona", "xpos"), sep = 1) |> 
  separate(xpos, into = c("numerus", "xpos"), sep = 1) |> 
  separate(xpos, into = c("tempus", "xpos"), sep = 1) |> 
  separate(xpos, into = c("modus", "xpos"), sep = 1) |> 
  separate(xpos, into = c("vox", "xpos"), sep = 1) |> 
  separate(xpos, into = c("genus", "xpos"), sep = 1) |> 
  separate(xpos, into = c("casus", "gradus"), sep = 1)

caesar_pos2_sep
```

Эти столбцы тоже можно использовать для поиска конкретных признаков. Посмотрим, например, в каком числе и падеже чаще всего стоит относительное местоимения.

```{r message=FALSE}
pron_rel_sum <- caesar_pos2_sep  |> 
  filter(upos == "PRON") |> 
  filter(str_detect(feats, "PronType=Rel")) |> 
  group_by(numerus, casus) |> 
  summarise(n = n()) |> 
  arrange(-n)

pron_rel_sum
```

Для удобства преобразуем сокращения. 

```{r}
pron_rel_sum <- pron_rel_sum |> 
  filter(casus != "-") |> 
  mutate(casus = case_when(casus == "n" ~ "nom",
                           casus == "g" ~ "gen",
                           casus == "d" ~ "dat",
                           casus == "a" ~ "acc",
                           casus == "b" ~ "abl")) |> 
  mutate(numerus = case_when(numerus == "s" ~ "sing",
                              numerus == "p" ~ "plur"))

pron_rel_sum
```
Функция `facet_wrap` позволяет разбить график на две части на основании значения переменной `numerus`.

```{r}
pron_rel_sum |> 
  ggplot(aes(casus, n, fill = casus)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +
  theme_light() +
  facet_wrap(~numerus) +
  labs(x = NULL, y = NULL, title = "Относительные местоимения в BG 1-7")
```


## Поле DEP_REL

Аналогичным образом можно отбирать синтаксические признаки и их комбинации, а также визуализировать деревья зависимостей для отдельных предложений. 

Дерево зависимостей -- это направленный граф, который имеет единственную корневую вершину (сказуемое главного предложения) без входящих дуг (рёбер), при этом все остальные вершины имеют ровно одну входящую дугу. Иными словами, каждое слово зависит от другого, но только от одного. Это выглядит примерно так:


```{r message=FALSE}
library(textplot)

sent <- caesar_pos |> 
  filter(doc_id == "doc1", sentence_id == 10) 

sent |> 
  distinct(sentence) |> 
  pull(sentence) 

textplot_dependencyparser(sent, size = 3)
```

Можно поспорить с тем, что _nobilissiumus_ и _ditissimus_ - это глаголы, хотя модель Perseus 2.5 верно опознала их в качестве именной части сказуемого при подлежащем "Оргеториг". Информация, которая на графе представлена стрелками, хранится в таблице в полях `token_id` и `head_token_id` и `dep_rel`. Корневой токен всегда имеет значение `0`, то есть ни от чего не зависит. 

```{r}
sent |> 
  select(token_id, token, head_token_id, dep_rel)
```


:::{.callout-info icon=false}
Правила синтаксической разметки для латинского языка доступны по [ссылке](https://github.com/PerseusDL/treebank_data/blob/master/v1/latin/docs/guidelines.pdf), а расшифровку сокращений (для всех языков) надо смотреть [здесь](https://universaldependencies.org/u/dep/).
:::


Вообще латынь (как и древнегреческий ) -- не очень ресурсный язык; для многих языков доступны хорошие предобученные модели.   
 

## Совместная встречаемость слов

Функция `cooccurence()` из пакета `udpipe` позволяет 
выяснить, сколько раз некий термин встречается совместно с другим термином, например:

- слова встречаются в одном и том же документе/предложении/параграфе;
- слова следуют за другим словом;
- слова находятся по соседству с другим словом на расстоянии n слов. 

Код ниже позволяет выяснить, какие существительные встречаются в одном предложении:

```{r echo=FALSE}
library(udpipe)
```

```{r warning=FALSE}
caesar_subset <-  subset(caesar_pos2, upos == "NOUN")
cooc <- cooccurrence(caesar_subset, term = "lemma", group = c("doc_id", "sentence_id")) |>   as_tibble() |> 
  filter(cooc > 25)

cooc
```

Этот результат легко визуализировать, используя пакет `ggraph` (подробнее о нем мы будем говорить в следующих уроках):

```{r message=FALSE, warning=FALSE, fig.width=9}
library(igraph)
library(ggraph)

wordnetwork <- graph_from_data_frame(cooc)
ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc), alpha = 0.8, edge_colour = "grey90", show.legend=FALSE) +
  geom_node_label(aes(label = name), col = "#1f78b4", size = 4) +
  theme_void() +
  labs(title = "Совместная встречаемость существительных", subtitle = "De Bello Gallico 1-7")
```

Чтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но [с другими аргументами](https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-postagging-lemmatisation.html):

```{r warning=FALSE}
cooc2 <- cooccurrence(caesar_subset$lemma, relevant = caesar_subset$upos %in% c("NOUN", "ADJ"), skipgram = 1) |> 
  as_tibble() |> 
  filter(cooc > 10)

cooc2
```

```{r fig.width=9}
wordnetwork <- graph_from_data_frame(cooc2)

ggraph(wordnetwork, layout = "fr") +
  geom_edge_link(aes(width = cooc), edge_colour = "grey90", edge_alpha=0.8, show.legend = F) +
  geom_node_label(aes(label = name), col = "#1f78b4", size = 4) +
  labs(title = "Слова, стоящие рядом в тексте", subtitle = "De Bello Gallico 1-7") +
  theme_void()
```




