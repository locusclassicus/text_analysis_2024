# Векторные представления слов

## Векторы 

Векторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе. 

В математике вектор -- это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство. 

Что в таком случае соответствует измерениям и координатам? Тут есть [несколько подходов](https://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf). 

Мы можем, например, создать _матрицу термин-документ_, где каждое слово "описывается" вектором его встречаемости в различных документах (разделах, параграфах...). Слова  считаются похожими, если "похожи" их векторы (о том, как сравивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.


|   | As You Like It  | Twelfth Night  | Julius Caesar   | Henry V   |
|---|---|---|---|---|
| battle   | 1  |  1 | 8  |  15 |
| soldier  | 2  | 2  | 12  | 36  |
| fool    |  37 |  58 |  1 |  5 |
| clown     | 6 | 117 | 0 | 0 |

Второй подход - зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим _матрицу термин-термин_. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше _синтаксической_ информации. Более широкое окно позволяет скорее судить о _семантике_: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей. 

И матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником "шума", поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности. 

В этом уроке мы рассмотрим алгоритм LSA, который использует матрицу термин-документ и снижение размерности при помощи SVD, а в следующий раз поговорим о других подходах, в том числе с использованием (поверхностных) нейросетей. 


## Латентно-семантический анализ

LSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) -- это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками). Слово "latent" (англ. "скрытый") в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить. 

Создатели [метода](http://cognaction.org/cogs105/readings/LSA.pdf) LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов  представляет собой своего рода сумму (или среднее) отдельных слов. 

На входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово "дорожное", например, служит лучшим показателем темы, чем слово "происшествие", которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции.

Дальше мы рассмотрим общий принцип действия алгоритма на очень простом [примере](https://www.engr.uvic.ca/~seng474/svd.pdf), после чего попытаемся его применить к реальным данным.


## LSA на простом примере

Дан "корпус" из пяти документов.

| doc | text | 
|---|---|
| d1 | Romeo and Juliet.  | 
| d2 | Juliet: O happy dagger!  | 
| d3 |  Romeo died by dagger. |
| d4 | "Live free or die", that's the New-Hampshire's motto. |
| d5 | Did you know, New Hampshire is in New-England. |

После удаления стоп-слов термдокументная матрица выглядит так.

```{r echo=FALSE}
df = data.frame(d1 = c(c(1, 1), rep(0, 6)),
                d2 = c(c(0, 1, 1, 1), rep(0, 4)),
                d3 = c(1, 0, 0, 1, 0, 1, 0, 0),
                d4 = c(rep(0, 4), rep(1, 4)),
                d5 = c(rep(0, 7), c(1)))
rownames(df) <- c("romeo", "juliet", "happy", "dagger", "live",
                  "die", "free", "new-hampshire")             
df
```

По этой матрице пока нельзя  сделать вывод о том, с какими темами связаны, с одной стороны, слова, а с другой - документы. Ее необходимо "переупорядочить" так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем. Примерно так.

![[Источник.](https://sysblok.ru/knowhow/kak-ponjat-o-chem-tekst-ne-chitaja-ego/)](https://sysblok.ru/wp-content/uploads/2019/07/image16.gif)

Для этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD).

## Сингулярное разложение матрицы

При сингулярном разложении исходная матрица $A_r$ проецируется в пространство меньшей размерности, так что получается новая матрица $A_k$, которая представляет собой малоранговую аппроксимацию исходной матрицы. 

Для получения новой матрицы применяется следующая процедура. Сначала для матрицы $A_r$ строится ее сингулярное разложение (Singular Value Decomposition) по формуле: $A = UΣV^t$ . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная. 

![Источник: Яндекс Практикум](https://pictures.s3.yandex.net/resources/3.3.7_skhema_5_1679315403.png)

Здесь `U` — матрица левых сингулярных векторов матрицы `A`; `Σ` — диагональная матрица сингулярных чисел матрицы `A`; `V` — матрица правых сингулярных векторов матрицы `A`. Мы пока не будем пытаться понять, что такое сингулярные векторы с математической точки зрения; достаточно думать о них как о топиках-измерениях, которые задают пространство для наших документов. 

Строки матрицы `U` соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в `V^t` столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы `U` показывает, как связаны слова с топиками, а столбцы `V^T` -- как связаны топики и документы.

Некоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется _усеченным_. Сколько топиков оставить при усечении, решает человек. 

Собственно _эмбеддингами_, или векторными представлениями слова, называют произведения каждой из строк матрицы `U` на `Σ`, а эмбеддингами документа -- произведение столбцов `V^t` на `Σ`. Таким образом мы как бы "вкладываем" (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов. 

## SVD в базовом R

Применим SVD к игрушечной термдокументной матрице, которую мы создали выше. В R для этого есть специальная функция (и не одна).

```{r}
my_svd = svd(df)

my_svd
```


Сингулярные значения меньше двух отсекаем, остается два значения. Это позволит нам визуализировать результат; в реальном исследовании используется больше измерений (от 50 до 1000 в зависимости от корпуса).


```{r}
my_svd$d[3:5] <- 0

s <- diag(my_svd$d) 

s
```

Матрицу правых сингулярных векторов транспонируем.

```{r}
vt <- t(my_svd$v)

vt
```
Теперь перемножим матрицы, чтобы получить эмбеддинги.

```{r}
# эмбеддинги слов
u <- my_svd$u
word_emb <- u %*% s |> 
  round(3)

rownames(word_emb) <- rownames(df)

word_emb
```

```{r}
# эмбеддинги документов
doc_emb <- s %*% vt |> 
  round(3)

colnames(doc_emb) <- colnames(df)

doc_emb 
```

Добавим условный поисковый запрос: _dies_, _dagger_. Очевидно, ближе всего к документы d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 -- ни одного.  Координаты поискового запроса (который рассматриваем как псевдодокумент) считаем как среднее арифметическое координат:

```{r}
q = c("die", "dagger")
q_doc <-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2
q_doc
```

Объединив все в единый датафрейм, можем визуализировать.

```{r warning=FALSE, message=FALSE}
library(tidyverse)

plot_tbl <- rbind(word_emb, t(doc_emb), q_doc) |> 
  as.data.frame() |> 
  rownames_to_column("item") |> 
  rename(dim1 = V1, dim2 = V2) |> 
  mutate(type = c(rep("word", 8), rep("doc", 6))) |> 
  select(!starts_with("V"))

plot_tbl
```


```{r echo=FALSE}
zero = tibble(dim1 = 0, dim2 = 0)

line1 = plot_tbl  |>
  filter(item == "d1") |> 
  select(-item, -type)  |> 
  bind_rows(zero)

line2 = plot_tbl  |> 
  filter(item == "d5") |> 
  select(-item, -type)  |> 
  bind_rows(zero)

lm1 = summary(lm(data=line1, dim2 ~ dim1))
lm2 = summary(lm(data=line2, dim2 ~ dim1))
```


```{r message=FALSE, echo=FALSE}
library(ggrepel)

plot_tbl |> 
  ggplot(aes(dim1, dim2, 
             color = as.factor(type), label = item)) +
  geom_point(show.legend = F) + 
  geom_text_repel(aes(fontface = "bold"), show.legend = F) +
  theme_bw() + 
  xlab(NULL) + 
  ylab(NULL) +
  geom_abline(slope = 0, intercept = 0, linetype = "dotted") +
  geom_vline(xintercept = -1.10, linetype = "dotted") +
  coord_cartesian(xlim = c(-1.7, 0.2), ylim = c(-1.5, 1.5)) +
  geom_abline(slope = lm1$coefficients[2], 
              intercept = lm1$coefficients[1], linetype = "dotted") + 
  geom_abline(slope = lm2$coefficients[2], 
              intercept = lm2$coefficients[1], linetype = "dotted")
```


Итак, "поисковый запрос" оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.

## Межвекторное расстояние

Мы исследовали наш небольшой корпус визуально, но там, где число измерений больше двух, это просто невозможно. На практике расстояние или сходство между векторами слов (или документов) вычисляется алгебраически. Наиболее известны манхэттенское и евклидово расстояние, а также косинусное сходство. Для анализа текстовых данных как правило применяется косинусное сходство.

![](./images/dist.png) 

Все их можно посчитать в R для заданной пары векторов. 

```{r}
doc_mx <- plot_tbl |> 
  filter(row_number() > 8 ) |> 
  column_to_rownames("item") |> 
  select(dim1, dim2) |> 
  as.matrix()

doc_mx
```


```{r message=FALSE}
dist_mx <- doc_mx |> 
  philentropy::distance(method = "cosine", use.row.names = T) 

dist_mx
```

Чтобы получить расстояние (а не сходство), вычитаем результат из единицы.

```{r}
round(1 - dist_mx, 3)
```

Аналогично вычисляются расстояния между словами. При желании все косинусы можно пересчитать в градусы, чтобы узнать точный угол между векторами. 

```{r}
acos(dist_mx[3,1])  # acos для d3 и d1 (cos = 0.872)
  
180 * acos(dist_mx[3,1]) / pi # переводим из радиан в градусы
```

## Подгтовка данных

Теперь, когда мы поняли общий принцип работы алгоритма LSA, оценим его возможности на [датасете](https://www.kaggle.com/datasets/marialevchenko/news-dataset-from-lenta-ru-2019-2023/data) с подборкой новостей на русском языке. Файл в формате `.Rdata` можно скачать в формате `.Rdata`  по [ссылке](https://github.com/locusclassicus/text_analysis_2024/raw/main/data/news.Rdata).
 
```{r}
load("../data/news.Rdata")

str(news_2019)
```

Исходный датасет содержит почти полмиллиона новостей на русском языке с 2019 по 2023 г.; для ускорения вычислений мы взяли данные только за один год. Добавим `id` для документов.

```{r}
news_2019 <- news_2019 |> 
  mutate(id = paste0("doc", row_number()))
```

Основные темы статей выглядят так: 

```{r}
news_2019 |> 
  group_by(topic) |> 
  summarise(n = n()) |> 
  arrange(-n) 
```
Составим список стоп-слов.

```{r}
library(stopwords)
stopwords_ru <- c(
  stopwords("ru", source = "snowball"),
  stopwords("ru", source = "marimo"),
  stopwords("ru", source = "nltk"), 
  stopwords("ru", source  = "stopwords-iso")
  )

stopwords_ru <- sort(unique(stopwords_ru))
length(stopwords_ru)
```

Разделим статьи на слова и удалим стоп-слова; это может занять несколько минут.

```{r eval=FALSE}
library(tidytext)
news_tokens <- news_2019 |> 
  unnest_tokens(token, text) |> 
  filter(!token %in% stopwords_ru)
```

```{r echo=FALSE}
library(tidytext)
```

Даже после удаления стоп-слов в нашем датасете осталось примерно 10 млн токенов; однако значительная их часть встречается лишь несколько раз и для тематического моделирования бесполезна. Поэтому можно от них избавиться.

```{r eval=FALSE}
news_tokens_pruned <- news_tokens |> 
  add_count(token) |> 
  filter(n > 10) |> 
  select(-n)
```

Также избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде `digit` и т.п. Токены на латинице тоже удаляем.

```{r eval=FALSE}
news_tokens_pruned <- news_tokens_pruned |> 
  filter(str_detect(token, "[\u0400-\u04FF]")) |> 
  filter(!str_detect(token, "\\d"))
```

```{r echo=FALSE, eval=FALSE}
save(news_tokens_pruned, file = "../data/news_tokens_pruned.Rdata")
```

```{r echo=FALSE}
rm(news_tokens)
```

```{r echo=FALSE}
load("../data/news_tokens_pruned.Rdata")
```

```{r}
news_tokens_pruned
```

Посмотрим на статистику по словам.

```{r}
news_tokens_pruned |> 
  group_by(token) |> 
  summarise(n = n()) |> 
  arrange(-n)
```


Этап подготовки данных -- самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели. 

## TF-IDF: опрятный подход

Вместо показателей абсолютной встречаемости при анализе больших текстовых данных применяется tf-idf. Эта статистическая мера [не используется](https://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf), если дана матрица термин-термин, но она хорошо работает с матрицами термин-документ, позволяя повысить веса для тех слов, которые служат хорошими дискриминаторами. Например, "заявил" и "отметил", хотя это не стоп-слова, могут встречаться в разных темах. 

```{r eval=FALSE}
news_counts <- news_tokens_pruned |>
  count(token, id)

news_counts
```

```{r echo=FALSE, eval=FALSE}
save(news_counts, file = "../data/news_counts.Rdata")
```

```{r echo=FALSE}
load("../data/news_counts.Rdata")
```

```{r}
news_counts |> 
  arrange(id)
```

Добавляем tf_idf. 

```{r eval=FALSE}
news_tf_idf <- news_counts |> 
  bind_tf_idf(token, id, n) |> 
  arrange(tf_idf) |> 
  select(-n, -tf, -idf)

news_tf_idf
```

```{r echo=FALSE, eval=FALSE}
save(news_tf_idf, file = "../data/news_tf_idf.Rdata")
```

```{r echo=FALSE}
load("../data/news_tf_idf.Rdata")
news_tf_idf
```

## DocumentTermMatrix

Посмотрим на размер получившейся таблицы.

```{r}
object.size(news_tf_idf)
format(object.size(news_tf_idf), units = "auto")
```

Чтобы вычислить SVD, такую таблицу необходимо преобразовать в матрицу термин-документ. Оценим ее размер:

```{r}
# число уникальных токенов
m <- unique(news_tf_idf$token) |> 
  length()
m

# число уникальных документов
n <- unique(news_tf_idf$id) |> 
  length()  
n


# число элементов в матрице 
m * n
```

Поэтому мы используем специальный формат для хранения разреженных матриц. 

```{r eval=FALSE}
dtm <- news_tf_idf |> 
  cast_sparse(token, id, tf_idf)
```

```{r echo=FALSE}
#save(dtm, file = "../data/dtm.Rdata")
rm(news_tf_idf)
```

```{r echo=FALSE}
load("../data/dtm.Rdata")
```

```{r}
# первые 10 рядов и 5 столбцов
dtm[1:10, 1:5]
```

Снова уточним размер матрицы.

```{r}
format(object.size(dtm), units = "auto")
```

## SVD с пакетом `irlba`

Метод для эффективного вычисления усеченного SVD на больших матрицах реализован в пакете `irlba`. Возможно, придется подождать `r emo::ji("time")`.

```{r message=FALSE, warning=FALSE}
library(irlba)
lsa_space<- irlba::svdr(dtm, 20)
```

Функция вернет список из трех элементов:

(@) `d`: k аппроксимированных сингулярных значений;
(@) `u`: k аппроксимированных левых сингулярных векторов;
(@) `v`: k аппроксимированных правых сингулярных векторов.

Полученную LSA-модель можно использовать для поиска наиболее близких слов и документов или для изучения тематики корпуса -- в последнем случае нас может интересовать, какие топики доминируют в тех или иных документах и какими словами они в первую очередь представлены. 

## Эмбеддинги слов

Вернем имена рядов матрице левых сингулярных векторов и добавим имена столбцов.

```{r}
rownames(lsa_space$u) <- rownames(dtm)
colnames(lsa_space$u) <- paste0("dim", 1:20)
```

Теперь посмотрим на эмбеддинги слов.

```{r warning=FALSE}
word_emb <- lsa_space$u |> 
  as.data.frame() |> 
  rownames_to_column("word") |> 
  as_tibble()

word_emb
```
Преобразуем наши данные в длинный формат. 

```{r}
word_emb_long <- word_emb |> 
  pivot_longer(-word, names_to = "dimension", values_to = "value") |>
  mutate(dimension = as.numeric(str_remove(dimension, "dim")))
  

word_emb_long
```

## Визуализация топиков

Визуализируем несколько топиков, чтобы понять, насколько они осмыслены. 

```{r}
word_emb_long |> 
  filter(dimension < 10) |> 
  group_by(dimension) |> 
  top_n(10, abs(value)) |> 
  ungroup() |> 
  mutate(word = reorder_within(word, value, dimension)) |> 
  ggplot(aes(word, value, fill = dimension)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~dimension, scales = "free_y", ncol = 3) +
  scale_x_reordered() +
  coord_flip() +
  labs(
    x = NULL, 
    y = "Value",
    title = "Первые 9 главных компонент за 2019 г.",
    subtitle = "Топ-10 слов"
  ) +
  scale_fill_viridis_c()
```

## Ближайшие соседи

Эмбеддинги можно использовать для поиска ближайших соседей.

```{r}
library(widyr)
nearest_neighbors <- function(df, feat, doc=F) {
  inner_f <- function() {
    widely(
        ~ {
          y <- .[rep(feat, nrow(.)), ]
          res <- rowSums(. * y) / 
            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))
          
          matrix(res, ncol = 1, dimnames = list(x = names(res)))
        },
        sort = TRUE
    )}
  if (doc) {
    df |> inner_f()(doc, dimension, value) }
  else {
    df |> inner_f()(word, dimension, value)
  }
}
```

```{r}
nearest_neighbors(word_emb_long, "посол")
```

## Похожие документы

Информация о документах хранится в матрице правых сингулярных векторов. 

```{r}
rownames(lsa_space$v) <- colnames(dtm)
colnames(lsa_space$v) <- paste0("dim", 1:20)
```

Посмотрим на эмбеддинги документов.

```{r warning=FALSE}
doc_emb <- lsa_space$v |> 
  as.data.frame() |> 
  rownames_to_column("doc") |> 
  as_tibble()

doc_emb
```
Преобразуем в длинный формат.

```{r}
doc_emb_long <- doc_emb |> 
  pivot_longer(-doc, names_to = "dimension", values_to = "value") |>
  mutate(dimension = as.numeric(str_remove(dimension, "dim")))
  

doc_emb_long
```

И найдем соседей для произвольного документа. 

```{r}
nearest_neighbors(doc_emb_long, "doc14", doc = TRUE)
```

Выведем документ 14 вместе с его соседями 1893 и 2043.

```{r}
news_2019 |> 
  filter(id %in% c("doc14", "doc1893", "doc2043")) |> 
  mutate(text = str_trunc(text, 70)) 
  
```

Поздравляем, вы построили свою первую рекомендательную систему `r emo::ji("drink")`.