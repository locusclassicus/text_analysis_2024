# Векторные представления слов

## Векторы 

Векторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе. 

В математике вектор -- это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство. 

Что в таком случае соответствует измерениям и координатам? Тут есть [несколько подходов](https://web.stanford.edu/~jurafsky/li15/lec3.vector.pdf). 

Мы можем, например, создать _матрицу термин-документ_, где каждое слово "описывается" вектором его встречаемости в различных документах (разделах, параграфах...). Слова  считаются похожими, если "похожи" их векторы (о том, как сравивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.


|   | As You Like It  | Twelfth Night  | Julius Caesar   | Henry V   |
|---|---|---|---|---|
| battle   | 1  |  1 | 8  |  15 |
| soldier  | 2  | 2  | 12  | 36  |
| fool    |  37 |  58 |  1 |  5 |
| clown     | 6 | 117 | 0 | 0 |

Второй подход - зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим _матрицу термин-термин_. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше _синтаксической_ информации. Более широкое окно позволяет скорее судить о _семантике_: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей. 

И матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником "шума", поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности. 

В этом уроке мы рассмотрим алгоритм LSA, который использует матрицу термин-документ и снижение размерности при помощи SVD, а в следующий раз поговорим о других подходах, в том числе с использованием (поверхностных) нейросетей. 


## Латентно-семантический анализ

LSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) -- это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками).
Слово "latent" (англ. "скрытый") в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить. 

Создатели метода [называют](http://cognaction.org/cogs105/readings/LSA.pdf) LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов  представляет собой своего рода сумму (или среднее) отдельных слов. 

На входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово "дорожное", например, служит лучшим показателем темы, чем слово "происшествие", которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции.

Дальше мы рассмотрим общий принцип действия алгоритма на очень простом [примере](https://www.engr.uvic.ca/~seng474/svd.pdf), после чего попытаемся его применить к реальным данным.


## LSA на простом примере

Дан "корпус" из пяти документов.

| doc | text | 
|---|---|
| d1 | Romeo and Juliet.  | 
| d2 | Juliet: O happy dagger!  | 
| d3 |  Romeo died by dagger. |
| d4 | "Live free or die", that's the New-Hampshire's motto. |
| d5 | Did you know, New Hampshire is in New-England. |

После удаления стоп-слов термдокументная матрица выглядит так.

```{r echo=FALSE}
df = data.frame(d1 = c(c(1, 1), rep(0, 6)),
                d2 = c(c(0, 1, 1, 1), rep(0, 4)),
                d3 = c(1, 0, 0, 1, 0, 1, 0, 0),
                d4 = c(rep(0, 4), rep(1, 4)),
                d5 = c(rep(0, 7), c(1)))
rownames(df) <- c("romeo", "juliet", "happy", "dagger", "live",
                  "die", "free", "new-hampshire")             
df
```

По этой матрице пока нельзя  сделать вывод о том, с какими темами связаны, с одной стороны, слова, а с другой - документы. Ее необходимо "переупорядочить" так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем. Примерно так.

![[Источник.](https://sysblok.ru/knowhow/kak-ponjat-o-chem-tekst-ne-chitaja-ego/)](https://sysblok.ru/wp-content/uploads/2019/07/image16.gif)

Для этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD).

## Сингулярное разложение матрицы

При сингулярном разложении исходная матрица $A_r$ проецируется в пространство меньшей размерности, так что получается новая матрица $A_k$, которая представляет собой малоранговую аппроксимацию исходной матрицы. 

Для получения новой матрицы применяется следующая процедура. Сначала для матрицы $A_r$ строится ее сингулярное разложение (Singular Value Decomposition) по формуле: $A = UΣV^t$ . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная. 

![Источник: Яндекс Практикум](https://pictures.s3.yandex.net/resources/3.3.7_skhema_5_1679315403.png)

Здесь `U` — матрица левых сингулярных векторов матрицы `A`; `Σ` — диагональная матрица сингулярных чисел матрицы `A`; `V` — матрица правых сингулярных векторов матрицы `A`. Мы пока не будем пытаться понять, что такое сингулярные векторы с математической точки зрения; достаточно думать о них как о топиках-измерениях, которые задают пространство для наших документов. 

Строки матрицы `U` соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в `V^t` столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы `U` показывает, как связаны слова с топиками, а столбцы `V^T` -- как связаны топики и документы.

Некоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется _усеченным_. Сколько топиков оставить при усечении, решает человек. 

Собственно _эмбеддингами_, или векторными представлениями слова, называют произведения каждой из строк матрицы `U` на `Σ`, а эмбеддингами документа -- произведение столбцов `V^t` на `Σ`. Таким образом мы как бы "вкладываем" (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов. 

## SVD в базовом R

Применим SVD к игрушечной термдокументной матрице, которую мы создали выше. В R для этого есть специальная функция (и не одна).

```{r}
my_svd = svd(df)

my_svd
```


Сингулярные значения меньше двух отсекаем, остается два значения. Это позволит нам визуализировать результат; в реальном исследовании используется больше измерений (от 50 до 1000 в зависимости от корпуса).


```{r}
my_svd$d[3:5] <- 0

s <- diag(my_svd$d) 

s
```

Матрицу правых сингулярных векторов транспонируем.

```{r}
vt <- t(my_svd$v)

vt
```
Теперь перемножим матрицы, чтобы получить эмбеддинги.

```{r}
# эмбеддинги слов
u <- my_svd$u
word_emb <- u %*% s |> 
  round(3)

rownames(word_emb) <- rownames(df)

word_emb
```

```{r}
# эмбеддинги документов
doc_emb <- s %*% vt |> 
  round(3)

colnames(doc_emb) <- colnames(df)

doc_emb 
```

Добавим условный поисковый запрос: _dies_, _dagger_. Очевидно, ближе всего к документы d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 -- ни одного.  Координаты поискового запроса (который рассматриваем как псевдодокумент) считаем как среднее арифметическое координат:

```{r}
q = c("die", "dagger")
q_doc <-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2
q_doc
```

Объединив все в единый датафрейм, можем визуализировать.

```{r warning=FALSE, message=FALSE}
library(tidyverse)

plot_tbl <- rbind(word_emb, t(doc_emb), q_doc) |> 
  as.data.frame() |> 
  rownames_to_column("item") |> 
  rename(dim1 = V1, dim2 = V2) |> 
  mutate(type = c(rep("word", 8), rep("doc", 6))) |> 
  select(!starts_with("V"))

plot_tbl
```


```{r echo=FALSE}
zero = tibble(dim1 = 0, dim2 = 0)

line1 = plot_tbl  |>
  filter(item == "d1") |> 
  select(-item, -type)  |> 
  bind_rows(zero)

line2 = plot_tbl  |> 
  filter(item == "d5") |> 
  select(-item, -type)  |> 
  bind_rows(zero)

lm1 = summary(lm(data=line1, dim2 ~ dim1))
lm2 = summary(lm(data=line2, dim2 ~ dim1))
```


```{r message=FALSE, echo=FALSE}
library(ggrepel)

plot_tbl |> 
  ggplot(aes(dim1, dim2, 
             color = as.factor(type), label = item)) +
  geom_point(show.legend = F) + 
  geom_text_repel(aes(fontface = "bold"), show.legend = F) +
  theme_bw() + 
  xlab(NULL) + 
  ylab(NULL) +
  geom_abline(slope = 0, intercept = 0, linetype = "dotted") +
  geom_vline(xintercept = -1.10, linetype = "dotted") +
  coord_cartesian(xlim = c(-1.7, 0.2), ylim = c(-1.5, 1.5)) +
  geom_abline(slope = lm1$coefficients[2], 
              intercept = lm1$coefficients[1], linetype = "dotted") + 
  geom_abline(slope = lm2$coefficients[2], 
              intercept = lm2$coefficients[1], linetype = "dotted")
```


Итак, "поисковый запрос" оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.

## Межвекторное расстояние

Мы исследовали наш небольшой корпус визуально, но там, где число измерений больше двух, это просто невозможно. На практике расстояние или сходство между векторами слов (или документов) вычисляется алгебраически. Наиболее известны манхэттенское и евклидово расстояние, а также косинусное сходство. Для анализа текстовых данных как правило применяется косинусное сходство.

![](./images/dist.png) 

Все их можно посчитать в R для заданной пары векторов. 

```{r}
doc_mx <- plot_tbl |> 
  filter(row_number() > 8 ) |> 
  column_to_rownames("item") |> 
  select(dim1, dim2) |> 
  as.matrix()

doc_mx
```


```{r message=FALSE}
dist_mx <- doc_mx |> 
  philentropy::distance(method = "cosine", use.row.names = T) 

dist_mx
```

Чтобы получить расстояние (а не сходство), вычитаем результат из единицы.

```{r}
round(1 - dist_mx, 3)
```

Аналогично вычисляются расстояния между словами. При желании все косинусы можно пересчитать в градусы, чтобы узнать точный угол между векторами. 

```{r}
acos(dist_mx[3,1])  # acos для d3 и d1 (cos = 0.872)
  
180 * acos(dist_mx[3,1]) / pi # переводим из радиан в градусы
```

## Подготовка данных

Теперь, когда мы поняли общий принцип работы алгоритма LSA, оценим его возможности на датасете с подборкой новостей на русском языке. [Источник](https://www.kaggle.com/datasets/yutkin/corpus-of-russian-news-articles-from-lenta); копия в репозитории курса.

```{r}
```

Датасет содержит более 800 тыс. новостей на русском языке с 

## SVD: опрятный подход




