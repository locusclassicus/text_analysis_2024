[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Компьютерный анализ текста",
    "section": "",
    "text": "Введение",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#об-этом-курсе",
    "href": "index.html#об-этом-курсе",
    "title": "Компьютерный анализ текста",
    "section": "Об этом курсе",
    "text": "Об этом курсе\nЭтот сайт содержит материалы к курсу “Компьютерный анализ текста в R” для магистерской программы НИУ ВШЭ “Цифровые методы в гуманитарных науках”. Предыдущую версию курса можно найти здесь.\nИ тексты, и инструменты для работы с ними подобраны таким образом, чтобы помочь студентам гуманитарных специальностей (филологам, философам, историкам и др.) как можно быстрее, но с полным пониманием дела перейти к применению количественных методов в собственной работе.\nЧтобы лучше понимать, какие из этих методов более всего востребованы в научной работе, преподаватели магистратуры “Цифровые методы в гуманитарных науках” – Б.В. Орехов, А.А. Осмоловская и О.В. Алиева – организовали в 2024 г. серию встреч с ведущими представителями отрасли. Видео этих встреч и литературу к семинарам можно найти на сайте http://criticaldh.ru/.\nТам мы собрали именно теоретические обсуждения и литературу к ним, а в этом курсе предлагаем приступить к практике DH (на языке R). Оба этих аспекта, в нашем представлении и в программе магистратуры тесно связаны: одного программирования не хватит, чтобы стать “цифровым гуманистом”, а теории недостаточно, чтобы судить об успешности тех или иных цифровых проектов. Поэтому этот курс старается стоять на двух ногах и соединять кодинг с теоретической рефлексией. Это почти невыполнимая задача но когда нам это мешало.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#ресурсы",
    "href": "index.html#ресурсы",
    "title": "Компьютерный анализ текста",
    "section": "Ресурсы",
    "text": "Ресурсы\nИ в теоретическом, и в практическом плане курс опирается на огромную работу, уже проделанную преподавателями магистратуры ЦМГН. Важнейшие наши достижения собрал Б.В. Орехов: https://github.com/nevmenandr/awesome-dh-hse. Здесь вы найдете ссылки на видео, научно-популярные и научные публикации и датасеты.\nЕсли вдруг вам не хватит практических заданий по R, то в качестве дополнения к оффлайн-курсу можно рекомендовать онлайн-курс Георгия Мороза “Введение в анализ данных на R для гуманитарных и социальных наук”. К этому курсу прилагается онлайн-ноутбук (https://agricolamz.github.io/daR4hs/) с комментариями и всем кодом, и он полностью открыт. Надо иметь в виду, однако, что онлайн-курс рассчитан всего на 9 недель, в то время как наш – на два семестра, так что его можно использовать лишь как вспомогательный ресурс, но не замену.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#программа",
    "href": "index.html#программа",
    "title": "Компьютерный анализ текста",
    "section": "Программа",
    "text": "Программа\nКурс 2025/2026 г. включает в себя четыре основных блока и 32 темы. Программа носит предварительный характер и может быть изменена.\nМодуль 1. Основы работы в R\n\nНачало работы. Контроль версий.\nТаблицы. Опрятные данные.\nВизуализации.\nРаспознавание текста.\nЦиклы, условия, функции.\nИмпорт из JSON.\nРазметка XML TEI.\nДобавление разметки с помощью LLM.\n\nМодуль 2. Текст-майнинг\n\nРегулярные выражения.\nВеб-скрапинг.\nТокенизация, лемматизация, POS-тэггинг и синтаксический анализ.\nРаспределения слов и анализ частотностей.\nЭмоциональная тональность (метод словарей).\nВекторные представления слов.\nТематическое моделирование c LDA.\nКонсолидация.\n\nМодуль 3. Деревья, сети, карты\n\nКластеризация и метод главных компонент.\nСтилометрический анализ с пакетом stylo.\nКонсенсусные деревья и сети.\nСетевые данные в igraph.\nГрафический дизайн сетей в ggraph.\nАнализ сетей и обнаружение сообществ.\nПространственные данные в R.\nКонсолидация.\n\nМодуль 4. Машинное обучение\n\nРегрессионный анализ.\nРегрессионные модели с tidymodels.\nБинарная классификация.\nМногоклассовая классификация.\nГлубокое обучение.\nРабота с LLM.\nПриложения Shiny.\nКонсолидация.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#оценивание",
    "href": "index.html#оценивание",
    "title": "Компьютерный анализ текста",
    "section": "Оценивание",
    "text": "Оценивание\nДомашние задания выполняются в GitHub Classroom. Еженедельно выполняются небольшие задания, которые оцениваются по бинарной шкале (1/0), раз в месяц – консолидирующие задания на весь пройденный материал (оценка 0-10). Все необходимые ссылки вы найдете в чате курса в Telegram.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#благодарности",
    "href": "index.html#благодарности",
    "title": "Компьютерный анализ текста",
    "section": "Благодарности",
    "text": "Благодарности\nЗа помощь в разработке курса и подготовке датасетов к нему автор благодарит Георгия Мороза, Бориса Орехова и Софью Порфирьеву. Даниилу Скоринкину я признательная за помощь в работе над главами, посвященными сетевому анализу. Идеей количественного сравнения британских эмпириков в десятой главе я обязана своей коллеге по Школе философии и культурологии НИУ ВШЭ Дарье Дроздовой. Вдохновением для некоторых технических и визуальных решений послужили телеграм-блоги “Наука и данные” (@naukaidannye) и “People Analytics” (@People_Analytics). За отдельные дельные замечания, которые помогли улучшить курс, спасибо Софии Федотовой, Дмитрию Бергу и Дарье Галкиной.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#обратная-связь",
    "href": "index.html#обратная-связь",
    "title": "Компьютерный анализ текста",
    "section": "Обратная связь",
    "text": "Обратная связь\nЕсли вы заметили ошибку или опечатку, можно сообщить по почте oalieva@hse.ru или оставить issue в репозитории курса на GitHub.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "1  Начало работы",
    "section": "",
    "text": "1.1 Установка R и RStudio\nМы будем использовать R, так что для занятий понадобятся:\nВместо RStudio можно поставить VS Code или Positron. По сути, Positron – это тот же VS Code, но без необходимости устанавливать расширения.\nМы будем использовать следующую версию R:\nR version 4.5.0 (2025-04-11)\nДля работы в облаке ☁️ можно использовать RStudio Cloud, но в бесплатной версии есть ограничения.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#установка-r-и-rstudio",
    "href": "start.html#установка-r-и-rstudio",
    "title": "1  Начало работы",
    "section": "",
    "text": "R\n\nна Windows\nна Mac\nна Linux.\n\nRStudio — IDE для R (можно скачать здесь)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#знакомство-с-rstudio",
    "href": "start.html#знакомство-с-rstudio",
    "title": "1  Начало работы",
    "section": "1.2 Знакомство с RStudio",
    "text": "1.2 Знакомство с RStudio\nRStudio — основная среда разработки (IDE) для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:\n\n\n\nRStudio при первом открытии\n\n\nПосле нажатия на двойное окошко чуть левее надписи Environment откроется окно скрипта.\n\n\n\nПодокна RStudio\n\n\nВсе следующие команды можно:\n\nвводить в окне консоли, и тогда для исполнения следует нажимать клавишу Enter.\nвводить в окне скрипта, и тогда для исполнения следует нажимать клавиши Ctrl/Cmd + Enter или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять Ctrl/Cmd + S.\n\nДля начала попробуйте получить информацию о сессии, введя в консоли такую команду:\n\nsessionInfo()\n\nsessionInfo() – это функция. О функциях можно думать как о глаголах (“сделай то-то!”). За названием функции всегда следуют круглые скобки, внутри которых могут находиться аргументы функции. Аргументы – это что-то вроде дополнений и обстоятельств. Аргументы могут быть обязательные и необязательные. Чтобы узнать, каких аргументов требует функция, надо вызывать help: ?mean(). В правой нижней панели появится техническая документация. Но также можно воспользоваться функцией args(). Попробуйте набрать в консоли args(round).\n\n\n\n\n\n\nВопрос\n\n\n\nСколько аргументов функции round() имеют значения по умолчанию?\n\n\nОтвет:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#пакеты",
    "href": "start.html#пакеты",
    "title": "1  Начало работы",
    "section": "1.3 Пакеты",
    "text": "1.3 Пакеты\nПосле установки R вы получите доступ к уже готовым методам статистического анализа и инструментам для визуализации. Если в базовой инсталляции R нет нужного решения – надо поискать в библиотеке пакетов. Пакет – это набор функций и иногда датасетов, созданный пользователями. На 1 июля 2023 г. в репозитории CRAN доступно 19789 пакетов. И это далеко не все: многие пакеты доступны только на GitHub.\n\n\n\n\n\n\nНа заметку\n\n\n\nНекоторые функции, которые вы найдете в пакетах, частично дублируют друг друга – это нормально, как и в естественном языке, “сказать” что-то можно разными способами.\n\n\nПо технической документации и так называемым “виньеткам” можно понять, какой пакет вам нужен. Например, вот так выглядит виньетка пакета RPerseus, при помощи которого можно получить доступ к корпусу греческой и латинской литературы.\nБывают еще “пакеты пакетов”, то есть очень большие семейства функций, своего рода “диалекты” R. Таково семейство tidyverse, объединяемое идеологией “опрятных” данных. Про него мы еще будем говорить.\nПакеты для работы устанавливаются один раз, однако подключать их надо во время каждой сессии. Чтобы установить новый пакет, можно воспользоваться меню Tools &gt; Install Packages. Также можно устанавливать пакеты из консоли. Установим пакет с интерактивными уроками программирования на языке R:\n\ninstall.packages(\"swirl\")\n\nДля подключения используем функцию library(), которой передаем в качестве аргумента название пакета без кавычек:\n\nlibrary(swirl)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#рабочая-директория",
    "href": "start.html#рабочая-директория",
    "title": "1  Начало работы",
    "section": "1.4 Рабочая директория",
    "text": "1.4 Рабочая директория\nПеред началом работы проверьте свою рабочую директорию при помощи getwd(). Для смены можно использовать как абсолютный, так и относительный путь:\n\nsetwd(\"/Users/name/folder\")\n\n# искать в текущей директории\nsetwd(\"./folder\")\n\n# перейти на уровень вверх\nsetwd(\"../\")\n\nТакже для выбора рабочей директории можно использовать меню R Session &gt; Set Working Directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#r-как-калькулятор",
    "href": "start.html#r-как-калькулятор",
    "title": "1  Начало работы",
    "section": "1.5 R как калькулятор",
    "text": "1.5 R как калькулятор\nМожно использовать R как калькулятор. Для этого вводим данные рядом с символом приглашения &gt;, который называется prompt.\n\nsqrt(4) # квадратный корень\n\n[1] 2\n\n2^3 # степень\n\n[1] 8\n\nlog10(100) #логарифм\n\n[1] 2\n\n\nЕсли в начале консольной строки стоит +, значит предыдущий код не завершен. Например, вы забыли закрыть скобку функции. Ее можно дописать на следующей строке. Попробуйте набрать sqrt(2 в консоли.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#операторы-присваивания",
    "href": "start.html#операторы-присваивания",
    "title": "1  Начало работы",
    "section": "1.6 Операторы присваивания",
    "text": "1.6 Операторы присваивания\nЧтобы в окружении появился новый объект, надо присвоить результат вычислений какой-нибудь переменной при помощи оператора присваивания &lt;- (Alt + - (Windows) или Option + - (Mac)). Знак = также работает как оператор присваивания, но не во всех контекстах, поэтому им лучше не пользоваться.\n\nx &lt;- 2 + 2 # создаем переменную\ny &lt;- 0.1 # создаем еще одну переменную\nx &lt;- y # переназначаем  \nx + y\n\n[1] 0.2\n\n\nСочетание клавиш для оператора присваивания: Option/Alt + -. Имя переменной, как и имя функции, может содержать прописные и строчные буквы, точку и знак подчеркивания.\nТеперь небольшое упражнение.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс программирования на R: install_course(\"R Programming\"). После этого привяжите пакет командой library(swirl) и наберите: swirl(). Укажите ваше имя. Пройдите урок 1 Basic Building Blocks.\n\n\nЕсли все получилось, можно двигаться дальше! Но сначала зафиксируем несколько новых функций из этих первого урока.\n\n\n\n\n\n\nВопрос\n\n\n\nЧто вычисляет функция abs()?\n\n\nОтвет: среднеемодульквадратный корень\n\n\n\n\n\n\nВопрос\n\n\n\nСколько значений вернет функция, если разделить c(2, 4, 6) на 2?\n\n\nОтвет: \n\n\n\n\n\n\nВопрос\n\n\n\nБуква “c” в названии функции c() означает…\n\n\nОтвет: covercollapseconcatenate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#пайпы-конвееры",
    "href": "start.html#пайпы-конвееры",
    "title": "1  Начало работы",
    "section": "1.7 Пайпы (конвееры)",
    "text": "1.7 Пайпы (конвееры)\nВ нашем коде мы часто будем использовать знаки конвеера (или пайпы): |&gt; (в вашей версии он может выглядить иначе: %&gt;%; переключить оператор можно в Global Options). Они призваны показывать последовательность действий. Сочетание клавиш: Ctrl/Cmd + Shift + M.\n\nmean(sqrt(abs(sin(1:100)))) \n\n[1] 0.7654264\n\n1:100 |&gt; \n  sin() |&gt; \n  abs() |&gt; \n  sqrt() |&gt; \n  mean()\n\n[1] 0.7654264",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#векторы",
    "href": "start.html#векторы",
    "title": "1  Начало работы",
    "section": "1.8 Векторы",
    "text": "1.8 Векторы\nВектор – это объект, предназначенный для хранения данных. К таким же объектам относятся также матрицы, списки, датафреймы и др. Заметим, что в языке R нет скаляров (отдельных чисел). Числа считаются векторами из одного элемента.\n\nx &lt;- 2\nclass(x) # числовой вектор\n\n[1] \"numeric\"\n\nlength(x) # длина вектора\n\n[1] 1\n\n\nКак вы уже поняли, функция c() позволяет собрать несколько элементов в единый вектор:\n\nx &lt;- c(3, 5, 7)\nx_mean &lt;- mean(x) \nx_mean\n\n[1] 5\n\n\n Над векторами можно совершать арифметические операции, но будьте внимательны, применяя операции к векторам разной длины: в этом случае более короткий вектор будет переработан, то есть повторен до тех пор, пока его длина не сравняется с длиной вектора большей длины.\n\nx &lt;- c(1.2, 2.51, 3.8)\ny &lt;- 4\nx + y \n\n[1] 5.20 6.51 7.80\n\n\nКак-то так:\n\\[ \\left(\n    \\begin{array}{c}\n      1.2 \\\\\n      2.51 \\\\\n      3.8\n    \\end{array}\n  \\right) + \\left(\n    \\begin{array}{c}\n      4 \\\\\n      4 \\\\\n      4\n    \\end{array}\n  \\right) \\]\nВекторы можно индексировать, то есть забирать из них какие-то элементы:\n\nx &lt;- seq(1, 5, 0.5)\nx[4:5] # индексы начинаются с 1 (в отличие от Python)\n\n[1] 2.5 3.0\n\n\nВектор может хранить данные разных типов:\n\nцелое число (integer);\nчисло с плавающей точкой (numeric, также называются double, то есть число двойной точности);\nстроку (character);\nлогическую переменную (logical);\nкатегориальную переменную, или фактор (factor).\n\n\n# проверить тип данных \nx &lt;- sqrt(2)\nclass(x)\n\n[1] \"numeric\"\n\nis.integer(x)\n\n[1] FALSE\n\nis.numeric(x)\n\n[1] TRUE\n\n\nСоздавать векторы можно не только при помощи c(). Вот еще два способа.\n\nseq(1, 5, 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nrep(\"foo\", 5)\n\n[1] \"foo\" \"foo\" \"foo\" \"foo\" \"foo\"\n\n\n\nФакторы внешне похожи на строки, но в отличие от них хранят информацию об уровнях категориальных переменных. Уровень может обозначаться как числом (например, 1 и 0), так и строкой.\n\nt &lt;- factor(c(\"A\", \"B\", \"C\"), levels = c(\"A\", \"B\", \"C\"))\nt\n\n[1] A B C\nLevels: A B C\n\n\nВажно: вектор может хранить данные только одного типа. При попытке объединить в единый вектор данные разных типов они будут принудительно приведены к одному типу:\n\nx &lt;- c(TRUE, 1, 3, FALSE)\nx # логические значения приведены к числовым\n\n[1] 1 1 3 0\n\ny &lt;- c(1, \"a\", 2, \"лукоморье\") \ny # числа превратились в строки\n\n[1] \"1\"         \"a\"         \"2\"         \"лукоморье\"\n\n\nЛогические векторы можно получить в результате применения логических операторов (== “равно”, != “не равно”, &lt;= “меньше или равно”) к данным других типов:\n\nx &lt;- 1:10 # числа от 1 до 10\ny &lt;- x &gt; 5\ny # значения TRUE соответствуют единице, поэтому их можно складывать\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum(y)\n\n[1] 5\n\n\nФункции all() и any() также возвращают логические значения:\n\nx &lt;- 10:20 \nany(x == 15)\n\n[1] TRUE\n\nall(x &gt; 9)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#списки",
    "href": "start.html#списки",
    "title": "1  Начало работы",
    "section": "1.10 Списки",
    "text": "1.10 Списки\nВ отличие от векторов списки могут хранить данные разных типов.\n\nmy_list &lt;-  list(\n  a = c(\"a\", \"b\", \"c\"), \n  b = c(1, 2, 3), \n  c = c(TRUE, FALSE, TRUE)\n  )\n\nmy_list\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] 1 2 3\n\n$c\n[1]  TRUE FALSE  TRUE\n\n\nМожно получить доступ как к элементам списка целиком, так и к их содержимому.\n\nmy_list$a # обращение к поименованным элементам \n\n[1] \"a\" \"b\" \"c\"\n\nmy_list[2] # одинарные квадратные скобки извлекают элемент списка целиком\n\n$b\n[1] 1 2 3\n\nclass(my_list[2])\n\n[1] \"list\"\n\nmy_list[[2]] #  элементы второго элемента \n\n[1] 1 2 3\n\nclass(my_list[[2]])\n\n[1] \"numeric\"\n\nmy_list$c[1]# первый элемент второго элемента\n\n[1] TRUE\n\n\nОбратите внимание, что my_list[2] и my_list[[2]] возвращают объекты разных классов. Нам это еще понадобится при работе с XML.\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите библиотеку rcorpora и загрузите список с названиями хлеба и сладкой выпечки.\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nУзнайте длину my_list и введите ее в поле ниже.\n\n\nОтвет: \n\n\n\n\n\n\nВопрос\n\n\n\nДостаньте из my_list элемент pastries и узнайте его длину.\n\n\nОтвет: \n\n\n\n\n\n\nВопрос\n\n\n\nА теперь извлеките пятый элемент из pastries и введите ниже его название.\n\n\nОтвет: \nСо списками покончено. Теперь можно пойти выпить кофе с my_list$pastries[13]. Дальше будет сложнее, но интереснее.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "tabular.html",
    "href": "tabular.html",
    "title": "2  Таблицы. Опрятные данные",
    "section": "",
    "text": "2.1 Tidyveryse\nСуществуют два основных “диалекта” R, один из которых опирается главным образом на функции и структуры данных базового R, а другой пользуется синтаксисом tidyverse. Tidyverse – это семейство пакетов (метапакет), разработанных Хадли Уикхемом и др., которое включает в себя в том числе пакеты dplyr, ggplot2 и многие другие.\n# загрузить все семейство\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#таблицы-датафреймы",
    "href": "tabular.html#таблицы-датафреймы",
    "title": "2  Таблицы",
    "section": "2.2 Таблицы (датафреймы)",
    "text": "2.2 Таблицы (датафреймы)\nЕсли матрица – это двумерный аналог вектора, то таблица (кадр данных, data frame) – это двумерный аналог списка. Как и список, датафрейм может хранить данные разного типа.\n\n# создание датафрейма\ndf &lt;- data.frame(names = c(\"John\", \"Mary\"), age = c(18, 25), sport = c(\"basketball\", \"tennis\"))\ndf\n\n\n  \n\n\n\nИзвлечение данных тоже напоминает работу со списком.\n\ndf$names # забирает весь столбец\n\n[1] \"John\" \"Mary\"\n\ndf[,\"names\"] # то же самое, другой способ\n\n[1] \"John\" \"Mary\"\n\ndf[1, ] # забирает ряд",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#импорт-табличных-данных",
    "href": "tabular.html#импорт-табличных-данных",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.2 Импорт табличных данных",
    "text": "2.2 Импорт табличных данных\nВ этом уроке мы будем работать с датасетом из Репозитория открытых данных по русской литературе и фольклору под названием “Программы по литературе для средней школы с 1919 по 1991 гг.” Этот датасет был использован при подготовке интерактивной карты российского школьного литературного канона (1852-2023). Карта была представлена в 2023 г. Лабораторией проектирования содержания образования ВШЭ. Подробнее о проекте можно посмотреть материал “Системного блока”.\nОсновная функция для скачивания файлов из Сети – download.file(), которой необходимо задать в качестве аргументов url, название сохраняемого файла, иногда также метод.\n\nurl &lt;- \"https://dataverse.pushdom.ru/api/access/datafile/4229\"\n\n# скачивание в папку files в родительской директории\ndownload.file(url, destfile = \"../files/curricula.tsv\") \n\nОсновные функции для чтения табличных данных в базовом R - это read.table() и read.csv(). Они вернут датафрейм.\nФайл, который мы скачали, имеет расширение .tsv (tab separated values). Чтобы его прочитать, используем read.table(), указав тип разделителя:\n\ncurricula_df &lt;- read.table(\"../files/curricula.tsv\", sep = \"\\t\", header = TRUE)\n\ncurricula_df\n\n\n  \n\n\n\nФункция read.csv() отличается лишь тем, что автоматически выставляет значения аргументов sep = \",\", header = TRUE.\nФункция class() позволяет убедиться, что перед нами датафрейм.\n\nclass(curricula_df)\n\n[1] \"data.frame\"\n\n\nОбратите внимание, как ведет себя датафрейм при индексации.\n\ncurricula_df[,1] |&gt; \n  class()\n\n[1] \"character\"\n\n\n\n# вывести сводку\nsummary(curricula_df)\n\n    author             title             comment           curriculum       \n Length:10306       Length:10306       Length:10306       Length:10306      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n       id            year               grade          priority        \n Min.   : 1.00   Length:10306       Min.   : 5.000   Length:10306      \n 1st Qu.:13.00   Class :character   1st Qu.: 8.000   Class :character  \n Median :31.00   Mode  :character   Median :10.000   Mode  :character  \n Mean   :28.01                      Mean   : 9.195                     \n 3rd Qu.:42.00                      3rd Qu.:10.000                     \n Max.   :50.00                      Max.   :11.000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#работа-с-датафреймами",
    "href": "tabular.html#работа-с-датафреймами",
    "title": "2  Таблицы",
    "section": "2.4 Работа с датафреймами",
    "text": "2.4 Работа с датафреймами\n\n# узнать имена столбцов\ncolnames(curricula_df) \n\n[1] \"author\"     \"title\"      \"comment\"    \"curriculum\" \"id\"        \n[6] \"year\"       \"grade\"      \"priority\"  \n\n\n\n# извлечь ряд(ы) по значению\ncurricula_df[curricula_df$year == \"1919\", ]\n\n\n  \n\n\n\n\n# извлечь столбец \ncurricula_df$year |&gt; head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\ncurricula_df[ , \"year\"] |&gt; head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\ncurricula_df[ , 6] |&gt;  head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\n\n\n# узнать тип данных в столбцах\nstr(curricula_df) \n\n'data.frame':   10306 obs. of  8 variables:\n $ author    : chr  \"Андреев Л.Н.\" \"Андреев Л.Н.\" \"Андреев Л.Н.\" \"Бальмонт К.Д.\" ...\n $ title     : chr  \"Жили-были\" \"Иуда\" \"Рассказ о семи повешенных\" \"\" ...\n $ comment   : chr  \"\" \"\" \"\" \"\" ...\n $ curriculum: chr  \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" ...\n $ id        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : chr  \"1919\" \"1919\" \"1919\" \"1919\" ...\n $ grade     : int  9 9 9 9 9 8 8 8 8 8 ...\n $ priority  : chr  \"\" \"\" \"*\" \"*\" ...\n\n\n\n# преобразовать тип данных в столбцах\ncurricula_df$year &lt;- as.numeric(curricula_df$year)\n\n\n# вывести сводку\nsummary(curricula_df)\n\n    author             title             comment           curriculum       \n Length:10306       Length:10306       Length:10306       Length:10306      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n       id             year          grade          priority        \n Min.   : 1.00   Min.   :1919   Min.   : 5.000   Length:10306      \n 1st Qu.:13.00   1st Qu.:1946   1st Qu.: 8.000   Class :character  \n Median :31.00   Median :1966   Median :10.000   Mode  :character  \n Mean   :28.01   Mean   :1963   Mean   : 9.195                     \n 3rd Qu.:42.00   3rd Qu.:1981   3rd Qu.:10.000                     \n Max.   :50.00   Max.   :1991   Max.   :11.000                     \n                 NA's   :12                                        \n\n\nНебольшое упражнение на кодинг позволит закрепить навыки работы с матрицами и датафреймами.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 7 Matrices and Data Frames.\n\n\nВсе ли вы запомнили?\n\n\n\n\n\n\nВопрос\n\n\n\nДля чего нужна функция cbind()?\n\n\n\n\nдля добавления рядов\n\n\nдля добавления столбцов\n\n\nдля извлечения имен столбцов\n\n\nдля извлечения имен рядов\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nФункция colnames() позволяет как назначать новые имена таблице, так и извлекать существующие.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Испанские писатели”.\n\n\n\n# устанавливаем и загружаем нужный пакет\ninstall.packages(\"languageR\")\nlibrary(languageR)\n\n# загружаем датасет\nmeta &lt;- spanishMeta\n\n# допишите ваш код ниже\n# посчитайте средний год публикации романов Камило Хосе Селы\n\n\n# вычислите суммарное число слов в романах Эдуардо Мендосы\n\n\n# извлеките ряды с текстами, опубликованными до 1980 г.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#tibble",
    "href": "tabular.html#tibble",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.3 Tibble",
    "text": "2.3 Tibble\nОсновная структура данных в tidyverse – это tibble, современный вариант датафрейма. Тиббл, как говорят его разработчики, это ленивые и недовольные датафреймы: они делают меньше и жалуются больше. Это позволяет решать проблемы на более ранних этапах, что, как правило, приводит к созданию более чистого и выразительного кода.\nОсновные отличия от обычного датафрейма:\n\nусовершенствованный метод print(), не нужно постоянно вызывать head();\nнет имен рядов;\nдопускает синтаксически “неправильные” имена столбцов;\nпри индексировании не превращается в вектор.\n\nПреобразуем наш датафрейм в тиббл для удобства работы с ним.\n\ncurricula_tbl &lt;- as_tibble(curricula_df)\n\nЧтобы избежать подобных преобразований, можно воспользоваться пакетом для импорта данных readr:\n\ncurricula_tbl &lt;- read_tsv(\"../files/curricula.tsv\")\n\nRows: 10306 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): author, title, comment, curriculum, year, priority\ndbl (2): id, grade\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nВоспользуйтесь любым способом, чтобы получить тиббл.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Getting and Cleaning Data\"). Загрузите библиотеку library(swirl), запустите swirl(), выберите этот курс и пройдите из него урок 1 Manipulating Data with dplyr. При попытке загрузить урок 1 вы можете получить сообщение об ошибке. В таком случае установите версию курса из github, как указано здесь, или загрузите файл вручную, как указано здесь.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#dplyr",
    "href": "tabular.html#dplyr",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.4 Dplyr",
    "text": "2.4 Dplyr\nВ уроке swirl выше вы уже немного познакомились с “грамматикой манипуляции данных”, лежащей в основе dplyr. Здесь об этом будет сказано подробнее. Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными:\n\nmutate() добавляет новые переменные, которые являются функциями существующих переменных;\nselect() выбирает переменные (столбцы) на основе их имен;\nfilter() выбирает наблюдения (ряды) на основе их значений;\nsummarise() обобщает значения;\narrange() изменяет порядок следования строк.\n\nВсе эти глаголы естественным образом сочетаются с функцией group_by(), которая позволяет выполнять любые операции “по группам”, и с оператором pipe |&gt; из пакета magrittr.\nВ итоге получается более лаконичный и читаемый код. Узнаем, за какие года у нас есть программы по литературе.\n\ncurricula_tbl |&gt; \n  count(curriculum, year) |&gt; \n  print()\n\n# A tibble: 52 × 3\n   curriculum    year        n\n   &lt;chr&gt;         &lt;chr&gt;   &lt;int&gt;\n 1 19 ИРЛ 2 ст   1919      160\n 2 22 РЛ 2 ст    1922      191\n 3 32 ФЗС 5-7    1932      122\n 4 33 ПСШ 8-10   1933      213\n 5 33 ПСШ ГС 5-8 1933      129\n 6 34 ПСШ 8-10   1934      216\n 7 37-38 ЛИТ 7   1937-38    12\n 8 38 ЛИТ        1938      107\n 9 39 ПСШ 8-10   1933        1\n10 39 ПСШ 8-10   1939      375\n# ℹ 42 more rows\n\n\nОтберем две программы для 8 класса и выясним, какие авторы в них представлены лучше всего.\n\ncurricula_tbl |&gt; \n  filter(year %in% c(1919, 1922), grade == 8) |&gt; \n  count(author, year) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nТеперь упражнения в swirl. Вам придется редактировать код, который предложит программа, так что сгруппируйтесь.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(), выберите курс Getting and Cleaning Data и пройдите из него урок 2 Grouping and Chaining with dplyr.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#опрятные-данные",
    "href": "tabular.html#опрятные-данные",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.5 Опрятные данные",
    "text": "2.5 Опрятные данные\n\nTidy datasets are all alike, but every messy dataset is messy in its own way.\n— Hadley Wickham\n\nTidyverse – это не только особый синтаксис, но и отдельная идеология “опрятных данных”. “Сырые” данные, с которыми мы работаем, редко бывают опрятны, и перед анализом их следует “почистить” и преобразовать.\nОсновные принципы опрятных данных:\n\nотдельный столбец для каждой переменной;\nотдельный ряд для каждого наблюдения;\nу каждого значения отдельная ячейка;\nодин датасет – одна таблица.\n\n\n\n\nПринципы опрятных данных. Источник\n\n\n\nПосмотрите на учебные тибблы из пакета tidyr и подумайте, какое из этих правил нарушено в каждом случае.\n\ndata(\"table2\")\ntable2 |&gt; \n  print()\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ndata(\"table3\")\ntable3 |&gt; \n  print()\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ndata(\"table4a\")\ntable4a |&gt; \n  print()\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ndata(\"table4b\")\ntable4b |&gt; \n  print()\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n\nВажные функции для преобразования данных из пакета tidyr:\n\nseparate() делит один столбец на новые;\nunite() объединяет столбцы;\npivot_longer() удлиняет таблицу;\npivot_wider() расширяет таблицу;\ndrop_na() и replace_na() указывают, что делать с NA и др.\n\nКроме того, в dplyr есть полезное семейство функций _join, позволяющих объединять данные в различных таблицах.\n\n\n\nИсточник.\n\n\nДальше мы потренируемся с ними работать, но сначала пройдем урок swirl. Это достаточно сложный урок (снова понадобится редактировать скрипт), но он нам дальше здорово поможет.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(), выберите курс Getting and Cleaning Data и пройдите из него урок 3 Tidying Data with tidyr.\n\n\nПрежде чем двигаться дальше, приведите в порядок table2, 3, 4a-4b, используя dplyr и tidyr.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#обобщение-данных",
    "href": "tabular.html#обобщение-данных",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.6 Обобщение данных",
    "text": "2.6 Обобщение данных\nТеперь вернемся к датасету curricula и попробуем частично воспроизвести результаты, полученные авторами проекта “Список чтения”, упомянутого выше.\nУ каких авторов больше всего произведений (во всех программах)?\n\ncurricula_tbl |&gt; \n  group_by(author, title) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n) |&gt; \n  print()\n\n# A tibble: 1,624 × 3\n# Groups:   author [461]\n   author          title                        n\n   &lt;chr&gt;           &lt;chr&gt;                    &lt;int&gt;\n 1 Горький М.      Мать                        51\n 2 Некрасов Н.А.   Кому на Руси жить хорошо    50\n 3 Пушкин А.С.     Евгений Онегин              49\n 4 Островский А.Н. Гроза                       48\n 5 Тургенев И.С.   Отцы и дети                 48\n 6 Гоголь Н.В.     Мертвые души                47\n 7 Грибоедов А.С.  Горе от ума                 47\n 8 Лермонтов М.Ю.  Герой нашего времени        46\n 9 Толстой Л.Н.    Война и мир                 46\n10 Толстой Л.Н.    Воскресение                 46\n# ℹ 1,614 more rows\n\n\nКакие произведения упоминаются в программах чаще всего?\n\ncurricula_tbl |&gt; \n  count(author, title) |&gt; \n  arrange(-n) |&gt; \n  print()\n\n# A tibble: 1,624 × 3\n   author          title                        n\n   &lt;chr&gt;           &lt;chr&gt;                    &lt;int&gt;\n 1 Горький М.      Мать                        51\n 2 Некрасов Н.А.   Кому на Руси жить хорошо    50\n 3 Пушкин А.С.     Евгений Онегин              49\n 4 Островский А.Н. Гроза                       48\n 5 Тургенев И.С.   Отцы и дети                 48\n 6 Гоголь Н.В.     Мертвые души                47\n 7 Грибоедов А.С.  Горе от ума                 47\n 8 Лермонтов М.Ю.  Герой нашего времени        46\n 9 Толстой Л.Н.    Война и мир                 46\n10 Толстой Л.Н.    Воскресение                 46\n# ℹ 1,614 more rows\n\n\nНа принятые в каких годах программы приходится больше всего произведений? (Объяснение здесь.)\n\ncurricula_tbl |&gt; \n  group_by(year) |&gt; \n  distinct(author, title) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n) |&gt; \n  print()\n\n# A tibble: 49 × 2\n   year      n\n   &lt;chr&gt; &lt;int&gt;\n 1 1991    518\n 2 1940    372\n 3 1939    371\n 4 1933    288\n 5 1982    282\n 6 1984    282\n 7 1983    281\n 8 1985    278\n 9 1986    278\n10 1987    278\n# ℹ 39 more rows\n\n\nВ заключение попробуйте сформулировать новые вопросы и ответить на них при помощи этого датасета.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "plot.html",
    "href": "plot.html",
    "title": "3  Визуализации",
    "section": "",
    "text": "3.1 Графические системы\nВ R есть несколько графических систем: базовый R, lattice и ggplot2. В этом курсе мы будем работать лишь с ggplot2 как с наиболее современной. Если вам интересны первые две, то вы можете обратиться к версии курса 2023/2024 г. и к интерактивным урокам swirl.\nНастоящая графическая сила R – это пакет ggplot2. В его основе лежит идея “грамматики графических элементов” Лиланда Уилкинсона (Мастицкий 2017) (отсюда “gg” в названии). С одной стороны, вы можете постепенно достраивать график, добавляя элемент за элементом (как в базовом R); с другой – множество параметров подбираются автоматически, как в Lattice.\nО различных видах графиков можно почитать по ссылке. В этом уроке мы научимся строить диаграмму рассеяния (scatter plot) и столбиковую диаграмму (bar chart). Вот к чему мы стремимся.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#графические-системы",
    "href": "plot.html#графические-системы",
    "title": "3  Визуализации",
    "section": "",
    "text": "Задание\n\n\n\nЗапустите swirl(); курс R Programming у вас уже установлен. Из него сделайте урок 15 Base Graphics. Также установите курс swirl::install_course(\"Exploratory Data Analysis\"). Из него можно пройти любые уроки: это необязательно, но поможет разобраться в теме.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#датасет-метаданные-романов-xix-xx-вв.",
    "href": "plot.html#датасет-метаданные-романов-xix-xx-вв.",
    "title": "3  Визуализации",
    "section": "3.2 Датасет: метаданные романов XIX-XX вв.",
    "text": "3.2 Датасет: метаданные романов XIX-XX вв.\nЗнакомиться с ggplot2 мы будем на примере датасета из коллекции “NovelTM Datasets for English-Language Fiction, 1700-2009”, подготовленного Тедом Андервудом, Патриком Кимутисом и Джессикой Уайт. Они собрали метаданные о 210,266 томах художественной прозы в HathiTrust Digital Library и составили из них несколько датасетов.\nМы возьмем небольшой датасет, который содержит провернные вручную метаданные, а также сведения о категории художественной прозы для 2,730 произведений, созданных в период 1799-2009 г. (равные выборки для каждого года). Об особенностях сбора и подготовки данных можно прочитать по ссылке.\nМы попробуем проверить наблюдение, сделанное Франко Моретти в статье “Корпорация стиля: размышления о 7 тысячах заглавий (британские романы 1740-1850)” (2009 г., рус. перевод в книге “Дальнее чтение”, 2016 г.). Моретти заметил, что на протяжении XVIII-XIX вв. названия становятся короче, причем уменьшается не только среднее, но и стандартное отклонение (т.е. разброс значений). В публикации он предлагает несколько возможных объяснений для этого тренда. В датасете NovelTM есть не только романы (и не только британские), но тем более интересно будет сравнить результат.\nДля этого урока данные были немного трансформированы: в частности, мы добавили столбец n_words, в котором хранятся сведения о числе слов в названии. Файл в формате .Rdata надо забрать из репозитория курса и прочитать в окружение.\n\nlibrary(tidyverse)\n\n\nload(\"../data/noveltm.Rdata\")\nnoveltm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#диаграмма-рассеяния",
    "href": "plot.html#диаграмма-рассеяния",
    "title": "3  Визуализации",
    "section": "3.3 Диаграмма рассеяния",
    "text": "3.3 Диаграмма рассеяния\nФункция ggplot() имеет два основных аргумента: data и mapping. Аргумент mapping задает эстетические атрибуты геометрических объектов. Обычно используется в виде mapping = aes(x, y), где aes() означает aesthetics.\nПод “эстетикой” подразумеваются графические атрибуты, такие как размер, форма или цвет. Вы не увидите их на графике, пока не добавите какие-нибудь “геомы” – геометрические объекты (точки, линии, столбики и т.п.). Эти объекты могут слоями накладываться друг на друга (Wickham и Grolemund 2016).\nДиаграмма рассеяния, которая подходит для отражения связи между двумя переменными, делается при помощи geom_point(). Попробуем настройки по умолчанию.\n\nnoveltm |&gt; \n  ggplot(aes(inferreddate, n_words)) + \n  geom_point()\n\n\n\n\n\n\n\n\nУпс. Точек очень много, и они накладываются друг на друга, так как число слов – дискретная величина. Поступим так же, как Моретти, который отразил на графике среднее для каждого года. Для этого нам надо снова поколдовать над данными.\n\nnoveltm_summary &lt;- noveltm |&gt;\n  group_by(inferreddate) |&gt;\n  summarise(n = n(),\n            mean_w = mean(n_words, na.rm = TRUE)) |&gt; \n  filter(n &gt; 1)\n\nnoveltm_summary\n\n\n  \n\n\n\nСнова построим диаграмму рассеяния. Добавим линию тренда, изменим внешний вид точек и тему оформления, а также уберем подпись оси X.\n\nnoveltm_summary |&gt; \n  ggplot(aes(inferreddate, mean_w)) +\n  geom_point(color = \"steelblue\", alpha = 0.7, size = 2) +\n  geom_smooth(color = \"tomato\") + \n  theme_bw() +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nНисходящая тенденция, о которой писал Моретти, хорошо прослеживается. Но, возможно, она характерна не для всех стран?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#сравнение-двух-групп",
    "href": "plot.html#сравнение-двух-групп",
    "title": "3  Визуализации",
    "section": "3.5 Сравнение двух групп",
    "text": "3.5 Сравнение двух групп\nВ столбце nationality хранятся данные о происхождении писателя.\n\nnoveltm |&gt; \n  count(nationality, sort = TRUE) |&gt; \n  print()\n\n# A tibble: 68 × 2\n   nationality     n\n   &lt;chr&gt;       &lt;int&gt;\n 1 uk            861\n 2 us            801\n 3 &lt;NA&gt;          630\n 4 ir             74\n 5 fr             68\n 6 au             45\n 7 de             32\n 8 ca             30\n 9 in             22\n10 ru             19\n# ℹ 58 more rows\n\n\nОтберем только английских и американских авторов и сравним тенденции в этих двух группах. Категориальную переменную (национальность) в нашем случае проще всего закодировать цветом. Также добавим заголовок и подзаголовок и поменяем тему.\n\nnoveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(inferreddate) |&gt; \n  # убираем года, для которых только одно наблюдение\n  filter(n &gt; 1) |&gt; \n  # код как выше, но убираем цвет для геомов\n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth(se = FALSE) +\n  # новая тема\n  theme_bw() +\n  # заголовки\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  )\n\n\n\n\n\n\n\n\nДля разведывательного анализа данных вполне достаточно настроек по умолчанию, но для публикации вы, вероятно, захотите вручную поправить шрифтовое и цветовое оформление.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#цветовые-шкалы",
    "href": "plot.html#цветовые-шкалы",
    "title": "3  Визуализации",
    "section": "3.6 Цветовые шкалы",
    "text": "3.6 Цветовые шкалы\nGgplot2 дает возможность легко поменять цветовую палитру и шрифтовое оформление, а также добавить фон.\nФункции scale_color_brewer() и scale_fill_brewer() позволяют использовать специально подобранные палитры хорошо сочетаемых цветов.\nОбщее правило для выбора таково.\n\nЕсли дана качественная переменная с упорядоченными уровнями (например, “холодный”, “теплый”, “горячий”) или количественная переменная, и необходимо подчеркнуть разницу между высокими и низкими значениями, то для визуализации подойдет последовательная шкала.\nЕсли дана количественная переменная с осмысленным средним значением, например нулем, 50%, медианой, целевым показателем и т.п., то выбираем расходящуюся шкалу.\nЕсли дана качественная переменная, уровни которой невозможно упорядочить (названия городов, имена авторов и т.п.), ищем качественную шкалу.\n\n\n\n\nИсточник.\n\n\nВот основные (но не единственные!) цветовые шкалы в R. Также цвета можно задавать и вручную – по названию или коду.\n\n\n# тут все по-старому\nnoveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  filter(n &gt; 1) |&gt; \n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  ) +\n  # тут новое\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#шрифты",
    "href": "plot.html#шрифты",
    "title": "3  Визуализации",
    "section": "3.7 Шрифты",
    "text": "3.7 Шрифты\nПакет ggplot2 и расширения для него дают возможность использовать пользовательские шрифты.\n\n# тут новое\nlibrary(showtext)\nfont_add_google(\"Special Elite\", family = \"special\")\nshowtext_auto()\n\n# тут почти все по-старому...\nnoveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  filter(n &gt; 1) |&gt; \n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth() +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  ) +\n  scale_color_brewer(palette = \"Dark2\") + \n  # кроме этих строк, тут новое\n  theme(\n    axis.title = element_text(family = \"special\"),\n    title = element_text(family = \"special\"),\n    axis.text = element_text(family = \"special\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#изображения",
    "href": "plot.html#изображения",
    "title": "3  Визуализации",
    "section": "3.8 Изображения",
    "text": "3.8 Изображения\nИзображения можно добавлять и в качестве фона, и вместо отдельных геомов, например точек. Поправим цвета, чтобы они лучше сочетались с цветом изображения.\n\nlibrary(ggimage)\nurl &lt;- \"./images/book.jpg\"\n\n# почти все по-старому...\nfont_add_google(\"Special Elite\", family = \"special\")\nshowtext_auto()\n\n# ...но график сохраняем в окружение под именем g\ng &lt;- noveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  filter(n &gt; 1) |&gt; \n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth() +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  ) +\n  # подбираем новые цвета, в т.ч. для текста\n  scale_color_manual(\"country\", values = c(\"#A03B37\", \"#50684E\")) + \n  theme(\n    axis.title = element_text(family = \"special\", color = \"#8B807C\"),\n    title = element_text(family = \"special\", color = \"#52211E\"),\n    axis.text = element_text(color = \"#52211E\"),\n    axis.ticks = element_blank(),\n    # расширяем правое поле, чтобы все влезло\n    plot.margin = unit(c(0.4, 3, 0.4, 0.4), \"inches\"), # t, r, b, l\n    # рамочка\n    panel.border = element_rect(color = \"#8B807C\"),\n    # перемещаем легенду\n    legend.position = c(0.8, 0.8)\n  )\n\n# let the magic start!\nggbackground(g, url)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#столбиковая-диаграмма",
    "href": "plot.html#столбиковая-диаграмма",
    "title": "3  Визуализации",
    "section": "3.9 Столбиковая диаграмма",
    "text": "3.9 Столбиковая диаграмма\nДля визуализации распределений качественных переменных подходит стобиковая диаграмма, которая наглядно показывает число наблюдений в каждой группе. В датасете NovelTM представлены следующие категории литературы.\n\nnoveltm |&gt; \n  ggplot(aes(category, fill = category)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nНас будет интересовать категория longfiction, т.к. именно сюда попадает популярный в XIX в. жанр романа. Известно, что примерно до 1840 г. почти половина романистов были женщинами, но к началу XX в. их доля снизилась (Underwood 2019, 133). Отчасти это объясняется тем, что после середины XIX в. профессия писателя становится более престижной, а его социальный статус повышается, что приводит к “джентрификации” романа. Посмотрим, что на этот счет могут сказать данные NovelTM. Переменная gender хранит данные о гендере автора.\n\nnoveltm |&gt; \n  ggplot(aes(gender, fill = gender)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nОтберем лишь одну категорию и два гендера.\n\nnoveltm_new &lt;- noveltm |&gt; \n  select(inferreddate, gender, category) |&gt; \n  filter(gender != \"u\", category == \"longfiction\") |&gt; \n  select(-category)\n\nnoveltm_new |&gt; \n  print()\n\n# A tibble: 1,875 × 2\n   inferreddate gender\n          &lt;dbl&gt; &lt;chr&gt; \n 1         1896 m     \n 2         1954 m     \n 3         1976 f     \n 4         1807 m     \n 5         1807 m     \n 6         1820 f     \n 7         1820 f     \n 8         1817 m     \n 9         1817 m     \n10         1815 m     \n# ℹ 1,865 more rows\n\n\nМожно предположить, что соотношение мужчин и женщин в разные десятилетия менялось. Чтобы это выяснить, нам надо преобразовать данные, указав для каждого года соответствующую декаду, и посчитать число мужчин и женщин в каждой декаде.\n\nnoveltm_new &lt;- noveltm_new |&gt; \n  mutate(decade = (inferreddate %/% 10) * 10) \n\nnoveltm_new |&gt; \n  print()\n\n# A tibble: 1,875 × 3\n   inferreddate gender decade\n          &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1         1896 m        1890\n 2         1954 m        1950\n 3         1976 f        1970\n 4         1807 m        1800\n 5         1807 m        1800\n 6         1820 f        1820\n 7         1820 f        1820\n 8         1817 m        1810\n 9         1817 m        1810\n10         1815 m        1810\n# ℹ 1,865 more rows\n\n\nЭтого уже достаточно для визуализации, но она будет не очень наглядная.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nСравните:\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = \"stack\")\n\n\n\n\n\n\n\n\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nДолю женщин можно посчитать и отдельно.\n\nnoveltm_new_prop &lt;- noveltm_new |&gt; \n  add_count(decade, name = \"total\") |&gt; \n  select(-inferreddate) |&gt; \n  add_count(gender, decade, name = \"counts\") |&gt; \n  distinct(gender, decade, counts, total) |&gt; \n  mutate(share = counts / total) \n\nnoveltm_new_prop |&gt; \n  print()\n\n# A tibble: 42 × 5\n   gender decade counts total share\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 m        1890     56    94 0.596\n 2 m        1950     63    89 0.708\n 3 f        1970     25    90 0.278\n 4 m        1800     52    82 0.634\n 5 f        1820     31    78 0.397\n 6 m        1810     42    97 0.433\n 7 m        1870     52    89 0.584\n 8 m        1840     53    83 0.639\n 9 f        1810     55    97 0.567\n10 f        1800     30    82 0.366\n# ℹ 32 more rows\n\nnoveltm_new_prop |&gt; \n  # тот же график, но...\n  ggplot(aes(decade, share, fill = gender)) +\n  # тут просим ничего не считать, а брать что дают\n  geom_bar(stat = \"identity\") + \n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#линейная-диаграмма",
    "href": "plot.html#линейная-диаграмма",
    "title": "3  Визуализации",
    "section": "3.9 Линейная диаграмма",
    "text": "3.9 Линейная диаграмма\nДанные о доли женщин-писателей можно представить и в виде линии: в нашем случае это не лишено смысла, поскольку ось x – это временная шкала.\n\nnoveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = gender)) +\n  geom_line(show.legend = FALSE)\n\n\n\n\n\n\n\n\nПо умолчанию ось y усекается, и создается впечатление, что доля женщин ок. 1900 падает чуть ли не до нуля. Поправим вручную границы оси.\n\nnoveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = gender)) +\n  geom_line(show.legend = FALSE) +\n  expand_limits(y = 0)\n\n\n\n\n\n\n\n\nГрафик, кажется, подтверждает, что доля женщин в литературе снижалась примерно до середины XX в. Однако при разделении данных на группы можно заметить другую тенденцию.\n\nnoveltm_nation &lt;- noveltm |&gt; \n  filter(category == \"longfiction\") |&gt; \n  select(inferreddate, gender, nationality) |&gt; \n  mutate(nationality = case_when(!nationality %in% c(\"uk\", \"us\") ~ \"other\",\n                                 .default = nationality)) |&gt; \n  filter(gender != \"u\") |&gt; \n  mutate(decade = (inferreddate %/% 10) * 10)\n\nnoveltm_nation\n\n\n  \n\n\ntotal_nation &lt;- noveltm_nation |&gt; \n  group_by(decade) |&gt; \n  summarise(total = n()) |&gt; \n  filter(total &gt; 1)\n\nsummary_nation &lt;- noveltm_nation |&gt; \n  group_by(decade, nationality, gender) |&gt; \n  summarise(counts = n()) |&gt; \n  filter(counts &gt; 1)\n\nsummary_nation\n\n\n  \n\n\nnoveltm_nation_share &lt;- summary_nation |&gt; \n  left_join(total) |&gt; \n  mutate(share = counts / total) |&gt; \n  select(-counts, -total)\n\nnoveltm_nation_share\n\n\n  \n\n\n\n\nnoveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line() \n\n\n\n\n\n\n\n\nДобавим название и немного поменяем оформление.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"rcartocolor::Pastel\")[1:3]\n\nnoveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_minimal() + \n  labs(\n    title = \"Female Writers' Share\",\n    subtitle = \"NovelTM Data 1800-2009 \\n \",\n    x = NULL,\n    y = NULL) +\n  theme(text=element_text(size=14, family=\"serif\")) + \n  scale_color_manual(values = cols)\n\n\n\n\n\n\n\n\nМожно добавить рамку и переместить легенду.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"rcartocolor::Pastel\")[c(1,3,5)]\n\n\ng &lt;- noveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_light() + \n  labs(\n    title = \"Female Writers' Share\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL,\n    y = NULL) +\n  theme(text=element_text(size=14, family=\"serif\"),\n        axis.text = element_text(color = \"#F0F0F0\"),\n        axis.title = element_text(color = \"#F0F0F0\"),\n        legend.position = c(0.5, 0.87), \n        legend.direction = \"horizontal\",\n        legend.title = element_blank(),\n        legend.text = element_text(color = \"#D2B48C\"),\n        legend.background = element_blank(),\n        plot.title = element_text(hjust=0.5, color = \"#F0F0F0\", face=\"bold\"),\n        plot.subtitle = element_text(hjust=0.5, color = \"#F0F0F0\"),\n        plot.background = element_rect(fill = \"#D2B48C\"),\n        panel.background = element_rect(fill = \"#F0F0F0\"),\n        panel.grid.major = element_line(color = \"#87CEEB\"),\n        panel.grid.minor = element_line(color = \"#87CEEB\"),\n        panel.grid.major.y = element_line(linewidth = 0.5)) + \n  scale_color_manual(values = cols)\n\ng",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#экспорт-графиков-из-среды-r",
    "href": "plot.html#экспорт-графиков-из-среды-r",
    "title": "3  Визуализации",
    "section": "3.12 Экспорт графиков из среды R",
    "text": "3.12 Экспорт графиков из среды R\nСпособы:\n\nреализованные в R драйверы стандартных графических устройств;\nфункция ggsave()\nменю программы RStudio.\n\n\n# код сохранит pdf в рабочую директорию \npdf(file = \"plot.pdf\")\n \ng \n\ndev.off()\n\nЕще один способ сохранить последний график из пакета ggplot2.\n\nggsave(\n  filename = \"plot.png\",\n  plot = last_plot(),\n  device = \"png\",\n  scale = 1,\n  width = NA,\n  height = 500,\n  units = \"px\",\n  dpi = 300\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "iterate.html",
    "href": "iterate.html",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "4.1 Датасет\nВ этом уроке мы познакомимся с итерационными конструкциями и функционалами, т.е. такими функциями, которые принимают в качестве аргумента другую функцию. Они нужны для того, чтобы можно было что-то сделать много раз: например, прочитать сразу 100 файлов из директории, построить и сохранить одной командой несколько графиков или сделать множество случайных выборок из большого текстового корпуса.\nОбщее правило таково: если вы скопировали кусок кода больше трех раз, то самое время задуматься об итерации. Это позволит избежать ошибок при копировании и сделает код более легким и читаемым.\nВ этом уроке мы исследуем датасет “Гарри Поттер”, который представляет собой набор файлов .csv, содержащих метаданные о ресурсах из коллекций Британской библиотеки, связанных с Гарри Поттером. Первоначально он был выпущен к 20-летию публикации книги «Гарри Поттер и философский камень» 26 июня 2017 года и с тех пор ежегодно обновлялся. Всего в датасете пять файлов, каждый из которых содержит разное представление данных.\nДатасет до 2023 г. был доступен на сайте Британской библиотеки (https://www.bl.uk/); в репозитории курса сохранена его копия. Скачаем архив.\nmy_url &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/HP.zip\"\ndownload.file(url = my_url, destfile = \"../files/HP.zip\")\nПосле этого переходим в директорию с архивом и распаковываем его.\nunzip(\"../files/HP.zip\")\nСохраним список всех файлов с расширением .csv, используя подходящую функцию из base R.\nmy_files &lt;- list.files(\"../files/HP\", pattern = \".csv\", full.names = TRUE)\nmy_files\n\n[1] \"../files/HP/classification.csv\" \"../files/HP/names.csv\"         \n[3] \"../files/HP/records.csv\"        \"../files/HP/titles.csv\"        \n[5] \"../files/HP/topics.csv\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#циклы-и-их-аналоги",
    "href": "iterate.html#циклы-и-их-аналоги",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "На заметку\n\n\n\nВ циклах часто используется буква i. Но никакой особой магии в ней нет, имя переменной можно изменить.\n\n\n\n\n\n\n\n\n\n\n\nОдин из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми.\n— Нормат Мэтлофф\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПройдите урок 10 lapply and sapply и урок 11 vapply and tapply из курса R Programming в swirl.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nПосчитайте среднее для всех столбцов в mtcars.\nОпределите тип данных во всех столбцах nycflights13::flights.\nПосчитайте число уникальных значений в каждом столбце iris.\nСгенерируйте 10 случайных чисел из нормального распределения - это делает функция rnorm() - со средним -10, 0, 10.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПопробуйте избавиться от цикла 😜.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#условия",
    "href": "iterate.html#условия",
    "title": "4  Циклы, условия, функции",
    "section": "4.9 Условия",
    "text": "4.9 Условия\nИногда необходимо ограничить выполнение функции неким условием. Короткие условия можно писать в одну строку без фигурных скобок.\n\nword &lt;-  \"Эйяфьятлайокудль\"\n\nif(is.character(word)) toupper(word)\n\n[1] \"ЭЙЯФЬЯТЛАЙОКУДЛЬ\"\n\n\nБолее сложные и множественные условия требуют фигурных скобок. Можно сравнить это с условным периодом: протасис (всегда либо TRUE, либо FALSE) в круглых скобках, аподосис в фигурных.\n\nif(is.character(word)) {\n  toupper(word)\n} else {\n  print(\"not a character\")\n}\n\n[1] \"ЭЙЯФЬЯТЛАЙОКУДЛЬ\"\n\n\nТеперь добавим условие внутрь нашей функции.\nВот исходный вариант. На этот раз обойдемся без облака, а только посчитаем статистику для столбца.\n\ncolumn_counts &lt;- function(data, colname) {\n  # загружаем пакеты\n  library(dplyr)\n  library(stringr)\n  library(tidytext)\n\n  # пишем код, подставляя имена переменных \n  count_data &lt;- data |&gt; \n  filter(str_detect(Languages, \"English\")) |&gt; \n  select(any_of(colname)) |&gt;  \n  unnest_tokens(output = \"word\", input = colname) |&gt; \n  anti_join(stop_words) |&gt; \n  count(word, sort = TRUE)\n  \n  return(count_data)\n}\n\n\ncolumn_counts(titles, \"Genre\")\n\n\n  \n\n\n\nДадим пользователю возможность выбрать, хочет ли он удалять стоп-слова.\n\ncolumn_counts &lt;- function(data, colname, remove_stopwords = TRUE) {\n  library(dplyr)\n  library(tidytext)\n  library(stringr)\n  \n  # базовая обработка\n  count_data &lt;- data |&gt; \n    select(any_of(colname)) |&gt;  \n    unnest_tokens(output = \"word\", input = colname) \n  \n  # удаляем стоп-слова, если требуется\n  if (remove_stopwords) {\n    count_data &lt;- count_data |&gt; \n      anti_join(stop_words)\n  }\n  \n  # частотности и сортировка\n  count_data &lt;- count_data |&gt;\n    count(word, sort = TRUE)\n  \n  return(count_data)\n}\n\nПрименим.\n\ncolumn_counts(titles, \"Title\", remove_stopwords = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#функции",
    "href": "iterate.html#функции",
    "title": "4  Циклы, условия, функции",
    "section": "4.3 Функции",
    "text": "4.3 Функции\nФункция и код – не одно и то же. Чтобы стать функцией, кусок кода должен получить имя. Но зачем давать имя коду, который и так работает?\nВот три причины, которые приводит Хадли Уикхем:\n\nу функции есть выразительное имя, которое облегчает понимание кода;\nпри изменении требований необходимо обновлять код только в одном месте, а не во многих;\nменьше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом)\n\n\nWriting good functions is a lifetime journey.\n— Hadley Wickham\n\nМашине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена!\nДалее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть).\nНаписание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию.\nНапример, нам нужна функция, которая ищет совпадения в двух векторах и возвращает совпавшие элементы. Сначала решим задачу для двух векторов.\n\nline1 &lt;- c(\"гнев\", \"богиня\", \"воспой\")\nline2 &lt;- c(\"в\", \"мысли\", \"ему\", \"то\", \"вложила\", \"богиня\", \"державная\", \"гера\")\nidx &lt;- which(line2 %in% line1) # 2\nline2[idx]\n\n[1] \"богиня\"\n\n\nТеперь заменяем фактические переменные на формальные.\n\ncommon_words &lt;- function(x, y){\n  idx &lt;- which(x %in% y)\n  x[idx]\n}\n\nИ применяем к новым данным.\n\nline3 &lt;- c(\"лишь\", \"явилась\", \"заря\", \"розоперстая\", \"вестница\", \"утра\")\nline4 &lt;- c(\"вестница\", \"утра\", \"заря\", \"на\", \"великий\", \"олимп\", \"восходила\")\ncommon_words(line4, line3)\n\n[1] \"вестница\" \"утра\"     \"заря\"    \n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗагрузите библиотеку swirl, выберите курс R Programming и пройдите из него урок 9 Functions.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДля просмотра исходного кода любой функции необходимо…\n\n\n\n\n\n\nнабрать имя функции без аргументов и без скобок\n\n\nиспользовать специальную функцию для просмотра кода\n\n\nвызвать help к функции\n\n\nединственный способ — найти код функции в репозитории на GitHub\n\n\n\n\n\n\n\n\nНапишем функцию, которая будет центрировать данные, то есть вычитать среднее из каждого значения (забудем на время, что это уже делает базовая scale()):\n\ncenter &lt;- function(x){ \n  n = x - mean(x)\n  return(n) \n}\n\nx &lt;- c(5, 10, 15)\ncenter(x) \n\n[1] -5  0  5\n\n\nВнутри нашей функции есть переменная n, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции.\nФункция может принимать произвольное число аргументов. Доработаем наш код:\n\ncenter &lt;- function(x, na.rm = F){\n  if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие\n  x - mean(x) # на этот раз без return()\n}\n\nx &lt;- c(5, 10, NA)\ncenter(x)\n\n[1] NA NA NA\n\n\nЧто произошло? Почему следующий код выдает другой результат?\n\ncenter(x, na.rm = T)\n\n[1] -2.5  2.5\n\n\nВычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет.\n\ncenter &lt;- function(x, na.rm = F, what_is_your_name){\n  if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие\n  x - mean(x) # на этот раз без return()\n}\n\ncenter(x, na.rm = T)\n\n[1] -2.5  2.5\n\ncenter(x, na.rm = T, what_is_your_name = \"Locusclassicus\")\n\n[1] -2.5  2.5\n\n\nЧасто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении.\n\ncenter &lt;- function(x){\n  if (length(x) == 1) {stop(\"Отстань, старушка, я в печали.\")}\n  x - mean(x) # на этот раз без return()\n}\n\nx &lt;- 10\ncenter(x) # вернет ошибку\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию awesome_plot, которая будет принимать в качестве аргументов два вектора, трансформировать их в тиббл и строить диаграмму рассеяния при помощи ggplot(). Задайте цвет и прозрачность точек, а в подзаголовке выведите коэффициент корреляции.\n\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nНапишите код, который распечатает стихи детской песни “Alice the Camel”.\nПревратите детскую потешку “Ted in the Bed” в функцию. Обобщите до любого числа спящих.\nЗапишите в виде функции текст песни “99 Bottles of Beer on the Wall”. Обобщите до любого числа любых напитков на любой поверхности.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию, которая будет говорить “доброе утро”, “добрый день” или “добрый вечер” в зависимости от времени суток. Используйте lubridate::now() в качестве значения аргумента по умолчанию.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#purrr",
    "href": "iterate.html#purrr",
    "title": "4  Циклы, условия, функции",
    "section": "4.7 Purrr",
    "text": "4.7 Purrr\nПо-настоящему мощный инструмент для итераций – это пакет purrr из семейства tidyverse. Разработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham и Grolemund 2016).\n\nYou should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work.\n— Hadley Wickham & Garrett Grolemund\n\nВ семействе функций map_ из этого пакета всего 23 вариации. Вот основные из них:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nВсе они принимают на входе данные и функцию (или формулу), которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_int() – целочисленный вектор, и т.д.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#гарри-поттер-цикл-vs.-map_",
    "href": "iterate.html#гарри-поттер-цикл-vs.-map_",
    "title": "4  Циклы, условия, функции",
    "section": "4.5 Гарри Поттер: цикл vs. map_()",
    "text": "4.5 Гарри Поттер: цикл vs. map_()\nКак вы уже поняли, одни и те же задачи можно решать при помощи циклов и при помощи map_. Мы потренируемся на датасете, который в 2023 г. был доступен на сайте Британской библиотеки (https://www.bl.uk/), но потом оттуда исчез (но у нас сохранилась копия).\nДатасет представляет собой набор файлов .csv, содержащих метаданные о ресурсах, связанных с Гарри Поттером, из коллекций Британской библиотеки. Первоначально он был выпущен к 20-летию публикации книги «Гарри Поттер и философский камень» 26 июня 2017 года и с тех пор ежегодно обновлялся. Всего в датасете пять файлов, каждый из которых содержит разное представление данных.\nСкачаем архив.\n\nmy_url &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/HP.zip\"\ndownload.file(url = my_url, destfile = \"../files/HP.zip\")\n\nПосле этого переходим в директорию с архивом и распаковываем его.\n\nunzip(\"../files/HP.zip\")\n\nСохраним список всех файлов с расширением .csv, используя подходящую функцию из base R.\n\nmy_files &lt;- list.files(\"../files/HP\", pattern = \".csv\", full.names = TRUE)\nmy_files\n\n[1] \"../files/HP/classification.csv\" \"../files/HP/names.csv\"         \n[3] \"../files/HP/records.csv\"        \"../files/HP/titles.csv\"        \n[5] \"../files/HP/topics.csv\"        \n\n\n\n4.5.1 Цикл\nТеперь напишем цикл, который\n\nпрочитает все файлы из my_files, используя для этого функцию read_csv() из пакета readr;\nдля каждого датасета выяснит количество рядов без NA в столбце BNB Number;\nразделит число таких рядов на общее число рядов;\nвернет таблицу c четырьми столбцами:\n\nназвание файла (id),\nчисло рядов (total),\nчисло рядов без NA (complete),\nдоля полных рядов (ratio).\n\n\nСначала создаем таблицу, в которую будем складывать результат.\n\nmy_files_short &lt;- list.files(\"../files/HP\", pattern = \".csv\")\n\nmy_df &lt;- data.frame(id = my_files_short, \n                    total = 0,\n                    complete = 0,\n                    ratio = 0)\n\nmy_df\n\n\n  \n\n\n\nТеперь тело цикла:\n\nfor (i in 1:length(my_files)) {\n\n  # читаем очередной файл из my_files\n  current_file &lt;- my_files[i]\n  current_df &lt;- readr::read_csv(current_file, show_col_types = FALSE) \n\n  # выявляем общее число рядов и число рядов без NA в BNB number\n  # из-за пробела в названии столбца BNB number нужно использовать\n  # с бэктиками ``, а не с \"такими\" или 'такими' кавычками \n  current_total &lt;- nrow(current_df)\n  current_complete &lt;- sum(!is.na(current_df$`BNB number`))\n    \n\n  # помещаем значения в нужное место в заранее созданном my_df вместо нулей\n  my_df$total[i] &lt;- current_total  \n  my_df$complete[i] &lt;- current_complete\n  my_df$ratio[i] &lt;- current_complete / current_total\n}\n\nСмотрим на результат.\n\nmy_df\n\n\n  \n\n\n\n\n\n4.5.2 map_()\nТеперь исследуем датасет при помощи функционалов. Прочитаем все файлы одним вызовом функции.\n\n# чтение файлов \nHP &lt;- map(my_files, read_csv, col_types = cols())\n\nОбъект HP – это список. В нем пять элементов, так как на входе у нас было пять файлов. Для удобства назначим имена элементам списка.\n\nnames(HP) &lt;- my_files_short\n\n\nНачнем с простого: при помощи map можно извлечь столбцы (по имени) или ряды (по условию) из всех пяти таблиц. Прежде чем выполнить код ниже, подумайте, как будет выглядеть результат.\n\n# извлечь столбцы\nmap(HP, select, `BNB number`)\n\n# извлечь ряды\nmap(HP, filter, !(is.na(`BNB number`)))\n\n\n\n\n\n\n\nЗадание\n\n\n\nИзвлеките все уникальные названия (столбец Title) из всех пяти таблиц в HP. Используйте функцию distinct.\n\n\nЧто, если мы не знаем заранее, какие столбцы есть во всех пяти таблицах, и хотим это выяснить? Для этого подойдет функция reduce() из того же purrr. Она принимает на входе вектор (или список) и функцию и применяет функцию последовательно к каждой паре значений.\n\n\n\nИсточник.\n\n\n\nВоспользуемся этим, чтобы найти общие для всех таблиц имена столбцов.\n\nmap(HP, colnames) |&gt; \n  reduce(intersect)\n\n [1] \"Dewey classification\"       \"BL record ID\"              \n [3] \"Type of resource\"           \"Content type\"              \n [5] \"Material type\"              \"BNB number\"                \n [7] \"ISBN\"                       \"ISSN\"                      \n [9] \"Name\"                       \"Dates associated with name\"\n[11] \"Type of name\"               \"Role\"                      \n[13] \"Title\"                      \"Series title\"              \n[15] \"Number within series\"       \"Country of publication\"    \n[17] \"Place of publication\"       \"Publisher\"                 \n[19] \"Date of publication\"        \"Edition\"                   \n[21] \"Physical description\"       \"BL shelfmark\"              \n[23] \"Genre\"                      \"Languages\"                 \n[25] \"Notes\"                     \n\n\nЕще одна неочевидная возможность функции reduce - объединение нескольких таблиц в одну одним вызовом. Например, так:\n\nHP_joined &lt;- HP |&gt; \n  reduce(left_join)\n\nHP_joined\n\n\n  \n\n\n\n\n\n4.5.3 EDA\nТеперь можно почистить данные и построить несколько разведывательных графиков.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(`Country of publication`, into = c(\"country\", NA), sep = \";\") |&gt;\n  mutate(country = str_squish(country)) |&gt; \n  mutate(country = \n           case_when(country == \"England\" ~ \"United Kingdom\",\n                     country == \"Scotland\" ~ \"United Kingdom\",\n                     TRUE ~ country)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(country)) |&gt; \n  group_by(year, country) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \n\ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = country)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nВ качестве небольшого бонуса к этому уроку построим облако слов. Вектор слов возьмем из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./book/images/Wizard-Hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "5  Импорт: JSON & XML",
    "section": "",
    "text": "5.1 JSON\nФормат JSON (JavaScript Object Notation) предназначен для представления структурированных данных. JSON имеет шесть основных типов данных. Четыре из них - скаляры:\nСтроки, числа и булевы значения в JSON очень похожи на символьные, числовые и логические векторы в R. Основное отличие заключается в том, что скаляры JSON могут представлять только одно значение. Для представления нескольких значений необходимо использовать один из двух оставшихся типов: массивы и объекты.\nИ массивы, и объекты похожи на списки в R, разница заключается в том, именованы они или нет. Массив подобен безымянному списку и записывается через []. Например, [1, 2, 3] - это массив, содержащий 3 числа, а [null, 1, \"string\", false] - массив, содержащий ноль, число, строку и булево значение.\nОбъект подобен именованному списку и записывается через {}. Имена (ключи в терминологии JSON) являются строками, поэтому должны быть заключены в кавычки. Например, {“x”: 1, “y”: 2} - это объект, который сопоставляет x с 1, а y – с 2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON & XML</span>"
    ]
  },
  {
    "objectID": "import.html#json",
    "href": "import.html#json",
    "title": "5  Импорт: JSON & XML",
    "section": "",
    "text": "cамый простой тип - null, который играет ту же роль, что и NA в R. Он представляет собой отсутствие данных;\ncтрока (string) похожа на строку в R, но в ней всегда должны использоваться двойные кавычки;\nчисло аналогично числам в R, при этом поддерживается целочисленная (например, 123), десятичная (например, 123.45) или научная (например, 1,23e3) нотация. JSON не поддерживает Inf, -Inf или NaN;\nлогическое значение аналогично TRUE и FALSE в R, но использует строчные буквы true и false.\n\n\n\n\n\n5.1.1 Пакет jsonlite\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\n\nlibrary(jsonlite)\n\npath &lt;- \"../files/TBBT.json\"\ntbbt &lt;- read_json(path)\n\nФункция read_json() вернула нам список со следующими элементами:\n\nsummary(tbbt)\n\n                          Length Class  Mode     \nname                        1    -none- character\nseason_count                1    -none- character\nepisodes_count_total        1    -none- character\nepisodes_count_per_season  12    -none- list     \ncasting                    11    -none- list     \nepisode_list              280    -none- list     \nreferences                  1    -none- list     \n\n\n\n\n5.1.2 От списка к таблице\nВыборочно преобразуем список в тиббл. Функция transpose() берет список списков и выворачивает его наизнанку: вместо списка, в котором для каждого из персонажей указан актер и первое появление, мы получаем три списка: с персонажами, актерами и эпизодами. На месте отсутствующих значений ставится NULL.\n\nlibrary(tidyverse)\n\ncast_tbl &lt;- tbbt$casting |&gt; \n  transpose() |&gt; \n  map(as.character) |&gt; \n  as_tibble()\n\ncast_tbl\n\n\n  \n\n\n\nПроделаем то же самое для списка эпизодов, но другим способом. Функция pluck() представляет собой аналог [[, который можно использовать в пайпе. Она позволяет эффективно индексировать многоуровневые списки. Поскольку списков много, мы используем ее в сочетании с map_chr().\n\nepisodes_tbl &lt;- tibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\"))\n\nepisodes_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСамостоятельно создайте тиббл, в котором будет храниться количество серий для каждого сезона.\n\n\nЕще один способ описан здесь.\n\n\n5.1.3 Датасет: Шедевры Пушкинского музея\nJSON – популярный формат для публикации открытых данных. В таком виде часто публикуют данные органы государственной власти, культурные и некоммерческие организации и др. Например, Пушкинский музей.\nВзглянем на датасет “Шедевры из коллекции музея”. JSON можно прочитать напрямую из Сети.\n\ndoc &lt;- read_json(\"https://pushkinmuseum.art/json/masterpieces.json\")\n\nДатасет содержит информацию о 97 единицах хранения.\n\nnames(doc)\n\n [1] \"3687\"  \"3675\"  \"3706\"  \"3708\"  \"3713\"  \"3716\"  \"4005\"  \"4011\"  \"4014\" \n[10] \"4023\"  \"4030\"  \"4131\"  \"4147\"  \"4149\"  \"4161\"  \"4163\"  \"4178\"  \"4180\" \n[19] \"4191\"  \"4193\"  \"4198\"  \"4209\"  \"4244\"  \"4255\"  \"4260\"  \"4262\"  \"4266\" \n[28] \"4291\"  \"4325\"  \"4338\"  \"4350\"  \"4421\"  \"4450\"  \"4518\"  \"4543\"  \"4641\" \n[37] \"4711\"  \"4724\"  \"4767\"  \"7563\"  \"4782\"  \"4783\"  \"4788\"  \"4844\"  \"4906\" \n[46] \"4932\"  \"4936\"  \"4941\"  \"4949\"  \"4950\"  \"5238\"  \"5239\"  \"5297\"  \"5347\" \n[55] \"5591\"  \"5798\"  \"5910\"  \"5913\"  \"5992\"  \"6187\"  \"6226\"  \"6564\"  \"6584\" \n[64] \"6586\"  \"6629\"  \"6632\"  \"6886\"  \"7034\"  \"7151\"  \"7457\"  \"7468\"  \"7564\" \n[73] \"7565\"  \"7566\"  \"7567\"  \"7568\"  \"7569\"  \"7570\"  \"9464\"  \"9415\"  \"9046\" \n[82] \"10253\" \"10284\" \"10266\" \"10277\" \"10282\" \"10278\" \"10279\" \"10280\" \"10281\"\n[91] \"10285\" \"10286\" \"10287\" \"10288\" \"10289\" \"10290\" \"10291\"\n\n\nДля каждого предмета дано подробное описание.\n\nsummary(doc[[1]])\n\n                   Length Class  Mode     \npath               1      -none- character\nm_parent_id        1      -none- character\nyear               1      -none- numeric  \nget_year           1      -none- character\ninv_num            1      -none- character\ntype               2      -none- list     \ncountry            2      -none- list     \nperiod             2      -none- list     \npaint_school       1      -none- character\ngraphics_type      1      -none- character\ndepartment         1      -none- character\nmasterpiece        1      -none- character\nshow_in_hall       1      -none- character\nshow_in_collection 1      -none- numeric  \nname               2      -none- list     \nnamecom            2      -none- list     \nsize               2      -none- list     \ntext               2      -none- list     \nannotation         2      -none- list     \nlitra              2      -none- list     \nrestor             2      -none- list     \naudioguide         2      -none- list     \nvideoguide         2      -none- list     \nlink               2      -none- list     \nlinktext           2      -none- list     \nproducein          2      -none- list     \nmaterial           2      -none- list     \nfrom               2      -none- list     \nmatvos             2      -none- list     \nsizevos            2      -none- list     \nprodcast           2      -none- list     \nsearcha            2      -none- list     \nseakeys            2      -none- list     \nhall               1      -none- character\nbuilding           1      -none- character\ngallery            1      -none- list     \nauthors            1      -none- character\ncollectors         1      -none- list     \ncast               1      -none- character\nshop               1      -none- character\n\n\nЗаберем только то, что нам интересно.\n\nmasterpieces &lt;- tibble(\n  name = map_chr(doc, pluck, \"name\", \"ru\"),\n  get_year = map_chr(doc, pluck, \"get_year\"),\n  year = map_int(doc, pluck, \"year\"),\n  period = map_chr(doc, pluck, \"period\", \"name\", \"ru\"),\n  country = map_chr(doc, pluck, \"country\", \"ru\"),\n  gallery = paste0(\"https://pushkinmuseum.art\", map_chr(doc, pluck, \"gallery\", 1, 1)))\n\nБиблиотека imager позволяет работать с изображениями из датасета. Вот так мы могли бы забрать одно из них.\n\nlibrary(imager)\nimg &lt;- load.image(masterpieces$gallery[1]) |&gt; \n  plot()\nimg\n\n\nВ пакете imager есть функция map_il(), которая похожа на свою родню из purrr, но возвращает список изображений.\n\nimg_gallery &lt;- map_il(masterpieces$gallery, ~load.image(.x))\n\nФункция walk() из пакета purrr – это аналог map() для тех случаев, когда нас интересует только вывод, т.е.не надо ничего сохранять в окружение.\n\npar(mfrow = c(10, 10), mar = rep(0,4))\nwalk(img_gallery, plot, axes = FALSE)\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПопробуйте самостоятельно узнать, когда приобретена большая часть шедевров и из каких регионов они происходят.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON & XML</span>"
    ]
  },
  {
    "objectID": "import.html#xml",
    "href": "import.html#xml",
    "title": "5  Импорт: JSON & XML",
    "section": "5.2 XML",
    "text": "5.2 XML\nXML (от англ. eXtensible Markup Language) — расширяемый язык разметки. Слово “расширяемый” означает, что список тегов не зафиксирован раз и навсегда: пользователи могут вводить свои собственные теги и создавать так называемые настраиваемые языки разметки (Холзнер 2004, 29). Один из таких настраиваемых языков – это TEI (Text Encoding Initiative), о котором будет сказано дальше.\nНазначение языков разметки заключается в описании структурированных документов. Структура документа представляется в виде набора вложенных в друг друга элементов (дерева XML). У элементов есть открывающие и закрывающие теги.\nВсе составляющие части документа обобщаются в пролог и корневой элемент. Корневой элемент — обязательная часть документа, в которую вложены все остальные элементы. Пролог может включать объявления, инструкции обработки, комментарии.\nВ правильно сформированном XML открывающий и закрывающий тег вложенного элемента всегда находятся внутри одного родительского элемента.\nСоздадим простой XML из строки. Сначала идет инструкция по обработке XML (со знаком вопроса), за ней следует объявление типа документа (с восклицательным знаком) и открывающий тег корневого элемента. В этот корневой элемент вложены все остальные элементы.\n\nstring_xml &lt;- '&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!DOCTYPE recipe&gt;\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n   &lt;title&gt;\n      Простой хлеб\n   &lt;/title&gt;\n   &lt;composition&gt;\n      &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n      &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n      &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n   &lt;/composition&gt;\n   &lt;instructions&gt;\n     &lt;step&gt;\n        Смешать все ингредиенты и тщательно замесить. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Закрыть тканью и оставить на один час в тёплом помещении. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Замесить ещё раз, положить на противень и поставить в духовку.\n     &lt;/step&gt;\n   &lt;/instructions&gt;\n&lt;/recipe&gt;'\n\n\n5.2.1 Библиотека XML\nДля работы с xml понадобится установить одноименную библиотеку. Функция xmlTreeParse() создаст R-структуру, представляющую дерево XML.\n\nlibrary(XML)\ndoc &lt;- xmlTreeParse(string_xml)\nclass(doc)\n\n[1] \"XMLDocument\"         \"XMLAbstractDocument\"\n\n\nФункция xmlRoot() позволяет извлечь корневой элемент вместе со всеми детьми.\n\nrootnode &lt;- xmlRoot(doc)\nrootnode\n\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n &lt;title&gt;Простой хлеб&lt;/title&gt;\n &lt;composition&gt;\n  &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n  &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n  &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n &lt;/composition&gt;\n &lt;instructions&gt;\n  &lt;step&gt;Смешать все ингредиенты и тщательно замесить.&lt;/step&gt;\n  &lt;step&gt;Закрыть тканью и оставить на один час в тёплом помещении.&lt;/step&gt;\n  &lt;step&gt;Замесить ещё раз, положить на противень и поставить в духовку.&lt;/step&gt;\n &lt;/instructions&gt;\n&lt;/recipe&gt;\n\n\nЕсли документ большой, бывает удобнее не распечатывать все дерево, а вывести имена дочерних элементов.\n\nnames(xmlChildren(rootnode))\n\n[1] \"title\"        \"composition\"  \"instructions\"\n\n\nРазмер узла – это число вложенных в него “детей”. Его можно узнать, применив к узлу функцию xmlSize() – или посчитав число “детей”.\n\nxmlSize(rootnode) == length(xmlChildren(rootnode))\n\n[1] TRUE\n\n\n\n\n5.2.2 Выбор элементов\nРаботать с xml можно как с обычным списком, то есть индексировать узлы по имени или по номеру элемента при помощи квадратных скобок. Так мы достаем узел по имени:\n\nrootnode[[\"composition\"]]\n\n&lt;composition&gt;\n &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n&lt;/composition&gt;\n\n\nА так – по индексу:\n\nrootnode[[2]]\n\n&lt;composition&gt;\n &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n&lt;/composition&gt;\n\n\nКак и с обычными списками, мы можем использовать последовательности квадратных скобок:\n\ningr_node &lt;- rootnode[[2]][[\"ingredient\"]]\ningr_node\n\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n\n\n\n5.2.3 Значения узлов и атрибутов\nНо обычно нам нужен не элемент как таковой, а его содержание (значение). Чтобы добраться до него, используем функцию xmlValue():\n\nxmlValue(ingr_node)\n\n[1] \"Мука\"\n\n\nМожно уточнить атрибуты узла при помощи xmlAttrs():\n\nxmlAttrs(ingr_node)\n\n  amount     unit \n     \"3\" \"стакан\" \n\n\nЧтобы извлечь значение атрибута, используем функцию xmlGetAttr(). Первым аргументом функции передаем xml-узел, вторым – имя атрибута.\n\nxmlGetAttr(ingr_node, \"unit\")\n\n[1] \"стакан\"\n\n\n\n\n5.2.4 Обход дерева узлов\nКак насчет того, чтобы применить функцию к набору узлов – например, ко всем инредиентам? Вспоминаем функции для работы со списками – sapply() из базового R или map() из пакета purrr:\n\ningr_nodes &lt;- xmlChildren(rootnode[[2]])\n\nsapply(ingr_nodes, xmlValue)\n\n   ingredient    ingredient    ingredient \n       \"Мука\"      \"Дрожжи\" \"Тёплая вода\" \n\n\n\nsapply(ingr_nodes, xmlGetAttr, \"unit\")\n\ningredient ingredient ingredient \n  \"стакан\"    \"грамм\"   \"стакан\" \n\n\n\n\n5.2.5 Синтаксис XPath\nДобраться до узлов определенного уровня можно также при помощи синтаксиса XPath. XPath – это язык запросов к элементам XML-документа. С его помощью можно описать “путь” до нужного узла: абсолютный (начиная с корневого элемента) или относительный. В пакете XML синтаксис XPath поддерживает функция getNodeSet().\n\n# абсолютный путь\ningr_nodes &lt;- getNodeSet(rootnode, \"/recipe//composition//ingredient\")\n\ningr_nodes\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n\n[[3]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n# относительный путь\ningr_nodes &lt;- getNodeSet(rootnode, \"//composition//ingredient\")\n\ningr_nodes\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n\n[[3]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВ большинстве случаев функция getNodeSet() требует задать пространство имен (namespace), но в нашем случае оно не определено, поэтому пока передаем только дерево и путь до узла. С пространством имен встретимся чуть позже!\n\n\n\nСинтаксис XPath позволяет отбирать узлы с определенными атрибутами. Допустим, нам нужны только те узлы, где значение атрибута unit = “стакан”:\n\ngetNodeSet(rootnode, \"//composition//ingredient[@unit='стакан']\")\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n\n5.2.6 От дерева к таблице\nПри работе с xml в большинстве случаев наша задача – извлечь значения определеннных узлов или их атрибутов и сохранить их в прямоугольном формате. Один из способов выглядит так.\n\ntibble(title = xmlValue(rootnode[[\"title\"]]), \n       ingredients = map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlValue),\n       unit = map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlGetAttr, \"unit\"),\n       amount = map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlGetAttr, \"amount\"))\n\n\n  \n\n\n\n\n\n5.2.7 Разметка TEI\nБольшая часть размеченных литературных корпусов хранится именно в формате XML. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п.\nДобавлять и удалять разметку может любой пользователь в редакторе XML кода или даже в простом текстовом редакторе. При этом в качестве универсального языка разметки в гуманитарных дисциплинах используется язык TEI (Скоринкин 2016). Корневой элемент в документах TEI называется TEI, внутри него располагается элемент teiHeader с метаинформацией о документе и элемент text. Последний содержит текст документа с элементами, определяющими его структурное членение.\n&lt;TEI&gt;\n  &lt;teiHeader&gt;&lt;/teiHeader&gt;\n  &lt;text&gt;&lt;/text&gt;\n&lt;/TEI&gt;\nПример оформления документа можно посмотреть по ссылке.\nУ teiHeader есть четыре главных дочерних элемента:\n\nfileDesc (описание документа c библиографической информацией)\nencodingDesc (описание способа кодирование первоисточника)\nprofileDesc (“досье” на текст, например отправитель и получатель для писем, жанр, используемые языки, обстоятельства создания, место написания и т.п.)\nrevisionDesc (история изменений документа).\n\nВ самом тексте язык TEI дает возможность представлять разные варианты (авторские, редакторские, корректорские и др.) Основным средством параллельного представления является элемент choice. Например, в тексте Лукреция вы можете увидеть такое:\nsic calor atque &lt;choice&gt;&lt;reg&gt;aer&lt;/reg&gt;&lt;orig&gt;aër&lt;/orig&gt;&lt;/choice&gt; et venti caeca potestas\nЗдесь reg указывает на нормализованное написание, а orig – на оригинальное.\n\n\n5.2.8 Датасет: “Война и мир”\nВ качестве примера загрузим датасет “Пушкинского дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”.\n\nfilename = \"../files/War_and_Peace.xml\"\ndoc &lt;- xmlTreeParse(filename, useInternalNodes = T)\nrootnode &lt;- xmlRoot(doc)\n\nТеперь можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена:\n\nnames(xmlChildren(rootnode)) \n\n[1] \"teiHeader\" \"text\"     \n\n\nОчевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее:\n\nnames(xmlChildren(rootnode[[\"text\"]])) \n\n[1] \"div\" \"div\" \"div\" \"div\" \"div\"\n\n\nИтак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это четыре тома и эпилог.\n\n# это список\ndivs &lt;-  rootnode[[\"text\"]][\"div\"]\n\nsapply(divs, xmlGetAttr, \"type\")\n\n       div        div        div        div        div \n  \"volume\"   \"volume\"   \"volume\"   \"volume\" \"epilogue\" \n\n\nКак мы уже знаем, добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – при помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на два уровня ниже: там тоже будет тег div, но с другим атрибутом. Как легко убедиться, теперь это главы, всего их 358.\n\ndivs &lt;- getNodeSet(doc, \"/tei:TEI//tei:text//tei:div//tei:div//tei:div\",\n                     namespaces = c(tei = \"http://www.tei-c.org/ns/1.0\")) \n\nlength(divs)\n\n[1] 358\n\nunique(sapply(divs, xmlGetAttr, \"type\"))\n\n[1] \"chapter\"\n\n\nОбратите внимание, что в данном случае надо прямо прописать пространство имен (namespaces). Это можно посмотреть в самом xml, а можно воспользоваться специальной функцией:\n\nxmlNamespace(rootnode)\n\n[1] \"http://www.tei-c.org/ns/1.0\"\nattr(,\"class\")\n[1] \"XMLNamespace\"\n\n\nЗабрать конкретную главу можно путем индексации, но лучше – по значению соответствующего атрибута.\n\nidx &lt;- which(map(divs, xmlGetAttr, \"xml:id\") == \"chapter1part1Volume1\")\nch1 &lt;- divs[[idx]]\n\nЧтобы извлечь текст, понадобится функция xmlValue.\n\nchapter_1 &lt;- xmlValue(ch1)\n\nРаспечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на параграфы и выведем первый и последний:\n\nlibrary(stringr)\nchapter_lines &lt;- str_split(chapter_1, pattern = \"\\n\")\n\nchapter_lines[[1]][[5]]\n\n[1] \"        — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j'y crois) — je ne vous connais plus, vous n'êtes plus mon ami, vous n'êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\"\n\nchapter_lines[[1]][[838]]\n\n[1] \"       Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.\"\n\n\nПервая и последняя реплика по-французски: все правильно!\n\n\n\n\n\n\nЗадание\n\n\n\nСкачайте по ссылке “Горе от ума” Грибоедова и преобразуйте xml в прямоугольный формат таким образом, чтобы для каждой реплики был указан акт, сцена и действующее лицо.\n\n\nПодбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan и Lang 2014) и (Холзнер 2004).\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON & XML</span>"
    ]
  },
  {
    "objectID": "import.html#tei",
    "href": "import.html#tei",
    "title": "5  Импорт",
    "section": "5.3 TEI",
    "text": "5.3 TEI\nБольшая часть размеченных литературных корпусов хранится именно в формате XML. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п.\nДобавлять и удалять разметку может любой пользователь в редакторе XML кода или даже в простом текстовом редакторе. При этом в качестве универсального языка разметки в гуманитарных дисциплинах используется язык TEI (Скоринкин 2016). Корневой элемент в документах TEI называется TEI, внутри него располагается элемент teiHeader с метаинформацией о документе и элемент text. Последний содержит текст документа с элементами, определяющими его структурное членение.\n&lt;TEI&gt;\n  &lt;teiHeader&gt;&lt;/teiHeader&gt;\n  &lt;text&gt;&lt;/text&gt;\n&lt;/TEI&gt;\nПример оформления документа можно посмотреть по ссылке.\nУ teiHeader есть четыре главных дочерних элемента:\n\nfileDesc (описание документа c библиографической информацией)\nencodingDesc (описание способа кодирование первоисточника)\nprofileDesc (“досье” на текст, например отправитель и получатель для писем, жанр, используемые языки, обстоятельства создания, место написания и т.п.)\nrevisionDesc (история изменений документа).\n\nВ самом тексте язык TEI дает возможность представлять разные варианты (авторские, редакторские, корректорские и др.) Основным средством параллельного представления является элемент choice. Например, в тексте Лукреция вы можете увидеть такое:\nsic calor atque &lt;choice&gt;&lt;reg&gt;aer&lt;/reg&gt;&lt;orig&gt;aër&lt;/orig&gt;&lt;/choice&gt; et venti caeca potestas\nЗдесь reg указывает на нормализованное написание, а orig – на оригинальное.\nВ качестве примера загрузим датасет “Пушкинского дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”.\n\nfilename = \"../files/War_and_Peace.xml\"\ndoc &lt;- xmlTreeParse(filename, useInternalNodes = T)\nrootnode &lt;- xmlRoot(doc)\n\nТеперь можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена:\n\nnames(xmlChildren(rootnode)) \n\n[1] \"teiHeader\" \"text\"     \n\n\nОчевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее:\n\nnames(xmlChildren(rootnode[[\"text\"]])) \n\n[1] \"div\" \"div\" \"div\" \"div\" \"div\"\n\n\nИтак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это четыре тома и эпилог.\n\n# это список\ndivs &lt;-  rootnode[[\"text\"]][\"div\"]\n\nsapply(divs, xmlGetAttr, \"type\")\n\n       div        div        div        div        div \n  \"volume\"   \"volume\"   \"volume\"   \"volume\" \"epilogue\" \n\n\nКак мы уже знаем, добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – при помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на два уровня ниже: там тоже будет тег div, но с другим атрибутом. Как легко убедиться, теперь это главы, всего их 358.\n\ndivs &lt;- getNodeSet(doc, \"/tei:TEI//tei:text//tei:div//tei:div//tei:div\",\n                     namespaces = c(tei = \"http://www.tei-c.org/ns/1.0\")) \n\nlength(divs)\n\n[1] 358\n\nunique(sapply(divs, xmlGetAttr, \"type\"))\n\n[1] \"chapter\"\n\n\nОбратите внимание, что в данном случае надо прямо прописать пространство имен (namespaces). Это можно посмотреть в самом xml, а можно воспользоваться специальной функцией:\n\nxmlNamespace(rootnode)\n\n[1] \"http://www.tei-c.org/ns/1.0\"\nattr(,\"class\")\n[1] \"XMLNamespace\"\n\n\nЗабрать конкретную главу можно путем индексации, но лучше – по значению соответствующего атрибута.\n\nidx &lt;- which(map(divs, xmlGetAttr, \"xml:id\") == \"chapter1part1Volume1\")\nch1 &lt;- divs[[idx]]\n\nЧтобы извлечь текст, понадобится функция xmlValue.\n\nchapter_1 &lt;- xmlValue(ch1)\n\nРаспечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на параграфы и выведем первый и последний:\n\nlibrary(stringr)\nchapter_lines &lt;- str_split(chapter_1, pattern = \"\\n\")\n\nchapter_lines[[1]][[5]]\n\n[1] \"        — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j'y crois) — je ne vous connais plus, vous n'êtes plus mon ami, vous n'êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\"\n\nchapter_lines[[1]][[838]]\n\n[1] \"       Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.\"\n\n\nПервая и последняя реплика по-французски: все правильно!\n\n\n\n\n\n\nЗадание\n\n\n\nСкачайте по ссылке “Горе от ума” Грибоедова и преобразуйте xml в прямоугольный формат таким образом, чтобы для каждой реплики был указан акт, сцена и действующее лицо.\n\n\nПодбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan и Lang 2014) и (Холзнер 2004).\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#бонус-gutenbergr",
    "href": "import.html#бонус-gutenbergr",
    "title": "5  Импорт",
    "section": "5.4 Бонус: GutenbergR",
    "text": "5.4 Бонус: GutenbergR\nПакет GutenbergR поможет достать тексты из библиотеки Gutenberg, но будьте осторожны: распознаны они не всегда хорошо и порой содержат много разного шума, например примечания редактора, номера страниц и т.п. В билингвах источник и перевод могут идти вперемешку. И если в XML подобные элементы будут окружены соответствующими тегами, которые позволят их легко отбросить при анализе, то Gutenberg дает вам сырой текст. Часто его надо хорошенько чистить при помощи регулярных выражений или даже вручную.\nРаботать с метаданными GutenbergR вы уже научились, теперь можете пользоваться пакетом и для скачивания текстов. Сначала узнаем id нужных текстов^ [https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html]\n\nlibrary(gutenbergr)\n\ncaesar &lt;- gutenberg_works(author == \"Caesar, Julius\", languages = \"la\") \n\ncaesar \n\n\n  \n\n\n\nЧтобы извлечь отдельный текст (тексты):\n\nde_bello_gallico &lt;- gutenberg_download(218, meta_fields = \"title\", mirror = \"ftp://mirrors.xmission.com/gutenberg/\")\nde_bello_gallico\n\n\n  \n\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nСуществует несколько зеркал библиотеки Gutenberg, и, если при выполнении функции gutenberg_download() возникает ошибка “could not download a book at http://aleph.gutenberg.org/”, то следует использовать аргумент mirror. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL\n\n\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "share.html",
    "href": "share.html",
    "title": "7  Публикационная система Quarto",
    "section": "",
    "text": "7.1 О воспроизводимости\nПолученный в результате количественных исследований результат должен быть проверяем и воспроизводим. Это значит, что в большинстве случаев недостаточно просто рассказать, что вы проделали. Теоретически читатель должен иметь возможность проделать тот же путь, что и автор: воcпроизвести его результаты, но в обратном направлении.\nДля этого должны выполняться три основных требования:\nУже на этапе планирования исследования очень важно продумать, как вы будете его документировать. Важно помнить, что код пишется не только для машин, но и для людей, поэтому стоит документировать не только то, что вы делали, но и почему. R дает для этого множество возможностей, главная из которых – это Markdown.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#о-воспроизводимости",
    "href": "share.html#о-воспроизводимости",
    "title": "7  Публикационная система Quarto",
    "section": "",
    "text": "На заметку\n\n\n\nВоспроизводимость (reproducibility) – это не то же, что повторяемость (replicability). Ученый, который повторяет исследование, проводит его заново на новых данных. Воспроизведение – гораздо более скромная задача, не требующая таких ресурсов, как повторение (Winter 2020, 47).\n\n\n\n\nдоступность данных и метаданных;\nдоступность компьютерного кода;\nдоступность программного обеспечения.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#markdown",
    "href": "share.html#markdown",
    "title": "7  Публикационная система Quarto",
    "section": "7.2 Markdown",
    "text": "7.2 Markdown\nMarkdown – это облегчённый язык разметки. Он позволяет создавать документы разного формата – не только HTML (веб-страницы), но и PDF и Word. Markdown дает возможность создания полностью воспроизводимых документов, сочетающих код и поясняющий текст. Этот язык используется для создания сайтов, статей, книг, презентаций, отчетов, дашбордов и т.п. Этот курс написан с использованием Markdown.\nЧтобы начать работать с документами .rmd, нужен пакет rmarkdown; в RStudio он уже предустановлен. Создание нового документа .rmd происходит из меню.\nПо умолчанию документ .rmd снабжен шапкой yaml. Она не обязательна. Здесь содержатся данные об авторе, времени создания, формате, сведения о файле с библиографией и т.п.\n---\ntitle: \"Demo\"\nauthor: \"My name\"\ndate: \"2025-11-21\"\noutput: html_document\n---\nТакже в документе .rmd скорее всего будет простой текст и блоки кода. Чтобы “сшить” html (pdf, doc), достаточно нажать кнопку knit либо запустить в консоли код: rmarkdown::render(\"Demo.Rmd\"). После этого в рабочей директории появится новый файл (html, pdf, или doc), которым можно поделиться с коллегами, грантодателями или друзьями.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#quarto",
    "href": "share.html#quarto",
    "title": "7  Публикационная система Quarto",
    "section": "7.3 Quarto",
    "text": "7.3 Quarto\nРаботать с маркдауном мы будем, используя издательскую систему Quarto с открытым исходным кодом. Она позволяет создавать и публиковать статьи, презентации, информационные панели, веб-сайты, блоги и книги в HTML, PDF, MS Word, ePub и других форматах. В общем, обычный Markdown тоже позволяет все это делать, но чуть сложнее. Quarto объединяет различные пакеты из экосистемы R Markdown воедино и значительно упрощает работу с ними. Подробнее см. практическое руководство “Quarto: The Definitive Guide”.\n\n\n\n\n\n\nЗадание\n\n\n\nСоздайте новый .qmd документ. Потренируйтесь запускать код и сшивать документ в .html, .pdf, .docx.\n\n\nДля .pdf может понадобиться установка LaTeX.\n\n# install.packages(\"tinytex\")\ntinytex::install_tinytex()\n# to uninstall TinyTeX, run\n# tinytex::uninstall_tinytex()\n\nМожно указать сразу несколько форматов для файла, как показано здесь, и “сшить” их одновременно:\n\nquarto::quarto_render(\n  \"untitled.qmd\", \n  output_format = c(\"pdf\", \"html\", \"docx\")\n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#шапка-yaml",
    "href": "share.html#шапка-yaml",
    "title": "7  Публикационная система Quarto",
    "section": "7.4 Шапка YAML",
    "text": "7.4 Шапка YAML\nОсновные параметры документа хранятся в YAML-шапке. К ним относятся format, title, subtitle, date, date-format, author, abstract, lang, toc, number-sections и другие.\nПопробуйте изменить шапку своего .qmd-документа и заново его сшить. Сравните с предыдущей версией.\n---\ntitle: \"Заголовок\"\nsubtitle: \"Подзаголовок\"\nformat: html\nauthor: locusclassicus\ndate: today\ndate-format: D.MM.YYYY\nabstract: Значенье бублика нам непонятно.\nlang: ru\ntoc: true\nnumber-sections: true\n---\n\nПоле execute позволяет задать параметры всех фрагментов кода в документе, например:\n---\nexecute:\n  echo: false\n  fig-width: 9\n---\n  \nНо для отдельных кусков кода эти настройки можно поменять:\n```\n#| echo: true\n\nsqrt(16)\n```\nПараметр df-print позволяет выбрать один из возможных способов отображения датафреймов:\n\ndefault — стандартный, как в консоли;\ntibble — стандартный, как в консоли, но в формате tibble;\nkable — минималистичный вариант, подходит для всех видов документов;\npaged — интерактивная таблица, подходит для html страниц.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#синтаксис-markdown",
    "href": "share.html#синтаксис-markdown",
    "title": "7  Публикационная система Quarto",
    "section": "7.5 Синтаксис Markdown",
    "text": "7.5 Синтаксис Markdown\n\n7.5.1 Заголовки\nЗаголовки разного уровня задаются при помощи решетки:\n# Заголовок первого уровня\n## Заголовок второго уровня\n### Заголовок третьего уровня\n#### Заголовок четвёртого уровня\nПример заголовка третьего уровня:\n\n\n7.5.2 Форматирование\n*курсив*  \n_курсив_\n\n**полужирный**  \n__полужирный__\n\n***полужирный курсив***  \n___полужирный курсив___\n\n~~зачеркнутый~~\n\n&lt;mark&gt;выделение&lt;/mark&gt;\nПример:\nкурсив\nполужирный\nуж и не знаю как выделить\nзачеркнутый\nвыделение\n\n\n7.5.3 Списки\nНумерованный список\n1. Пункт первый\n2. Пункт второй\n3. Пункт третий\nПример:\n\nПункт первый\nПункт второй\nПункт третий\n\nМаркированный список\n- Пункт первый\n- Пункт второй\n- Пункт третий\nПример:\n\nПункт первый\nПункт второй\nПункт третий\n\nТакже Markdown позволяет делать вложенные списки:\n1. Пункт первый\n    - Подпункт первый\n    - Подпункт второй\n2. Пункт второй\nПример:\n\nПункт первый\n\nПодпункт первый\nПодпункт второй\n\nПункт второй\n\nСамое удобное, что элементы списка не обязательно нумеровать:\n(@) Пункт первый.\n(@) Пункт не знаю какой.\n\nПункт первый.\nПункт не знаю какой.\n\n\n\n7.5.4 Ссылки\n[Текст ссылки](http://antibarbari.ru/)\nПример:\nТекст ссылки\n\n\n7.5.5 Изображения\n![Текст описания](https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Holbein-erasmus.jpg/548px-Holbein-erasmus.jpg)\nПример:\n\n\n\nМоя картинка\n\n\nИзображения можно вставлять, пользуясь непосредственно разметкой html.\n&lt;img src=\"images/my_image.jpg\" width=40%&gt;\n\n\n7.5.6 Блоки кода\nМожно вставлять непосредственно в текст; для этого код выделяют одинарным обратным апострофом (грависом). Но чаще код дают отдельным блоком. Эти блоки можно именовать; тогда в случае ошибки будет сразу понятно, где она случилась.\n```{}\nsome code here\n```\nВ фигурных скобках надо указать язык, например {r}, только в этом случае код будет подсвечиваться и выполняться.\nТам же в фигурных скобках можно задать следующие параметры:\n\neval = FALSE код будет показан, но не будет выполняться;\ninclude = FALSE код будет выполнен, но ни код, ни результат не будут показаны;\necho = FALSE код будет выполнен, но не показан, результаты при этом видны;\nmessage = FALSE или warning = FALSE прячет сообщения или предупреждения;\nresults = 'hide' не распечатывает результат, а fig.show = 'hide' прячет графики;\nerror = TRUE “сшивание” продолжается, даже если этот блок вернул ошибку.\n\n\n\n7.5.7 Цитаты\n&gt; Omnia praeclara rara.\nПример:\n\nOmnia praeclara rara.\n\nЦитата с подписью может быть оформлена так:\n&gt; Omnia praeclara rara.\n&gt;\n&gt; --- Cicero\nПример:\n\nOmnia praeclara rara.\n— Cicero\n\n\n\n7.5.8 Разделители\nЧтобы создать горизонтальную линию, можно использовать ---, *** или ___.\nПример:\n\n\n\n7.5.9 Таблицы\nТаблицы можно задать вручную при помощи дефисов - и вертикальных линий |; идеальная точность при этом не нужна. Перед таблицей обязательно оставляйте пустую строку, иначе волшебство не сработает.\n\n| Фрукты   | Калории  |\n| -----  | ---- |\n| Яблоко   | 52  |\n| Апельсин | 47  |\nПример:\n\n\n\nФрукты\nКалории\n\n\n\n\nЯблоко\n52\n\n\nАпельсин\n47\n\n\n\nПо умолчанию Markdown распечатывает таблицы так, как они бы выглядели в консоли.\n\ndata(\"iris\")\nhead(iris)\n\n\n  \n\n\n\nДля дополнительного форматирования можно использовать функцию knitr::kable():\n\nknitr::kable(iris[1:6, ], caption = \"Таблица knitr\")\n\n\nТаблица knitr\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\nИнтерактивную таблицу можно создать так:\n\nDT::datatable(iris[1:6,])\n\n\n\n\n\n\n\n7.5.10 Чек-листы\n- [x] Таблицы\n- [ ] Графики\nПример:\n\nТаблицы\nГрафики\n\n\n\n7.5.11 Внутренние ссылки\nУдобны для навигации по документу. К названию любого раздела можно добавить {#id}.\n[Вернуться к чек-листам](#id)\nПример:\nВернуться к чек-листам\n\n\n7.5.12 Графики\nMarkdown позволяет встраивать любые графики.\n\nlibrary(ggplot2)\nggplot(aes(x = Sepal.Length, y = Petal.Length, col = Species), data = iris) +\n  geom_point(show.legend = F)\n\n\n\n\n\n\n\n\nДля интерактивных графиков понадобится пакет plotly:\n\nlibrary(plotly)\nplot_ly(data=iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species)\n\n\n\n\n\nПодробное руководство по созданию интерактивных графиков можно найти на сайте https://plotly.com/r/.\n\n\n7.5.13 Математические формулы\nПишутся с использованием синтаксиса LaTeX, о котором можно прочитать подробнее здесь.\nФормулы заключаются в одинарный $, если пишутся в строку, и в двойной $$, если отдельным блоком.\n\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\nВот так это выглядит в тексте: \\(\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\).\nА вот так – блоком:\n\\[\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\]\n\n\n7.5.14 Смайлы\nУдобнее вставлять через визуальный редактор (“шестеренка” &gt; Use Visual Editor), но можно и без него:\n\n# devtools::install_github(\"hadley/emo\")\nlibrary(emo)\nemo::ji(\"apple\")\n\n🍎 \n\n\nКод можно записать в строку, тогда смайл появится в тексте: 💀.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#библиография",
    "href": "share.html#библиография",
    "title": "7  Публикационная система Quarto",
    "section": "7.7 Библиография",
    "text": "7.7 Библиография\nMarkdown позволяет добавлять библиографию в формате BibTeX. BibTeX — программное обеспечение для создания форматированных списков библиографии; обычно используется совместно с LaTeX’ом. Многие сайты, например GoogleScholar, позволяют экспортировать библиографические записи в формате BibTeX. При необходимости запись можно исправить вручную.\nКаждая запись имеет следующую форму.\n@book{winter2020,\n  author = {Bodo Winter},\n  title = \"{Statistics for Linguists: An Introduction Using R}\",\n  year = {2020},\n  publisher = {Routledge}\n}\nЗдесь book — тип записи («книга»), winter2020 — метка-идентификатор записи, дальше список полей со значениями.\nОдна запись описывает ровно одну публикацию статью, книгу, диссертацию, и т. д. Подробнее о типах записей можно посмотреть вот здесь.\nПодобные записи хранятся в текстовом файле с расширением .bib. Чтобы привязать библиографию, нужно указать имя файла в шапке yaml.\n---\nbibliography: bibliography.bib\n---\nДальше, чтобы добавить ссылку, достаточно ввести ключ публикации после @ (в квадратных скобках, чтобы публикация отражалась в круглых): [@wickham2016].\nПример:\n(Wickham и Grolemund 2016).\nМожно интегрировать BibTex с Zotero или другим менеджером библиографии. Для этого придется установить специальное расширение.\nЧтобы изменить стиль цитирования, необходимо добавить в шапку yaml название csl-файла (CSL - Citation Style Language), например:\n---\noutput: html_document\nbibliography: references.bib\ncsl: archiv-fur-geschichte-der-philosophie.csl\n---\nНайти необходимый csl-файл можно, например, в репозитории стилей Zotero.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#публикация-html",
    "href": "share.html#публикация-html",
    "title": "7  Публикационная система Quarto",
    "section": "7.8 Публикация html",
    "text": "7.8 Публикация html\nДля публикации на RPubs понадобится установить пакеты packrat, rsconnect.\nПри публикации страницы на https://rpubs.com/ следует добавить в шапку две строчки:\n\n---\nembed-resources: true\nstandalone: true\n---\n\nЭто позволит корректно отобразить локальные фото, графики и сохранит оформление.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "8  Регулярные выражения",
    "section": "",
    "text": "8.1 str_c(), str_trunc(), str_squish()\nЕсть старая шутка, ее приписывают программисту Джейми Завински: если у вас есть проблема, и вы собираетесь ее решать при помощи регулярных выражений, то у вас две проблемы. Регулярные выражения – это формальный язык, который используется для того, чтобы находить, извлекать и заменять части текста.\nРегулярные выражения (regex, regexp) объединяют литералы (буквальные символы) и метасимволы (специальные символы, задающие правила поиска). Для поиска используется строка-шаблон (англ. pattern), которая определяет правило сопоставления и поиска нужного фрагмента текста.\nДля работы нам понадобится пакет {stringr} из библиотеки tidyverse. Также установите библиотеку {tokenizers}.\nЗагрузим текст романа “Гордость и предубеждение”.\nДля объединения строк используется функция str_c():\nnames &lt;- c(\"Jane\", \"Elizabeth\")\n\nstr_c(names, \"Bennet\", sep = \" \")\n\n[1] \"Jane Bennet\"      \"Elizabeth Bennet\"\n\nstr_c(names, collapse = \" and \")\n\n[1] \"Jane and Elizabeth\"\nОбъединим текст романа в единый вектор и воспользуемся функцией str_trunc(), чтобы распечатать самое начало.\npp &lt;- str_c(pp, collapse = \" \")\nstr_trunc(pp, 120)\n\n[1] \"PRIDE AND PREJUDICE  By Jane Austen    Chapter 1   It is a truth universally acknowledged, that a single man in posse...\"\nИзбавимся от лишних пробелов.\npp &lt;- str_squish(pp)\nstr_trunc(pp, 120)\n\n[1] \"PRIDE AND PREJUDICE By Jane Austen Chapter 1 It is a truth universally acknowledged, that a single man in possession ...\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#литералы-и-классы",
    "href": "regex.html#литералы-и-классы",
    "title": "7  Регулярные выражения",
    "section": "7.2 Литералы и классы",
    "text": "7.2 Литералы и классы\nБуквальные символы – это то, что вы ожидаете увидеть (или не увидеть – для управляющих и пробельных символов); можно сказать, что это символы, которые ничего не “имеют в виду”. Их можно объединять в классы при помощи квадратных скобок, например, так: [abc].\n\nvec &lt;- c(\"a\", \"d\", \"c\")\ngrepl(\"[abc]\", vec)\n\n[1]  TRUE FALSE  TRUE\n\ngrep(\"[abc]\", vec)\n\n[1] 1 3\n\n\nДля некоторых классов есть специальные обозначения.\n\n\n\n\n\n\n\n\nКласс\nЭквивалент\nЗначение\n\n\n\n\n[:upper:]\n[A-Z]\nСимволы верхнего регистра\n\n\n[:lower:]\n[a-z]\nСимволы нижнего регистра\n\n\n[:alpha:]\n[[:upper:][:lower:]]\nБуквы\n\n\n[:digit:]\n[0-9], т. е. \\d\nЦифры\n\n\n[:alnum:]\n[[:alpha:][:digit:]]\nБуквы и цифры\n\n\n[:word:]\n[[:alnum:]_], т. е. \\w\nСимволы, образующие «слово»\n\n\n[:punct:]\n[-!“#$%&’()*+,./:;&lt;=&gt;?@[\\]_`{|}~]\nЗнаки пунктуации\n\n\n[:blank:]\n[\\s\\t]\nПробел и табуляция\n\n\n[:space:]\n[[:blank:]\\v\\r\\n\\f], т. е. \\s\nПробельные символы\n\n\n[:cntrl:]\n\nУправляющие символы (перевод строки, табуляция и т.п.)\n\n\n[:graph:]\n\nПечатные символы\n\n\n[:print:]\n\nПечатные символы с пробелом\n\n\n\nЭти классы тоже можно задавать в качестве паттерна.\n\nvec &lt;- c(\"жираф\", \"верблюд1\", \"0зебра\")\ngsub( \"[[:digit:]]\",  \"\", vec)\n\n[1] \"жираф\"   \"верблюд\" \"зебра\"  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nВ пакете stringr есть небольшой датасет words. Найдите все слова с последовательностью символов wh. Сколько слов содержат два гласных после w?\n\n\n\nВ качестве классов можно рассматривать и следующие обозначения:\n\n\n\n\n\n\n\n\nПредставление\nЭквивалент\nЗначение\n\n\n\n\n\\d\n[0-9]\nЦифра\n\n\n\\D\n[^\\\\d]\nЛюбой символ, кроме цифры\n\n\n\\w\n[A-Za-zА-Яа-я0-9_]\nСимволы, образующие «слово» (буквы, цифры и символ подчёркивания)\n\n\n\\W\n[^\\\\w]\nСимволы, не образующие «слово»\n\n\n\\s\n[ \\t\\v\\r\\n\\f]\nПробельный символ\n\n\n\\S\n[^\\\\s]\nНепробельный символ\n\n\n\n\ngsub( \"\\\\d\",  \"\", vec) # вторая косая черта \"экранирует\" первую\n\n[1] \"жираф\"   \"верблюд\" \"зебра\"  \n\n\nВнутри квадратных скобор знак ^ означает отрицание:\n\ngsub( \"[^[:digit:]]\",  \"\", vec) \n\n[1] \"\"  \"1\" \"0\"\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, в которых за w следует согласный. Замените всю пунктуацию в строке “tomorrow?and-tomorrow_and!tomorrow” на пробелы.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#якоря",
    "href": "regex.html#якоря",
    "title": "7  Регулярные выражения",
    "section": "7.3 Якоря",
    "text": "7.3 Якоря\nЯкоря позволяют искать последовательности символов в начале или в конце строки. Знак ^ (вне квадратных скобок!) означает начало строки, а знак $ – конец. Мнемоническое правило: First you get the power (^) and then you get the money ($).\n\nvec &lt;- c(\"The spring is a lovely time\", \n         \"Fall is a time of peace\")\ngrepl(\"time$\", vec)\n\n[1]  TRUE FALSE\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, которые заканчиваются на x. Найдите все слова, которые начинаются на b или на g.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#метасимволы",
    "href": "regex.html#метасимволы",
    "title": "7  Регулярные выражения",
    "section": "7.4 Метасимволы",
    "text": "7.4 Метасимволы\nВсе метасимволы представлены в таблице ниже.\n\n\n\nОписание\nСимвол\n\n\n\n\nоткрывающая квадратная скобка\n[\n\n\nзакрывающая квадратная скобка\n]\n\n\nобратная косая черта\n\\\n\n\nкарет\n^\n\n\nзнак доллара\n$\n\n\nточка\n.\n\n\nвертикальная черта\n|\n\n\nзнак вопроса\n?\n\n\nастериск\n*\n\n\nплюс\n+\n\n\nоткрывающая фигурная скобка\n{\n\n\nзакрывающая фигурная скобка\n}\n\n\nоткрывающая круглая скобка\n(\n\n\nзакрывающая круглая скобка\n)\n\n\n\nКвадратные скобки используются для создания классов, карет и знак доллара – это якоря, но карет внутри квадратных скобок может также быть отрицанием. Точка – это любой знак.\n\nvec &lt;- c(\"жираф\", \"верблюд1\", \"0зебра\")\ngrep(\".б\", vec) \n\n[1] 2 3\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, в которых есть любые два символа между b и k.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#экранирование",
    "href": "regex.html#экранирование",
    "title": "7  Регулярные выражения",
    "section": "7.5 Экранирование",
    "text": "7.5 Экранирование\nЕсли необходимо найти буквальную точку, буквальный знак вопроса и т.п., то используется экранирование: перед знаком ставится косая черта. Но так как сама косая черта – это метасимвол, но нужно две косые черты, первая из которых экранирует вторую.\n\nvec &lt;- c(\"жираф?\", \"верблюд.\", \"зебра\")\ngrep(\"\\\\?\", vec) \n\n[1] 1\n\ngrepl(\"\\\\.\", vec)\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУзнайте, все ли предложения в sentences (входит в stringr) кончаются на точку.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#квантификация",
    "href": "regex.html#квантификация",
    "title": "7  Регулярные выражения",
    "section": "7.6 Квантификация",
    "text": "7.6 Квантификация\nКвантификатор после символа, символьного класса или группы определяет, сколько раз предшествующее выражение может встречаться. Квантификатор может относиться более чем к одному символу в регулярном выражении, только если это символьный класс или группа.\n\n\n\nПредставление\nЧисло повторений\nЭквивалент\n\n\n\n\n?\nНоль или одно\n{0,1}\n\n\n*\nНоль или более\n{0,}\n\n\n+\nОдно или более\n{1,}\n\n\n\nПример:\n\nvec &lt;- c(\"color\", \"colour\", \"colouur\")\ngrepl(\"ou?r\", vec) # ноль или одно \n\n[1]  TRUE  TRUE FALSE\n\ngrepl(\"ou+r\", vec) # одно или больше\n\n[1] FALSE  TRUE  TRUE\n\ngrepl(\"ou*r\", vec) # ноль или больше\n\n[1] TRUE TRUE TRUE\n\n\nТочное число повторений (интервал) можно задать в фигурных скобках:\n\n\n\nПредставление\nЧисло повторений\n\n\n\n\n{n}\nРовно n раз\n\n\n{m,n}\nОт m до n включительно\n\n\n{m,}\nНе менее m\n\n\n{,n}\nНе более n\n\n\n\n\nvec &lt;- c(\"color\", \"colour\", \"colouur\", \"colouuuur\")\ngrepl(\"ou{1}r\", vec)\n\n[1] FALSE  TRUE FALSE FALSE\n\ngrepl(\"ou{1,2}r\", vec)\n\n[1] FALSE  TRUE  TRUE FALSE\n\ngrepl(\"ou{,2}r\", vec) # это включает и ноль!\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nЧасто используется последовательность .* для обозначения любого количества любых символов между двумя частями регулярного выражения.\n\n\n\n\n\n\nЗадание\n\n\n\nУзнайте, в каких предложениях в sentences за пробелом следует ровно три согласных.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#жадная-и-ленивая-квантификация",
    "href": "regex.html#жадная-и-ленивая-квантификация",
    "title": "7  Регулярные выражения",
    "section": "7.7 Жадная и ленивая квантификация",
    "text": "7.7 Жадная и ленивая квантификация\nВ регулярных выражениях квантификаторам соответствует максимально длинная строка из возможных (квантификаторы являются жадными, англ. greedy). Это может оказаться значительной проблемой. Например, часто ожидают, что выражение &lt;.*&gt; найдёт в тексте теги HTML. Однако если в тексте есть более одного HTML-тега, то этому выражению соответствует целиком строка, содержащая множество тегов.\n\nvec &lt;- c(\"&lt;p&gt;&lt;b&gt;Википедия&lt;/b&gt; — свободная энциклопедия, в которой &lt;i&gt;каждый&lt;/i&gt; может изменить или дополнить любую статью.&lt;/p&gt;\")\ngsub(\"&lt;.*&gt;\", \"\", vec) # все исчезло!\n\n[1] \"\"\n\n\nЧтобы этого избежать, надо поставить после квантификатора знак вопроса. Это сделает его ленивым.\n\n\n\nregex\nзначение\n\n\n\n\n??\n0 или 1, лучше 0\n\n\n*?\n0 или больше, как можно меньше\n\n\n+?\n1 или больше, как можно меньше\n\n\n{n,m}?\nот n до m, как можно меньше\n\n\n\nПример:\n\ngsub(\"&lt;.*?&gt;\", \"\", vec) # все получилось!\n\n[1] \"Википедия — свободная энциклопедия, в которой каждый может изменить или дополнить любую статью.\"\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nДана строка “tomorrow (and) tomorrow (and) tomorrow”. Необходимо удалить первые скобки с их содержанием.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#regex-в-stringr-основы",
    "href": "regex.html#regex-в-stringr-основы",
    "title": "7  Регулярные выражения",
    "section": "7.8 Regex в stringr: основы",
    "text": "7.8 Regex в stringr: основы\nПакет stringr является частью tidyverse1:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nЭто очень удобный инструмент для работы со строками. Вот так можно узнать длину строки или объединить ее с другими строками:\n\nvec &lt;- c(\"жираф\", \"верблюд\")\nstr_length(vec)\n\n[1] 5 7\n\nstr_c(\"красивый_\", vec)\n\n[1] \"красивый_жираф\"   \"красивый_верблюд\"\n\n\nЭлементы вектора можно объединить в одну строку:\n\nstr_c(vec, collapse = \", \") # теперь у них общие кавычки\n\n[1] \"жираф, верблюд\"\n\n\nС помощью str_sub() и str_sub_all() можно выбрать часть строки2.\n\nvec &lt;- c(\"жираф\", \"верблюд\")\nstr_sub(vec, 1, 3)\n\n[1] \"жир\" \"вер\"\n\nstr_sub(vec, 1, -2)\n\n[1] \"жира\"   \"верблю\"\n\n\nФункции ниже меняют начертание с прописного на строчное или наоборот:\n\nVEC &lt;- str_to_upper(vec)\nVEC\n\n[1] \"ЖИРАФ\"   \"ВЕРБЛЮД\"\n\nstr_to_lower(VEC)\n\n[1] \"жираф\"   \"верблюд\"\n\nstr_to_title(vec)\n\n[1] \"Жираф\"   \"Верблюд\"\n\n\nОдна из полезнейших функций в этом пакете – str_view(); она помогает увидеть, что поймало регулярное выражение – до того, как вы внесете какие-то изменения в строку.\n\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n\n[2] │ &lt;a.c&gt;\n\n\nНапример, с помощью этой функции можно убедиться, что вертикальная черта выступает как логический оператор “или”:\n\nstr_view(c(\"grey\", \"gray\"), \"gr(e|a)y\")\n\n[1] │ &lt;grey&gt;\n[2] │ &lt;gray&gt;\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСоздайте тиббл с двумя столбцами: letters и numbers (1:26). Преобразуйте, чтобы в третьем столбце появился результат соединения первых двух через подчеркивание, например a_1. Отфильтруйте, чтобы остались только ряды, где есть цифра 3 или буква x.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_detect-и-str_count",
    "href": "regex.html#str_detect-и-str_count",
    "title": "7  Регулярные выражения",
    "section": "7.9 str_detect() и str_count()",
    "text": "7.9 str_detect() и str_count()\nАналогом grepl() в stringr является функция str_detect()\n\nlibrary(rcorpora)\ndata(\"fruit\")\nhead(fruit)\n\n[1] \"apple\"       \"apricot\"     \"avocado\"     \"banana\"      \"bell pepper\"\n[6] \"bilberry\"   \n\nstr_detect(head(fruit), \"[aeiou]$\")\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n\n# какая доля слов заканчивается на гласный?\nmean(str_detect(fruit, \"[aeiou]$\"))\n\n[1] 0.35\n\n# сколько всего слов заканчивается на гласный?\nsum(str_detect(fruit, \"[aeiou]$\"))\n\n[1] 28\n\n\nОтрицание можно задать двумя способами:\n\ndata(\"words\")\n\nno_vowels1 &lt;- !str_detect(words, \"[aeiou]\") # слова без гласных\n\nno_vowels2 &lt;- str_detect(words, \"^[^aeiou]+$\") # слова без гласных\n\nsum(no_vowels1 != no_vowels2)\n\n[1] 0\n\n\nЛогический вектор можно использовать для индексирования:\n\nwords[!str_detect(words, \"[aeiou]\")]\n\n[1] \"by\"  \"dry\" \"fly\" \"mrs\" \"try\" \"why\"\n\n\nЭту функцию можно применять вместе с функцией filter() из пакета dplyr:\n\nlibrary(dplyr)\ngods &lt;- corpora(which = \"mythology/greek_gods\")\n\ndf &lt;- tibble(god = as.character(gods$greek_gods), \n             i = seq_along(god)\n             )\n\ndf |&gt; \n  filter(str_detect(god, \"s$\"))\n\n\n  \n\n\n\nВариацией этой функции является str_count():\n\nstr_count(as.character(gods$greek_gods), \"[Aa]\")\n\n [1] 1 1 1 1 2 0 0 1 1 1 0 1 0 0 1 2 1 0 0 0 0 1 2 1 0 2 3 2 1 0 0\n\n\nЭту функцию удобно использовать вместе с mutate() из dplyr:\n\ndf |&gt; \n  mutate(\n    vowels = str_count(god, \"[AEIOYaeiou]\"),\n    consonants = str_count(god, \"[^AEIOYaeiou]\")\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПреобразуйте sentences из пакета stringr в тиббл; в новом столбце сохраните количество пробелов в каждом предложении.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_extract-str_subset-и-str_match",
    "href": "regex.html#str_extract-str_subset-и-str_match",
    "title": "7  Регулярные выражения",
    "section": "7.10 str_extract(), str_subset() и str_match()",
    "text": "7.10 str_extract(), str_subset() и str_match()\nФункция str_extract() извлекает совпадения3.\nСначала зададим паттерн для поиска.\n\ncolours &lt;- c(\" red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\")\ncolour_match &lt;- str_c(colours, collapse = \"|\")\ncolour_match\n\n[1] \" red|orange|yellow|green|blue|purple\"\n\n\nИ применим к предложениями. Используем str_extract_all(), т.к. str_extract() возвращает только первое вхождение.\n\nhas_colour &lt;- str_subset(sentences, colour_match)\nmatches &lt;- str_extract_all(has_colour, colour_match)\nhead(unlist(matches))\n\n[1] \"blue\"   \"blue\"   \"blue\"   \"yellow\" \"green\"  \" red\"  \n\n\nКруглые скобки используются для группировки. Например, мы можем задать шаблон для поиска существительного или прилагательного с артиклем.\n\nnoun &lt;- \"(a|the) ([^ ]+)\" # как минимум один непробельный символ после пробела\n\nhas_noun &lt;- sentences |&gt;\n  str_subset(noun) |&gt;\n  head(10)\nhas_noun\n\n [1] \"The birch canoe slid on the smooth planks.\"       \n [2] \"Glue the sheet to the dark blue background.\"      \n [3] \"It's easy to tell the depth of a well.\"           \n [4] \"These days a chicken leg is a rare dish.\"         \n [5] \"The box was thrown beside the parked truck.\"      \n [6] \"The boy was there when the sun rose.\"             \n [7] \"The source of the huge river is the clear spring.\"\n [8] \"Kick the ball straight and follow through.\"       \n [9] \"Help the woman get back to her feet.\"             \n[10] \"A pot of tea helps to pass the evening.\"          \n\n\nДальше можно воспользоваться уже известной функцией str_extract() или применить str_match. Результат будет немного отличаться: вторая функция вернет матрицу, в которой хранится не только сочетание слов, но и каждый компонент отдельно.\n\nhas_noun |&gt; \n  str_extract(noun)\n\n [1] \"the smooth\" \"the sheet\"  \"the depth\"  \"a chicken\"  \"the parked\"\n [6] \"the sun\"    \"the huge\"   \"the ball\"   \"the woman\"  \"a helps\"   \n\nhas_noun |&gt; \n  str_match(noun)\n\n      [,1]         [,2]  [,3]     \n [1,] \"the smooth\" \"the\" \"smooth\" \n [2,] \"the sheet\"  \"the\" \"sheet\"  \n [3,] \"the depth\"  \"the\" \"depth\"  \n [4,] \"a chicken\"  \"a\"   \"chicken\"\n [5,] \"the parked\" \"the\" \"parked\" \n [6,] \"the sun\"    \"the\" \"sun\"    \n [7,] \"the huge\"   \"the\" \"huge\"   \n [8,] \"the ball\"   \"the\" \"ball\"   \n [9,] \"the woman\"  \"the\" \"woman\"  \n[10,] \"a helps\"    \"a\"   \"helps\"  \n\n\nФункция tidyr::extract() работает похожим образом, но требует дать имена для каждого элемента группы. Этим удобно пользоваться, если ваши данные хранятся в виде тиббла.\n\ntibble(sentence = sentences) |&gt; \n  tidyr::extract(\n    sentence, c(\"article\", \"noun\"), \"(a|the) ([^ ]+)\", \n    remove = FALSE\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите в sentences все предложения, где есть to, и выберите следующее за этим слово. Переведите в нижний регистр. Узнайте, сколько всего уникальных сочетаний.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_replace",
    "href": "regex.html#str_replace",
    "title": "7  Регулярные выражения",
    "section": "7.11 str_replace",
    "text": "7.11 str_replace\nФункции str_replace() и str_replace_all() позволяют заменять совпадения на новые символы.\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_replace(x, \"[aeiou]\", \"-\")\n\n[1] \"-pple\"  \"p-ar\"   \"b-nana\"\n\nstr_replace_all(x, \"[aeiou]\", \"-\")\n\n[1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\n\nЭтим можно воспользоваться, если вы хотите, например, удалить из текста все греческие символы. Для стандартного греческого алфавита хватит [Α-Ωα-ω], но для древнегреческого этого, например, не хватит. Попробуем на отрывке из письма Цицерона Аттику, которое содержит греческий текст.\n\ncicero &lt;- \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀπεραντολογίας ἀηδοῦς! \"\n\nstr_replace_all(cicero, \"[Α-Ωα-ω]\", \"\")\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀί ἀῦ! \"\n\n\nὢ ἀί ἀῦ! Не все у нас получилось гладко. Попробуем иначе:\n\nstr_replace_all(cicero, \"[\\u0370-\\u03FF]\", \"\")\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀ ἀῦ! \"\n\n\nУдалилась (буквально была заменена на пустое место) та диакритика, которая есть в новогреческом (ί). Но остались еще буквы со сложной диакритикой, которой современные греки не пользуются.\n\nno_greek &lt;- str_replace_all(cicero, \"[[\\u0370-\\u03FF][\\U1F00-\\U1FFF]]\", \"\")\nno_greek\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius.   ! \"\n\n\n! Мы молодцы. Избавились от этого непонятного греческого.\nНа самом деле, конечно, str_replace хорош тем, что он позволяет производить осмысленные замены. Например, мы можем в оставшемся латинском текст заменить гласные с макроном (черточка, означающая долготу) на обычные гласные.\n\nstr_replace_all(no_greek, c(\"ā\" = \"a\", \"ū\" = \"u\", \"ī\" = \"i\", \"ō\" = \"o\"))\n\n[1] \"nihil hac solitudine iucundius, nisi paulum interpellasset Amyntae filius.   ! \"\n\n\nКрасота. О более сложных заменах с перемещением групп можно посмотреть видео здесь и здесь. Это помогает даже в таком скорбном деле, как переоформление библиографии.\n\n\n\n\n\n\nЗадание\n\n\n\nДана библиографическая запись:\nAst, Friedrich. 1816. Platon’s Leben und Schriften. Leipzig, Weidmann.\nИспользуя регулярные выражения, замените полное имя на инициал. Запятую перед инициалом удалите. Уберите название издательства. Год поставьте в круглые скобки.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_split",
    "href": "regex.html#str_split",
    "title": "7  Регулярные выражения",
    "section": "7.12 str_split",
    "text": "7.12 str_split\nФункция str_split() помогает разбить текст на предложения, слова или просто на бессмысленные наборы символов. Это важный этап подготовки текста для анализа, и проводится он нередко именно с применением регулярных выражений.\n\nsentences |&gt;\n  head(2) |&gt; \n  str_split(\" \")\n\n[[1]]\n[1] \"The\"     \"birch\"   \"canoe\"   \"slid\"    \"on\"      \"the\"     \"smooth\" \n[8] \"planks.\"\n\n[[2]]\n[1] \"Glue\"        \"the\"         \"sheet\"       \"to\"          \"the\"        \n[6] \"dark\"        \"blue\"        \"background.\"\n\n\nНо можно обойтись и без регулярных выражений.\n\nx &lt;- \"This is a sentence.  This is another sentence.\"\nstr_view_all(x, boundary(\"word\"))\n\n[1] │ &lt;This&gt; &lt;is&gt; &lt;a&gt; &lt;sentence&gt;.  &lt;This&gt; &lt;is&gt; &lt;another&gt; &lt;sentence&gt;.\n\nstr_view_all(x, boundary(\"sentence\"))\n\n[1] │ &lt;This is a sentence.  &gt;&lt;This is another sentence.&gt;\n\n\nОчень удобно, но убедитесь, что в вашем языке границы слов и предложения выглядят как у людей. С древнегреческим эта штука не справится (как делить на предложения греческие и латинские тексты, я рассказывала здесь):\n\napology &lt;- c(\"νῦν δ' ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ' ὅς. “Τίς,” ἦν δ' ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;\")\n\nstr_view_all(apology, boundary(\"sentence\"))\n\n[1] │ &lt;νῦν δ' ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ' ὅς. &gt;&lt;“Τίς,” ἦν δ' ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&gt;\n\n\nПолный крах 💩",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#footnotes",
    "href": "regex.html#footnotes",
    "title": "7  Регулярные выражения",
    "section": "",
    "text": "https://r4ds.had.co.nz/strings.html↩︎\nhttps://stringr.tidyverse.org/reference/str_sub.html↩︎\nhttps://r4ds.had.co.nz/strings.html#extract-matches↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "scrape.html",
    "href": "scrape.html",
    "title": "9  Веб-скрапинг",
    "section": "",
    "text": "9.1 Структура html\nФайлы html, как и XML, хранят данные в структурированном виде. Извлечь их позволяет пакет rvest. С его помощью мы добудем архив телеграм-канала Antibarbari HSE. Канал публичный, и Telegram дает возможность скачать архив в формате html при помощи кнопки export (эта функция может быть недоступна на MacOS, в этом случае стоит попробовать Telegram Lite). Данные (в формате zip) для этого урока можно забрать по ссылке.\nЭта глава опирается в основом на второе издание книги R for Data Science Хадли Викхема.\nДокументы html (HyperText Markup Language) имеют иерархическую структуру, состоящую из элементов. В каждом элементе есть открывающий тег (&lt;tag&gt;), опциональные атрибуты (id='first') и закрывающий тег (&lt;/tag&gt;). Все, что находится между открывающим и закрывающим тегом, называется содержанием элемента.\nВажнейшие теги, о которых стоит знать:\nЧтобы увидеть структуру веб-страницы, надо нажать правую кнопку мыши и выбрать View Source (это работает и для тех html, которые хранятся у вас на компьютере).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#структура-html",
    "href": "scrape.html#структура-html",
    "title": "9  Веб-скрапинг",
    "section": "",
    "text": "&lt;html&gt; (есть всегда), с двумя детьми (дочерними элементами): &lt;head&gt; и &lt;body&gt;;\nэлементы, отвечающие за структуру: &lt;h1&gt; (заголовок), &lt;section&gt;, &lt;p&gt; (параграф), &lt;ol&gt; (упорядоченный список);\nэлементы, отвечающие за оформление: &lt;b&gt; (bold), &lt;i&gt; (italics), &lt;a&gt; (ссылка).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#каскадные-таблицы-стилей",
    "href": "scrape.html#каскадные-таблицы-стилей",
    "title": "9  Веб-скрапинг",
    "section": "9.2 Каскадные таблицы стилей",
    "text": "9.2 Каскадные таблицы стилей\nУ тегов могут быть именованные атрибуты; важнейшие из них – это id и class, которые в сочетании с CSS контролируют внешний вид страницы.\n\n\n\n\n\n\nНа заметку\n\n\n\nCSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML).\n\n\nПример css-правила (такие инфобоксы использованы в предыдущей версии курса):\n\n.infobox {\n  padding: 1em 1em 1em 4em;\n  background: aliceblue 5px center/3em no-repeat;\n  color: black;\n}\n\n\nПроще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — селектор и блок объявлений. Селектор, расположенный в левой части правила до знака {, определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;».\nСелекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так:\n\np выберет все элементы &lt;p&gt;\n.title выберет элементы с классом “title”\n#title выберет все элементы с атрибутом id=‘title’\n\nВажно: если изменится структура страницы, откуда вы скрапили информацию, то и код придется переписывать.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#чтение-html",
    "href": "scrape.html#чтение-html",
    "title": "9  Веб-скрапинг",
    "section": "9.3 Чтение html",
    "text": "9.3 Чтение html\nЧтобы прочесть файл html, используем одноименную функцию (и пакет purrr, чтобы прочитать сразу три файла из архива).\n\nantibarbari_files &lt;- list.files(\"../files/antibarbari_2024-08-18\", pattern = \"html\", full.names = TRUE)\nantibarbari_archive &lt;- map(antibarbari_files, read_html)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#парсинг-html-отдельные-элементы",
    "href": "scrape.html#парсинг-html-отдельные-элементы",
    "title": "9  Веб-скрапинг",
    "section": "9.4 Парсинг html: отдельные элементы",
    "text": "9.4 Парсинг html: отдельные элементы\nНа следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельный html; rvest позволяет это сделать (но внутри двойных кавычек должны быть только одинарные):\n\nexample_html &lt;-  minimal_html(\"\n&lt;div class='message default clearfix' id='message83'&gt;\n      &lt;div class='pull_left userpic_wrap'&gt;\n       &lt;div class='userpic userpic2' style='width: 42px; height: 42px'&gt;\n        &lt;div class='initials' style='line-height: 42px'&gt;\nA\n        &lt;/div&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class='body'&gt;\n       &lt;div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'&gt;\n11:18\n       &lt;/div&gt;\n       &lt;div class='from_name'&gt;\nAntibarbari HSE \n       &lt;/div&gt;\n       &lt;div class='text'&gt;\nЭтот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. &lt;br&gt;&lt;br&gt;«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).&lt;br&gt;&lt;a href='http://antibarbari.ru/2022/05/19/digest_1/'&gt;http://antibarbari.ru/2022/05/19/digest_1/&lt;/a&gt;\n       &lt;/div&gt;\n       &lt;div class='signature details'&gt;\nOlga Alieva\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n\")\n\nИз всего этого мне может быть интересно id сообщения (\\&lt;div class='message default clearfix' id='message83'\\&gt;), текст сообщения (\\&lt;div class='text'\\&gt;), дата публикации (\\&lt;div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'\\&gt;), а также, если указан, автор сообщения (\\&lt;div class='signature details'\\&gt;). Извлекаем текст (для этого рекомендуется использовать функцию html_text2()):\n\nexample_html |&gt;\n  html_element(\".text\") |&gt; \n  html_text2()\n\n[1] \"Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора.\\n\\n«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).\\nhttp://antibarbari.ru/2022/05/19/digest_1/\"\n\n\nВ классе signature details есть пробел, достаточно на его месте поставить точку:\n\nexample_html |&gt;\n  html_element(\".signature.details\") |&gt; \n  html_text2()\n\n[1] \"Olga Alieva\"\n\n\nОсталось добыть дату и message id:\n\nexample_html |&gt; \n  html_element(\".pull_right.date.details\") |&gt; \n  html_attr(\"title\")\n\n[1] \"19.05.2022 11:18:07 UTC+03:00\"\n\n\n\nexample_html |&gt;\n  html_element(\".message.default.clearfix\") |&gt;\n  html_attr(\"id\")\n\n[1] \"message83\"\n\n\nТеперь мы можем сохранить все нужные нам данные в таблицу.\n\ntibble(id = example_html |&gt; \n         html_element(\".message.default.clearfix\") |&gt; \n         html_attr(\"id\"),\n       date = example_html |&gt; \n         html_element(\".pull_right.date.details\") |&gt; \n         html_attr(\"title\"),\n       signature = example_html |&gt;\n         html_element(\".signature.details\") |&gt; \n         html_text2(),\n       text = example_html |&gt; \n         html_element(\".text\") |&gt;\n         html_text2()\n)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#парсинг-html-вложенные-элементы",
    "href": "scrape.html#парсинг-html-вложенные-элементы",
    "title": "9  Веб-скрапинг",
    "section": "9.5 Парсинг html: вложенные элементы",
    "text": "9.5 Парсинг html: вложенные элементы\nДо сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, и нам надо извлечь только такие div, которым соответствует определенный класс. Также не будем забывать, что архив выгрузился в виде трех html-файлов, так что понадобится наше знание итераций в purrr. Пока пробуем на одном из них:\n\narchive_1 &lt;- antibarbari_archive[[1]]\n\narchive_1 |&gt;\n  html_elements(\"div.message.default\") |&gt; \n  head()\n\n{xml_nodeset (6)}\n[1] &lt;div class=\"message default clearfix\" id=\"message3\"&gt;\\n\\n      &lt;div class= ...\n[2] &lt;div class=\"message default clearfix\" id=\"message5\"&gt;\\n\\n      &lt;div class= ...\n[3] &lt;div class=\"message default clearfix\" id=\"message6\"&gt;\\n\\n      &lt;div class= ...\n[4] &lt;div class=\"message default clearfix\" id=\"message7\"&gt;\\n\\n      &lt;div class= ...\n[5] &lt;div class=\"message default clearfix\" id=\"message8\"&gt;\\n\\n      &lt;div class= ...\n[6] &lt;div class=\"message default clearfix\" id=\"message9\"&gt;\\n\\n      &lt;div class= ...\n\n\nУже из этого набора узлов можем доставать все остальное.\n\narchive_1_tbl &lt;- tibble(\n  id = archive_1 |&gt;\n    html_elements(\"div.message.default\") |&gt;\n    html_attr(\"id\"),\n  \n  date = archive_1 |&gt;\n    html_elements(\"div.message.default\") |&gt;\n    html_element(\".pull_right.date.details\") |&gt;\n    html_attr(\"title\"),\n  \n  signature = archive_1 |&gt;\n    html_elements(\"div.message.default\") |&gt;\n    html_element(\".signature.details\") |&gt;\n    html_text2(),\n  \n  text = archive_1 |&gt;\n    html_elements(\"div.message.default\") |&gt;\n    html_element(\".text\") |&gt;\n    html_text2()\n)\n\narchive_1_tbl\n\n\n  \n\n\n\nОбратите внимание, что мы сначала извлекаем нужные элементы при помощи html_elements(), а потом применяем к каждому из них html_element(). Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений.\nКак вы уже поняли, теперь нам надо проделать то же самое для двух других файлов из архива антиварваров, а значит пришло время превратить наш код в функцию.\n\nscrape_antibarbari &lt;- function(html_file){\n  messages_tbl &lt;- tibble(\n    id = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_attr(\"id\"),\n    date = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".pull_right.date.details\") |&gt;\n      html_attr(\"title\"),\n    signature = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".signature.details\") |&gt;\n      html_text2(),\n    text = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".text\") |&gt;\n      html_text2()\n  )\n  messages_tbl\n}\n\n\nmessages_tbl &lt;- map_df(antibarbari_archive, scrape_antibarbari)\n\nВот что у нас получилось.\n\nmessages_tbl",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#разведывательный-анализ",
    "href": "scrape.html#разведывательный-анализ",
    "title": "9  Веб-скрапинг",
    "section": "9.6 Разведывательный анализ",
    "text": "9.6 Разведывательный анализ\nСоздатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. Кроме того, в некоторых постах только фото, для них в столбце text – NA, их можно сразу отсеять.\n\nmessages_tbl &lt;- messages_tbl |&gt;\n  filter(!is.na(text))\n\nmessages_tbl\n\n\n  \n\n\n\nТакже преобразуем столбец, в котором хранится дата и время. Разделим его на два и выясним, в какое время и день недели чаще всего публикуются сообщения.\n\n\n\n\n\n\nЗадание\n\n\n\nИз курса Getting and Cleaning Data в swirl будет полезно пройти урок Dates and Times with lubridate.\n\n\n\nmessages_tbl2 &lt;- messages_tbl |&gt; \n  separate(date, into = c(\"date\", \"time\", NA), sep = \" \") |&gt; \n  mutate(date = dmy(date), \n         time = hms(time)) |&gt; \n  mutate(year = year(date), \n        month = month(date, label = TRUE),\n        wday = wday(date, label = TRUE),\n        hour = hour(time),\n        length = str_count(text, \" \") + 1) |&gt; \n  mutate(wday = factor(wday, levels = c(\"Sun\", \"Sat\", \"Fri\", \"Thu\", \"Wed\", \"Tue\", \"Mon\")))\n\n\nmessages_tbl2\n\n\n  \n\n\n\n\nsummary1 &lt;- messages_tbl2 |&gt; \n  group_by(year, month) |&gt; \n  summarise(n = n()) \n\nsummary1\n\n\n  \n\n\nsummary2 &lt;- messages_tbl2 |&gt; \n  group_by(year, hour) |&gt; \n  summarise(n = n()) |&gt; \n  mutate(hour = case_when(hour == 0 ~ 24,\n                          .default = hour))\n\nsummary2\n\n\n  \n\n\nsummary3 &lt;- messages_tbl2 |&gt; \n   group_by(wday) |&gt; \n   summarise(n = n())\n\nsummary3\n\n\n  \n\n\n\n\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"khroma::okabeitoblack\")\n\np1 &lt;- summary1 |&gt; \n  ggplot(aes(month, n, color = as.factor(year), group = year)) +\n  geom_line(show.legend = FALSE, linewidth = 1.2, alpha = 0.8) +\n  labs(title = \"Число постов в месяц\") +\n  theme(legend.title = element_blank(), \n        legend.position = c(0.8, 0.3),\n        title = element_text(face=\"italic\")) +\n  labs(x = NULL, y = NULL) +\n  scale_color_manual(values = cols[1:3])\n\n\np2 &lt;- summary2 |&gt; \n  ggplot(aes(hour, n, color = as.factor(year), group = year)) + \n  geom_line(linewidth = 1.2, alpha = 0.8) +\n  scale_x_continuous(breaks = seq(1,24,1)) +\n  labs(x = NULL, y = NULL, title = \"Время публикации поста\") + \n  theme(legend.title = element_blank(), \n        legend.position = \"left\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        title = element_text(face=\"italic\")\n        ) +\n  coord_polar(start = 0) +\n  scale_color_manual(values = cols[1:3])\n\n\np3 &lt;- summary3 |&gt; \n  ggplot(aes(wday, n, fill = wday)) + \n  geom_bar(stat = \"identity\", \n           show.legend = FALSE) + \n  coord_flip() + \n  labs(x = NULL, y = NULL, title  = \"Публикации по дням недели\") +\n  scale_fill_manual(values = cols) +\n  theme(title = element_text(face=\"italic\"))\n\n\np4 &lt;- messages_tbl2 |&gt; \n  ggplot(aes(as.factor(year), length, fill = as.factor(year))) +\n  geom_boxplot(show.legend = FALSE) +\n  labs(title = \"Длина поста по годам\") + \n  labs(x = NULL, y = NULL) + \n  scale_fill_manual(values = cols[1:3]) + \n  theme(title = element_text(face=\"italic\"))\n\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2,\n             top =  textGrob(\"Телеграм-канал Antibarbari HSE\",\n                    gp=gpar(fontsize=16)),\n             bottom = textGrob(\"@Rantiquity\",\n                    gp = gpar(fontface = 3, fontsize = 9), hjust = 1, x = 1))",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#html-таблицы",
    "href": "scrape.html#html-таблицы",
    "title": "9  Веб-скрапинг",
    "section": "9.7 Html таблицы",
    "text": "9.7 Html таблицы\nЕсли вам повезет, то ваши данные уже будут храниться в HTML-таблице, и их можно будет просто считать из этой таблицы. Распознать таблицу в браузере обычно несложно: она имеет прямоугольную структуру из строк и столбцов, и ее можно скопировать и вставить в такой инструмент, как Excel.\nТаблицы HTML строятся из четырех основных элементов: &lt;table&gt;, &lt;tr&gt; (строка таблицы), &lt;th&gt; (заголовок таблицы) и &lt;td&gt; (данные таблицы). Мы достанем программу курса “Количественные методы в гуманитарных науках: критическое введение” (2023/2024).\n\nhtml &lt;- read_html(\"http://criticaldh.ru/program/\")\nmy_table &lt;- html |&gt;  \n  html_table() |&gt; \n  pluck(1)\n\nmy_table\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nС сайта Новой философской энциклопедии извлеките список слов на букву П. Используйте map_df() для объединения таблиц.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько всего слов на букву П в НФЭ?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#selector-gadget",
    "href": "scrape.html#selector-gadget",
    "title": "9  Веб-скрапинг",
    "section": "9.8 Selector Gadget",
    "text": "9.8 Selector Gadget\nМногие тексты доступны на сайте &lt;wikisource.org&gt;. Попробуем извлечь латинский текст “Записок о Галльской войне” Цезаря: он пригодится нам в следующем уроке.\n\nurl &lt;- \"https://la.wikisource.org/wiki/Commentarii_de_bello_Gallico\"\nhtml = read_html(url)\n\nДля того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот тут можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни.\n\ntoc &lt;- html |&gt; \n  html_elements(\"#mw-content-text li\") \n\ntoc\n\n{xml_nodeset (8)}\n[1] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_I\" title=\"Commentar ...\n[2] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_II\" title=\"Commenta ...\n[3] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_III\" title=\"Comment ...\n[4] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_IV\" title=\"Commenta ...\n[5] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_V\" title=\"Commentar ...\n[6] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VI\" title=\"Commenta ...\n[7] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VII\" title=\"Comment ...\n[8] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VIII\" title=\"Commen ...\n\n\nИзвлекаем путь и имя файла для web-страниц.\n\nlibri &lt;- tibble(\n  title = toc |&gt; \n    html_text2(),\n  href = toc |&gt; \n    html_element(\"a\") |&gt; \n    html_attr(\"href\")\n) \n\nlibri\n\n\n  \n\n\n\nТеперь добавляем протокол доступа и доменное имя для каждой страницы.\n\nlibri &lt;- libri |&gt; \n  mutate(link = str_c(\"https://la.wikisource.org\", href)) |&gt; \n  select(-href)\n\nlibri\n\n\n  \n\n\n\nДальше необходимо достать текст для каждой книги. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила.\n\nurls &lt;- libri |&gt; \n  pull(link)\n\ntext &lt;- read_html(urls[1]) |&gt; \n  html_elements(\".mw-heading3+ p\") |&gt; \n  html_text2() \n\ntext[1]\n\n[1] \"Gallia est omnis dīvīsa in partēs trēs, quārum ūnam incolunt Belgae, aliam Aquītānī, tertiam quī ipsōrum linguā Celtae, nostrā Gallī appellantur. Hī omnēs linguā, īnstitūtīs, lēgibus inter sē differunt. Gallōs ab Aquītānīs Garumna flūmen, ā Belgīs Mātrona et Sēquana dīvidit. Hōrum omnium fortissimī sunt Belgae, proptereā quod ā cultū atque hūmānitāte prōvinciae longissimē absunt, minimēque ad eōs mercātōrēs saepe commeant atque ea quae ad effēminandōs animōs pertinent important, proximīque sunt Germānīs, quī trāns Rhēnum incolunt, quibuscum continenter bellum gerunt. Quā dē causā Helvētiī quoque reliquōs Gallōs virtūte praecēdunt, quod fere cotīdiānīs proeliīs cum Germānīs contendunt, cum aut suīs fīnibus eōs prohibent aut ipsī in eōrum fīnibus bellum gerunt. Eōrum ūna, pars, quam Gallōs obtinēre dictum est, initium capit ā flūmine Rhodano, continētur Garumna flūmine, Ōceanō, fīnibus Belgārum, attingit etiam ab Sēquanīs et Helvētiīs flūmen Rhēnum, vergit ad septentriōnēs. Belgae ab extrēmīs Galliae fīnibus oriuntur, pertinent ad īnferiōrem partem flūminis Rhēnī, spectant in septentriōnem et orientem sōlem. Aquītānia ā Garumnā flūmine ad Pȳrēnaeōs montēs et eam partem Ōceanī quae est ad Hispāniam pertinet; spectat inter occāsum sōlis et septentriōnēs.\"\n\n\nУбедившись, что параграфы извлечены верно, обобщаем: пишем функцию для извлечения текстов и применяем ее ко всем книгам.\n\nget_text &lt;- function(url) {\n  Sys.sleep(3)\n  res = tibble(\n    liber = str_extract(url, \"Liber_.+$\"),\n    text = read_html(url) |&gt;\n      html_elements(\".mw-heading3+ p\") |&gt;\n      html_text2())\n  return(res)\n}\n\nЭто займет некоторое время.\n\nlibri_text &lt;- map_df(urls, get_text)\n\nСоединим две таблицы.\n\ncaesar &lt;- libri_text |&gt; \n  group_by(liber) |&gt; \n  mutate(cap = row_number(), .after = liber)\n\ncaesar\n\n\n  \n\n\n\nСохраним подготовленный датасет для дальнейшего анализа.\n\nsave(caesar, file = \"../data/caesar.Rdata\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html",
    "href": "tokenize.html",
    "title": "10  Токенизация и лемматизация",
    "section": "",
    "text": "10.1 Токенизация и анализ частотностей\nОсновные этапы NLP включают в себя токенизацию, морфологический и синтаксический анализ, а также семантический анализ В этом уроке речь пойдет про первые три этапа. Мы научимся разбивать текст на токены (слова), определять морфологические характеристики слов и находить их начальные формы (леммы), а также анализировать структуру предложения с использованием синтаксических парсеров.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#токенизация",
    "href": "tokenize.html#токенизация",
    "title": "10  Лемматизация и POS-тэггинг",
    "section": "",
    "text": "unnest_tokens(\n  tbl,\n  output,\n  input,\n  token = \"words\",\n  format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"),\n  to_lower = TRUE,\n  drop = TRUE,\n  collapse = NULL,\n  ...\n)\n\n\n“words” (default),\n“characters”,\n“character_shingles”,\n“ngrams”,\n“skip_ngrams”,\n“sentences”,\n“lines”,\n“paragraphs”,\n“regex”,\n“ptb” (Penn Treebank).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лемматизация и POS-тэггинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#лемматизация-и-частеречная-разметка",
    "href": "tokenize.html#лемматизация-и-частеречная-разметка",
    "title": "10  Токенизация и лемматизация",
    "section": "10.2 Лемматизация и частеречная разметка",
    "text": "10.2 Лемматизация и частеречная разметка\nЛемматизация – приведение слов к начальной форме (лемме). Как правило, она проводится одновременно с частеречной разметкой (POS-tagging). Все это умеет делать UDPipe – обучаемый конвейер (trainable pipeline), для которого существует одноименный пакет в R.\nОсновным форматом файла для него является CoNLL-U. Файлы в таком формате хранятся в так называемых трибанках, то есть коллекциях уже размеченных текстов (название объясняется тем, что синтаксическая структура предложений представлена в них в виде древовидных графов). Файлы CoNLL-U используются для обучения нейросетей, но для большинства языков доступны хорошие предобученные модели, работать с которыми достаточно просто.\n\n10.2.1 Udpipe\nПакет udpipe позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных трибанках. Среди этих языков есть и латинский.\nПрежде всего нужно выбрать и загрузить модель (список); в нашем случае это модель Perseus, но можно попробовать и другие доступные на сайте https://universaldependencies.org/.\nАннотировать лучше не разбитый на слова текст. Чтобы не потерять информацию о происхождении текста, добавим уникальный id к каждому параграфу.\n\ncaesar_id &lt;- caesar |&gt; \n  unite(id, c(\"liber\", \"cap\"), sep = \"_\")\n\ncaesar_id\n\n\n  \n\n\n\n\n#  скачиваем модель в рабочую директорию\n#udpipe_download_model(language = \"latin-perseus\")\n\n# загружаем модель\nlatin_perseus &lt;- udpipe_load_model(file = \"latin-perseus-ud-2.5-191206.udpipe\")\n\n# аннотируем\ncaesar_annotate &lt;- udpipe_annotate(latin_perseus, caesar_id$text, doc_id = caesar_id$id)\n\nРезультат возвращается в формате CoNLL-U; это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. Вот пример разбора предложения:\n\nCтроки слов содержат следующие поля:\n\nID: индекс слова, целое число, начиная с 1 для каждого нового предложения; может быть диапазоном токенов с несколькими словами.\nFORM: словоформа или знак препинания.\nLEMMA: Лемма или основа словоформы.\nUPOSTAG: тег части речи из универсального набора проекта UD, который создавался для того, чтобы аннотации разных языков были сравнимы между собой.\nXPOSTAG: тег части речи, который выбрали исследователи под конкретные нужды языка\nFEATS: список морфологических характеристик.\nHEAD: идентификатор (номер) синтаксической вершины текущего токена. Если такой вершины нет, то ставят ноль (0).\nDEPREL: характер синтаксической зависимости.\nDEPS: Список вторичных зависимостей.\nMISC: любая другая аннотация.\n\nДля работы данные удобнее трансформировать в прямоугольный формат.\n\ncaesar_pos &lt;- caesar_annotate |&gt; \n  as_tibble() |&gt; \n  select(-paragraph_id)\n\ncaesar_pos \n\n\n\n\n  \n\n\n\n\n\n10.2.2 Обучение модели\nМожно заметить, что модель Perseus 2.5 справилась не безупречно: все бельги оказались женского рода, а кельты и вовсе признаны глаголом. Есть ошибки в падежах и числах: например, “provinciae” в четвертом предложении, конечно, не именительный, а родительный падеж. Множество топонимов не опознано в качестве имен собственных.\nЗдесь есть два пути. Первый: пробовать другие модели, доступные в пакете udpipe. Например, для латыни это PROIEl, обученная не только на классических авторах, но и на Вульгате, или ITTB, обученная на сочинениях Фомы. Но если тексты в трибанках не очень похожи на ваш корпус, то это вряд ли сработает.\nВторой путь - обучить модель самостоятельно. Например, для трибанка Perseus доступны более свежие версии (2.13 на момент написания этой главы) на GitHub.\nВот некоторые изменения:\n\nпоявилась метка dep_rel для ablativus absolutus (advcl:abs);\nисправлены аннотации для супина (VerbForm=Conv, Aspect=Prosp), а также герундия и герундива (VerbForm=Part, Aspect=Prosp);\nдобавлен тип для местоимения (PronType) и вид для глагола (Aspect) и др.\n\nИнструкцию по обучению модели при помощи udpipe можно найти здесь (нужен VPN). Следуя этой инструкции и используя трибанк Perseus 2.13, мы обучили новую модель (это заняло около 8 часов на персональном компьютере), которую можно загрузить и использовать для аннотации.\nНадо иметь в виду, что само по себе обновление трибанка еще не гарантирует того, что модель будет лучше справляться с парсингом: многое зависит от параметров обучения. В нашем случае, впрочем, некоторые улучшения есть: например, “provinciae” корректно опознано как родительный падеж. Но есть и потери: “fortissimi” в том же предложении выше - это nominativus pluralis, который ошибочно опознан как генитив единственного числа.\n\nlatin_perseus_new &lt;- udpipe_load_model(\"../latin_model/la_perseus-2.13-20231115.udpipe\")\n\ncaesar_annotate2 &lt;- udpipe_annotate(latin_perseus_new, caesar_id$text, doc_id = caesar_id$id)\n\ncaesar_pos2 &lt;- as_tibble(caesar_annotate2) |&gt; \n  select(-paragraph_id)\n\n\ncaesar_pos2\n\n\n  \n\n\n\nДля многих задач достигнутой точности хватит, но есть способы ее повысить (часто за пределами R). Например, для латинского языка разработан пайплайн под названием LatinPipe, в 2024 г. победивший в конкурсе как лучший парсер для латинского языка. Это сложная конфигурация из различных нейросетей, которые учатся не на одном, а сразу на нескольких трибанках, что позволяет достичь большой точности. Мы обучили подобную модель и передали ей “Записки Цезаря”. Результат возвращается в формате CoNLL-U: прочитаем его в окружение и посмотрим, что получилось (скачать можно здесь).\n\ncaesar_pos3 &lt;- udpipe_read_conllu(\"../files/bg_latinpipe.conllu\") |&gt; \n  select(-paragraph_id)\ncaesar_pos3\n\n\n  \n\n\n\nКельты признаны существительным, бельги мужского рода (в поле FEATS), а provinciae – генитив. Очень хорошо.\n\n\n10.2.3 Лемматизация по API\nПоследние модели для лемматизации доступны здесь https://lindat.mff.cuni.cz/services/udpipe/ (нужен VPN). Можно загружать файлы вручную или через API.\n\n\nПодробнее о лемматизации по API.\n\n\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# функция для лемматизации латинского текста\nlemmatize_latin_text &lt;- function(text, model = \"latin-perseus-ud-2.15-241121\") {\n  url &lt;- \"https://lindat.mff.cuni.cz/services/udpipe/api/process\"\n  \n  # Параметры запроса\n  params &lt;- list(\n    model = model,  \n    data = text,\n    tokenizer = \"\",\n    tagger = \"\",\n    parser = \"\"\n  )\n  \n  # Отправка POST запроса\n  response &lt;- POST(\n    url = url,\n    body = params,\n    encode = \"form\"\n  )\n  \n  # Проверка статуса ответа\n  if (status_code(response) == 200) {\n    # Парсинг JSON ответа\n    result &lt;- content(response, \"parsed\")\n    \n    # Извлечение данных\n    if (!is.null(result$result)) {\n      return(result$result)\n    } else {\n      stop(\"Ошибка: результат не содержит данных\")\n    }\n  } else {\n    stop(paste(\"Ошибка API:\", status_code(response)))\n  }\n}\n\n\n# пример использования\nlatin_text &lt;- \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.\"\n\n# лемматизация текста\nresult &lt;- lemmatize_latin_text(latin_text)\n\ncat(result)\n\n# # generator = UDPipe 2, https://lindat.mff.cuni.cz/services/udpipe\n# # udpipe_model = latin-perseus-ud-2.15-241121\n# # udpipe_model_licence = CC BY-NC-SA\n# # newdoc\n# # newpar\n# # sent_id = 1\n# # text = Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.\n# 1 Gallia  Gallia  PROPN   n-s---fn-   Case=Nom|Gender=Fem|Number=Sing 4   nsubj:pass  _   _\n# 2 est sum AUX v3spia---   Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin    4   aux:pass    _   _\n# 3 omnis   omnis   DET a-s---fn-   Case=Acc|Gender=Fem|Number=Plur|PronType=Tot    1   det _   _\n# 4 divisa  divido  VERB    v-srppfn-   Aspect=Perf|Case=Nom|Gender=Fem|Number=Sing|VerbForm=Part|Voice=Pass    0   root    _   _\n# 5 in  in  ADP r--------   _   6   case    _   _\n# 6 partes  pars    NOUN    n-p---fa-   Case=Acc|Gender=Fem|Number=Plur 4   obl _   _\n# 7 tres    tres    NUM C1|grn1|casM|gen2|vgr1  Case=Acc|Gender=Fem|Number=Plur|NumForm=Word|NumType=Card   6   nummod  _   SpaceAfter=No\n# 8 ,   ,   PUNCT   u--------   _   11  punct   _   _\n# 9 quarum  qui PRON    p-p---fg-   Case=Gen|Gender=Fem|Number=Plur|PronType=Rel    10  nmod    _   _\n# 10    unam    unus    DET p-s---fa-   Case=Acc|Gender=Fem|InflClass=LatPron|Number=Sing|NumType=Card|NumValue=1|PronType=Ind  11  obj _   _\n# 11    incolunt    incolo  VERB    v3ppia---   Aspect=Imp|Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act  6   acl:relcl   _   _\n# 12    Belgae  Belgae  NOUN    n-p---mn-   Case=Nom|Gender=Masc|Number=Plur    11  nsubj   _   SpaceAfter=No\n# 13    ,   ,   PUNCT   u--------   _   15  punct   _   _\n# 14    aliam   alius   DET a-s---fa-   Case=Acc|Gender=Fem|Number=Sing|PronType=Con    15  det _   _\n# 15    Aquitani    Aquitani    NOUN    n-p---mn-   Case=Nom|Gender=Masc|Number=Plur    11  nsubj   _   SpaceAfter=No\n# 16    ,   ,   PUNCT   u--------   _   17  punct   _   _\n# 17    tertiam tertius ADJ a-s---fa-   Case=Acc|Gender=Fem|Number=Sing|NumType=Ord 15  amod    _   _\n# 18    qui qui PRON    p-p---mn-   Case=Nom|Gender=Masc|Number=Plur|PronType=Rel   25  nsubj:pass  _   _\n# 19    ipsorum ipse    DET p-p---mg-   Case=Gen|Gender=Masc|Number=Plur|Person=3|PronType=Prs  20  nmod    _   _\n# 20    lingua  lingua  NOUN    n-s---fb-   Case=Abl|Gender=Fem|Number=Sing 21  obl _   _\n# 21    Celtae  Celta   ADJ a-s---fg-   Case=Gen|Gender=Fem|Number=Sing 25  xcomp   _   SpaceAfter=No\n# 22    ,   ,   PUNCT   u--------   _   21  punct   _   _\n# 23    nostra  noster  DET p-p---nn-   Case=Abl|Gender=Fem|Number=Sing|Number[psor]=Plur|Person[psor]=1|Poss=Yes|PronType=Prs  24  det _   _\n# 24    Galli   Galli   NOUN    n-p---mn-   Case=Nom|Gender=Masc|Number=Plur    25  nsubj:pass  _   _\n# 25    appellantur appello VERB    v3ppip---   Aspect=Imp|Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Pass 15  acl:relcl   _   SpaceAfter=No\n# 26    .   .   PUNCT   u--------   _   4   punct   _   SpaceAfter=No\n\nСохраняем и парсим CoNLL-U как было показано выше.\n\nwrite_lines(result, \"test_conllu_caesar.txt\")\nudpipe_read_conllu(\"test_conllu_caesar.txt\")\n\n\n\n\n\n10.2.4 Поле UPOS\nМорфологическая аннотация, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, местоимения.\n\ncaesar_pos3 |&gt; \n  filter(upos == \"PRON\") |&gt; \n  select(token, lemma, upos, xpos)\n\n\n  \n\n\n\nПосчитать части речи можно так:\n\nupos_counts &lt;- caesar_pos3 |&gt; \n  group_by(upos) |&gt; \n  count() |&gt; \n  arrange(-n)\n\nupos_counts\n\n\n  \n\n\n\nСтолбиковая диаграмма позволяет наглядно представить результаты подсчетов:\n\nupos_counts |&gt; \n  ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) +\n  geom_bar(stat = \"identity\", show.legend = F) +\n  coord_flip() +\n  labs(x = NULL) +\n  theme_bw() \n\n\n\n\n\n\n\n\nОтберем наиболее частотные имена и имена собственные.\n\nnouns &lt;- caesar_pos3  |&gt; \n  filter(upos %in% c(\"NOUN\", \"PROPN\")) |&gt; \n  count(lemma) |&gt; \n  arrange(-n) \n\nnouns\n\n\n  \n\n\n\n\nlibrary(wordcloud)\nlibrary(RColorBrewer)\n\npal &lt;- RColorBrewer::brewer.pal(8, \"Dark2\")\n\nwordcloud(nouns$lemma, nouns$n, colors = pal, max.words = 130)\n\n\n\n\n\n\n\n\n\n\n10.2.5 Поле FEATS\nДопустим, нам нужны не все местоимения, а лишь определенные их формы: например, относительные.\n\nrel_pron &lt;- caesar_pos3  |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  as_tibble()\n\nrel_pron \n\n\n  \n\n\n\nПосмотрим на некоторые местоимения в контексте.\nrel_pron |&gt; \n  filter(row_number() %in% c(1, 7)) |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.”\n[2] “Eorum una pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones.”\n\n\n10.2.6 Поле XPOS\nЧтение xpos требует сноровки: например причастие sublata там описывается так: v-srppfb-, где\n\nv = verbum;\n- на месте лица;\ns = singularis;\nr = perfectum (не p, потому что p = praesens);\np = participium;\np = passivum;\nf = femininum;\nb = ablativus (не a, потому что a = accusativus).\n\nСравним с описанием личной формы глагола differunt v3ppia---:\n\nv = verbum;\n3 = 3. persona;\np = pluralis;\np = praesens;\ni = indicativus;\na = activum;\n-- на месте рода и падежа, т.к. форма личная.\n\nПоследнее “место” (Degree) у глаголов всегда свободно; в первой книге там стоит s (superlativus) лишь у florentissimis, что явно ошибка, потому что это не глагол.\n\n\n\n\n\n\nНа заметку\n\n\n\nСпецификацию всех xpos-тегов для латинского языка можно найти по ссылке.\n\n\nДля удобства разобьем xpos на 9 столбцов.\n\ncaesar_pos3_sep &lt;- caesar_pos3 |&gt; \n  separate(xpos, into = c(\"POS\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"persona\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"numerus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"tempus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"modus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"vox\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"genus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"casus\", \"gradus\"), sep = 1)\n\ncaesar_pos3_sep\n\n\n  \n\n\n\nЭти столбцы тоже можно использовать для поиска конкретных признаков. Посмотрим, например, в каком числе и падеже чаще всего стоит относительное местоимения.\n\npron_rel_sum &lt;- caesar_pos3_sep  |&gt; \n  filter(upos == \"PRON\") |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  group_by(numerus, casus) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\npron_rel_sum\n\n\n  \n\n\n\nДля удобства преобразуем сокращения.\n\npron_rel_sum &lt;- pron_rel_sum |&gt; \n  filter(casus != \"-\") |&gt; \n  mutate(casus = case_when(casus == \"n\" ~ \"nom\",\n                           casus == \"g\" ~ \"gen\",\n                           casus == \"d\" ~ \"dat\",\n                           casus == \"a\" ~ \"acc\",\n                           casus == \"b\" ~ \"abl\")) |&gt; \n  mutate(numerus = case_when(numerus == \"s\" ~ \"sing\",\n                              numerus == \"p\" ~ \"plur\"))\n\npron_rel_sum\n\n\n  \n\n\n\nФункция facet_wrap позволяет разбить график на две части на основании значения переменной numerus.\n\npron_rel_sum |&gt; \n  ggplot(aes(casus, n, fill = casus)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  coord_flip() +\n  theme_light() +\n  facet_wrap(~numerus) +\n  labs(x = NULL, y = NULL, title = \"Относительные местоимения в BG 1-7\")\n\n\n\n\n\n\n\n\n\n\n10.2.7 Поле DEP_REL\nАналогичным образом можно отбирать синтаксические признаки и их комбинации, а также визуализировать деревья зависимостей для отдельных предложений.\nДерево зависимостей – это направленный граф, который имеет единственную корневую вершину (сказуемое главного предложения) без входящих дуг (рёбер), при этом все остальные вершины имеют ровно одну входящую дугу. Иными словами, каждое слово зависит от другого, но только от одного. Это выглядит примерно так:\n\nlibrary(textplot)\n\nsent &lt;- caesar_pos3 |&gt; \n  filter(doc_id == \"doc1\", sentence_id == 10) \n\nsent |&gt; \n  distinct(sentence) |&gt; \n  pull(sentence) \n\n[1] \"Apud Helvetios longe nobilissimus fuit et ditissimus Orgetorix.\"\n\ntextplot_dependencyparser(sent, size = 3)\n\n\n\n\n\n\n\n\nПрилагательные “nobilissiumus” и “ditissimus” верно опознаны в качестве именной части сказуемого при подлежащем “Оргеториг”. Информация, которая на графе представлена стрелками, хранится в таблице в полях token_id и head_token_id и dep_rel. Корень синтаксического дерева всегда имеет значение 0, то есть ни от чего не зависит.\n\nsent |&gt; \n  select(token_id, token, head_token_id, dep_rel)\n\n\n  \n\n\n\n\nПравила синтаксической разметки для латинского языка доступны по ссылке, а расшифровку сокращений (для всех языков) надо смотреть здесь.\n\n\n\n10.2.8 Сочетания признаков\nДобудем все сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nexample_sentence &lt;- acl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-sentence, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\nexample_sentence\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например достать такие относительные местоимения, сразу за которыми стоит их вершина:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent) |&gt; \n  head(5)\n[1] “Erant omnino itinera duo, quibus itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:” [4] “Ita sive casu sive consilio deorum immortalium quae pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\nТак мы кое-что полезное поймали, но не все, потому что между местоимением и его антецедентом возможны другие слова (например, “каковыми опасными путями”). С другой стороны, есть и кое-что лишнее, а именно случаи инкорпорации антецедента в придаточное предложение (“quae pars …, ea” вместо “ea pars, quae…” ). В общем, условие можно дальше дорабатывать, но мы пока не будем этого делать.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#обучение-модели",
    "href": "tokenize.html#обучение-модели",
    "title": "10  Лемматизация и POS-тэггинг",
    "section": "10.3 Обучение модели",
    "text": "10.3 Обучение модели\nМожно заметить, что модель Perseus 2.5 справилась не безупречно: все бельги оказались женского рода, а кельты и вовсе признаны глаголом. Есть ошибки в падежах и числах: например, “provinciae” в четвертом предложении, конечно, не именительный, а родительный падеж. Множество топонимов не опознано в качестве имен собственных.\nЗдесь есть два пути. Первый: пробовать другие модели, доступные в пакете udpipe. Например, для латыни это PROIEl, обученная не только на классических авторах, но и на Вульгате, или ITTB, обученная на сочинениях Фомы. Но если тексты в трибанках не очень похожи на ваш корпус, то это вряд ли сработает.\nВторой путь - обучить модель самостоятельно. Например, для трибанка Perseus доступны более свежие версии (2.13 на момент написания этой главы) на GitHub. Вот некоторые изменения:\n\nпоявилась метка dep_rel для ablativus absolutus (advcl:abs);\nисправлены аннотации для супина (VerbForm=Conv, Aspect=Prosp), а также герундия и герундива (VerbForm=Part, Aspect=Prosp);\nдобавлен тип для местоимения (PronType) и вид для глагола (Aspect) и др.\n\nИнструкцию по обучению модели при помощи udpipe можно найти здесь. Следуя этой инструкции и используя трибанк Perseus 2.13, мы обучили новую модель (это заняло около 8 часов на персональном компьютере), которую можно загрузить и использовать для аннотации.\nНадо иметь в виду, что само по себе обновление трибанка еще не гарантирует того, что модель будет лучше справляться с парсингом: многое зависит от параметров обучения. В нашем случае, впрочем, некоторые улучшения есть: например, “provinciae” корректно опознано как родительный падеж. Но есть и потери: “fortissimi” в том же предложении выше - это nominativus pluralis, который ошибочно опознан как генитив единственного числа.\n\nlatin_perseus_new &lt;- udpipe_load_model(\"../latin_model/la_perseus-2.13-20231115.udpipe\")\n\ncaesar_annotate2 &lt;- udpipe_annotate(latin_perseus_new, caesar$text[1])\n\ncaesar_pos2 &lt;- as_tibble(caesar_annotate2) |&gt; \n  select(-paragraph_id)\n\n\ncaesar_pos2\n\n\n  \n\n\n\nДля многих задач достигнутой точности хватит, но есть способы ее повысить (часто за пределами R). Например, для латинского языка разработан пайплайн под названием LatinPipe, в 2024 г. победивший в конкурсе как лучший парсер для латинского языка. Это сложная конфигурация из различных нейросетей, которые учатся не на одном, а сразу на нескольких трибанках, что позволяет достичь большой точности. Мы обучили подобную модель и передали ей “Записки Цезаря”. Результат возвращается в формате CoNLL-U: прочитаем его в окружение и посмотрим, что получилось (скачать можно здесь).\n\nlibrary(udpipe)\ncaesar_pos3 &lt;- udpipe_read_conllu(\"../files/bg_latinpipe.conllu\") |&gt; \n  select(-paragraph_id)\ncaesar_pos3\n\n\n  \n\n\n\nКельты признаны существительным, бельги мужского рода (в поле FEATS), а provinciae – генитив.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лемматизация и POS-тэггинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-upos",
    "href": "tokenize.html#поле-upos",
    "title": "10  Токенизация и лемматизация",
    "section": "10.3 Поле UPOS",
    "text": "10.3 Поле UPOS\nМорфологическая аннотация, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, местоимения.\n\ncaesar_pos3 |&gt; \n  filter(upos == \"PRON\") |&gt; \n  select(token, lemma, upos, xpos)\n\n\n  \n\n\n\nПосчитать части речи можно так:\n\nupos_counts &lt;- caesar_pos3 |&gt; \n  group_by(upos) |&gt; \n  count() |&gt; \n  arrange(-n)\n\nupos_counts\n\n\n  \n\n\n\nСтолбиковая диаграмма позволяет наглядно представить результаты подсчетов:\n\nupos_counts |&gt; \n  ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) +\n  geom_bar(stat = \"identity\", show.legend = F) +\n  coord_flip() +\n  labs(x = NULL) +\n  theme_bw() \n\n\n\n\n\n\n\n\nОтберем наиболее частотные имена и имена собственные.\n\nnouns &lt;- caesar_pos3  |&gt; \n  filter(upos %in% c(\"NOUN\", \"PROPN\")) |&gt; \n  count(lemma) |&gt; \n  arrange(-n) \n\nnouns\n\n\n  \n\n\n\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(RColorBrewer)\n\npal &lt;- RColorBrewer::brewer.pal(8, \"Dark2\")\n\nwordcloud(nouns$lemma, nouns$n, colors = pal, max.words = 130)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-feats",
    "href": "tokenize.html#поле-feats",
    "title": "10  Токенизация и лемматизация",
    "section": "10.4 Поле FEATS",
    "text": "10.4 Поле FEATS\nДопустим, нам нужны не все местоимения, а лишь определенные их формы: например, относительные.\n\nrel_pron &lt;- caesar_pos3  |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  as_tibble()\n\nrel_pron \n\n\n  \n\n\n\nПосмотрим на некоторые местоимения в контексте.\nrel_pron |&gt; \n  filter(row_number() %in% c(1, 7)) |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.”\n[2] “Eorum una pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones.”",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-xpos",
    "href": "tokenize.html#поле-xpos",
    "title": "10  Токенизация и лемматизация",
    "section": "10.5 Поле XPOS",
    "text": "10.5 Поле XPOS\nЧтение xpos требует сноровки: например причастие sublata там описывается так: v-srppfb-, где\n\nv = verbum;\n- на месте лица;\ns = singularis;\nr = perfectum (не p, потому что p = praesens);\np = participium;\np = passivum;\nf = femininum;\nb = ablativus (не a, потому что a = accusativus).\n\nСравним с описанием личной формы глагола differunt v3ppia---:\n\nv = verbum;\n3 = 3. persona;\np = pluralis;\np = praesens;\ni = indicativus;\na = activum;\n-- на месте рода и падежа, т.к. форма личная.\n\nПоследнее “место” (Degree) у глаголов всегда свободно; в первой книге там стоит s (superlativus) лишь у florentissimis, что явно ошибка, потому что это не глагол.\n\n\n\n\n\n\nНа заметку\n\n\n\nСпецификацию всех xpos-тегов для латинского языка можно найти по ссылке.\n\n\nДля удобства разобьем xpos на 9 столбцов.\n\ncaesar_pos3_sep &lt;- caesar_pos3 |&gt; \n  separate(xpos, into = c(\"POS\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"persona\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"numerus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"tempus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"modus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"vox\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"genus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"casus\", \"gradus\"), sep = 1)\n\ncaesar_pos3_sep\n\n\n  \n\n\n\nЭти столбцы тоже можно использовать для поиска конкретных признаков. Посмотрим, например, в каком числе и падеже чаще всего стоит относительное местоимения.\n\npron_rel_sum &lt;- caesar_pos3_sep  |&gt; \n  filter(upos == \"PRON\") |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  group_by(numerus, casus) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\npron_rel_sum\n\n\n  \n\n\n\nДля удобства преобразуем сокращения.\n\npron_rel_sum &lt;- pron_rel_sum |&gt; \n  filter(casus != \"-\") |&gt; \n  mutate(casus = case_when(casus == \"n\" ~ \"nom\",\n                           casus == \"g\" ~ \"gen\",\n                           casus == \"d\" ~ \"dat\",\n                           casus == \"a\" ~ \"acc\",\n                           casus == \"b\" ~ \"abl\")) |&gt; \n  mutate(numerus = case_when(numerus == \"s\" ~ \"sing\",\n                              numerus == \"p\" ~ \"plur\"))\n\npron_rel_sum\n\n\n  \n\n\n\nФункция facet_wrap позволяет разбить график на две части на основании значения переменной numerus.\n\npron_rel_sum |&gt; \n  ggplot(aes(casus, n, fill = casus)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  coord_flip() +\n  theme_light() +\n  facet_wrap(~numerus) +\n  labs(x = NULL, y = NULL, title = \"Относительные местоимения в BG 1-7\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-dep_rel",
    "href": "tokenize.html#поле-dep_rel",
    "title": "10  Токенизация и лемматизация",
    "section": "10.6 Поле DEP_REL",
    "text": "10.6 Поле DEP_REL\nАналогичным образом можно отбирать синтаксические признаки и их комбинации, а также визуализировать деревья зависимостей для отдельных предложений.\nДерево зависимостей – это направленный граф, который имеет единственную корневую вершину (сказуемое главного предложения) без входящих дуг (рёбер), при этом все остальные вершины имеют ровно одну входящую дугу. Иными словами, каждое слово зависит от другого, но только от одного. Это выглядит примерно так:\n\nlibrary(textplot)\n\nsent &lt;- caesar_pos3 |&gt; \n  filter(doc_id == \"doc1\", sentence_id == 10) \n\nsent |&gt; \n  distinct(sentence) |&gt; \n  pull(sentence) \n\n[1] \"Apud Helvetios longe nobilissimus fuit et ditissimus Orgetorix.\"\n\ntextplot_dependencyparser(sent, size = 3)\n\nWarning in ggraph::geom_node_label(ggplot2::aes(label = token), col =\nvertex_color, : Ignoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\nПрилагательные “nobilissiumus” и “ditissimus” верно опознаны в качестве именной части сказуемого при подлежащем “Оргеториг”. Информация, которая на графе представлена стрелками, хранится в таблице в полях token_id и head_token_id и dep_rel. Корень синтаксического дерева всегда имеет значение 0, то есть ни от чего не зависит.\n\nsent |&gt; \n  select(token_id, token, head_token_id, dep_rel)\n\n\n  \n\n\n\n\nПравила синтаксической разметки для латинского языка доступны по ссылке, а расшифровку сокращений (для всех языков) надо смотреть здесь.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#совместная-встречаемость-слов",
    "href": "tokenize.html#совместная-встречаемость-слов",
    "title": "10  Токенизация и лемматизация",
    "section": "10.3 Совместная встречаемость слов",
    "text": "10.3 Совместная встречаемость слов\nФункция cooccurence() из пакета udpipe позволяет выяснить, сколько раз некий термин встречается совместно с другим термином, например:\n\nслова встречаются в одном и том же документе/предложении/параграфе;\nслова следуют за другим словом;\nслова находятся по соседству с другим словом на расстоянии n слов.\n\nКод ниже позволяет выяснить, какие существительные встречаются в одном предложении:\n\ncaesar_subset &lt;-  caesar_pos3 |&gt; \n  filter(upos == \"NOUN\")\n\n\ncooc &lt;- cooccurrence(caesar_subset, term = \"lemma\", \n                     group = c(\"doc_id\", \"sentence_id\")) |&gt;\n  as_tibble() |&gt; \n  filter(cooc &gt; 25)\n\ncooc\n\n\n  \n\n\n\nЭтот результат легко визуализировать, используя пакет ggraph (подробнее о нем мы будем говорить в следующих уроках):\n\nlibrary(igraph)\nlibrary(ggraph)\n\nwordnetwork &lt;- graph_from_data_frame(cooc, directed = FALSE)\nwordnetwork\n\nIGRAPH 41c0d94 UN-- 22 40 -- \n+ attr: name (v/c), cooc (e/n)\n+ edges from 41c0d94 (vertex names):\n [1] castra   --hostis   castra   --locus    castra   --dies    \n [4] hostis   --locus    hostis   --res      flumen   --pars    \n [7] castra   --legio    locus    --pars     hostis   --miles   \n[10] castra   --pars     hostis   --pars     castra   --res     \n[13] causa    --res      castra   --copia    hostis   --proelium\n[16] civitas  --res      bellum   --res      consilium--res     \n[19] locus    --res      hostis   --legio    dies     --res     \n[22] miles    --res      hostis   --eques    legio    --res     \n+ ... omitted several edges\n\n\n\nggraph(wordnetwork) +\n  geom_edge_arc(aes(width = cooc), alpha = 0.8, \n                edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость существительных\", subtitle = \"De Bello Gallico 1-7\")\n\nUsing \"stress\" as default layout\n\n\nWarning in geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4):\nIgnoring unknown parameters: `label.size`\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nЧтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но с другими аргументами:\n\ncooc2 &lt;- cooccurrence(caesar_subset$lemma, \n                      relevant = caesar_subset$upos %in% c(\"NOUN\", \"ADJ\"),\n                      skipgram = 1) |&gt; \n  as_tibble() |&gt; \n  filter(cooc &gt; 10)\n\ncooc2\n\n\n  \n\n\n\n\nwordnetwork &lt;- graph_from_data_frame(cooc2, directed = FALSE)\nwordnetwork\n\nIGRAPH 3c45052 UN-- 29 67 -- \n+ attr: name (v/c), cooc (e/n)\n+ edges from 3c45052 (vertex names):\n [1] res     --res       castra  --hostis    castra  --locus    \n [4] castra  --dies      castra  --copia     res     --hostis   \n [7] castra  --hostis    res     --dies      res     --locus    \n[10] tribunus--miles     res     --locus     res     --castra   \n[13] res     --civitas   castra  --locus     res     --consilium\n[16] pars    --pars      pars    --flumen    res     --causa    \n[19] castra  --legio     hostis  --numerus   res     --causa    \n[22] locus   --hostis    castra  --legio     locus   --hostis   \n+ ... omitted several edges\n\nggraph(wordnetwork, layout = \"fr\") +\n  geom_edge_link(aes(width = cooc), edge_colour = \"grey90\", edge_alpha=0.8, show.legend = F) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  labs(title = \"Слова, стоящие рядом в тексте\", subtitle = \"De Bello Gallico 1-7\") +\n  theme_void()\n\nWarning in geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4):\nIgnoring unknown parameters: `label.size`",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "count.html",
    "href": "count.html",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "",
    "text": "10.1 Подготовка данных\nЗа основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты:\nИсточники для этого урока доступны в библиотеке Gutengerg; чтобы их извлечь, следует выяснить gutenberg_id. Пример ниже; таким же образом можно найти id для трактатов Локка и Беркли.\n# install.packages(\"gutenbergr\")\nlibrary(gutenbergr)\nlibrary(tidyverse)\nlibrary(stringr)\n\ngutenberg_works(str_detect(author, \"Hume\"), languages = \"en\")\nКогда id найдены, gutenbergr позволяет загрузить сочинения; на этом этапе часто возникают ошибки – в таком случае надо воспользоваться одним из зеркал. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL.\nmy_corpus &lt;- gutenberg_download(meta_fields = c(\"author\", \"title\"), c(9662, 4723, 10615), mirror = \"https://www.gutenberg.org/dirs/\")\n\nmy_corpus\nВ этом тиббле хранятся все три текста, которые нам нужны. Уточнить уникальные называния можно при помощи функции distinct() из tidyverse.\nmy_corpus |&gt; \n  distinct(author)\nПрежде чем приступать к анализу, придется немного прибраться. Для этого используем инструменты tidyverse, о которых шла речь в главе про опрятные данные.\nmy_corpus &lt;- my_corpus |&gt; \n  select(-gutenberg_id) |&gt; \n  select(-title) |&gt; \n  relocate(text, .after = author) |&gt; \n  mutate(author = str_remove(author, \",.+$\")) |&gt; \n  filter(text != \"\")\n\nmy_corpus\nВ случае с Юмом отрезаем предисловия, оглавление и индексы, а также номера разделов (везде прописными). Многие слова, которые в оригинале были выделены курсивом, окружены знаками подчеркивания (_), их тоже удаляем.\nHume &lt;- my_corpus |&gt; \n  filter(author == \"Hume\")|&gt; \n  filter(!row_number() %in% c(1:25),\n         !row_number() %in% c(4814:nrow(my_corpus))) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"_\", \" \")) |&gt; \n  filter(!str_detect(text, \"SECTION .{1,4}\"))\nВ случае с Беркли отрезаем метаданные и посвящение в самом начале, а также удаляем нумерацию параграфов. Кроме того, текст содержит примечания следующего вида: Note: Vide Hobbes’ Tripos, ch. v. sect. 6., от них тоже следует избавиться.\nBerkeley &lt;- my_corpus |&gt; \n  filter(author == \"Berkeley\") |&gt; \n  filter(!row_number() %in% c(1:38)) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]+?\\\\.\", \" \"))  |&gt; \n  mutate(text = str_replace_all(text, \"\\\\[.+?\\\\]\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]+\", \" \"))\nЧто касается Локка, то здесь удаляем метаданные и оглавление в самом начале, а также посвящение и подчеркивания вокруг слов. “Письмо к читателю” уже содержит некоторые философские положения, и его можно оставить.\nLocke &lt;- my_corpus  |&gt;  \n  filter(author == \"Locke\") |&gt; \n  filter(!row_number() %in% c(1:135)) |&gt; \n  mutate(text = str_replace_all(text, \"_\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]\", \" \"))\nСоединив обратно все три текста, замечаем некоторые орфографические нерегулярности; исправляем.\ncorpus_clean &lt;- bind_rows(Hume, Berkeley, Locke) |&gt; \n  mutate(text = str_replace_all(text, c(\"[Mm]an’s\" = \"man's\", \"[mM]en’s\" = \"men's\", \"[hH]ath\" = \"has\")))\n\ncorpus_clean\nПосле этого делим корпус на слова.\nlibrary(tidytext)\n\ncorpus_words &lt;- corpus_clean |&gt; \n  unnest_tokens(word, text)\n\ncorpus_words",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#подготовка-данных",
    "href": "count.html#подготовка-данных",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "",
    "text": "“Опыт о человеческом разумении” Джона Локка (1690), первые две книги;\n“Трактат о принципах человеческого знания” Джорджа Беркли (1710);\n“Исследование о человеческом разумении” Дэвида Юма (1748).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nСкачать подготовленный таким образом корпус можно здесь.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#cтоп-слова",
    "href": "count.html#cтоп-слова",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.2 Cтоп-слова",
    "text": "10.2 Cтоп-слова\nБольшая часть слов, которые мы сейчас видим в корпусе, нам пока не интересна – это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. Функция anti_join() позволяет от них избавиться; в случае с английским языком список стоп-слов уже доступен в пакете tidytext; в других случаях их следует загружать отдельно.\nДля многих языков стоп-слова доступны в пакете stopwords. Пример удаления стопслов на русском языке можно посмотреть здесь.\nФункция anti_join() работает так:\n\n\nother &lt;- c(\"section\", \"chapter\", 0:40, \"edit\", 1710, \"v.g\", \"v.g.a\")\n\ncorpus_words_tidy &lt;- corpus_words  |&gt;  \n  anti_join(stop_words) |&gt; \n  filter(!word %in% other)\n\ncorpus_words_tidy\n\n\n  \n\n\n\nУборка закончена, мы готовы к подсчетам.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#абсолютная-частотность",
    "href": "count.html#абсолютная-частотность",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.3 Абсолютная частотность",
    "text": "10.3 Абсолютная частотность\nДля начала посмотрим на самые частотные слова во всем корпусе.\n\nlibrary(ggplot2)\n\ncorpus_words_tidy |&gt; \n  count(word, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = F) + \n  coord_flip() \n\n\n\n\n\n\n\n\nЭтот график уже дает общее представление о тематике нашего корпуса: это теория познания, в центре которой для всех трех авторов стоит понятие “idea”.\nОднако можно заподозрить, что высокие показатели для слов “simple”, “distinct” и “powers” – это заслуга прежде всего Локка, который вводит понятия “простой идеи” и “отчетливой идеи”, а также говорит о “силах” вещей, благодаря которым они воздействуют как друг на друга, так и на разум. Силы для Локка – это причины идей, и как таковые они часто упоминаются в его тексте. Понятие врожденности (“innate”) также занимает в первую очередь его: вся первая книга “Опыта” – это опровержение теории врожденных идей. Беркли о врожденности не говорит вообще, а Юм – очень кратко.\nКроме того, хотя мы взяли только две книги из “Опыта” Локка – это самый длинный текст в нашем корпусе, что создает значительный перекос:\n\ncorpus_words_tidy |&gt; \n  group_by(author) |&gt; \n  summarise(sum = n())\n\n\n  \n\n\n\nПосмотрим статистику по отдельным авторам.\n\ncorpus_words_tidy  |&gt; \n  group_by(author) |&gt; \n  count(word, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder_within(word, n, author), n, fill = word)) +\n  geom_col(show.legend = F) + \n  facet_wrap(~author, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nНаиболее частотные слова (при условии удаления стоп-слов) дают вполне адекватное представление о тематике каждого из трех трактатов.\nСогласно Локку, объектом мышления является идея (желательно отчетливая, но тут уж как получится). Все идеи приобретены умом из опыта, направленного на либо на внешние предметы (ощущения, или чувства), либо на внутренние действия разума (рефлексия, или внутреннее чувство). Никаких врожденных идей у человека нет, изначально его душа похожа на чистый лист (антинативизм). Идеи могут быть простыми и сложными; они делятся на модусы, субстанции и отношения. К числу простых модусов относятся пространство, в котором находятся тела, а также продолжительность; измеренная продолжительность представляет собой время.\nБеркли спорит с мнением, согласно котором ум способен образовывать абстрактные идеи. В том числе, утверждает он, невозможна абстрактная идея движения, отличная от движущегося тела. Он пытается устранить заблуждение Локка, согласно которому слова являются знаками абстрактных общих идей. В мыслящей душе (которую он также называет умом и духом) существуют не абстрактные идеи, а ощущения, и существование немыслящих вещей безотносительно к их воспринимаемости совершенно невозможно. Нет иной субстанции, кроме духа; немыслящие вещи ее совершенно лишены. По этой причине нельзя допустить, что существует невоспринимающая протяженная субстанция, то есть материя. Идеи ощущений возникают в нас согласно с некоторыми правилами, которые мы называем законами природы. Действительные вещи – это комбинации ощущений, запечатлеваемые в нас могущественным духом.\nСогласно Юму, все объекты, доступные человеческому разуму, могут быть разделены на два вида, а именно: на отношения между идеями и факты. К суждениям об отношениях можно прийти благодаря одной только мыслительной деятельности, в то время как все заключения о фактах основаны на отношениях причины и действия. В свою очередь знание о причинности возникает всецело из опыта: только привычка заставляет нас ожидать наступления одного события при наступлении другого. Прояснение этого позволяет добиться большей ясности и точности в философии.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#стемминг",
    "href": "count.html#стемминг",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.4 Стемминг",
    "text": "10.4 Стемминг\nПоскольку мы не лемматизировали текст, то единственное и множественное число слова idea рассматриваются как разные токены. Один из способов справиться с этим - стемминг.\nСтемминг (англ. stemming “поиск происхождения”) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. Стемминг применяется в поисковых системах для расширения поискового запроса пользователя, является частью процесса нормализации текста. Один из наиболее популярных алгоритмов стемминга был написан Мартином Портером и опубликован в 1980 году.\nВ R стеммер Портера доступен в пакете snowball. К сожалению, он поддерживает не все языки, но русский, французский, немецкий и др. там есть. Не для всех языков, впрочем, и не для всех задач стемминг – это хорошая идея. Но попробуем применить его к нашему корпусу.\n\nlibrary(SnowballC)\n\ncorpus_stems &lt;- corpus_words_tidy |&gt; \n  mutate(stem = wordStem(word)) \n\ncorpus_stems |&gt; \n  count(stem, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder(stem, n), n, fill = stem)) +\n  geom_col(show.legend = F) + \n  coord_flip() \n\n\n\n\n\n\n\n\nВсе слова немного покромсаны, но вполне узнаваемы. При этом общее количество уникальных токенов стало значительно меньше:\n\n# до стемминга\nn_distinct(corpus_words_tidy$word)\n\n[1] 8132\n\n# после стемминга\nn_distinct(corpus_stems$stem)\n\n[1] 5229\n\n\nСтемминг применяется в некоторых алгоритмах машинного обучения, но сегодня - все реже, потому что современные компьютеры прекрасно справляются с лемматизацией.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#относительная-частотность",
    "href": "count.html#относительная-частотность",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.5 Относительная частотность",
    "text": "10.5 Относительная частотность\nАбсолютная частотность – плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте.\nCначала считаем частотность для всех токенов по авторам.\n\nauthor_word_counts &lt;- corpus_words  |&gt; \n  count(author, word, sort = T) |&gt; \n  filter(!word %in% other) |&gt; \n  ungroup()\n\nauthor_word_counts\n\n\n  \n\n\n\nЗатем - число токенов в каждой книге.\n\ntotal_counts &lt;- author_word_counts |&gt; \n  group_by(author) |&gt; \n  summarise(total = sum(n))\n\ntotal_counts\n\n\n  \n\n\n\nСоединяем два тиббла:\n\nauthor_word_counts &lt;- author_word_counts |&gt; \n  left_join(total_counts)\n\nauthor_word_counts\n\n\n  \n\n\n\nСчитаем относительную частотность:\n\nauthor_word_tf &lt;- author_word_counts |&gt; \n  mutate(tf = round((n / total), 5))\n\nauthor_word_tf\n\n\n  \n\n\n\nНаиболее частотные слова – это служебные части речи. На графике видно, что подавляющее большинство слов встречается очень редко, а слов с высокой частотностью - мало.\n\nauthor_word_tf |&gt; \n  ggplot(aes(tf, fill = author)) +\n  geom_histogram(show.legend = FALSE) +\n  facet_wrap(~author, scales = \"free_y\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#закон-ципфа",
    "href": "count.html#закон-ципфа",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.6 Закон Ципфа",
    "text": "10.6 Закон Ципфа\nПодобная картина характерна для естественных языков. Распределения слов в них подчиняются закону Ципфа. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) и утверждает следующее: если все слова языка или длинного текста упорядочить по убыванию частоты использования, частота (tf) n-го слова в списке окажется обратно пропорциональной его рангу (r) в степени α. Это значит (в самом общем случае), что если ранг увеличится в n раз, то частотность во столько же раз должна упасть: второе слово в корпусе встречается примерно в два раза реже, чем первое (Savoy 2020, 24).\n\\[tf_{r_i} = \\frac{c}{r^α_i}\\]\nЗдесь c - это константа, которая оценивается для каждого случая отдельно, как и параметр α. Иначе говоря:\n\\[ tf_{r_i} \\times r^α_i = c \\] Посмотрим на ранги и частотность первых 50 слов.\n\nauthor_word_tf_rank &lt;- author_word_tf |&gt; \n  group_by(author) |&gt; \n  mutate(rank = row_number()) \n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(linewidth = 1.1, alpha = 0.7) +\n  coord_cartesian(xlim = c(NA, 50)) +\n  scale_x_continuous(breaks = seq(0,50,5))\n\n\n\n\n\n\n\n\nВспомнив, что логарифм дроби равен разности логарифмов числителя и знаменателя, запишем:\n\\[log(tf_{r_i}) = c - α \\times log(r_i) \\] Таким образом, мы получаем близкую к линейность зависимость, где константа c определяет точку пересечения оси y, a коэффициентα - угол наклона прямой. Графически это выглядит так:\n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЧтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом поговорим подробнее в следующих уроках):\n\nlm_zipf &lt;- lm(data = author_word_tf_rank, \n              formula = log10(tf) ~ log10(rank))\n\ncoefficients(lm_zipf)\n\n(Intercept) log10(rank) \n -0.2501025  -1.2728170 \n\n\nМы получили коэффициент наклона α чуть больше -1 (на практике точно -1 встречается редко). Добавим линию регрессии на график:\n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  geom_abline(intercept = coefficients(lm_zipf)[1],\n              slope = coefficients(lm_zipf)[2], \n              linetype = 2, \n              color = \"grey50\") +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЗдесь видно, что отклонения наиболее заметны в “хвостах” графика. Это характерно для многих корпусов: как очень редких, так и самых частотных слов не так много, как предсказывает закон Ципфа. Кроме того, внизу кривая почти всегда приобретает ступенчатый вид, потому что слова встречаются в корпусе дискретное число раз: ранг у них разный, а частотности одинаковые.\n\n\n\n\n\n\nВопрос\n\n\n\nДальше всего вправо уходит кривая Локка. Почему?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#меры-лексического-разнообразия",
    "href": "count.html#меры-лексического-разнообразия",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.7 Меры лексического разнообразия",
    "text": "10.7 Меры лексического разнообразия\nКоэффициент наклона α для кривой Ципфа колеблется в достаточно узком диапазоне между 0.7 и 2 и, как полагают, может быть связан с “когнитивным усилием” говорящего: например, для устной речи α чуть больше, для письменной - ниже. Однако рассматривать этот наклон как индивидуальную характеристику стиля не стоит: как и другие меры лексического разнообразия, он сильно коррелирует с длиной текста.\nДело в том, что редкие слова (события) встречаются очень часто; это явление известно под названием Large Number of Rare Events (LNRE). И чем длиннее текст, тем больше в нем будет редких слов, но скорость их прибавления постепенно уменьшается: чем дальше, тем сложнее встретить слово, которого еще не было.\nЧтобы в этом убедиться, взглянем на наиболее известную мера лексического разнообразия под названием type-token ratio (TTR).\n\\[ TTR(T) = \\frac{Voc(T)}{n} \\] Здесь n - общее число токенов, а Voc (т.е. словарь) - число уникальных токенов (типов).\nВ пакете languageR, написанном лингвистом Гаральдом Баайеном, есть функция, позволяющая быстро производить такие вычисления. Она требует на входе вектор, а не тиббл, поэтому для эксперимента извлечем один из текстов.\n\nlocke_words &lt;- corpus_words %&gt;% \n  filter(author == \"Locke\") %&gt;% \n  pull(word)\n\nlength(locke_words)\n\n[1] 148171\n\n\nФункция считает различные меры лексического разнообразия, из которых нас сейчас будет интересовать наклон Ципфа и TTR.\n\nlibrary(languageR)\nlocke.growth = growth.fnc(text = locke_words, size = 1000, nchunks = 40)\n\n........................................\n\ngrowth_df &lt;- locke.growth@data$data \ngrowth_df\n\n\n  \n\n\n\nБыстро визуализировать результат можно при помощи plot(locke.growth), но мы воспользуемся ggplot2.\n\nlibrary(gridExtra)\n\np1 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, Types)) + \n  geom_point(color = \"steelblue\")\n\np2 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, Zipf)) + \n  geom_point(color = \"#B44682\") +\n  ylab(\"Zipf Slope\")\n\np3 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, TypeTokenRatio)) + \n  geom_point(color = \"#81B446\")\n\np4 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, HapaxLegomena / Tokens)) + \n  geom_point(color = \"#B47846\") +\n  ylab(\"Growth Rate\")\n\ngrid.arrange(p1, p2, p3, p4, nrow=2)\n\n\n\n\n\n\n\n\nПодробнее о различных мерах лексического разнообразия см.: (Baayen 2008, 222–36) и (Savoy 2020).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#tf-idf",
    "href": "count.html#tf-idf",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.8 TF-IDF",
    "text": "10.8 TF-IDF\nНаиболее частотные слова (с низким рангом) наименее подвержены влиянию тематики, поэтому их используют для стилометрического анализа. Если отобрать наиболее частотные после удаления стоп-слов, то мы получим достаточно адекватное отражение тематики документов. Если же мы необходимо найти наиболее характерные для документов токены, то применяется другая мера, которая называется tf-idf (term frequency - inverse document frequency).\n\nЛогарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для документа. При этом относительная частотность тоже учитывается! Например, Беркли один раз упоминает “сахарные бобы”, а Локк – “миндаль”, но из-за редкой частотности tf-idf для подобных слов будет низкой.\nФункция bind_tf_idf() принимает на входе тиббл с абсолютной частотностью для каждого слова.\n\nauthor_word_tfidf &lt;- author_word_tf |&gt; \n  bind_tf_idf(word, author, n)\n\nauthor_word_tfidf\n\n\n  \n\n\n\nПосмотрим на слова с высокой tf-idf:\n\nauthor_word_tfidf |&gt; \n  select(-total) |&gt; \n  arrange(desc(tf_idf))\n\n\n  \n\n\n\nСнова визуализируем.\n\nauthor_word_tfidf |&gt; \n  arrange(-tf_idf) |&gt; \n  group_by(author) |&gt; \n  top_n(15) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) +\n  geom_col(show.legend = F) +\n  labs(x = NULL, y = \"tf-idf\") +\n  facet_wrap(~author, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip()\n\n\n\n\n\n\n\n\nНа такой диаграмме авторы совсем не похожи друг на друга, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#сравнение-при-помощи-диаграммы-рассеяния",
    "href": "count.html#сравнение-при-помощи-диаграммы-рассеяния",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.9 Сравнение при помощи диаграммы рассеяния",
    "text": "10.9 Сравнение при помощи диаграммы рассеяния\nСтолбиковая диаграмма – не единственный способ сравнить частотности слов. Еще один наглядный метод – это диаграмма рассеяния с относительными частотностями. Сначала “расширим” наш тиббл.\n\nspread_freq &lt;- author_word_tf  |&gt; \n  anti_join(stop_words) |&gt; \n  filter(!word %in% other) |&gt; \n  filter(tf &gt; 0.0001) |&gt; \n  select(-n, -total) |&gt; \n  pivot_wider(names_from = author, values_from = tf, values_fill = 0) \n\nspread_freq\n\n\n  \n\n\n\nТеперь “удлиним”.\n\nlong_freq &lt;- spread_freq |&gt; \n  pivot_longer(c(\"Hume\", \"Locke\"), names_to = \"author\", values_to = \"tf\")\n\nlong_freq\n\n\n  \n\n\n\nМожно визуализировать. Обратите внимание, что частотности для Юма и Лока упаковываются в один столбец (ось X), по оси Y на обеих панелях даны частотности для Беркли (ось y).\n\nlibrary(scales)\n\nlong_freq |&gt; \n  ggplot(aes(x = tf, y = Berkeley)) +\n  geom_abline(color = \"grey40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, \n              height = 0.3, color = \"darkblue\") +\n  geom_text(aes(label = word), check_overlap = TRUE, \n            vjust = 1.5, color = \"grey30\") +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position = \"none\") +\n  theme_minimal() +\n  labs(y = \"Berkeley\", x = NULL)\n\n\n\n\n\n\n\n\nСлова, расположенные ближе к линии, примерно одинаково представлены в обоих текстах (например, “ум” и “душа” у Беркли и Локка); слова, которые находятся дальше от линии, более свойственны одному из двух авторов: например, у Беркли чаще встречается “абстрактный” по сравнению с первой книгой Локка, а у Локка чаще используется слово “простой”.\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press.\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "sentiment.html",
    "href": "sentiment.html",
    "title": "12  Эмоциональная тональность",
    "section": "",
    "text": "12.1 Анализ тональности\nАнализ тональности текста (англ. Sentiment analysis) — задача компьютерной лингвистики, заключающаяся в определении эмоциональной окраски (тональности) текста и, в частности, в выявлении эмоциональной оценки авторов по отношению к объектам, описываемым в тексте.\nАнализ тональности – это частный случай бинарной (позитивная / негативная) или многоклассовой (радость / гнев / обида и т.п.) классификации, хотя иногда бывает также необходимо оценить эмоциональную окрашенность текста по заданной шкале.\nО том, как в решении подобных задач могут быть полезны методы машинного обучения, мы поговорим в следующих уроках, а здесь речь пойдет о достаточно простом и в то же време эффективном подходе, основанном на тональных словарях (англ. affective lexicons). Тональный словарь представляет из себя список слов со значением тональности для каждого слова.\nСравнивая текст (или отрывок текста) со словарем, мы можем вычислить тональность для всего текста (или отрывка). Словари эмоциональной тональности размечаются вручную, полуавтоматически или автоматически на основании уже существующих тезаурусов, при этом используются различные шкалы:\nВ некоторых случаях дополнительно вводятся различия между оценочной лексикой (“неряшливый”) и негативным фактом (“кража”) и т.п.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#анализ-тональности",
    "href": "sentiment.html#анализ-тональности",
    "title": "12  Эмоциональная тональность",
    "section": "",
    "text": "бинарная: negative / positive (-1 / 1)\nтринарная: бинарная + 0 (neutral)\nранжированная: например, от -5 до 5",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#лексиконы-для-русского-языка",
    "href": "sentiment.html#лексиконы-для-русского-языка",
    "title": "11  Эмоциональная тональность",
    "section": "11.2 Лексиконы для русского языка",
    "text": "11.2 Лексиконы для русского языка\nПакет с лексиконами устанавливается напрямую из GitHub.\n\nremotes::install_github(\"dmafanasyev/rulexicon\")\n\nНачало работы.\n\nlibrary(rulexicon)\nlibrary(tidyverse)\nlibrary(tidytext)\n\nРусский язык входит в языков, для которых Й. Чен и С. Скиена собрали оценочную лексику (Chen и Skiena 2014). Их лексикон построен на основе графа знаний, связывающего слова на разных языках (на основе Wiktionary, Google Translate, транслитерационных ссылок и WordNet). Слова оцениваются по бинарной шкале ( -1 / 1).\n\nset.seed(0211)\nchen_skiena &lt;- hash_sentiment_chen_skiena\nsample_n(chen_skiena, 10)\n\n\n  \n\n\n\nДля русского языка в свободном доступе находится “РуСентиЛекс” («Создание лексикона оценочных слов русского языка РуСентилекс» 2016). Он содержит около 15000 уникальных слов или фраз, среди которых оценочные слова, а также слова и выражения, не передающие оценочное отношения автора, но имеющие положительную или отрицательную ассоциацию (коннотацию). Возможные значения переменной sentiment: neutral, positive, negative, positive/negative.\n\nset.seed(1102)\nrusenti2017 &lt;- hash_rusentilex_2017\nsample_n(rusenti2017, 10) |&gt; \n  select(-source, -token)\n\n\n  \n\n\n\nПри работе с этим лексиконом следует учитывать, что для отдельных слов он содержит несколько вхождений, как положительных, так и отрицательных, например:\n\nrusenti2017 %&gt;% \n  filter(token == \"нежный\") |&gt; \n  select(-source, -token)\n\n\n  \n\n\n\nСловарь AFINN содержит 7268 оценочных слов. Их тональность оценивается по шкале от -5 (крайне негативная) до 5 (в высшей степени положительная). Например, слово “адский” имеет оценку -5, а слово “ангельский” – +5.\n\nset.seed(0211)\nafinn &lt;- hash_sentiment_afinn_ru\nsample_n(afinn, 10)\n\n\n  \n\n\n\nNRC** для русского языка – это переведенная версия списка положительных и отрицательных слов Mohammad & Turney (2010). Таблица содержит 5179 слов с не нейтральными оценками. Бинарная шкала: -1 / 1.\n\nset.seed(1102)\nnrc &lt;- hash_sentiment_nrc_emolex_ru\nsample_n(nrc, 10)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#анализ-тональности-опрятный-подход",
    "href": "sentiment.html#анализ-тональности-опрятный-подход",
    "title": "11  Эмоциональная тональность",
    "section": "11.3 Анализ тональности: опрятный подход",
    "text": "11.3 Анализ тональности: опрятный подход\nСогласно Silge и Robinson (2017), анализе эмоциональной тональности в духе tidy data предполагает следующий алгоритм работы:\n\nПрежде всего текст делится на токены (или лемматизируется), затем каждому токену присваивается некое значение тональности, после чего эти значения суммируются и визуализируются.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#подготовка-текста",
    "href": "sentiment.html#подготовка-текста",
    "title": "11  Эмоциональная тональность",
    "section": "11.4 Подготовка текста",
    "text": "11.4 Подготовка текста\nПрежде всего текст необходимо токенизировать, лемматизировать и привести в опрятный формат. Можно загрузить уже подготовленные данные по ссылке.\n\nload(\"../data/liza_tbl.Rdata\")\n\nРазделим весь текст “Лизы” на отрывки по 100 слов: это позволит понять, как меняется эмоциональная тональность произведения по мере развития сюжета.\n\nliza_tbl &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  select(lemma) |&gt;  \n  rename(token = lemma)  |&gt;  \n  mutate(chunk = round(((row_number() + 50) / 100), 0))\n\nliza_tbl\n\n\n  \n\n\n\nВ тексте чуть более 5000 слов, у нас получился 51 отрывок.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#модификация-лексикона",
    "href": "sentiment.html#модификация-лексикона",
    "title": "11  Эмоциональная тональность",
    "section": "11.5 Модификация лексикона",
    "text": "11.5 Модификация лексикона\nСовременные лексиконы могут не очень подходят для анализа классической литературы. Например, в лексиконе AFINN, доступном в пакете rulexicon, слово “старый” имеет отрицательную оценку, как и слово “чувствительный”.\nКод ниже показывает, как можно удалить слово или поменять его знак в R. Разумеется, все то же самое можно сделать вручную, сохранив лексикон локально в виде файла.\n\nlex &lt;- hash_sentiment_afinn_ru |&gt; \n  filter(token != \"старый\")\n\nlex &lt;- lex  |&gt; \n  mutate_at(vars(score), ~ case_when(token == \"чувствительный\" ~  1.7,\n                 TRUE ~ .))\n\nlex |&gt; \n  filter(str_detect(token, \"чувств\"))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#соединение-лексикона-с-документом",
    "href": "sentiment.html#соединение-лексикона-с-документом",
    "title": "11  Эмоциональная тональность",
    "section": "11.6 Соединение лексикона с документом",
    "text": "11.6 Соединение лексикона с документом\nСтоп-слова, то есть слова, не несущие никакой смысловой нагрузки, нам не нужны, но удалять их отдельно нет смысла: мы соединим, при помощи функции inner_join(), документ с одним из лексиконов, в котором не будет стоп-слов. Функция inner_join() работает так:\n\n\nliza_sent &lt;- liza_tbl |&gt; \n  inner_join(lex)\n\nliza_sent\n\n\n  \n\n\n\nЗдесь “горе” – ошибка лемматизации (“стоя на сей горе…”).\nСложив положительно и отрицательно окрашенную лексику для каждого отрывка, получаем значение, позволяющее судить о доминирующей тональности:\n\nliza_chunk_sent &lt;- liza_sent |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  arrange(sum)\n\nliza_chunk_sent\n\n\n  \n\n\n\nДовольно неожиданно, что самый негативный отрывок находится не в конце повести, ближе к трагической развязке, а почти в начале (отрывок 5, ср. отрывки 3 и 4 рядом). Представим эмоционально окрашенную лексику отрывков 3-5 в виде сравнительного облака слов. Палитру берем отсюда.\n\nlibrary(reshape2)\nlibrary(wordcloud)\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\n# добавляем новый столбец для удобства визуализации\nliza_sent_class &lt;- liza_sent |&gt; \n  mutate(tone = case_when( score &gt;= 0 ~ \"pos\",\n                           score &lt; 0 ~ \"neg\"))\nset.seed(0211)\nliza_sent_class |&gt; \n  filter(chunk  %in%  c(3, 4, 5)) |&gt; \n  count(token, tone, sort = T) |&gt; \n  acast(token ~ tone, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = c(pal[1], pal[5]),\n                   max.words = 99)\n\n\n\n\n\n\n\n\nЗдесь видно, что негативная тональность в этой части не связана с судьбой героев: об этом говорят такие слова, как “лютый”, “враг”, “свирепый”. Рассказчик, глядя на заброшенный Симонов монастырь, вспоминает о “печальной истории” Москвы. Если верить нашей модели, самый мрачный фрагмент повести посвящен не судьбе бедной девушки, а “глухому стону времен”:\n\nИногда на вратах храма рассматриваю изображение чудес, в сем монастыре случившихся, там рыбы падают с неба для насыщения жителей монастыря, осажденного многочисленными врагами; тут образ богоматери обращает неприятелей в бегство. Все сие обновляет в моей памяти историю нашего отечества — печальную историю тех времен, когда свирепые татары и литовцы огнем и мечом опустошали окрестности российской столицы и когда несчастная Москва, как беззащитная вдовица, от одного бога ожидала помощи в лютых своих бедствиях.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#тональность-на-оси-времени",
    "href": "sentiment.html#тональность-на-оси-времени",
    "title": "11  Эмоциональная тональность",
    "section": "11.7 Тональность на оси времени",
    "text": "11.7 Тональность на оси времени\nТаблица, которую мы подготовили, позволяет наглядно показать, как меняется тональность во времени – разумеется, речь идет о повествовательном времени, которое измеряется не в минутах, а в словах.\nОбозначим как положительный или отрицательный каждый из отрывков, как мы это делали для слов.\n\nliza_chunk_sent &lt;- liza_chunk_sent |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_chunk_sent\n\n\n  \n\n\n\nПалитра у нас уже сохранена.\n\nlibrary(showtext)\nfont_add(family = \"vibes\", \"GreatVibes-Regular.ttf\")\nshowtext_auto()\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\np1 &lt;- liza_chunk_sent |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (без учета отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\np1\n\n\n\n\n\n\n\n\nВ целом график получился осмысленным. Мы уже сказали выше про отрывки 3-4. Дальше немного скорби в отрывке 8 посвящено покойному отцу Лизы. В 11-м отрывке отразилась тревога матери за судьбу дочери: “коварно”, “обидеть”, “дурной” вносят вклад в настроение этого фрагмента. Это достаточно характерно для сентиментальной прозы с ее противопоставлением пороков городской жизни и пасторальных добродетелей.\n\nУ меня всегда сердце бывает не на своем месте, когда ты ходишь в город; я всегда ставлю свечу перед образ и молю господа бога, чтобы он сохранил тебя от всякой беды и напасти.\n\nЕще два минимума: отрывки 31 и 34. В первом из них Лиза встревожена вестью о возможном замужестве с сыном крестьянина. Отрывок 34 – это падение Лизы:\n\nГрозно шумела буря, дождь лился из черных облаков — казалось, что натура сетовала о потерянной Лизиной невинности.\n\nНа графике видно, что это место гораздо более эмоционально, чем эпизод самоубийства Лизы: именно после знаменитых карамзинских многоточий и тире события устремляются к трагическому финалу. О самой смерти девушки Карамзин говорит, конечно, с грустью, но без надрыва: “Тут она бросилась в воду”.\nОтрывки 38, 39, 42 – Эраст отправляется на войну. Все, как положено, плачут, что зафиксировал и наш график.\nНаконец, в отрывках 49-51 доминирует тема смерти, причем часть этих слов относится не к самой девушке, а к ее матери.\n\nliza_sent_class %&gt;% \n  filter(chunk %in% c(49:51)) %&gt;% \n  filter(tone == \"neg\") %&gt;% \n  count(token, sort = T) %&gt;% \n  with(wordcloud(token, n, max.words = 100, colors = pal[2]))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#отрицания",
    "href": "sentiment.html#отрицания",
    "title": "11  Эмоциональная тональность",
    "section": "11.8 Отрицания",
    "text": "11.8 Отрицания\nВ отрывке 15 несколько негативных слов имеют перед собой отрицания (“не подозревая”, “никакого худого намерения” и т.п.), поэтому к числу отрицательно окрашенных он отнесен ошибочно. К сожалению, это недостаток подхода, основанного на словарях, не принимающего в учет синтаксические связи в предложении.\nОдно из самых простых решений заключается в том, что бы соединить отрицание и следующее за ним слово (или добавить отрицание ко всем словам до следующего знака препинания).\n\nneg_sent &lt;- \"Старушка с охотою приняла сие предложение, не подозревая в нем никакого худого намерения.\"\n\nstr_replace_all(neg_sent, \"( не | никакого )(\\\\w+)\", \" NEG_\\\\2\")\n\n[1] \"Старушка с охотою приняла сие предложение, NEG_подозревая в нем NEG_худого намерения.\"\n\n\nЧтобы систематически применить этот подход ко всему документу (или коллекции документов), необходим список отрицаний для выбранного языка. Список ниже не претендует на полноту, но иллюстрирует общий принцип.\n\nnegations &lt;- c(\"никто\", \"никого\", \"никем\", \"ничто\", \"ничем\", \"ничего\", \"ни\", \"никакой\", \"никакого\", \"никаких\", \"никаким\", \"никак\", \"ничей\", \"ничьих\", \"нисколько\", \"никогда\", \"нигде\", \"никуда\", \"некого\", \"нельзя\", \"нечего\", \"незачем\", \"нет\", \"едва\", \"не\", \"ничуть\")\n\nregex &lt;- str_c(negations, collapse = \" | \")\nregex &lt;- paste0(\"( \", regex, \"  )(\\\\w+)\")\nregex\n\n[1] \"( никто | никого | никем | ничто | ничем | ничего | ни | никакой | никакого | никаких | никаким | никак | ничей | ничьих | нисколько | никогда | нигде | никуда | некого | нельзя | нечего | незачем | нет | едва | не | ничуть  )(\\\\w+)\"\n\n\n\nload(\"../data/liza_tbl.Rdata\") \ntext &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  pull(lemma) |&gt; \n  str_c(collapse = \" \")\n\nЗаменяем отрицания и считаем статистику по отрывкам.\n\ntext &lt;-  str_replace_all(text, regex, \" NEG_\\\\2\")\n\n\nliza_NEG &lt;- tibble(text = text) |&gt; \n  unnest_tokens(token, text) |&gt; \n  mutate(chunk = round(((row_number() + 50) / 100), 0)) |&gt; \n  inner_join(lex) \n\nJoining with `by = join_by(token)`\n\nliza_NEG_chunk &lt;- liza_NEG |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_NEG_chunk \n\n\n  \n\n\n\nОсталось заново построить график. Для сравнения оставим рядом старую версию.\n\nlibrary(gridExtra)\n\np2 &lt;- liza_NEG_chunk |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (с учетом отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nИз-за изменения числа токенов отрывки сдвинулись, но незначительно. Бывший отрывок 15, как мы и ожидали, перешел в число положительно окрашенных (несмотря на ошибочную оценку слова “левый”).\n\n\nБыло:\n\n\n\n  \n\n\n\n\nСтало:\n\n\n\n  \n\n\n\n\n\nПомимо этого, повысилось абсолютное значение негативной тональности в последних отрывках, хотя на это повлияли не столько отрицания, сколько изменение числа слов и перераспределение их по отрывкам.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#пакет-ggpage",
    "href": "sentiment.html#пакет-ggpage",
    "title": "11  Эмоциональная тональность",
    "section": "11.9 Пакет ggpage",
    "text": "11.9 Пакет ggpage\n\nlibrary(ggpage)\n\npage_data &lt;- liza_tbl |&gt; \n  select(lemma) |&gt; \n  rename(text = lemma)  # required by ggpage_build()\n\npage_data |&gt; \n  ggpage_build(lpp = 22, character_height = 3) |&gt; \n  rename(token = word) |&gt; # required by join\n  left_join(lex) |&gt; \n  rename(text = token) |&gt; \n  mutate(neg = case_when(score &lt; 0 ~ TRUE,\n                         .default = FALSE)) |&gt; \n  ggpage_plot(aes(fill = neg), page.number = \"top-left\") +\n  labs(title = \"Негативная лексика в «Бедной Лизе»\", x = NULL, y = NULL) +\n  scale_fill_manual(values = c(pal[5], pal[1]),\n                    labels = c(\"другая\", \"негативная\"),\n                    name = NULL) + \n  theme(axis.title = element_blank(), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_blank(),\n        text = element_text(family = \"vibes\", size = 12, color = \"grey30\"),\n        )",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#p.s.-для-других-языков",
    "href": "sentiment.html#p.s.-для-других-языков",
    "title": "11  Эмоциональная тональность",
    "section": "11.10 P.S.: Для других языков",
    "text": "11.10 P.S.: Для других языков\nДля языков, которые используют латиницу, в R есть отличный пакет под названием syuzhet, разработанный Мэтью Джокерсом. Название пакета, как говорит его разработчик, подсмотрено у русских формалистов Виктора Шкловского и Владимира Проппа. Возможности и ограничения этого пакета обсуждались в специальной литературе.\n\n\n\n\nChen, Y., и S. Skiena. 2014. «Building Sentiment Lexicons for All Major Languages». Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, 383–89.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.\n\n\n«Создание лексикона оценочных слов русского языка РуСентилекс». 2016. Труды конференции OSTIS-2016, 377–82.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "lsa.html",
    "href": "lsa.html",
    "title": "12  Латентно-семантический анализ",
    "section": "",
    "text": "12.1 Векторы в лингвистике\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Есть несколько возможных решений.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравнивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход – зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности.\nВ этом уроке мы рассмотрим алгоритм LSA, а в следующем – векторные модели на основе PMI. В первом случае анализируется матрица термин-документ, во втором – матрица термин-термин. Оба подхода предполагают использование SVD.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#векторы",
    "href": "lsa.html#векторы",
    "title": "12  Латентно-семантический анализ",
    "section": "",
    "text": "As You Like It\nTwelfth Night\nJulius Caesar\nHenry V\n\n\n\n\nbattle\n1\n1\n8\n15\n\n\nsoldier\n2\n2\n12\n36\n\n\nfool\n37\n58\n1\n5\n\n\nclown\n6\n117\n0\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#латентно-семантический-анализ",
    "href": "lsa.html#латентно-семантический-анализ",
    "title": "12  Латентно-семантический анализ",
    "section": "12.3 Латентно-семантический анализ",
    "text": "12.3 Латентно-семантический анализ\nLSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) – это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками). Слово “latent” (англ. “скрытый”) в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить.\nСоздатели метода LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов представляет собой своего рода сумму (или среднее) отдельных слов. Этот принцип является общим для всех векторных моделей.\nНа входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово “дорожное”, например, служит лучшим показателем темы, чем слово “происшествие”, которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции. Общий принцип действия алгоритма подробно объясняется на очень простом примере по ссылке; мы же перейдем сразу к анализу реальных данных.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#lsa-на-простом-примере",
    "href": "lsa.html#lsa-на-простом-примере",
    "title": "12  Латентно-семантический анализ",
    "section": "12.3 LSA на простом примере",
    "text": "12.3 LSA на простом примере\nДан “корпус” из пяти документов.\n\n\n\ndoc\ntext\n\n\n\n\nd1\nRomeo and Juliet.\n\n\nd2\nJuliet: O happy dagger!\n\n\nd3\nRomeo died by dagger.\n\n\nd4\n“Live free or die”, that’s the New-Hampshire’s motto.\n\n\nd5\nDid you know, New Hampshire is in New-England.\n\n\n\nПосле удаления стоп-слов термдокументная матрица выглядит так.\n\n\n\n  \n\n\n\nПо этой матрице пока нельзя сделать вывод о том, с какими темами связаны, с одной стороны, слова, а с другой - документы. Ее необходимо “переупорядочить” так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем. Примерно так.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#сингулярное-разложение-матрицы",
    "href": "lsa.html#сингулярное-разложение-матрицы",
    "title": "12  Латентно-семантический анализ",
    "section": "12.4 Сингулярное разложение матрицы",
    "text": "12.4 Сингулярное разложение матрицы\nПри сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы.\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. Мы пока не будем пытаться понять, что такое сингулярные векторы с математической точки зрения; достаточно думать о них как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-в-базовом-r",
    "href": "lsa.html#svd-в-базовом-r",
    "title": "12  Латентно-семантический анализ",
    "section": "12.5 SVD в базовом R",
    "text": "12.5 SVD в базовом R\nПрименим SVD к игрушечной термдокументной матрице, которую мы создали выше. В R для этого есть специальная функция (и не одна).\n\nmy_svd = svd(df)\n\nmy_svd\n\n$d\n[1] 2.2852979 2.0102582 1.3606993 1.1181404 0.7965768\n\n$u\n           [,1]       [,2]        [,3]        [,4]        [,5]\n[1,] -0.3961528  0.2800574  0.57117132  0.44968498  0.10183880\n[2,] -0.3142681  0.4495321 -0.41059055  0.51301824 -0.20390607\n[3,] -0.1782395  0.2689915 -0.49732052 -0.25699778 -0.04305233\n[4,] -0.4383638  0.3685083 -0.01287918 -0.57732882  0.21964021\n[5,] -0.2638806 -0.3459214 -0.14578908  0.04748488 -0.41748402\n[6,] -0.5240048 -0.2464047  0.33865227 -0.27284616 -0.15479149\n[7,] -0.2638806 -0.3459214 -0.14578908  0.04748488 -0.41748402\n[8,] -0.3263732 -0.4596688 -0.31700297  0.23724380  0.72485145\n\n$v\n           [,1]       [,2]       [,3]        [,4]        [,5]\n[1,] -0.3108657  0.3629332  0.1180134  0.86098600 -0.12813236\n[2,] -0.4073304  0.5407425 -0.6767037 -0.28735960 -0.03429449\n[3,] -0.5944614  0.2000544  0.6591790 -0.35817507  0.20925479\n[4,] -0.6030458 -0.6953914 -0.1983751  0.05309476 -0.33255810\n[5,] -0.1428143 -0.2286616 -0.2329706  0.21217712  0.90995798\n\n\nСингулярные значения меньше двух отсекаем, остается два значения. Это позволит нам визуализировать результат; в реальном исследовании используется больше измерений (от 50 до 1000 в зависимости от корпуса).\n\nmy_svd$d[3:5] &lt;- 0\n\ns &lt;- diag(my_svd$d) \n\ns\n\n         [,1]     [,2] [,3] [,4] [,5]\n[1,] 2.285298 0.000000    0    0    0\n[2,] 0.000000 2.010258    0    0    0\n[3,] 0.000000 0.000000    0    0    0\n[4,] 0.000000 0.000000    0    0    0\n[5,] 0.000000 0.000000    0    0    0\n\n\nМатрицу правых сингулярных векторов транспонируем.\n\nvt &lt;- t(my_svd$v)\n\nvt\n\n           [,1]        [,2]       [,3]        [,4]       [,5]\n[1,] -0.3108657 -0.40733041 -0.5944614 -0.60304575 -0.1428143\n[2,]  0.3629332  0.54074246  0.2000544 -0.69539140 -0.2286616\n[3,]  0.1180134 -0.67670369  0.6591790 -0.19837510 -0.2329706\n[4,]  0.8609860 -0.28735960 -0.3581751  0.05309476  0.2121771\n[5,] -0.1281324 -0.03429449  0.2092548 -0.33255810  0.9099580\n\n\nТеперь перемножим матрицы, чтобы получить эмбеддинги.\n\n# эмбеддинги слов\nu &lt;- my_svd$u\nword_emb &lt;- u %*% s |&gt; \n  round(3)\n\nrownames(word_emb) &lt;- rownames(df)\n\nword_emb\n\n                [,1]   [,2] [,3] [,4] [,5]\nromeo         -0.905  0.563    0    0    0\njuliet        -0.718  0.904    0    0    0\nhappy         -0.407  0.541    0    0    0\ndagger        -1.002  0.741    0    0    0\nlive          -0.603 -0.695    0    0    0\ndie           -1.198 -0.495    0    0    0\nfree          -0.603 -0.695    0    0    0\nnew-hampshire -0.746 -0.924    0    0    0\n\n\n\n# эмбеддинги документов\ndoc_emb &lt;- s %*% vt |&gt; \n  round(3)\n\ncolnames(doc_emb) &lt;- colnames(df)\n\ndoc_emb \n\n        d1     d2     d3     d4     d5\n[1,] -0.71 -0.931 -1.359 -1.378 -0.326\n[2,]  0.73  1.087  0.402 -1.398 -0.460\n[3,]  0.00  0.000  0.000  0.000  0.000\n[4,]  0.00  0.000  0.000  0.000  0.000\n[5,]  0.00  0.000  0.000  0.000  0.000\n\n\nДобавим условный поисковый запрос: dies, dagger. Очевидно, ближе всего к документы d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 – ни одного. Координаты поискового запроса (который рассматриваем как псевдодокумент) считаем как среднее арифметическое координат:\n\nq = c(\"die\", \"dagger\")\nq_doc &lt;-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2\nq_doc\n\n[1] -1.100  0.123  0.000  0.000  0.000\n\n\nОбъединив все в единый датафрейм, можем визуализировать.\n\nlibrary(tidyverse)\n\nplot_tbl &lt;- rbind(word_emb, t(doc_emb), q_doc) |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"item\") |&gt; \n  rename(dim1 = V1, dim2 = V2) |&gt; \n  mutate(type = c(rep(\"word\", 8), rep(\"doc\", 6))) |&gt; \n  select(!starts_with(\"V\"))\n\nplot_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nИтак, “поисковый запрос” оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#межвекторное-расстояние",
    "href": "lsa.html#межвекторное-расстояние",
    "title": "12  Латентно-семантический анализ",
    "section": "12.6 Межвекторное расстояние",
    "text": "12.6 Межвекторное расстояние\nМы исследовали наш небольшой корпус визуально, но там, где число измерений больше двух, это просто невозможно. На практике расстояние или сходство между векторами слов (или документов) вычисляется алгебраически. Наиболее известны манхэттенское и евклидово расстояние, а также косинусное сходство. Для анализа текстовых данных как правило применяется косинусное сходство.\n\nВсе их можно посчитать в R для заданной пары векторов.\n\ndoc_mx &lt;- plot_tbl |&gt; \n  filter(row_number() &gt; 8 ) |&gt; \n  column_to_rownames(\"item\") |&gt; \n  select(dim1, dim2) |&gt; \n  as.matrix()\n\ndoc_mx\n\n        dim1   dim2\nd1    -0.710  0.730\nd2    -0.931  1.087\nd3    -1.359  0.402\nd4    -1.378 -1.398\nd5    -0.326 -0.460\nq_doc -1.100  0.123\n\n\n\ndist_mx &lt;- doc_mx |&gt; \n  philentropy::distance(method = \"cosine\", use.row.names = T) \n\ndist_mx\n\n               d1         d2        d3          d4         d5     q_doc\nd1     1.00000000  0.9979996 0.8719223 -0.02109092 -0.1817325 0.7725616\nd2     0.99799957  1.0000000 0.8392224 -0.08425530 -0.2435368 0.7308749\nd3     0.87192229  0.8392224 1.0000000  0.47114573  0.3230341 0.9845083\nd4    -0.02109092 -0.0842553 0.4711457  1.00000000  0.9869622 0.6185045\nd5    -0.18173249 -0.2435368 0.3230341  0.98696218  1.0000000 0.4839672\nq_doc  0.77256165  0.7308749 0.9845083  0.61850449  0.4839672 1.0000000\n\n\nЧтобы получить расстояние (а не сходство), вычитаем результат из единицы.\n\nround(1 - dist_mx, 3)\n\n         d1    d2    d3    d4    d5 q_doc\nd1    0.000 0.002 0.128 1.021 1.182 0.227\nd2    0.002 0.000 0.161 1.084 1.244 0.269\nd3    0.128 0.161 0.000 0.529 0.677 0.015\nd4    1.021 1.084 0.529 0.000 0.013 0.381\nd5    1.182 1.244 0.677 0.013 0.000 0.516\nq_doc 0.227 0.269 0.015 0.381 0.516 0.000\n\n\nАналогично вычисляются расстояния между словами. При желании все косинусы можно пересчитать в градусы, чтобы узнать точный угол между векторами.\n\nacos(dist_mx[3,1])  # acos для d3 и d1 (cos = 0.872)\n\n[1] 0.5116817\n\n180 * acos(dist_mx[3,1]) / pi # переводим из радиан в градусы\n\n[1] 29.3172",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#подготовка-данных",
    "href": "lsa.html#подготовка-данных",
    "title": "12  Латентно-семантический анализ",
    "section": "12.4 Подготовка данных",
    "text": "12.4 Подготовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для простоты возьмем из него лишь один год). Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nlibrary(tidyverse)\nload(\"../data/news.Rdata\")\n\nnews_2019 |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\nДобавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\nПодготовленные данные можно забрать по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-опрятный-подход",
    "href": "lsa.html#svd-опрятный-подход",
    "title": "12  Векторные представления слов",
    "section": "12.8 SVD: опрятный подход",
    "text": "12.8 SVD: опрятный подход",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "index.html#дополнительные-материалы",
    "href": "index.html#дополнительные-материалы",
    "title": "Компьютерный анализ текста",
    "section": "Дополнительные материалы",
    "text": "Дополнительные материалы\nЭтот курс опирается на четыре книги, к которым можно обращаться за дополнительной информацией. Все они находятся в открытом доступе.\n\n\n\n\n\n\n\n\n\n1.\n\n\n\n\n\n\n\n2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.\n\n\n\n\n\n\n\n4.\n\n\n\n\n\nЭти книги объединяет общий подход, основанный на идеологии tidy data, и использовать описанные там инструменты можно, не опасаясь проблем совместимости.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "lsa.html#подгтовка-данных",
    "href": "lsa.html#подгтовка-данных",
    "title": "12  Латентно-семантический анализ",
    "section": "12.4 Подгтовка данных",
    "text": "12.4 Подгтовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для простоты возьмем из него лишь один год). Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nlibrary(tidyverse)\nload(\"../data/news.Rdata\")\n\nnews_2019 |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\nДобавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\nПодготовленные данные можно забрать по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#tf-idf-опрятный-подход",
    "href": "lsa.html#tf-idf-опрятный-подход",
    "title": "12  Латентно-семантический анализ",
    "section": "12.5 TF-IDF: опрятный подход",
    "text": "12.5 TF-IDF: опрятный подход\nВместо показателей абсолютной встречаемости при анализе больших текстовых данных применяется tf-idf. Эта статистическая мера не используется, если дана матрица термин-термин, но она хорошо работает с матрицами термин-документ, позволяя повысить веса для тех слов, которые служат хорошими дискриминаторами. Например, “заявил” и “отметил”, хотя это не стоп-слова, могут встречаться в разных темах.\n\nnews_counts &lt;- news_tokens_pruned |&gt;\n  count(token, id)\n\nnews_counts\n\n\nnews_counts |&gt; \n  arrange(id)\n\n\n  \n\n\n\nДобавляем tf_idf.\n\nnews_tf_idf &lt;- news_counts |&gt; \n  bind_tf_idf(token, id, n) |&gt; \n  arrange(tf_idf) |&gt; \n  select(-n, -tf, -idf)\n\nnews_tf_idf",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#documenttermmatrix",
    "href": "lsa.html#documenttermmatrix",
    "title": "12  Латентно-семантический анализ",
    "section": "12.6 DocumentTermMatrix",
    "text": "12.6 DocumentTermMatrix\nПосмотрим на размер получившейся таблицы.\n\nobject.size(news_tf_idf)\n\n5369712 bytes\n\nformat(object.size(news_tf_idf), units = \"auto\")\n\n[1] \"5.1 Mb\"\n\n\nЧтобы вычислить SVD, такую таблицу необходимо преобразовать в матрицу термин-документ. Оценим ее размер:\n\n# число уникальных токенов\nm &lt;- unique(news_tf_idf$token) |&gt; \n  length()\nm\n\n[1] 6299\n\n# число уникальных документов\nn &lt;- unique(news_tf_idf$id) |&gt; \n  length()  \nn\n\n[1] 3407\n\n# число элементов в матрице \nm * n\n\n[1] 21460693\n\n\nИспользуем специальный формат для хранения разреженных матриц.\n\ndtm &lt;- news_tf_idf |&gt; \n  cast_sparse(token, id, tf_idf)\n\n\n# первые 10 рядов и 5 столбцов\ndtm[1:10, 1:5]\n\n10 x 5 sparse Matrix of class \"dgCMatrix\"\n               doc608     doc1670     doc2170     doc2184     doc2219\nранее     0.003530193 .           .           0.005002585 .          \nроссии    0.010127611 0.003675658 0.004689633 0.004783897 0.005471238\nсловам    .           .           0.011776384 .           0.006869557\nрублей    0.006686328 .           0.027865190 .           .          \nрассказал 0.007250151 .           .           0.010274083 .          \nданным    0.007406320 .           .           .           0.012003345\nиздание   0.007759457 .           .           .           0.012575671\nходе      0.008675929 .           .           .           .          \nзаявил    0.016729327 .           .           .           .          \nчастности 0.008860985 .           0.012309349 .           .          \n\n\nСнова уточним размер матрицы.\n\nformat(object.size(dtm), units = \"auto\")\n\n[1] \"3 Mb\"",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-с-пакетом-irlba",
    "href": "lsa.html#svd-с-пакетом-irlba",
    "title": "12  Латентно-семантический анализ",
    "section": "12.7 SVD с пакетом irlba",
    "text": "12.7 SVD с пакетом irlba\nМетод для эффективного вычисления усеченного SVD на больших матрицах реализован в пакете irlba. Возможно, придется подождать ⏳.\n\nlibrary(irlba)\nlsa_space&lt;- irlba::irlba(dtm, 50) \n\nФункция вернет список из трех элементов:\n\nd: k аппроксимированных сингулярных значений;\nu: k аппроксимированных левых сингулярных векторов;\nv: k аппроксимированных правых сингулярных векторов.\n\nПолученную LSA-модель можно использовать для поиска наиболее близких слов и документов или для изучения тематики корпуса – в последнем случае нас может интересовать, какие топики доминируют в тех или иных документах и какими словами они в первую очередь представлены.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#эмбеддинги-слов",
    "href": "lsa.html#эмбеддинги-слов",
    "title": "12  Латентно-семантический анализ",
    "section": "12.8 Эмбеддинги слов",
    "text": "12.8 Эмбеддинги слов\nВернем имена рядов матрице левых сингулярных векторов и добавим имена столбцов.\n\nrownames(lsa_space$u) &lt;- rownames(dtm)\ncolnames(lsa_space$u) &lt;- paste0(\"dim\", 1:50)\n\nТеперь посмотрим на эмбеддинги слов.\n\nword_emb &lt;- lsa_space$u |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"word\") |&gt; \n  as_tibble()\n\nword_emb\n\n\n  \n\n\n\nПреобразуем наши данные в длинный формат.\n\nword_emb_long &lt;- word_emb |&gt; \n  pivot_longer(-word, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\nword_emb_long",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#визуализация-топиков",
    "href": "lsa.html#визуализация-топиков",
    "title": "12  Латентно-семантический анализ",
    "section": "12.9 Визуализация топиков",
    "text": "12.9 Визуализация топиков\nВизуализируем несколько топиков, чтобы понять, насколько они осмыслены.\n\nword_emb_long |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#ближайшие-соседи",
    "href": "lsa.html#ближайшие-соседи",
    "title": "12  Латентно-семантический анализ",
    "section": "12.10 Ближайшие соседи",
    "text": "12.10 Ближайшие соседи\nЭмбеддинги можно использовать для поиска ближайших соседей.\n\nlibrary(widyr)\n\nnearest_neighbors &lt;- function(df, feat, doc=F) {\n  inner_f &lt;- function() {\n    widely(\n        ~ {\n          y &lt;- .[rep(feat, nrow(.)), ]\n          res &lt;- rowSums(. * y) / \n            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))\n          \n          matrix(res, ncol = 1, dimnames = list(x = names(res)))\n        },\n        sort = TRUE\n    )}\n  if (doc) {\n    df |&gt; inner_f()(doc, dimension, value) }\n  else {\n    df |&gt; inner_f()(word, dimension, value)\n  } |&gt; \n    select(-item2)\n}\n\n\nnearest_neighbors(word_emb_long, \"сборная\")\n\n\n  \n\n\nnearest_neighbors(word_emb_long, \"завод\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#похожие-документы",
    "href": "lsa.html#похожие-документы",
    "title": "12  Латентно-семантический анализ",
    "section": "12.11 Похожие документы",
    "text": "12.11 Похожие документы\nИнформация о документах хранится в матрице правых сингулярных векторов.\n\nrownames(lsa_space$v) &lt;- colnames(dtm)\ncolnames(lsa_space$v) &lt;- paste0(\"dim\", 1:50)\n\nПосмотрим на эмбеддинги документов.\n\ndoc_emb &lt;- lsa_space$v |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  as_tibble()\n\ndoc_emb\n\n\n  \n\n\n\nПреобразуем в длинный формат.\n\ndoc_emb_long &lt;- doc_emb |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\ndoc_emb_long\n\n\n  \n\n\n\nИ найдем соседей для произвольного документа.\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nВыведем документ 14 вместе с его соседями.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc14\", \"doc392\", \"doc2043\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nПоздравляем, вы построили свою первую рекомендательную систему 🥛.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#сложные-запросы",
    "href": "tokenize.html#сложные-запросы",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.8 Сложные запросы",
    "text": "9.8 Сложные запросы\nДобудем все сложные сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nacl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-id, -sentence, -lemma, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;\\\\\\\\mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Erant omnino itinera duo, quibus&lt;\\mark&gt; itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua&lt;\\mark&gt; die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius&lt;\\mark&gt; legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:”\n[4] “Ita sive casu sive consilio deorum immortalium quae&lt;\\mark&gt; pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius&lt;\\mark&gt; legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\n[6] “Ubi se diutius duci intellexit et diem instare quo&lt;\\mark&gt; die frumentum militibus metiri oporteret, convocatis eorum principibus, quorum magnam copiam in castris habebat, in his Diviciaco et Lisco, qui summo magistratui praeerat, quem vergobretum appellant Haedui, qui creatur annuus et vitae necisque in suos habet potestatem, graviter eos accusat, quod, cum neque emi neque ex agris sumi possit, tam necessario tempore, tam propinquis hostibus ab iis non sublevetur, praesertim cum magna ex parte eorum precibus adductus bellum susceperit [;”\n[7] “In castris Helvetiorum tabulae repertae sunt litteris Graecis confectae et ad Caesarem relatae, quibus in tabulis nominatim ratio confecta erat, qui numerus domo exisset eorum qui arma ferre possent, et item separatim, quot&lt;\\mark&gt; pueri, senes mulieresque.”\n[8] “Quibus&lt;\\mark&gt; proeliis calamitatibusque fractos, qui et sua virtute et populi Romani hospitio atque amicitia plurimum ante in Gallia potuissent, coactos esse Sequanis obsides dare nobilissimos civitatis et iure iurando civitatem obstringere sese neque obsides repetituros neque auxilium a populo Romano imploraturos neque recusaturos quo minus perpetuo sub illorum dicione atque imperio essent.”\n[9] “Quod si decessisset et liberam possessionem Galliae sibi tradidisset, magno se illum praemio remuneraturum et quaecumque&lt;\\mark&gt; bella geri vellet sine ullo eius labore et periculo confecturum.”\n[10] “Eo circiter hominum XVI milia expedita cum omni equitatu Ariovistus misit, quae&lt;\\mark&gt; copiae nostros terrerent et munitione prohiberent.”\n[11] “Ubi prima impedimenta nostri exercitus ab iis qui in silvis abditi latebant visa sunt, quod&lt;\\mark&gt; tempus inter eos committendi proelii convenerat, ut intra silvas aciem ordinesque constituerant atque ipsi sese confirmaverant, subito omnibus copiis provolaverunt impetumque in nostros equites fecerunt.”\n[12] “quibusnam manibus aut quibus viribus praesertim homines tantulae staturae (nam plerumque omnibus Gallis prae magnitudine corporum quorum&lt;\\mark&gt; brevitas nostra contemptui est) tanti oneris turrim in muro sese posse conlocare confiderent?”\n[13] “quarum&lt;\\mark&gt; rerum a nostris propter paucitatem fieri nihil poterat, ac non modo defesso ex pugna excedendi, sed ne saucio quidem eius loci ubi constiterat relinquendi ac sui recipiendi facultas dabatur.”\n[14] “Ita commutata fortuna eos qui in spem potiundorum castrorum venerant undique circumventos intercipiunt, et ex hominum milibus amplius XXX, quem&lt;\\mark&gt; numerum barbarorum ad castra venisse constabat, plus tertia parte interfecta reliquos perterritos in fugam coiciunt ac ne in locis quidem superioribus consistere patiuntur.”\n[15] “superiorum dierum Sabini cunctatio, perfugae confirmatio, inopia cibariorum, cui&lt;\\mark&gt; rei parum diligenter ab iis erat provisum, spes Venetici belli, et quod fere libenter homines id quod volunt credunt.”\n[16] “Qua&lt;\\mark&gt; re concessa laeti, ut explorata victoria, sarmentis virgultisque collectis, quibus fossas Romanorum compleant, ad castra pergunt.”\n[17] “qui complures annos Sueborum vim sustinuerunt, ad extremum tamen agris expulsi et multis locis Germaniae triennium vagati ad Rhenum pervenerunt, quas&lt;\\mark&gt; regiones Menapii incolebant.”\n[18] “Qua&lt;\\mark&gt; spe adducti Germani latius iam vagabantur et in fines Eburonum et Condrusorum, qui sunt Treverorum clientes, pervenerant.”\n[19] “Quod ubi Caesar comperit, omnibus iis rebus confectis, quarum&lt;\\mark&gt; rerum causa exercitum traducere constituerat, ut Germanis metum iniceret, ut Sugambros ulcisceretur, ut Ubios obsidione liberaret, diebus omnino XVIII trans Rhenum consumptis, satis et ad laudem et ad utilitatem profectum arbitratus se in Galliam recepit pontemque rescidit.”\n[20] “Quibus&lt;\\mark&gt; rebus nostri perterriti atque huius omnino generis pugnae imperiti, non eadem alacritate ac studio quo in pedestribus uti proeliis consuerant utebantur.”\n[21] “Nostri tamen, quod neque ordines servare neque firmiter insistere neque signa subsequi poterant atque alius alia ex navi quibuscumque&lt;\\mark&gt; signis occurrerat se adgregabat, magnopere perturbabantur;”\n[22] “Eadem nocte accidit ut esset luna plena, qui&lt;\\mark&gt; dies maritimos aestus maximos in Oceano efficere consuevit, nostrisque id erat incognitum.”\n[23] “Quibus&lt;\\mark&gt; rebus cognitis, principes Britanniae, qui post proelium ad Caesarem convenerant, inter se conlocuti, cum et equites et naves et frumentum Romanis deesse intellegerent et paucitatem militum ex castrorum exiguitate cognoscerent, quae hoc erant etiam angustior quod sine impedimentis Caesar legiones transportaverat, optimum factu esse duxerunt rebellione facta frumento commeatuque nostros prohibere et rem in hiemem producere, quod his superatis aut reditu interclusis neminem postea belli inferendi causa in Britanniam transiturum confidebant.”\n[24] “Qui cum propter siccitates paludum quo&lt;\\mark&gt; se reciperent non haberent, quo perfugio superiore anno erant usi, omnes fere in potestatem Labieni venerunt.”\n[25] “Qua&lt;\\mark&gt; re nuntiata Pirustae legatos ad eum mittunt qui doceant nihil earum rerum publico factum consilio, seseque paratos esse demonstrant omnibus rationibus de iniuriis satisfacere.”\n[26] “complures praeterea minores subiectae insulae existimantur, de quibus&lt;\\mark&gt; insulis nonnulli scripserunt dies continuos triginta sub bruma esse noctem.”\n[27] “Ex his omnibus longe sunt humanissimi qui Cantium incolunt, quae&lt;\\mark&gt; regio est maritima omnis, neque multum a Gallica differunt consuetudine.”\n[28] “Dum haec in his locis geruntur, Cassivellaunus ad Cantium, quod esse ad mare supra demonstravimus, quibus&lt;\\mark&gt; regionibus quattuor reges praeerant, Cingetorix, Carvilius, Taximagulus, Segovax, nuntios mittit atque eis imperat uti coactis omnibus copiis castra navalia de improviso adoriantur atque oppugnent.”\n[29] “habere sese, quae de re communi dicere vellent, quibus&lt;\\mark&gt; rebus controversias minui posse sperarent.”\n[30] “Interim ad Labienum per Remos incredibili celeritate de victoria Caesaris fama perfertur, ut, cum ab hibernis Ciceronis milia passuum abesset circiter LX, eoque post horam nonam diei Caesar pervenisset, ante mediam noctem ad portas castrorum clamor oreretur, quo&lt;\\mark&gt; clamore significatio victoriae gratulatioque ab Remis Labieno fieret.”\n[31] “Hi certo anni tempore in finibus Carnutum, quae&lt;\\mark&gt; regio totius Galliae media habetur, considunt in loco consecrato.”\n[32] “Viri, quantas&lt;\\mark&gt; pecunias ab uxoribus dotis nomine acceperunt, tantas ex suis bonis aestimatione facta cum dotibus communicant.”\n[33] “Quae&lt;\\mark&gt; civitates commodius suam rem publicam administrare existimantur, habent legibus sanctum, si quis quid de re publica a finitimis rumore aut fama acceperit, uti ad magistratum deferat neve cum quo alio communicet, quod saepe homines temerarios atque imperitos falsis rumoribus terreri et ad facinus impelli et de summis rebus consilium capere cognitum est.”\n[34] “sed magistratus ac principes in annos singulos gentibus cognationibusque hominum, qui una coierunt, quantum et quo&lt;\\mark&gt; loco visum est agri attribuunt atque anno post alio transire cogunt.”\n[35] “His rebus agitatis profitentur Carnutes se nullum periculum communis salutis causa recusare principesque ex omnibus bellum facturos pollicentur et, quo&lt;\\mark&gt;niam in praesentia obsidibus cavere inter se non possint ne res efferatur, ut iureiurando ac fide sanciatur, petunt, collatis militaribus signis, quo more eorum gravissima caerimonia continetur, ne facto initio belli ab reliquis deserantur.”\n[36] “Nam quae Cenabi oriente sole gesta essent, ante primam confectam vigiliam in finibus Arvernorum audita sunt, quod&lt;\\mark&gt; spatium est milium passuum circiter centum LX. Simili ratione ibi Vercingetorix, Celtilli filius, Arveruus, summae potentiae adulescens, cuius pater principatum Galliae totius obtinuerat et ob eam causam, quod regnum appetebat, ab civitate erat interfectus, convocatis suis clientibus facile incendit.”\n[37] “Eo cum venisset, timentes confirmat, praesidia in Rutenis provincialibus, Volcis Arecomicis, Tolosatibus circumque Narbonem, quae&lt;\\mark&gt; loca hostibus erant finitima, constituit;”\n[38] “Qua&lt;\\mark&gt; re per exploratores nuntiata Caesar legiones quas expeditas esse iusserat portis incensis intromittit atque oppido potitur, perpaucis ex hostium numero desideratis quin cuncti caperentur, quod pontis atque itinerum angustiae multitudinis fugam intercluserant.”\n[39] “Cum is murum hostium paene contingeret, et Caesar ad opus consuetudine excubaret militesque hortaretur, ne quod omnino tempus ab opere intermitteretur, paulo ante tertiam vigiliam est animadversum fumare aggerem, quem cuniculo hostes succenderant, eodemque tempore toto muro clamore sublato duabus portis ab utroque latere turrium eruptio fiebat, alii faces atque aridam materiem de muro in aggerem eminus iaciebant, picem reliquasque res, quibus ignis excitari potest, fundebant, ut quo primum curreretur aut cui&lt;\\mark&gt; rei ferretur auxilium vix ratio iniri posset.” [40] “Non virtute neque in acie vicisse Romanos, sed artificio quodam et scientia oppugnationis, cuius&lt;\\mark&gt; rei fuerint ipsi imperiti.”\n[41] “Sibi numquam placuisse Avaricum defendi, cuius&lt;\\mark&gt; rei testes ipsos haberet;”\n[42] “Hi similitudine armorum vehementer nostros perterruerunt, ac tametsi dextris humeris ex sertis animadvertebantur, quod&lt;\\mark&gt; insigne pactum esse consuerat, tamen id ipsum sui fallendi causa milites ab hostibus factum existimabant.”\n[43] “Quanto&lt;\\mark&gt; opere eorum animi magnitudinem admiraretur, quos non castrorum munitiones, non altitudo montis, non murus oppidi tardare potuisset, tanto opere licentiam arrogantiamque reprehendere, quod plus se quam imperatorem de victoria atque exitu rerum sentire existimarent;”\n[44] “Namque altera ex parte Bellovaci, quae&lt;\\mark&gt; civitas in Gallia maximam habet opinionem virtutis, instabant, alteram Camulogenus parato atque instructo exercitu tenebat;”\n[45] “ab sinistro, quem&lt;\\mark&gt; locum duodecima legio tenebat, cum primi ordines hostium transfixi telis concidissent, tamen acerrime reliqui resistebant, nec dabat suspicionem fugae quisquam.”\n[46] “Sub muro, quae&lt;\\mark&gt; pars collis ad orientem solem spectabat, hunc omnem locum copiae Gallorum compleverant fossamque et maceriam sex in altitudinem pedum praeduxerant.”\n[47] “Subito clamore sublato, qua&lt;\\mark&gt; significatione qui in oppido obsidebantur de suo adventu cognoscere possent, crates proicere, fundis, sagittis, lapidibus nostros de vallo proturbare reliquaque quae ad oppugnationem pertinent parant administrare.”\n[48] “Eius adventu ex colore vestitus cognito, quo&lt;\\mark&gt; insigni in proeliis uti consuerat, turmisque equitum et cohortibus visis quas se sequi iusserat, ut de locis superioribus haec declivia et devexa cernebantur, hostes proelium committunt.”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#сочетание-условий",
    "href": "tokenize.html#сочетание-условий",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.8 Сочетание условий",
    "text": "9.8 Сочетание условий\nДобудем все сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nacl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-id, -sentence, -lemma, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Erant omnino itinera duo, quibus itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:”\n[4] “Ita sive casu sive consilio deorum immortalium quae pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\n[6] “Ubi se diutius duci intellexit et diem instare quo die frumentum militibus metiri oporteret, convocatis eorum principibus, quorum magnam copiam in castris habebat, in his Diviciaco et Lisco, qui summo magistratui praeerat, quem vergobretum appellant Haedui, qui creatur annuus et vitae necisque in suos habet potestatem, graviter eos accusat, quod, cum neque emi neque ex agris sumi possit, tam necessario tempore, tam propinquis hostibus ab iis non sublevetur, praesertim cum magna ex parte eorum precibus adductus bellum susceperit [;”\n[7] “In castris Helvetiorum tabulae repertae sunt litteris Graecis confectae et ad Caesarem relatae, quibus in tabulis nominatim ratio confecta erat, qui numerus domo exisset eorum qui arma ferre possent, et item separatim, quot pueri, senes mulieresque.”\n[8] “Quibus proeliis calamitatibusque fractos, qui et sua virtute et populi Romani hospitio atque amicitia plurimum ante in Gallia potuissent, coactos esse Sequanis obsides dare nobilissimos civitatis et iure iurando civitatem obstringere sese neque obsides repetituros neque auxilium a populo Romano imploraturos neque recusaturos quo minus perpetuo sub illorum dicione atque imperio essent.”\n[9] “Quod si decessisset et liberam possessionem Galliae sibi tradidisset, magno se illum praemio remuneraturum et quaecumque bella geri vellet sine ullo eius labore et periculo confecturum.”\n[10] “Eo circiter hominum XVI milia expedita cum omni equitatu Ariovistus misit, quae copiae nostros terrerent et munitione prohiberent.”\n[11] “Ubi prima impedimenta nostri exercitus ab iis qui in silvis abditi latebant visa sunt, quod tempus inter eos committendi proelii convenerat, ut intra silvas aciem ordinesque constituerant atque ipsi sese confirmaverant, subito omnibus copiis provolaverunt impetumque in nostros equites fecerunt.”\n[12] “quibusnam manibus aut quibus viribus praesertim homines tantulae staturae (nam plerumque omnibus Gallis prae magnitudine corporum quorum brevitas nostra contemptui est) tanti oneris turrim in muro sese posse conlocare confiderent?”\n[13] “quarum rerum a nostris propter paucitatem fieri nihil poterat, ac non modo defesso ex pugna excedendi, sed ne saucio quidem eius loci ubi constiterat relinquendi ac sui recipiendi facultas dabatur.”\n[14] “Ita commutata fortuna eos qui in spem potiundorum castrorum venerant undique circumventos intercipiunt, et ex hominum milibus amplius XXX, quem numerum barbarorum ad castra venisse constabat, plus tertia parte interfecta reliquos perterritos in fugam coiciunt ac ne in locis quidem superioribus consistere patiuntur.”\n[15] “superiorum dierum Sabini cunctatio, perfugae confirmatio, inopia cibariorum, cui rei parum diligenter ab iis erat provisum, spes Venetici belli, et quod fere libenter homines id quod volunt credunt.”\n[16] “Qua re concessa laeti, ut explorata victoria, sarmentis virgultisque collectis, quibus fossas Romanorum compleant, ad castra pergunt.”\n[17] “qui complures annos Sueborum vim sustinuerunt, ad extremum tamen agris expulsi et multis locis Germaniae triennium vagati ad Rhenum pervenerunt, quas regiones Menapii incolebant.”\n[18] “Qua spe adducti Germani latius iam vagabantur et in fines Eburonum et Condrusorum, qui sunt Treverorum clientes, pervenerant.”\n[19] “Quod ubi Caesar comperit, omnibus iis rebus confectis, quarum rerum causa exercitum traducere constituerat, ut Germanis metum iniceret, ut Sugambros ulcisceretur, ut Ubios obsidione liberaret, diebus omnino XVIII trans Rhenum consumptis, satis et ad laudem et ad utilitatem profectum arbitratus se in Galliam recepit pontemque rescidit.”\n[20] “Quibus rebus nostri perterriti atque huius omnino generis pugnae imperiti, non eadem alacritate ac studio quo in pedestribus uti proeliis consuerant utebantur.”\n[21] “Nostri tamen, quod neque ordines servare neque firmiter insistere neque signa subsequi poterant atque alius alia ex navi quibuscumque signis occurrerat se adgregabat, magnopere perturbabantur;”\n[22] “Eadem nocte accidit ut esset luna plena, qui dies maritimos aestus maximos in Oceano efficere consuevit, nostrisque id erat incognitum.”\n[23] “Quibus rebus cognitis, principes Britanniae, qui post proelium ad Caesarem convenerant, inter se conlocuti, cum et equites et naves et frumentum Romanis deesse intellegerent et paucitatem militum ex castrorum exiguitate cognoscerent, quae hoc erant etiam angustior quod sine impedimentis Caesar legiones transportaverat, optimum factu esse duxerunt rebellione facta frumento commeatuque nostros prohibere et rem in hiemem producere, quod his superatis aut reditu interclusis neminem postea belli inferendi causa in Britanniam transiturum confidebant.”\n[24] “Qui cum propter siccitates paludum quo se reciperent non haberent, quo perfugio superiore anno erant usi, omnes fere in potestatem Labieni venerunt.”\n[25] “Qua re nuntiata Pirustae legatos ad eum mittunt qui doceant nihil earum rerum publico factum consilio, seseque paratos esse demonstrant omnibus rationibus de iniuriis satisfacere.”\n[26] “complures praeterea minores subiectae insulae existimantur, de quibus insulis nonnulli scripserunt dies continuos triginta sub bruma esse noctem.”\n[27] “Ex his omnibus longe sunt humanissimi qui Cantium incolunt, quae regio est maritima omnis, neque multum a Gallica differunt consuetudine.”\n[28] “Dum haec in his locis geruntur, Cassivellaunus ad Cantium, quod esse ad mare supra demonstravimus, quibus regionibus quattuor reges praeerant, Cingetorix, Carvilius, Taximagulus, Segovax, nuntios mittit atque eis imperat uti coactis omnibus copiis castra navalia de improviso adoriantur atque oppugnent.”\n[29] “habere sese, quae de re communi dicere vellent, quibus rebus controversias minui posse sperarent.”\n[30] “Interim ad Labienum per Remos incredibili celeritate de victoria Caesaris fama perfertur, ut, cum ab hibernis Ciceronis milia passuum abesset circiter LX, eoque post horam nonam diei Caesar pervenisset, ante mediam noctem ad portas castrorum clamor oreretur, quo clamore significatio victoriae gratulatioque ab Remis Labieno fieret.”\n[31] “Hi certo anni tempore in finibus Carnutum, quae regio totius Galliae media habetur, considunt in loco consecrato.”\n[32] “Viri, quantas pecunias ab uxoribus dotis nomine acceperunt, tantas ex suis bonis aestimatione facta cum dotibus communicant.”\n[33] “Quae civitates commodius suam rem publicam administrare existimantur, habent legibus sanctum, si quis quid de re publica a finitimis rumore aut fama acceperit, uti ad magistratum deferat neve cum quo alio communicet, quod saepe homines temerarios atque imperitos falsis rumoribus terreri et ad facinus impelli et de summis rebus consilium capere cognitum est.”\n[34] “sed magistratus ac principes in annos singulos gentibus cognationibusque hominum, qui una coierunt, quantum et quo loco visum est agri attribuunt atque anno post alio transire cogunt.”\n[35] “His rebus agitatis profitentur Carnutes se nullum periculum communis salutis causa recusare principesque ex omnibus bellum facturos pollicentur et, quoniam in praesentia obsidibus cavere inter se non possint ne res efferatur, ut iureiurando ac fide sanciatur, petunt, collatis militaribus signis, quo more eorum gravissima caerimonia continetur, ne facto initio belli ab reliquis deserantur.”\n[36] “Nam quae Cenabi oriente sole gesta essent, ante primam confectam vigiliam in finibus Arvernorum audita sunt, quod spatium est milium passuum circiter centum LX. Simili ratione ibi Vercingetorix, Celtilli filius, Arveruus, summae potentiae adulescens, cuius pater principatum Galliae totius obtinuerat et ob eam causam, quod regnum appetebat, ab civitate erat interfectus, convocatis suis clientibus facile incendit.”\n[37] “Eo cum venisset, timentes confirmat, praesidia in Rutenis provincialibus, Volcis Arecomicis, Tolosatibus circumque Narbonem, quae loca hostibus erant finitima, constituit;”\n[38] “Qua re per exploratores nuntiata Caesar legiones quas expeditas esse iusserat portis incensis intromittit atque oppido potitur, perpaucis ex hostium numero desideratis quin cuncti caperentur, quod pontis atque itinerum angustiae multitudinis fugam intercluserant.”\n[39] “Cum is murum hostium paene contingeret, et Caesar ad opus consuetudine excubaret militesque hortaretur, ne quod omnino tempus ab opere intermitteretur, paulo ante tertiam vigiliam est animadversum fumare aggerem, quem cuniculo hostes succenderant, eodemque tempore toto muro clamore sublato duabus portis ab utroque latere turrium eruptio fiebat, alii faces atque aridam materiem de muro in aggerem eminus iaciebant, picem reliquasque res, quibus ignis excitari potest, fundebant, ut quo primum curreretur aut cui rei ferretur auxilium vix ratio iniri posset.” [40] “Non virtute neque in acie vicisse Romanos, sed artificio quodam et scientia oppugnationis, cuius rei fuerint ipsi imperiti.”\n[41] “Sibi numquam placuisse Avaricum defendi, cuius rei testes ipsos haberet;”\n[42] “Hi similitudine armorum vehementer nostros perterruerunt, ac tametsi dextris humeris ex sertis animadvertebantur, quod insigne pactum esse consuerat, tamen id ipsum sui fallendi causa milites ab hostibus factum existimabant.”\n[43] “Quanto opere eorum animi magnitudinem admiraretur, quos non castrorum munitiones, non altitudo montis, non murus oppidi tardare potuisset, tanto opere licentiam arrogantiamque reprehendere, quod plus se quam imperatorem de victoria atque exitu rerum sentire existimarent;”\n[44] “Namque altera ex parte Bellovaci, quae civitas in Gallia maximam habet opinionem virtutis, instabant, alteram Camulogenus parato atque instructo exercitu tenebat;”\n[45] “ab sinistro, quem locum duodecima legio tenebat, cum primi ordines hostium transfixi telis concidissent, tamen acerrime reliqui resistebant, nec dabat suspicionem fugae quisquam.”\n[46] “Sub muro, quae pars collis ad orientem solem spectabat, hunc omnem locum copiae Gallorum compleverant fossamque et maceriam sex in altitudinem pedum praeduxerant.”\n[47] “Subito clamore sublato, qua significatione qui in oppido obsidebantur de suo adventu cognoscere possent, crates proicere, fundis, sagittis, lapidibus nostros de vallo proturbare reliquaque quae ad oppugnationem pertinent parant administrare.”\n[48] “Eius adventu ex colore vestitus cognito, quo insigni in proeliis uti consuerat, turmisque equitum et cohortibus visis quas se sequi iusserat, ut de locis superioribus haec declivia et devexa cernebantur, hostes proelium committunt.”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#сочетания-признаков",
    "href": "tokenize.html#сочетания-признаков",
    "title": "10  Токенизация и лемматизация",
    "section": "10.7 Сочетания признаков",
    "text": "10.7 Сочетания признаков\nДобудем все сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nexample_sentence &lt;- acl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-sentence, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\nexample_sentence\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например достать такие относительные местоимения, сразу за которыми стоит их вершина:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent) |&gt; \n  head(5)\n[1] “Erant omnino itinera duo, quibus itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:” [4] “Ita sive casu sive consilio deorum immortalium quae pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\n\nТак мы кое-что полезное поймали, но не все, потому что между местоимением и его антецедентом возможны другие слова (например, “каковыми опасными путями”). С другой стороны, есть и кое-что лишнее, а именно случаи инкорпорации антецедента в придаточное предложение (“quae pars …, ea” вместо “ea pars, quae…” ). В общем, условие можно дальше дорабатывать, но мы пока не будем этого делать.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "pmi.html",
    "href": "pmi.html",
    "title": "12  Векторные представления слов",
    "section": "",
    "text": "12.1 Векторы в лингвистике\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Есть несколько возможных решений.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравнивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход – зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы, или эмбеддинги. Для этого к исходной матрице применяются различные методы снижения размерности.\nПодробнее о векторных моделях можено почитать статью В. Селеверстова на “Системном блоке”, а также посмотреть видео с лекцией Д. Рыжовой.\nВ этом уроке мы изучим несколько способов пострения эмбеддингов:\nТакже мы поговорим об использовании готовых (предобученных) эмбеддингов для разных языков.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(stopwords)\nlibrary(widyr)\nlibrary(uwot)\nlibrary(text)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "pmi.html#что-такое-pmi",
    "href": "pmi.html#что-такое-pmi",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.2 Что такое PMI",
    "text": "13.2 Что такое PMI\nОбычная мера ассоциации между словами, которой пользуются лингвисты, — точечная взаимная информация, или PMI (pointwise mutual information). Она рассчитывается по формуле:\n\\[PMI\\left(x;y\\right)=\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}}\\]\nВ числителе — вероятность встретить два слова вместе (например, в пределах одного документа или одного «окна» длинной n слов). В знаменателе — произведение вероятностей встретить каждое из слов отдельно. Если слова чаще встречаются вместе, логарифм будет положительным; если по отдельности — отрицательным.\nПосчитаем PMI на наших данных, воспользовавшись подходящей функцией из пакета widyr.\n\nlibrary(widyr)\nnews_pmi  &lt;- news_windows  |&gt; \n  pairwise_pmi(token, window_id)\n\n\nnews_pmi |&gt; \n  arrange(-abs(pmi))",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#почему-ppmi",
    "href": "pmi.html#почему-ppmi",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.3 Почему PPMI",
    "text": "13.3 Почему PPMI\nВ отличие от коэффициента корреляции, например, PMI может варьироваться от \\(-\\infty\\) до \\(+\\infty\\), но негативные значения проблематичны. Они означают, что вероятность встретить эти два слова вместе меньше, чем мы бы ожидали в результате случайного совпадения. Проверить это без огромного корпуса невозможно: если у нас есть \\(w_1\\) и \\(w_2\\), каждое из которых встречается с вероятностью \\(10^{-6}\\), то трудно удостовериться в том, что \\(p(w_1, w_2)\\) значимо отличается от \\(10^{-12}\\). Поэтому негативные значения PMI принято заменять нулями. В таком случае формула выглядит так:\n\\[ PMI\\left(x;y\\right)=max(\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}},0) \\] Для подобной замены подойдет векторизованное условие.\n\nnews_ppmi &lt;- news_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi)) \n\nnews_ppmi |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\nЕсли мы развернем такую матрицу вширь, то она получится очень разреженной; чтобы получить плотные векторы слов, необходимо прибегнуть к SVD.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#svd-на-матрице-с-ppmi",
    "href": "pmi.html#svd-на-матрице-с-ppmi",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.4 SVD на матрице с PPMI",
    "text": "13.4 SVD на матрице с PPMI\nДля этого можно передать тиббл фунции widely_svd() для вычисления сингулярного разложения. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов:\n\nword_emb &lt;- news_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n\n\nword_emb",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#визуализация-топиков",
    "href": "pmi.html#визуализация-топиков",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.5 Визуализация топиков",
    "text": "13.5 Визуализация топиков\nСнова визуализируем главные компоненты нашего векторного пространства.\n\nword_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#ближайшие-соседи",
    "href": "pmi.html#ближайшие-соседи",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.6 Ближайшие соседи",
    "text": "13.6 Ближайшие соседи\nИсследуем наши эмбеддинги, используя уже знакомую функцию, которая считает косинусное сходство между словами.\n\nsource(\"../helper_scripts/nearest_neighbors.R\")\n\n\nword_emb |&gt; \n  nearest_neighbors(\"сборная\")\n\n\n  \n\n\nword_emb |&gt; \n  nearest_neighbors(\"завод\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#от-эмбеддингов-слов-к-эмбеддингам-документов",
    "href": "pmi.html#от-эмбеддингов-слов-к-эмбеддингам-документов",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.7 От эмбеддингов слов к эмбеддингам документов",
    "text": "13.7 От эмбеддингов слов к эмбеддингам документов\nИспользуя документы как суммы входящих в них слов, мы может использовать эмбеддинги для нахождения ближайших документов в корпусе.\nПервая матрица хранит информацию о встречаемости слов в документах; в рядах здесь документы; в столбцах – слова (всего 6299).\n\ncounts_mx &lt;- news_tokens_pruned %&gt;%\n  count(id, token) %&gt;%\n  cast_sparse(id, token, n)\ndim(counts_mx)\n\n[1] 3407 6299\n\n\nВторая матрица – это наши эмбеддинги в разреженном виде. Число столбцов соответствует числу сингулярных векторов, которое мы задали при факторизации.\n\nembedding_mx &lt;- word_emb %&gt;%\n  cast_sparse(word, dimension, value)\ndim(embedding_mx)\n\n[1] 6299  100\n\n\nПеремножение матрицы \\((3407 \\times 6299)\\) на \\((6299 \\times 20)\\) вернет матрицу размером \\((3407 \\times 20)\\), то есть мы получим эмбеддинги для документов.\n\ndoc_mx &lt;- counts_mx %*% embedding_mx\ndim(doc_mx)\n\n[1] 3407  100",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "pmi.html#похожие-документы",
    "href": "pmi.html#похожие-документы",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.8 Похожие документы",
    "text": "13.8 Похожие документы\nПревратим разреженную матрицу обратно в датафрейм.\n\ndoc_emb_long &lt;- doc_mx |&gt; \n  as.matrix() |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\")\n\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nПознакомимся с соседями.\n\nload(\"../data/news.Rdata\")\nnews_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number())) |&gt; \n  filter(id %in% c(\"doc14\", \"doc1\", \"doc1000\", \"doc1142\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nРелевантная выдача здесь – только первый документ. С этой задачей LSA в нашем случае справилась лучше. Посмотрим теперь на семантическое пространство слов. Для этого придется снизить размерность со 100 до 2 измерений.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "pmi.html#d-визуализации-пространства-слов",
    "href": "pmi.html#d-визуализации-пространства-слов",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.7 2D-визуализации пространства слов",
    "text": "13.7 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы снова используем алгоритм UMAP.\n\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🏈 Теперь попробуем построить векторное пространство с использованием поверхностных нейросетей.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#математическое-волшебство",
    "href": "pmi.html#математическое-волшебство",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.10 Математическое волшебство",
    "text": "13.10 Математическое волшебство\nЕсли слова – это векторы, то их можно складывать и вычитать, как мы это делали в школе. Согласно известному примеру, если из вектора слова “король” вычесть вектор “мужчина” и прибавить вектор “женщина”, получатся числа, соответствующие слову “королева”. Правда, у нас слишком небольшой датасет, так что на озарения лучше не рассчитывать.\n\nlibrary(plyr)\n\nmystery_word &lt;- unrowname(word_emb_mx[\"спортсмен\",] + word_emb_mx[\"мяч\",]) \n\nhead(mystery_word)\n\n          1           2           3           4           5           6 \n 0.02899943 -0.01946437  0.12604935  0.02450570 -0.03578517 -0.01520623 \n\n\n\nmystery_tbl &lt;- tibble(\n  word = \"mystery\",\n  dimension = as.numeric(names(mystery_word)),\n  value = as.numeric(mystery_word)\n)\n\n\nword_emb_new &lt;- word_emb |&gt; \n  bind_rows(mystery_tbl)\n\nnearest_neighbors(word_emb_new, \"mystery\")\n\n\n  \n\n\n\nОтличная работа 🏈",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "pmi.html#сглаженная-pmi-dpf",
    "href": "pmi.html#сглаженная-pmi-dpf",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.11 Сглаженная PMI (DPF)",
    "text": "13.11 Сглаженная PMI (DPF)\nКак уже было сказано, для редких слов PMI оказывается завышена: т.к. вероятность встретить их в корпусе очень мала, знаменатель в формуле PMI тоже уменьшается. Иными словами, существует негативная корреляция между частотностью слова и его PMI.\nЕсть несколько способов если не совсем избавиться от негативной корреляции, то по крайней мере уменьшить ее абсолютное значение. Один из них заключается в том, чтобы чуть завысить знаменатель в формуле PMI, например, возведя его в дробную степень. Такой подход применялся в недавнем исследовании по цифровой истории идей, авторы которого предлагают использовать меру под названием DPF (Distributional Probability Factor). Показатель степени α при этом устанавливается в районе α = 0.75.\n\\[ DPF\\ \\left(x;y\\right)=\\frac{P\\left(x,y\\right)}{(P\\left(x\\right)P{\\left(y\\right))}^\\alpha} \\]\nВторой способ известен как сглаживание Лапласа, оно же аддитивное сглаживание. В этом случае мы прибавляем единицу к частоте каждого слова, как будто видели его на один раз больше (этот способ задействуется и в некоторых алгоритмах машинного обучения).\nПрименять сглаживание функция pairwise_pmi() не умеет, поэтому на этот раз мы посчитаем взаимную информацию чуть иначе.\n\n# вероятность для каждого слова\nunigram_probs &lt;- news_tokens_pruned  |&gt; \n  dplyr::count(token, sort = TRUE)  |&gt; \n  mutate(p = n / sum(n))\n\nunigram_probs\n\n\n  \n\n\n# вероятность встретить слова вместе\nbigram_probs &lt;- news_tokens_pruned  |&gt; \n  pairwise_count(token, id, diag = TRUE, sort = TRUE)  |&gt; \n  mutate(p = n / sum(n))\n\nbigram_probs |&gt; \n  arrange(-n) # на этом этапе можно отфильтровать n &lt; 2\n\n\n  \n\n\n\nПосчитаем нормализованную вероятность: это вероятность встретить два слова вместе, деленная на произведение вероятностей встретить каждое из них отдельно.\n\n# взаимная информация\nnormalized_probs &lt;- bigram_probs  |&gt; \n  # filter(n &gt; 4)  |&gt; \n  dplyr::rename(word1 = item1, word2 = item2)  |&gt; \n  left_join(unigram_probs  |&gt; \n              select(word1 = token, p1 = p),\n            by = \"word1\")  |&gt; \n  left_join(unigram_probs %&gt;%\n              select(word2 = token, p2 = p),\n            by = \"word2\")  |&gt; \n  mutate(p_together = p / p1 / p2)\n\nnormalized_probs\n\n\n  \n\n\n\nРассчитаем заново PMI и добавим сглаживание.\n\n# pmi & dpf\npmi_data &lt;- normalized_probs |&gt; \n  mutate(pmi = log(p_together)) |&gt; \n  mutate(dpf = p / (p1 * p2)^0.75) |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi))\n\npmi_data |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\nСравним зависимость от частотности в трех случаях.\n\nlibrary(ggpubr)\n# корреляция\npmi_data |&gt;\n  filter(word1 == \"футболист\") |&gt; \n  filter(!word1 == word2) |&gt; \n  select(word2, pmi, ppmi, dpf, p2) |&gt; \n  pivot_longer(cols = c(pmi, dpf, ppmi), names_to = \"measure\", values_to = \"value\") |&gt; \n  ggplot(aes(p2, value, color = value)) +\n   geom_jitter(width = 0.0002, height = 0.0002, \n              show.legend = FALSE, alpha = 0.5) +\n  xlim(NA, 0.0045) + # zoom in\n  facet_wrap(~ measure, scales = \"free_y\") +\n  stat_cor(aes(label = ..r.label..),\n           method = \"pearson\",\n           label.x = 0.002,\n           color = \"tomato\",\n           geom = \"label\"\n  ) +\n  theme_bw() +\n  labs(title = \"Корреляция между частотностью слова и PMI / DPF\", \n       y = NULL)\n\n\n\n\n\n\n\n\nАналогично тому, что мы делали выше, можно снизить размерность DPF-матрицы при помощи SVD или же сохранить ее в разреженном виде для изучения совместной встречаемости слов.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "embeddings.html",
    "href": "embeddings.html",
    "title": "12  Векторные представления слов",
    "section": "",
    "text": "12.1 Векторы в лингвистике\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Есть несколько возможных решений.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравнивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход – зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности.\nВ этом уроке мы рассмотрим алгоритм LSA и векторные модели на основе PMI. В первом случае анализируется матрица термин-документ, во втором – матрица термин-термин. Оба подхода предполагают использование SVD.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#svd",
    "href": "embeddings.html#svd",
    "title": "12  Векторные представления слов",
    "section": "12.2 SVD",
    "text": "12.2 SVD\nДля любых текстовых данных и матрица термин-термин и матрица термин-документ будет очень разряженной (то есть большая часть значений будет равна нулю). Необходимо “переупорядочить” ее так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD). При сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы (К. Маннинг, П. Рагхаван, Х. Шютце 2020, 407).\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. О сингулярных векторах можно думать как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.\nПосмотрим теперь, как SVD применяется при анализе текста.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#латентно-семантический-анализ",
    "href": "embeddings.html#латентно-семантический-анализ",
    "title": "12  Векторные представления слов",
    "section": "12.3 Латентно-семантический анализ",
    "text": "12.3 Латентно-семантический анализ\nLSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) – это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками). Слово “latent” (англ. “скрытый”) в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить.\nСоздатели метода LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов представляет собой своего рода сумму (или среднее) отдельных слов. Этот принцип является общим для всех векторных моделей.\nНа входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово “дорожное”, например, служит лучшим показателем темы, чем слово “происшествие”, которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции. Общий принцип действия алгоритма подробно объясняется на очень простом примере по ссылке; мы же перейдем сразу к анализу реальных данных.\n\n12.3.1 Подгтовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для простоты возьмем из него лишь один год). Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nlibrary(tidyverse)\nload(\"../data/news.Rdata\")\n\nnews_2019 |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\nДобавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\n\nWarning in rm(news_tokens): object 'news_tokens' not found\n\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\n\n\n12.3.2 TF-IDF: опрятный подход\nВместо показателей абсолютной встречаемости при анализе больших текстовых данных применяется tf-idf. Эта статистическая мера не используется, если дана матрица термин-термин, но она хорошо работает с матрицами термин-документ, позволяя повысить веса для тех слов, которые служат хорошими дискриминаторами. Например, “заявил” и “отметил”, хотя это не стоп-слова, могут встречаться в разных темах.\n\nnews_counts &lt;- news_tokens_pruned |&gt;\n  count(token, id)\n\nnews_counts\n\n\nnews_counts |&gt; \n  arrange(id)\n\n\n  \n\n\n\nДобавляем tf_idf.\n\nnews_tf_idf &lt;- news_counts |&gt; \n  bind_tf_idf(token, id, n) |&gt; \n  arrange(tf_idf) |&gt; \n  select(-n, -tf, -idf)\n\nnews_tf_idf\n\n\n\n\n  \n\n\n\n\n\n12.3.3 DocumentTermMatrix\nПосмотрим на размер получившейся таблицы.\n\nobject.size(news_tf_idf)\n\n5369712 bytes\n\nformat(object.size(news_tf_idf), units = \"auto\")\n\n[1] \"5.1 Mb\"\n\n\nЧтобы вычислить SVD, такую таблицу необходимо преобразовать в матрицу термин-документ. Оценим ее размер:\n\n# число уникальных токенов\nm &lt;- unique(news_tf_idf$token) |&gt; \n  length()\nm\n\n[1] 6299\n\n# число уникальных документов\nn &lt;- unique(news_tf_idf$id) |&gt; \n  length()  \nn\n\n[1] 3407\n\n# число элементов в матрице \nm * n\n\n[1] 21460693\n\n\nИспользуем специальный формат для хранения разреженных матриц.\n\ndtm &lt;- news_tf_idf |&gt; \n  cast_sparse(token, id, tf_idf)\n\n\n# первые 10 рядов и 5 столбцов\ndtm[1:10, 1:5]\n\n10 x 5 sparse Matrix of class \"dgCMatrix\"\n               doc608     doc1670     doc2170     doc2184     doc2219\nранее     0.003530193 .           .           0.005002585 .          \nроссии    0.010127611 0.003675658 0.004689633 0.004783897 0.005471238\nсловам    .           .           0.011776384 .           0.006869557\nрублей    0.006686328 .           0.027865190 .           .          \nрассказал 0.007250151 .           .           0.010274083 .          \nданным    0.007406320 .           .           .           0.012003345\nиздание   0.007759457 .           .           .           0.012575671\nходе      0.008675929 .           .           .           .          \nзаявил    0.016729327 .           .           .           .          \nчастности 0.008860985 .           0.012309349 .           .          \n\n\nСнова уточним размер матрицы.\n\nformat(object.size(dtm), units = \"auto\")\n\n[1] \"3 Mb\"\n\n\n\n\n12.3.4 SVD с пакетом irlba\nМетод для эффективного вычисления усеченного SVD на больших матрицах реализован в пакете irlba. Возможно, придется подождать ⏳.\n\nlibrary(irlba)\nlsa_space&lt;- irlba::irlba(dtm, 50) \n\nФункция вернет список из трех элементов:\n\nd: k аппроксимированных сингулярных значений;\nu: k аппроксимированных левых сингулярных векторов;\nv: k аппроксимированных правых сингулярных векторов.\n\nПолученную LSA-модель можно использовать для поиска наиболее близких слов и документов или для изучения тематики корпуса – в последнем случае нас может интересовать, какие топики доминируют в тех или иных документах и какими словами они в первую очередь представлены.\n\n\n12.3.5 Эмбеддинги слов\nВернем имена рядов матрице левых сингулярных векторов и добавим имена столбцов.\n\nrownames(lsa_space$u) &lt;- rownames(dtm)\ncolnames(lsa_space$u) &lt;- paste0(\"dim\", 1:50)\n\nТеперь посмотрим на эмбеддинги слов.\n\nword_emb &lt;- lsa_space$u |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"word\") |&gt; \n  as_tibble()\n\nword_emb\n\n\n  \n\n\n\nПреобразуем наши данные в длинный формат.\n\nword_emb_long &lt;- word_emb |&gt; \n  pivot_longer(-word, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\nword_emb_long\n\n\n  \n\n\n\n\n\n12.3.6 Визуализация топиков\nВизуализируем несколько топиков, чтобы понять, насколько они осмыслены.\n\nword_emb_long |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n12.3.7 Ближайшие соседи\nЭмбеддинги можно использовать для поиска ближайших соседей.\n\nlibrary(widyr)\n\nnearest_neighbors &lt;- function(df, feat, doc=F) {\n  inner_f &lt;- function() {\n    widely(\n        ~ {\n          y &lt;- .[rep(feat, nrow(.)), ]\n          res &lt;- rowSums(. * y) / \n            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))\n          \n          matrix(res, ncol = 1, dimnames = list(x = names(res)))\n        },\n        sort = TRUE\n    )}\n  if (doc) {\n    df |&gt; inner_f()(doc, dimension, value) }\n  else {\n    df |&gt; inner_f()(word, dimension, value)\n  } |&gt; \n    select(-item2)\n}\n\n\nnearest_neighbors(word_emb_long, \"сборная\")\n\n\n  \n\n\nnearest_neighbors(word_emb_long, \"завод\")\n\n\n  \n\n\n\n\n\n12.3.8 Похожие документы\nИнформация о документах хранится в матрице правых сингулярных векторов.\n\nrownames(lsa_space$v) &lt;- colnames(dtm)\ncolnames(lsa_space$v) &lt;- paste0(\"dim\", 1:50)\n\nПосмотрим на эмбеддинги документов.\n\ndoc_emb &lt;- lsa_space$v |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  as_tibble()\n\ndoc_emb\n\n\n  \n\n\n\nПреобразуем в длинный формат.\n\ndoc_emb_long &lt;- doc_emb |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\ndoc_emb_long\n\n\n  \n\n\n\nИ найдем соседей для произвольного документа.\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nВыведем документ 14 вместе с его соседями.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc14\", \"doc392\", \"doc2043\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nПоздравляем, вы построили свою первую рекомендательную систему 🍸.\n\n\n12.3.9 2D-визуализация пространства документов\nДля снижения размерности мы используем алгоритм UMAP. В отличие от PCA, он снижает размерность нелинейно, и в этом отношении похож на t-SNE.\n\nlibrary(uwot)\nset.seed(07062024)\nviz_lsa &lt;- umap(lsa_space$v ,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все документы вложены теперь в двумерное пространство.\n\ndim(viz_lsa)\n\n[1] 3407    2\n\n\nЗакодировав цветом рубрики новостного сайта, нанесем документы на “карту”.\n\n\ntibble(doc = rownames(viz_lsa),\n       topic = news_2019$topic,\n       V1 = viz_lsa[, 1], \n       V2 = viz_lsa[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = doc, color = topic)) + \n  geom_text(size = 2, alpha = 0.8, position = position_jitter(width = 1.5, height = 1.5)) +\n  annotate(geom = \"rect\", ymin = -10, ymax = 0, xmin = -20, xmax = -10, alpha = 0.2, color = \"tomato\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nВот несколько новостей из небольшого тематического кластера, выделенного на карте квадратом.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc718\", \"doc2437\", \"doc2918\")) |&gt; \n  mutate(text = str_trunc(text, 70))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#эмбеддинги-на-основе-pmi-матрицы",
    "href": "embeddings.html#эмбеддинги-на-основе-pmi-матрицы",
    "title": "12  Векторные представления слов",
    "section": "12.4 Эмбеддинги на основе PMI-матрицы",
    "text": "12.4 Эмбеддинги на основе PMI-матрицы\nТеперь рассмотрим второй способ построения эмбеддингов, когда за основу берется матрица термин-термин.\n\n12.4.1 Скользящее окно\nПрежде всего разделим новости на контекстные окна фиксированной величины. Чем меньше окно, тем больше синтаксической информации оно хранит.\n\nlibrary(tidyr)\n\nnested_news &lt;- news_tokens_pruned |&gt; \n  dplyr::select(-topic) |&gt; \n  nest(tokens = c(token))\n\nnested_news\n\n\n  \n\n\n\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %&gt;%\n    transpose() %&gt;%\n    pluck(\"result\") %&gt;%\n    compact() %&gt;%\n    bind_rows()\n}\n\nДеление на окна может потребовать нескольких минут. Чем больше окно, тем больше потребуется времени и тем больше будет размер таблицы.\n\nnews_windows &lt;- nested_news |&gt; \n  mutate(tokens = map(tokens, slide_windows, 10L)) %&gt;% \n  unnest(tokens) %&gt;% \n  unite(window_id, id, window_id)\n\nnews_windows\n\n\nload(\"../data/news_windows.Rdata\")\n\n\n\n12.4.2 Что такое PMI\nОбычная мера ассоциации между словами, которой пользуются лингвисты, — точечная взаимная информация, или PMI (pointwise mutual information). Она рассчитывается по формуле:\n\\[PMI\\left(x;y\\right)=\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}}\\]\nВ числителе — вероятность встретить два слова вместе (например, в пределах одного документа или одного «окна» длинной n слов). В знаменателе — произведение вероятностей встретить каждое из слов отдельно. Если слова чаще встречаются вместе, логарифм будет положительным; если по отдельности — отрицательным.\nПосчитаем PMI на наших данных, воспользовавшись подходящей функцией из пакета widyr.\n\nlibrary(widyr)\nnews_pmi  &lt;- news_windows  |&gt; \n  pairwise_pmi(token, window_id)\n\n\nnews_pmi |&gt; \n  arrange(-abs(pmi))\n\n\n  \n\n\n\n\n\n12.4.3 Почему PPMI\nВ отличие от коэффициента корреляции, например, PMI может варьироваться от \\(-\\infty\\) до \\(+\\infty\\), но негативные значения проблематичны. Они означают, что вероятность встретить эти два слова вместе меньше, чем мы бы ожидали в результате случайного совпадения. Проверить это без огромного корпуса невозможно: если у нас есть \\(w_1\\) и \\(w_2\\), каждое из которых встречается с вероятностью \\(10^{-6}\\), то трудно удостовериться в том, что \\(p(w_1, w_2)\\) значимо отличается от \\(10^{-12}\\). Поэтому негативные значения PMI принято заменять нулями. В таком случае формула выглядит так:\n\\[ PMI\\left(x;y\\right)=max(\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}},0) \\] Для подобной замены подойдет векторизованное условие.\n\nnews_ppmi &lt;- news_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi)) \n\nnews_ppmi |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\nЕсли мы развернем такую матрицу вширь, то она получится очень разреженной; чтобы получить плотные векторы слов, необходимо прибегнуть к SVD.\n\n\n12.4.4 SVD на матрице с PPMI\nДля этого можно передать тиббл фунции widely_svd() для вычисления сингулярного разложения. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов:\n\nword_emb &lt;- news_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n\n\nword_emb\n\n\n  \n\n\n\n\n\n12.4.5 Визуализация топиков\nСнова визуализируем главные компоненты нашего векторного пространства.\n\nword_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n12.4.6 Ближайшие соседи\nИсследуем наши эмбеддинги, используя уже знакомую функцию, которая считает косинусное сходство между словами.\n\nsource(\"../helper_scripts/nearest_neighbors.R\")\n\n\nword_emb |&gt; \n  nearest_neighbors(\"сборная\")\n\n\n  \n\n\nword_emb |&gt; \n  nearest_neighbors(\"завод\")\n\n\n  \n\n\n\n\n\n12.4.7 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы снова используем алгоритм UMAP.\n\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🏈 Позже мы научимся строить векторное пространство с использованием поверхностных нейросетей.\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#d-визуализации-пространства-слов",
    "href": "embeddings.html#d-визуализации-пространства-слов",
    "title": "12  Векторные представления слов",
    "section": "12.5 2D-визуализации пространства слов",
    "text": "12.5 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы используем алгоритм UMAP. В отличие от PCA, он снижает размерность нелинейно, и в этом отношении похож на t-SNE.\n\nlibrary(uwot)\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все наши слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🏈 Позже мы научимся строить векторное пространство с использованием поверхностных нейросетей.\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lda.html",
    "href": "lda.html",
    "title": "14  Тематическое моделирование c LDA",
    "section": "",
    "text": "14.1 Что такое LDA\nТематическое моделирование — семейство методов обработки больших коллекций текстовых документов. Эти методы позволяют определить, к каким темам относится каждый документ и какие слова образуют каждую тему.\nОдним из таких методов является Латентное размещение Дирихле (Latent Dirichlet Allocation, LDA). Это вероятностная модель, которая позволяет выявить заданное количество тем в корпусе. В основе метода лежит предположение о том, что каждый документ представляет собой комбинацию ограниченного числа топиков (тем), а каждый топик — это распределение вероятностей для слов. При этом, как и в естественном языке, документы могут перекрывать друг друга по темам, а темы — по словам. Например, слово «мяч» может быть связано не только со спортивным топиком, но и, например, с политическим («клятва в зале для игры в мяч»).\nСоздатели метода поясняют это на примере публикации из журнала Science.\nНа картинке голубым выделена тема «анализ данных»; розовым — «эволюционная биология», а желтым — «генетика». Если разметить все слова в тексте (за исключением «шумовых», таких как союзы, артикли и т.п.), то можно увидеть, что документ представляет собой сочетание нескольких тем. Цветные «окошки» слева — это распределение вероятностей для слов в теме. Гистограмма справа — это распределение вероятностей для тем в документе. Все документы в коллекции представляют собой сочетание одних и тех же тем — но в разной пропорции. Например, в этом примере почти нет зеленого «текстовыделителя», что хорошо видно на гистограмме.\nАссоциацию тем с документами, с одной стороны, и слов с темами, с другой, и рассчитывает алгоритм. При этом LDA относится к числу методов обучения без учителя (unsupervised), то есть не требует предварительной разметки корпуса: машина сама «находит» скрытые в корпусе темы и аннотирует каждый документ. Это делает метод востребованным в тех случаях, когда мы сами точно не знаем, что ищем — например, в исследованиях электронных архивов.\nСложность при построении модели обычно заключается в том, чтобы установить оптимальное число тем: для этого предлагались различные количественные метрики, но важнейшим условием является также интерпретируемость результата. Единственно правильного решения здесь нет: например, моделируя архив газетных публикаций, мы можем подобрать темы так, чтобы они примерно соответствовали рубрикам («спорт», «политика», «культура» и т.п.), но в некоторых случаях бывает полезно сделать zoom in, чтобы разглядеть отдельные сюжеты (например, «фигурное катание» и «баскетбол» в спортивной рубрике…)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#что-такое-lda",
    "href": "lda.html#что-такое-lda",
    "title": "14  Тематическое моделирование c LDA",
    "section": "",
    "text": "Источник: Blei, D. M. (2012), Probabilistic topic models",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#распределение-дирихле",
    "href": "lda.html#распределение-дирихле",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.2 Распределение Дирихле",
    "text": "14.2 Распределение Дирихле\nМатематические и статистические основания LDA достаточно хитроумны; общие принципы на русском языке хорошо изложены в статье “Как понять, о чем текст, не читая его” на сайте “Системный блок”, а лучшее объяснение на английском языке можно найти здесь и здесь.\n\nАльфа и бета на этой схеме - гиперпараметры распределения Дирихле. Гиперпараметры регулируют распределения документов по темам и тем по словам. Наглядно это можно представить так (при числе тем &gt; 3 треугольник превращается в n-мерный тетраэдр):\n\n\n\n\n\nПри α = 1 получается равномерное распределение: темы распределены равномерно (поэтому α называют “параметром концентрации”). При значениях α &gt; 1 выборки начинают концентрироваться в центре треугольника, представляя собой равномерную смесь всех тем. При низких значениях альфа α &lt; 1 большинство наблюдений находится в углах – скорее всего, в в этом случае в документах будет меньше смешения тем.\nРаспределение документов по топикам θ зависит от значения α; из θ выбирается конкретная тема Z. Аналогичным образом гиперпараметр β определяет связь тем со словами. Чем выше бета, тем с большим числом слов связаны темы. При меньших значениях беты темы меньше похожи друг на друга. Конкретное слово W “выбирается” из распределения слов φ в теме Z.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#подтоговка-данных",
    "href": "lda.html#подтоговка-данных",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.3 Подтоговка данных",
    "text": "14.3 Подтоговка данных\nЧтобы понять возможности алгоритма, мы попробуем передать ему тот же новостной архив: на новостях сразу видно адекватность модели; но это не значит, что применение LDA ограничено подобными сюжетами. Этот метод с успехом применяется, например, в историко-научных или литературоведческих исследованиях. Он хорошо подходит, если необходимо на основе журнального архива описать развитие некоторой области знания. Но сейчас нам подойдет пример попроще 👶\n\nlibrary(tidyverse)\nload(\"../data/news_tokens_pruned.Rdata\")\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПоскольку LDA – вероятностная модель, то на входе она принимает целые числа. В самом деле, не имеет смысла говорить о том, что некое распределение породило 0.5 слов или того меньше. Поэтому мы считаем абсолютную, а не относительную встречаемость – и не tf_idf.\n\nnews_counts &lt;- news_tokens_pruned |&gt; \n  count(token, id)\n\nnews_counts",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#матрица-встречаемости",
    "href": "lda.html#матрица-встречаемости",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.4 Матрица встречаемости",
    "text": "14.4 Матрица встречаемости\nДля работы с LDA в R устанавливаем пакет topicmodels. На входе нужная нам функция этого пакета принимает такую структуру данных, как document-term matrix (dtm), которая используется для хранения сильно разреженных данных и происходит из популярного пакета для текст-майнинга tm.\nПоэтому “тайдифицированный” текст придется для моделирования преобразовать в этот формат, а полученный результат вернуть в опрятный формат для визуализаций.\nДля преобразования подготовленного корпуса в формат dtm воспользуемся возможностями пакета tidytext:\n\nlibrary(tidytext)\n\nnews_dtm &lt;- news_counts |&gt; \n  cast_dtm(id, term = token, value = n)\n\nnews_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 3407, terms: 6299)&gt;&gt;\nNon-/sparse entries: 196774/21263919\nSparsity           : 99%\nMaximal term length: 20\nWeighting          : term frequency (tf)\n\n\nУбеждаемся, что почти все ячейки в нашей матрице – нули (99-процентная разреженность).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#оценка-perplexity",
    "href": "lda.html#оценка-perplexity",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.5 Оценка perplexity",
    "text": "14.5 Оценка perplexity\nКоличество тем для модели LDA задается вручную. Здесь на помощь приходит функция perplexity() из topicmodels. Она показывает, насколько подогнанная модель не соответствует данным – поэтому чем значение меньше, тем лучше.\nПодгоним сразу несколько моделей с разным количеством тем и посмотрим, какая из них покажет себя лучше. Чтобы ускорить дело, попробуем запараллелить вычисления.\n\nlibrary(topicmodels)\nlibrary(furrr)\n\nplan(multisession, workers = 6)\n\nn_topics &lt;- c(2, 4, 8, 16, 32, 64)\nnews_lda_models &lt;- n_topics  |&gt; \n  future_map(topicmodels::LDA, x = news_dtm, \n      control = list(seed = 0211), .progress = TRUE)\n\n\ndata_frame(k = n_topics,\n           perplex = map_dbl(news_lda_models, perplexity))  |&gt; \n  ggplot(aes(k, perplex)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Оценка LDA модели\",\n       x = \"Число топиков\",\n       y = \"Perplexity\")\n\n\nЕсли верить графику, предпочтительны 32 темы или больше. Посмотрим, сколько тем задано редакторами вручную.\n\nnews_tokens_pruned |&gt; \n  count(topic) |&gt; \n  arrange(-n)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#выбор-числа-тем-с-ldatuning",
    "href": "lda.html#выбор-числа-тем-с-ldatuning",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.6 Выбор числа тем с ldatuning",
    "text": "14.6 Выбор числа тем с ldatuning\nЕще одну возможность подобрать оптимальное число тем предлагает пакет ldatuning. Снова придется подождать.\n\nlibrary(ldatuning)\n\nresult &lt;- FindTopicsNumber(\n  news_dtm,\n  topics = n_topics,\n  metrics = c(\"Griffiths2004\", \"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 05092024),\n  verbose = TRUE\n)\n\n\nresult\n\n\n  \n\n\n\n\nFindTopicsNumber_plot(result)\n\n\n\n\n\n\n\n\nЭтот график тоже говорит о том, что модель требует не меньше 32 тем.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#модель-lda",
    "href": "lda.html#модель-lda",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.7 Модель LDA",
    "text": "14.7 Модель LDA\n\nnews_lda &lt;- topicmodels::LDA(news_dtm, k = 32, control = list(seed = 05092024))\n\nТеперь наша тематическая модель готова. Ее можно скачать в формате .Rdata отсюда; это примерно 2.5 Mb.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#слова-и-темы",
    "href": "lda.html#слова-и-темы",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.8 Слова и темы",
    "text": "14.8 Слова и темы\nПакет tidytext дает возможность “тайдифицировать” объект lda с использованием разных методов. Метод β (“бета”) показывает связь топиков с отдельными словами.\n\nnews_topics &lt;- tidy(news_lda, matrix = \"beta\")\n\nnews_topics |&gt; \n  filter(term == \"чай\")  |&gt;  \n  arrange(-beta)\n\n\n  \n\n\n\nНапример, слово “чай” с большей вероятностью порождено темой 22, чем остальными темами 🍵\nПосмотрим на главные термины в первых девяти топиках.\n\nnews_top_terms &lt;- news_topics |&gt; \n  filter(topic &lt; 10) |&gt; \n  group_by(topic) |&gt; \n  arrange(-beta) |&gt; \n  slice_head(n = 12) |&gt; \n  ungroup()\n\nnews_top_terms\n\n\n  \n\n\n\n\nnews_top_terms |&gt; \n  mutate(term = reorder(term, beta)) |&gt; \n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) + \n  facet_wrap(~ topic, scales = \"free\", ncol=3) +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#сравнение-топиков",
    "href": "lda.html#сравнение-топиков",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.9 Сравнение топиков",
    "text": "14.9 Сравнение топиков\nСравним два топика по формуле: \\(log_2\\left(\\frac{β_2}{β_1}\\right)\\). Если \\(β_2\\) в 2 раза больше \\(β_1\\), то логарифм будет равен 1; если наоборот, то -1. На всякий случай напомним: \\(\\frac{1}{2} = 2^{-1}\\).\nДля подсчетов снова придется трансформировать данные.\n\nbeta_wide &lt;- news_topics |&gt; \n  filter(topic %in% c(5, 7)) |&gt; \n  mutate(topic = paste0(\"topic_\", topic)) |&gt; \n  pivot_wider(names_from = topic, values_from = beta) |&gt; \n  filter(topic_5 &gt; 0.001  | topic_7 &gt; 0.001)  |&gt; \n  mutate(log_ratio = log2(topic_7 / topic_5))\n\nbeta_wide\n\n\n  \n\n\n\nНа графике выглядит понятнее:\n\nbeta_log_ratio &lt;- beta_wide  |&gt; \n  filter(!log_ratio %in% c(-Inf, Inf, 0)) |&gt; \n  mutate(sign = case_when(log_ratio &gt; 0 ~ \"pos\",\n                          log_ratio &lt; 0 ~ \"neg\"))  |&gt; \n  select(-topic_5, -topic_7) |&gt; \n  group_by(sign) |&gt; \n  arrange(desc(abs(log_ratio))) |&gt; \n  slice_head(n = 12)\n\n\nbeta_log_ratio |&gt; \n  ggplot(aes(reorder(term, log_ratio), log_ratio, fill = sign)) +\n  geom_col(show.legend = FALSE) +\n  xlab(\"термин\") +\n  ylab(\"log2 (beta_7 / beta_5)\") +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#темы-и-документы",
    "href": "lda.html#темы-и-документы",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.10 Темы и документы",
    "text": "14.10 Темы и документы\nРаспределение тем по документам хранит матрица gamma.\n\nnews_documents &lt;- tidy(news_lda, matrix = \"gamma\")\n\nnews_documents |&gt; \n  filter(topic == 1) |&gt; \n  arrange(-gamma)\n\n\n  \n\n\n\nЗначение gamma можно понимать как долю слов в документе, происходящую из данного топика, при этом каждый документ в рамках LDA рассматривается как собрание всех тем. Значит, сумма всех гамм для текста должна быть равна единице. Проверим.\n\nnews_documents |&gt; \n  group_by(document) |&gt; \n  summarise(sum = sum(gamma))\n\n\n  \n\n\n\nВсе верно!\nТеперь отберем несколько новостей и посмотрим, какие топики в них представлены.\n\nlong_news_id &lt;- news_tokens_pruned  |&gt; \n  group_by(id) |&gt; \n  summarise(nwords = n()) |&gt; \n  arrange(-nwords) |&gt; \n  slice_head(n = 4) |&gt; \n  pull(id)\n\nlong_news_id\n\n[1] \"doc608\"  \"doc1670\" \"doc389\"  \"doc2200\"\n\n\n\nnews_documents |&gt; \n  filter(document  %in%  long_news_id) |&gt; \n  arrange(-gamma) |&gt; \n  ggplot(aes(as.factor(topic), gamma, color = document)) + \n  geom_boxplot(show.legend = F) +\n  facet_wrap(~document, nrow = 2) +\n  xlab(NULL)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#распределения-вероятности-для-топиков",
    "href": "lda.html#распределения-вероятности-для-топиков",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.11 Распределения вероятности для топиков",
    "text": "14.11 Распределения вероятности для топиков\n\nnews_documents  |&gt;  \n  filter(topic &lt; 10) |&gt; \n  ggplot(aes(gamma, fill = as.factor(topic))) +\n  geom_histogram(show.legend = F) +\n  facet_wrap(~ topic, ncol = 3) + \n  scale_y_log10() +\n  labs(title = \"Распределение вероятностей для каждого топика\",\n       y = \"Число документов\")\n\n\n\n\n\n\n\n\nПочти ни одна тема не распределена равномерно: гамма чаще всего принимает значения либо около нуля, либо в районе единицы.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#интерактивные-визуализации",
    "href": "lda.html#интерактивные-визуализации",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.12 Интерактивные визуализации",
    "text": "14.12 Интерактивные визуализации\nБолее подробно изучить полученную модель можно при помощи интерактивной визуализации. Пакет LDAvis установим из репозитория.\n\ndevtools::install_github(\"cpsievert/LDAvis\")\n\nЭта функция поможет преобразовать объект lda в файл json.\n\ntopicmodels2LDAvis &lt;- function(x, ...){\n  svd_tsne &lt;- function(x) tsne(svd(x)$u)\n  post &lt;- topicmodels::posterior(x)\n  if (ncol(post[[\"topics\"]]) &lt; 3) stop(\"The model must contain &gt; 2 topics\")\n  mat &lt;- x@wordassignments\n  \n  LDAvis::createJSON(\n    phi = post[[\"terms\"]], \n    theta = post[[\"topics\"]],\n    vocab = colnames(post[[\"terms\"]]),\n    doc.length = slam::row_sums(mat, na.rm = TRUE),\n    term.frequency = slam::col_sums(mat, na.rm = TRUE),\n    mds.method = svd_tsne,\n    reorder.topics = FALSE\n  )\n}\n\nИнтерактивная визуализация сохранится в отдельной папке.\n\nlibrary(LDAvis)\nLDAvis::serVis(topicmodels2LDAvis(news_lda), out.dir = \"ldavis\")\n\nЭто приложение можно опубликовать на GitHub Pages.\n\n\n\nОб этом приложении см. здесь.\nЗначения лямбды, очень близкие к нулю, показывают термины, наиболее специфичные для выбранной темы. Это означает, что вы увидите термины, которые “важны” для данной конкретной темы, но не обязательно “важны” для всего корпуса.\nЗначения лямбды, близкие к единице, показывают те термины, которые имеют наибольшее соотношение между частотой терминов по данной теме и общей частотой терминов из корпуса.\nСами разработчики советуют выставлять значение лямбды в районе 0.6.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#footnotes",
    "href": "lda.html#footnotes",
    "title": "13  Тематическое моделирование",
    "section": "",
    "text": "https://stackoverflow.com/questions/50726713/meaning-of-bar-width-for-pyldavis-for-lambda-0↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Тематическое моделирование</span>"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "",
    "text": "15.1 Виды кластерного анализа\nВсе методы машинного обучения делятся на методы обучения с учителем и методы обучения без учителя. В первом случае у нас есть некоторое количество признаков X, измеренных у n объектов, и некоторый отклик Y. Задача заключается в предсказании Y по X. Например, мы измерили вес и пушистость у сотни котов известных пород, и хотим предсказать породу других котов, зная их вес и пушистость.\nОбучение без учителя предназначено для случаев, когда у нас есть только некоторый набор признаков X, но нет значения отклика. Например, есть группа котов, для которых мы измерили вес и пушистость, но мы не знаем, на какие породы они делятся.\nКластеризация относится к числу методов для обнаружения неизвестных групп (кластеров) в данных. Точнее, это целый набор методов. Мы рассмотрим два из них:\nВ случае с кластеризацией по методу K средних мы пытаемся разбить наблюдения на некоторое заранее заданное число кластеров. Иерархическая кластеризация возвращает результат в виде дерева (дендрограммы), которая позволяет увидеть все возможные кластеры.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#виды-кластерного-анализа",
    "href": "cluster.html#виды-кластерного-анализа",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "",
    "text": "кластеризация по методу K средних\nиерархическая кластеризация",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#кластеризация-по-методу-k-средних",
    "href": "cluster.html#кластеризация-по-методу-k-средних",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.2 Кластеризация по методу K средних",
    "text": "15.2 Кластеризация по методу K средних\nАлгоритм кластеризации:\n\nКаждому наблюдению присваивается случайно выбранное число из интервала от 1 до K (число кластеров). Это исходные метки.\n\n\n\nВычисляется центроид для каждого из кластеров. Центроид k-го класса – вектор из p средних значений признаков, описывающих наблюдения из этого кластера.\nКаждому наблюдению присваивается метка того кластера, чей центроид находится ближе всего к этому наблюдению (удаленность выражается обычно в виде евклидова расстояния).\nШаги 2-3 до тех пор, пока метки классов не станут изменяться.\n\nЭто дает возможность минимизировать внутрикластерный разброс: хорошей считается такая кластеризация, при которой такой разброс минимален.\nКогда центроиды двигаются, кластеры приобретают и теряют документы.\n\n\nВнутрикластерный разброс в кластере k – это сумма квадратов евклидовых расстояний между всеми парами наблюдений в этом кластере, разделенная на общее число входящих в него наблюдений.\n\n\n15.2.1 K-means в R\nРассмотрим это сначала на симулированных, а затем на реальных данных.\n\nset.seed(07092024)\nx = matrix(rnorm(50 * 2), ncol = 2)\nx[1:25, 1:2] = x[1:25, 1:2] + 3\nx[26:50, 1:2] = x[1:25, 1:2] - 4\n\n\nkm.out &lt;- kmeans(x, centers = 2, nstart = 20)\n\nkm.out$cluster\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nНаблюдения разделились идеально. Вот так выглядят наши центроиды:\n\nlibrary(tidyverse)\n\nas_tibble(x)  |&gt; \n  ggplot(aes(V1, V2, color = as.factor(km.out$cluster))) +\n  geom_point(show.legend = F) +\n  geom_point(data = as.data.frame(km.out$centers), color = \"grey40\", size = 3, alpha = 0.7) +\n  theme_light()\n\n\n\n\n\n\n\n\nАргумент nstart позволяет запустить алгоритм функции несколько раз с разными начальными метками кластеров; функция вернет наилучший результат.\n\n\n15.2.2 Кластеризация текстов\nЯ воспользуюсь датасетом из пакета stylo, в котором хранятся частотности 3000 наиболее частотных слов для 26 книг 5 авторов. Один из этих авторов – таинственный Роберт Гэлбрейт, как выяснилось – псевдоним Джоан Роулинг.\n\nlibrary(stylo)\ndata(\"galbraith\")\n\ngalbraith &lt;- as.data.frame.matrix(galbraith) |&gt; \n  select(1:150)\n\ngalbraith[1:10, 1:10]\n\n\n  \n\n\n\nЕсли одни признаки имеют больший разброс значений, чем другие, то при вычислении расстояний будут преобладать элементы с более широкими диапазонами. Поэтому перед применением алгоритма в некоторых случаях рекомендуется нормализовать данные по Z-оценке: из значения признака Х вычитается среднее арифметическое, а результат разделить на стандартное отклонение Х. Это делает функция scale().\n\\[ X_{new} = \\frac{X - Mean(X)}{StDev(X)}\\]\n\nset.seed(07092024)\nkm.out &lt;- kmeans(scale(galbraith), centers = 5, nstart = 20)\n\nkm.out$cluster\n\n       coben_breaker       coben_dropshot       coben_fadeaway \n                   3                    3                    3 \n     coben_falsemove    coben_goneforgood coben_nosecondchance \n                   3                    3                    3 \n     coben_tellnoone    galbraith_cuckoos         lewis_battle \n                   3                    4                    5 \n       lewis_caspian          lewis_chair          lewis_horse \n                   5                    5                    5 \n          lewis_lion         lewis_nephew         lewis_voyage \n                   5                    5                    5 \n      rowling_casual      rowling_chamber       rowling_goblet \n                   4                    2                    2 \n     rowling_hallows        rowling_order       rowling_prince \n                   2                    2                    2 \n    rowling_prisoner        rowling_stone        tolkien_lord1 \n                   2                    2                    1 \n       tolkien_lord2        tolkien_lord3 \n                   1                    1 \n\n\n\nexpected &lt;- str_remove_all(names(km.out$cluster), \"_.*\")\n\ntibble(expected = expected, \n       predicted = km.out$cluster)  |&gt;  \n  group_by(expected) |&gt; \n  count(predicted)\n\n\n  \n\n\n\nПочти все авторы разошлись по разным кластерам (кроме Роулинг), при этом Гэлбрейт в одном кластере с Роулинг. Результат кластеризации по методу k-средних можно визуализировать в двумерном пространстве, прибегнув к методу главных компонент.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#метод-главных-компонент",
    "href": "cluster.html#метод-главных-компонент",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.3 Метод главных компонент",
    "text": "15.3 Метод главных компонент\n\n15.3.1 PCA: общий смысл\nМетод главных компонент (англ. principal component analysis, PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. Этот метод привлекается, в частности, когда надо визуализировать многомерные данные.\nОбщий принцип хорошо объясняет Гаральд Баайен (Baayen 2008, 119).\n\nСерый цвет верхнего левого куба означает, что точки распределены равномерно – нужны все три измерения для того, чтобы описать положение точки в кубе. Куб справа сверху по-прежнему имеет три измерения, но нам достаточно только двух, вдоль которых рассеяны данные. Куб слева снизу тоже имеет два измерения, но вдоль оси y разброс данных меньше, чем вдоль x. Наконец, для куба справа снизу достаточно только одного измерения.\nМетод главных компонент ищет такие измерения, вдоль которых наблюдается наибольший разброс данных, причем каждая следующая компонента будет объяснять меньше разброса.\n\n\n15.3.2 PCA в базовом R\n\npca_fit &lt;- prcomp(galbraith, scale = T, center = T)\n\nnames(pca_fit)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nПервый элемент хранит данные о стандартном отклонении, соответствующем каждой компоненте.\n\nround(pca_fit$sdev, 3)\n\n [1] 7.100 5.586 4.055 3.147 2.891 2.318 1.799 1.720 1.691 1.653 1.385 1.345\n[13] 1.293 1.259 1.230 1.137 1.074 1.034 0.927 0.904 0.833 0.812 0.753 0.738\n[25] 0.612 0.000\n\n\nЭто можно узнать также, вызвав функцию summary.\n\nsummary(pca_fit)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     7.1000 5.5857 4.0551 3.14673 2.89110 2.31817 1.79909\nProportion of Variance 0.3361 0.2080 0.1096 0.06601 0.05572 0.03583 0.02158\nCumulative Proportion  0.3361 0.5441 0.6537 0.71971 0.77543 0.81126 0.83283\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.71973 1.69124 1.65255 1.38483 1.34501 1.29297 1.25850\nProportion of Variance 0.01972 0.01907 0.01821 0.01279 0.01206 0.01115 0.01056\nCumulative Proportion  0.85255 0.87162 0.88983 0.90261 0.91467 0.92582 0.93637\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     1.22957 1.13749 1.07351 1.03397 0.92701 0.90422 0.83317\nProportion of Variance 0.01008 0.00863 0.00768 0.00713 0.00573 0.00545 0.00463\nCumulative Proportion  0.94645 0.95508 0.96276 0.96989 0.97562 0.98107 0.98570\n                         PC22    PC23    PC24   PC25      PC26\nStandard deviation     0.8121 0.75291 0.73778 0.6122 6.838e-15\nProportion of Variance 0.0044 0.00378 0.00363 0.0025 0.000e+00\nCumulative Proportion  0.9901 0.99387 0.99750 1.0000 1.000e+00\n\n\nТаким образом, первые две компоненты объясняют почти половину дисперсии, а последняя почти не имеет объяснительной ценности.\n\nplot(pca_fit)\n\n\n\n\n\n\n\n\nКоординаты текстов в новом двумерном пространстве, определяемом первыми двумя компонентами, хранятся в элементе под названием x.\n\npca_fit$x[,1:2]\n\n                           PC1       PC2\ncoben_breaker        -8.757336  4.352396\ncoben_dropshot       -9.459904  5.276560\ncoben_fadeaway       -8.964170  4.572739\ncoben_falsemove      -8.738214  4.707646\ncoben_goneforgood    -8.113474  7.115332\ncoben_nosecondchance -6.783907  7.946904\ncoben_tellnoone      -7.779098  5.535273\ngalbraith_cuckoos    -3.828020 -5.113776\nlewis_battle          8.179671  2.898561\nlewis_caspian         6.991680  3.005847\nlewis_chair           6.743976  3.867256\nlewis_horse           6.451278  2.783159\nlewis_lion            6.281176  2.401030\nlewis_nephew          5.682845  4.443366\nlewis_voyage          7.947284  3.612055\nrowling_casual       -1.807538 -6.273038\nrowling_chamber      -3.168879 -7.780693\nrowling_goblet       -2.249544 -8.491719\nrowling_hallows      -1.350855 -7.684606\nrowling_order        -2.364382 -7.381456\nrowling_prince       -1.366061 -6.124957\nrowling_prisoner     -3.373330 -9.270778\nrowling_stone        -3.152846 -5.291951\ntolkien_lord1        10.392543  0.650889\ntolkien_lord2        10.799804  1.429537\ntolkien_lord3        11.787300 -1.185577\n\n\n\n\n15.3.3 PCA и кластеры K-means\nФункция augment() из пакета broom позволяет соединить результат анализа с исходными данными.\n\nlibrary(broom)\n\npca_fit  |&gt; \n  augment(galbraith) |&gt; \n  mutate(expected = str_remove_all(.rownames, \"_.+\")) |&gt; \n  ggplot(aes(.fittedPC1, .fittedPC2,\n             color = expected, \n             shape = as.factor(km.out$cluster))) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_discrete(name = \"автор\") +\n  scale_shape_discrete(name = \"кластер\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nЕще один способ представить наблюдения.\n\n# install.packages(\"FactoMineR\")\n# install.packages(\"factoextra\")\nlibrary(FactoMineR)\nlibrary(factoextra)\n\nfviz_pca_ind(pca_fit, geom = c(\"text\"),\n             habillage = as.factor(km.out$cluster),\n             addEllipses = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nАналогично можно представить и нагрузки компонент.\n\nfviz_pca_var(pca_fit, col.var=\"contrib\",\n             select.var = list(contrib = 40),\n             repel = TRUE)+\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nПри интерпретации этого графика следует учитывать, что положительно коррелированные переменные находятся рядом, а отрицательно коррелированные переменные находятся в противоположных квадрантах. Например, для первого измерения “his” и “as” коррелированы отрицательно. Это можно проверить, достав матрицу c нагрузками компонент из объекта pca_fit (в качестве координат используются коэффициенты корреляции между переменными и компонентами):\n\npca_fit$rotation[c(\"his\", \"as\"),1:2]\n\n            PC1         PC2\nhis -0.04033987 -0.15077917\nas   0.11665675 -0.07410514\n\n\nТеперь - наблюдения и переменные на одном графике.\n\nfviz_pca_biplot(pca_fit,  geom = c(\"text\"),\n                select.var = list(cos2 = 40),\n                habillage = as.factor(km.out$cluster),\n                col.var = \"steelblue\",\n                alpha.var = 0.3,\n                repel = TRUE,\n                ggtheme = theme_minimal()) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nПоработать над оформлением такого графика вы сможете в домашнем задании.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#иерархическая-кластеризация",
    "href": "cluster.html#иерархическая-кластеризация",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.4 Иерархическая кластеризация",
    "text": "15.4 Иерархическая кластеризация\n\n15.4.1 Интерпретация дендрограммы\nОдним из недостатков кластеризации по методу k-средних является то, что она требует предварительно указать число кластеров. Этого недостатка лишена иерархическая кластеризация. Если такая кластеризация происходит “снизу вверх”, она называется агломеративной. При этом построение дендрограммы начинается с “листьев” и продолжается вплоть до самого “ствола”.\n\nПри интерпретации дерева надо иметь в виду, что существует \\(2^{n-1}\\) способов упорядочения ветвей дендрограммы, где n – это число листьев. В каждой из точек слияния можно поменять местами наблюдения, не изменяя смысла дендрограммы. Поэтому выводы о сходстве двух наблюдений нельзя делать на основе из близости по горизонтальной оси. См. рис. из книги (Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани 2017, 423)). На рисунке видно, что наблюдение 9 похоже на наблюдение 2 не больше, чем оно похоже на наблюдения 8, 5 и 7. Выводы делаются, исходя из положения на вертикальной оси той точки, где происходит слияние наблюдений.\nКоличество кластеров определяется высотой, на которой мы разрезаем дендрограмму. Из этого следует, что одну и ту же дендрограмму можно использовать для получения разного числа кластеров.\n\n\n15.4.2 Алгоритм кластеризации\n\nВычислить меру различия для всех пар наблюдений. На первом шаге все наблюдения рассматриваются как отдельный кластер.\nНайти пару наиболее похожих кластеров и объединить их. Различие между кластерами соответствует высоте, на которой происходит их слияние в дендрограмме.\nПовторить шаги 1-2, пока не останется 1 кластер.\n\n\n\n\n15.4.3 Тип присоединения\nВид дерева будет зависеть от типа присоединения. На рисунке ниже представлено три способа: полное, одиночное, среднее.\n\nОбычно предпочитают среднее и полное, т.к. они приводят к более сбалансированным дендрограммам.\n\nДля функции hclust() в R по умолчанию выставлено значение аргумента method = \"complete\".\n\n\n15.4.4 Иерархическая кластеризация в R\nПрименим алгоритм к симулированным данным, которые мы создали выше. Функция dist() по умолчанию считает евклидово расстояние.\n\nhc.complete &lt;- hclust(dist(x), method = \"complete\")\nplot(hc.complete)\n\n\n\n\n\n\n\n\nНа картинке видно, что наблюдения из верхих и нижних рядов расходятся на два больших кластера.\n\n\n15.4.5 Иерархическая кластеризация текстов\nДля вычисления расстояния между текстами лучше подойдет не евклидово, а косинусное расстояние на нормализованных данных. В базовой dist() его нет, поэтому воспользуемся пакетом philentropy.\n\ndist_mx &lt;- galbraith  |&gt; \n  scale() |&gt; \n  philentropy::distance(method = \"cosine\", use.row.names = T) \n\nMetric: 'cosine'; comparing: 26 vectors.\n\n\nПреобразуем меру сходства в меру расстояния и передадим на кластеризацию.\n\ndist_mx &lt;- as.dist(1 - dist_mx)\nhc &lt;- hclust(dist_mx)\n\nplot(hc)\n\n\n\n\n\n\n\n\nДля получения меток кластеров, возникающих в результате рассечения дендрограммы на той или иной высоте, можно воспользоваться функцией cutree().\n\ncutree(hc, 5)\n\n       coben_breaker       coben_dropshot       coben_fadeaway \n                   1                    1                    1 \n     coben_falsemove    coben_goneforgood coben_nosecondchance \n                   1                    1                    1 \n     coben_tellnoone    galbraith_cuckoos         lewis_battle \n                   1                    2                    3 \n       lewis_caspian          lewis_chair          lewis_horse \n                   3                    3                    3 \n          lewis_lion         lewis_nephew         lewis_voyage \n                   3                    3                    3 \n      rowling_casual      rowling_chamber       rowling_goblet \n                   2                    4                    4 \n     rowling_hallows        rowling_order       rowling_prince \n                   4                    4                    4 \n    rowling_prisoner        rowling_stone        tolkien_lord1 \n                   4                    4                    5 \n       tolkien_lord2        tolkien_lord3 \n                   5                    5 \n\n\nЭтим меткам можно назначить свой цвет.\n\nlibrary(dendextend)\nhcd &lt;- as.dendrogram(hc)\npar(mar=c(2,2,2,7))\nhcd |&gt; \n  set(\"branches_k_color\", k = 5) |&gt; \n  set(\"labels_col\", k=5) |&gt; \n  plot(horiz = TRUE)\nabline(v=0.8, col=\"pink4\",lty=2)\n\n\n\n\n\n\n\n\n\n\n15.4.6 PCA и иерархическая кластеризация\nКод почти как выше, но надо указать, на сколько кластеров мы разрезаем дерево.\n\npca_fit |&gt; \n  augment(galbraith) |&gt; \n  ggplot(aes(.fittedPC1, .fittedPC2, \n             color = expected, shape = as.factor(cutree(hc, 5)))) +\n  geom_point(size = 3, alpha = 0.7)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#многомерное-шкалирование",
    "href": "cluster.html#многомерное-шкалирование",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.5 Многомерное шкалирование",
    "text": "15.5 Многомерное шкалирование\nКроме этого, для визуализации многомерных данных применяют многомерное шкалирование (cmd = classical multidimensional scaling). Функция получает на входе матрицу расстояний.\n\ncmd_fit &lt;- cmdscale(dist_mx)  |&gt; \n  as_tibble()\n\ncmd_fit\n\n\n  \n\n\n\n\ncmd_fit  |&gt; \n  ggplot(aes(V1, V2, \n             color = expected, \n             shape = as.factor(cutree(hc, 5)))) +\n  geom_point(size = 3, alpha = 0.7)\n\n\n\n\n\n\n\n\nМногомерное шкалирование стремится отразить расстояния между наблюдениями.\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press.\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd",
    "href": "lsa.html#svd",
    "title": "12  Латентно-семантический анализ",
    "section": "12.2 SVD",
    "text": "12.2 SVD\nДля любых текстовых данных и матрица термин-термин и матрица термин-документ будет очень разряженной (то есть большая часть значений будет равна нулю). Необходимо “переупорядочить” ее так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD). При сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы (К. Маннинг, П. Рагхаван, Х. Шютце 2020, 407).\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. О сингулярных векторах можно думать как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.\nПосмотрим теперь, как SVD применяется при анализе текста.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#d-визуализация-пространства-документов",
    "href": "lsa.html#d-визуализация-пространства-документов",
    "title": "12  Латентно-семантический анализ",
    "section": "12.12 2D-визуализация пространства документов",
    "text": "12.12 2D-визуализация пространства документов\nДля снижения размерности мы используем алгоритм UMAP. В отличие от PCA, он снижает размерность нелинейно, и в этом отношении похож на t-SNE.\n\nlibrary(uwot)\nset.seed(07062024)\nviz_lsa &lt;- umap(lsa_space$v ,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все документы вложены теперь в двумерное пространство.\n\ndim(viz_lsa)\n\n[1] 3407    2\n\n\nЗакодировав цветом рубрики новостного сайта, нанесем документы на “карту”.\n\n\ntibble(doc = rownames(viz_lsa),\n       topic = news_2019$topic,\n       V1 = viz_lsa[, 1], \n       V2 = viz_lsa[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = doc, color = topic)) + \n  geom_text(size = 2, alpha = 0.8, position = position_jitter(width = 1.5, height = 1.5)) +\n  annotate(geom = \"rect\", ymin = -10, ymax = 0, xmin = -20, xmax = -10, alpha = 0.2, color = \"tomato\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nВот несколько новостей из небольшого тематического кластера, выделенного на карте квадратом.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc718\", \"doc2437\", \"doc2918\")) |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "stylo.html",
    "href": "stylo.html",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "",
    "text": "16.1 Установка stylo\nВ этом уроке мы рассмотрим возможности стилометрического анализа с использованием пакета stylo. К пакету прилагается внятный HOWTO.\nlibrary(stylo)\n\n\n### stylo version: 0.7.5 ###\n\nIf you plan to cite this software (please do!), use the following reference:\n    Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R:\n    a package for computational text analysis. R Journal 8(1): 107-121.\n    &lt;https://journal.r-project.org/archive/2016/RJ-2016-007/index.html&gt;\n\nTo get full BibTeX entry, type: citation(\"stylo\")\nВозможности этого инструмента мы исследуем на корпусе древнегреческой литературы, подробнее о котором можно прочитать в препринте (upd.: опубликованная версия). Для этого эксперимента корпус был немного урезан и разложен по папкам. Корпус в формате .zip надо забрать по ссылке и сделать рабочей директорией.\nТексты могут быть на любом языке, но обязательно в кодировке Unicode.\nНа Mac может потребоваться поставить XQuartz.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#stylo",
    "href": "stylo.html#stylo",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.2 stylo()",
    "text": "16.2 stylo()\nГлавная рабочая лошадка этого пакета – функция stylo(). Если вызвать ее без аргументов, то запустится GUI (который можно отключить).\n\nstylo()\n\nНа вкладке Input & Language выбираете формат файла и язык.\n\nНа вкладке Features указываете, как разбивать текст: на слова, символы, словесные или символьные энграмы. Также можно уточнить, что делать с прописными буквами (в нашем случае это нерелевантно). Параметр MFW указывает, сколько слов использовать для анализа. CULLING задает порог отсечения для слов: 20 означает, что будут использованы слова, которые встречаются как минимум в 20% текстов, 0 – все слова, 100 - только те, которые есть во всех текстах корпуса.\n\nСледующая вкладка определяет метод, который будет использоваться для анализа.\n\nМожно также уточнить метод выборки.\n\nИ, наконец, формат, в котором следует вернуть результат.\n\nБез графического интерфейса команда будет выглядеть так.\n\nstylo(corpus.dir = \"corpus\", \n      analysis.type = \"CA\",\n      analyzed.features=\"w\", \n      ngram.size=1,\n      culling.min=20,\n      culling.max=20,\n      mfw.min = 100,\n      mfw.max = 100,\n      mfw.incr = 0,\n      distance.measure = \"dist.delta\",\n      write.png.file = FALSE,\n      gui = FALSE,\n      corpus.lang = \"Other\",\n      plot.custom.height=8,\n      plot.custom.width=7\n      )\n\n\nПодписи и цвета функция подбирает автоматически. Попробуйте использовать другие меры расстояния и другие статистические методы, и сравните результат (пока можно не обращать внимания на Consensus Tree – об этом поговорим на следующем занятии). После каждого запуска функции в рабочей директории сохраняются файлы с конфигурацией, признаками, которые использовались для анализа и, опционально, визуализация.\n\nstylo(corpus.dir = \"corpus\", \n      analysis.type = \"PCR\",\n      analyzed.features=\"w\", \n      ngram.size=1,\n      mfw.min = 100,\n      mfw.max = 100,\n      mfw.incr = 0,\n      #pca.visual.flavour=\"loadings\",\n      write.png.file = FALSE,\n      gui = FALSE,\n      corpus.lang = \"Other\",\n      plot.custom.height=8,\n      plot.custom.width=8,\n      save.analyzed.freqs=FALSE\n      )\n\n\nЗаглянуть внутрь функции stylo() можно здесь.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#section",
    "href": "stylo.html#section",
    "title": "16  Деревья и сети в stylo",
    "section": "16.3 ",
    "text": "16.3",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Деревья и сети в `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#classify",
    "href": "stylo.html#classify",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.4 classify()",
    "text": "16.4 classify()\nЕсли stylo() возвращает результат, который должен интерпретировать человек, то classify() используется для машинного обучения с учителем. Вызов функции без аргументов вернет GUI, похожий на тот, что мы видели выше. Отличие будет на вкладке “Статистика”.\n\nСреди доступных методов классификации: Delta, k-NN, SVM, Наивный Байес, метод ближайших центроидов. Подробнее о них мы будем говорить позже, а пока можно поэкспериментировать с Delta.\nПеред запуском функции необходимо создать в рабочей директории две папки: primary_set и secondary_set (они есть в архиве, который вы уже скачали). В первой находится так называемые обучающие данные, во второй - тестовые (контрольные) данные. Обычно это тексты неизвестного авторства, но к ним можно добавить и несколько произведений известного авторства для дополнительного контроля. Мы примем за спорные отрывок из “Греческой истории” Ксенофонта, диалог “Софист” Платона, “Наблюдателей” Лукиана и “Против софистов” Исократа.\n\nclassify_result &lt;- classify(\n      training.corpus.dir = \"primary_set\",\n      test.corpus.dir = \"secondary_set\",\n      classification.method = \"delta\",\n      culling.of.all.samples = FALSE,\n      analyzed.features=\"w\", \n      culling.max=20,\n      culling.min=20,\n      ngram.size=1,\n      mfw.min = 100,\n      mfw.max = 100,\n      mfw.incr = 0,\n      gui = FALSE,\n      corpus.lang = \"Other\")\n\nПосле того, как функция вернет управление, в рабочей директории появится несколько файлов, среди них – final_results.txt. В нашем случае успех 100%, но не стоит переоценивать этот результат: пример был совсем игрушечный. О подводных камнях поговорим в модуле про машинное обучение.\n\nclassify_result$expected == classify_result$predicted\n\n[1] TRUE TRUE TRUE TRUE\n\n\nТеперь попробуйте поэкспериментировать с разными методами и настройками.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#rolling.delta",
    "href": "stylo.html#rolling.delta",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.6 rolling.delta()",
    "text": "16.6 rolling.delta()\nЕще одна “фирменная” функция stylo называется rolling.delta(). Она подходит для тех случаев, когда текст написан в соавторстве (или мы предполагаем, что это так). Delta “прокатится” по всему тексту и для каждого его отрывка оценит вероятность того, что он создан тем или иным автором. Разумеется, это имеет смысл лишь в том случае, если у нас, во-первых, достаточно длинный спорный текст, а, во-вторых, есть понятные кандидаты.\nДля демонстрации работы функции мы составили “монстра” из “Бусириса” Исократа и “Софиста” Платона: первая тысяча слов из Исократа, потом две тысячи из Платона, потом еще тысяча из Исократа и тысяча из Платона. Монстр лежит в папке test_set. Обучающие данные находятся в папке reference_set.\n\nrolling.classify(training.corpus.dir = \"reference_set\",\n                 test.corpus.dir = \"test_set\",\n                 write.png.file = FALSE, \n                 classification.method = \"delta\", \n                 mfw = 150, \n                 corpus.lang=\"Other\", \n                 slice.size = 500, \n                 slice.overlap = 200,\n                 plot.legend = FALSE,\n                 milestone.points = seq(0, 5000, 500),\n                 shading = TRUE\n                 )\npar(mar=c(0,0,0,0))\nlegend('top', c(\"Isocrates\",\"Plato\"), \n       col=c('red', 'green'), lty = 2)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#oppose",
    "href": "stylo.html#oppose",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.7 oppose()",
    "text": "16.7 oppose()\nФункция oppose() реализует контрастивный анализ, помогая понять, каких слов авторы избегают, а какие – предпочитают. Функция возвращает два файла: words-preferred.txt и words-avoided.txt. Она тоже поддерживает графический интерфейс, но с древнегреческим бывают трудности токенизации, поэтому прописываем правило при помощи регулярных выражений.\nДля сравнения возьмем Платона и Исократа (они тоже есть в архиве).\n\noppose(corpus.format = \"plain\",\n       corpus.lang = \"Other\",\n       primary.corpus.dir = \"Plato\" , \n       secondary.corpus.dir = \"Isocrates\", \n       splitting.rule = \"[ \\t\\n]+\",\n       text.slice.length = 1000,\n       text.slice.overlap = 0,\n       rare.occurrences.threshold = 3,\n       zeta.filter.threshold = 0.05,\n       oppose.method = \"craig.zeta\",\n       display.on.screen = TRUE,\n       gui = FALSE)\n\n\nИзменим настройки визуализации и проверим, к кому из двух ближе Демосфен.\n\noppose(corpus.format = \"plain\",\n       corpus.lang = \"Other\",\n       primary.corpus.dir = \"Plato\" , \n       secondary.corpus.dir = \"Isocrates\", \n       test.corpus.dir = \"Demosthenes\",\n       splitting.rule = \"[ \\t\\n]+\",\n       text.slice.length = 1000,\n       text.slice.overlap = 0,\n       rare.occurrences.threshold = 3,\n       visualization=\"markers\", # изменение тут\n       zeta.filter.threshold = 0.05,\n       oppose.method = \"craig.zeta\",\n       display.on.screen = TRUE,\n       gui = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#imposters",
    "href": "stylo.html#imposters",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.9 imposters()",
    "text": "16.9 imposters()\nФункция imposters() реализует метод верификации авторства, предложенный в статье М. Коппеля и Я. Винтера и апробированный на корпусе Цезаря.\nНазвание метода отражает его суть: вместо того чтобы сравнивать текст неизвестного автора с текстами предполагаемых авторов, метод использует “импостеров” — случайно выбранные тексты, не принадлежащие ни одному из кандидатов, — для создания фона, на котором оценивается вероятность принадлежности текста конкретному автору.\nОсновные идеи метода imposters:\n\nСоздание обучающей выборки. Для проверки гипотезы о принадлежности текста определенному автору берется его текст, тексты кандидатов и большое количество “самозванцев” (то есть случайные тексты, которые заведомо не принадлежат ни одному из кандидатов).\nБустреп-подход. Метод много раз случайным образом выбирает подмножества признаков и случайные наборы “импостеров”, а затем выполняет этап классификации.\nПроверка гипотезы. Если текст подозреваемого автора чаще всего классифицируется как принадлежащий этому автору, можно с высокой вероятностью утверждать, что это действительно так. Если же нет, значит, автором с большей долей вероятности является кто-то другой.\n\nПочему такой метод эффективен? Классические методы (например, Delta-классификатор Бэрроуза) могут быть чувствительны к дисбалансу классов или менее устойчивы к вариативности текстов. Кроме того, использование “самозванцев” позволяет создать “естественный уровень шумова”, на фоне которого можно оценить значимость конкретной атрибуции.\nАвтор считается установленным, если атрибуция одному автору превышает некий установленный порог; значение этого порога устанавливается в зависимости от того, какова цена ошибки, то есть что для нас важнее – точность, precision (доля объектов, названными классификатором положительными и при этом действительно являющимися положительными) или полнота, recall (доля объектов положительного класса из всех объектов положительного класса). В качестве подмоги можно использовать функцию imposters.optimize().\nОб этой функции см. подробнее виньетку и документацию. На входе она требует уже подготовленные таблицы с частотностями. Обратите внимание: если не задать значение аргументу candidate.set, функция проверит на авторство все доступные в reference.set классы.\n\ndata(\"galbraith\")\n\n# забираем 8-й ряд из датасета:\nmy_text_to_be_tested = galbraith[8,]\n\n# исключаем 8-й ряд из датасета\nmy_frequency_table = galbraith[-c(8),]\n\n# поехали:\nimposters(reference.set = my_frequency_table, \n          test = my_text_to_be_tested,\n          iterations = 100,\n          features = 0.5)\n\n  coben   lewis rowling tolkien \n   0.34    0.00    1.00    0.00 \n\n\nФункция возвращает вектор вероятностей, где значения, близкие к 1, соответствуют наиболее правдоподобным кандидатам на авторство.\n\nlibrary(stringr)\n\n# кандидаты\nidx = str_detect(rownames(my_frequency_table), \"rowling\")\nmy_candidates &lt;- my_frequency_table[idx, ]\nmy_imposters &lt;- my_frequency_table[-idx, ]\n\n# поехали:\nimposters(reference.set = my_imposters, \n          test = my_text_to_be_tested,\n          # вот тут задаем кандидатов\n          candidate.set = my_candidates,\n          iterations = 100,\n          features = 0.5,\n          # доля импостеров для каждой итерации\n          imposters = 1)\n\nTesting a given candidate against imposters...\n\n\nrowling      1\n\n\nrowling \n      1 \n\n\n\n\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#что-такое-delta",
    "href": "stylo.html#что-такое-delta",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.3 Что такое Delta",
    "text": "16.3 Что такое Delta\nDelta Берроуза – это мера стилистической близости между текстами. Метод был предложен в 2001 году австралийским филологом Джоном Бёрроузом. С тех пор дельту используют во многих исследованиях, большая часть которых посвящена установлению авторства различных произведений.\nСуть метода заключается в том, что для корпуса текстов рассчитывается частотность ряда признаков; это могут быть слова (словоформы) или так называемые n-граммы, то есть последовательности n символов подряд. Для сравнения берутся самые частотные слова, среди которых будет значительная доля служебных, в наименьшей степени связанных с тематикой текста (предлоги, союзы, частицы и т.п.). Поскольку сравниваемые тексты, как правило, имеют разную длину, в стилометрических исследованиях принято брать для сравнения относительную, а не абсолютную частотность; Берроуз идет еще дальше, предлагая использовать так называемые z-scores, то есть стандартизированные оценки, показывающие разброс значений относительно средних. Z-score вычисляется по формуле:\n\\[Ζ =\\frac{x-\\mu}{sd}\\]\nЗдесь случайная величина x — это значение частотности, μ — математическое ожидание (среднее), а sd — стандартное отклонение. Иными словами, z-score показывает, на сколько стандартных отклонений x отстоит от ожидаемого. Зная z-scores для заданных слов у известных авторов/текстов, можно сравнить их с z-scores спорного текста; искомая дистанция Delta вычисляется как сумма взятых по модулю разниц между z-scores у двух сравниваемых текстов, поделенная на количество слов:\n\\[ΔB = \\frac{1}{n}\\times\\ \\sum_{i}^{n}{|z_{i,\\ A-}}z_{i,\\ B}|\\],\nгде i – конкретное слово, n – общее число слов, а A и B – сравниваемые авторы (знак | указывает, что суммируется абсолютное значение разницы). Чем больше дистанция, тем менее вероятно авторство.\nПростота метода позволяет использовать его в традиционных методах обучения без учителя, таких как кластерный анализ, так и с машинно-обучаемыми классификаторами, когда для каждого значения предиктора \\(x_i\\) имеется значение отклика \\(y_i\\). Это позволяет, имея показатели предикторов, прогнозировать отклик, то есть, в нашем примере, определять наиболее вероятного автора. Количество классов формально не ограничено: мы можем сравнивать спорные тексты (test set) как с двумя, так и с двадцатью кандидатами, которые включаются в обучающую выборку (training set).\nПакет stylo дает возможность работать не только с классической Delta, но и с ее вариациями, из которых заслуживает внимание т.н. “вюрцбургская Delta”. В отличие от Delta Берроуза, она использует не манхэттенское, а косинусное расстояние, что во многих случаях позволяет повысить точность классификации. Подробнее о разных расстояниях (на примере древнегреческого корпуса) см. наш препринт.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#size.penalize",
    "href": "stylo.html#size.penalize",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.5 size.penalize()",
    "text": "16.5 size.penalize()\nФункция size.penalize() позволяет проверить эффективность метода на отрывках разной длины при работе с различными машинно-обучаемыми классификаторами, в том числе Delta.\nФункция извлекает из текста случайные выборки все большей и большей длины и сравнивает их с обучающей выборкой для классификации с применением разного числа mfw; по умолчанию для каждой заданной длины отрывка проводится 100 итераций. На выходе функция возвращает матрицы с указанием количество успешных классификаций для каждой длины отрывка и заданного количества mfw, а также матрицы смешения, позволяющие судить о том, между какими авторами чаще возникала путаница.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#что-такое-zeta",
    "href": "stylo.html#что-такое-zeta",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.8 Что такое Zeta",
    "text": "16.8 Что такое Zeta\nКонтрастивный анализ, который проводит функция oppose(), тоже основан на методе, предложенном Д. Берроузом. Этот метод был описан и доработан рядом других исследователей, в том числе Хью Крейгом.\nЕго смысл подробно объясняет Savoy (2020), а более популярное объяснение (со ссылками на специальную литературу) можно найти на сайте https://zeta-project.eu. Общий смысл такой. Берутся два корпуса, которые необходимо сравнить. Это может быть корпус мужской и женской прозы, корпус Шекспира и других драматургов его времени, корпус американских и британских детективов… you name it. Один из корпусов (назовем его primary set) принимается за основу сравнения.\nВсе тексты делятся на фрагменты фиксированной длины, обычно от 900 до 6000 слов (Savoy 2020, 154). Дальше считается, в какой доле фрагментов из primary set слово встретилось и в какой доле фрагментов из secondary set оно не встретилось. Затем доли суммируются (тогда \\(0 \\leqslant  z \\leqslant  2\\)). Допустим, мы сравниваем Шекспира и Марлоу. Если у Шекспира слово есть во всех фрагментах, а у Марлоу – ни в одном, то \\(1 + 1 = 2\\). Если наоборот, то \\(0 + 0 = 0\\). На практике крайние значения встречаются очень редко.\nДругой вариант с примерно тем же смыслом. Считаем долю документов, в которых слово встречается у Шекспира и у Марлоу. Например, у Шекспира в 100%, а у Марло - ни в одном. Вторая доля вычитается из первой: \\(1 - 0 = 1\\). Если наоборот, то Zeta равна \\(-1\\). Таким образом, \\(-1 \\leqslant  z \\leqslant  1\\).\nДостоинство этого метода в том, что результат легко интерпретировать: мы сразу видим слова-дискриминаторы. Но надо помнить, что Zeta работает не с самыми частотными словами (точнее, не только с ними), а значит подвержена влиянию тематики и жанра.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#кто-такие-самозванцы",
    "href": "stylo.html#кто-такие-самозванцы",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.10 Кто такие самозванцы",
    "text": "16.10 Кто такие самозванцы\nФункция imposters() реализует метод верификации авторства, предложенный в статье М. Коппеля и Я. Винтера и апробированный на корпусе Цезаря.\nМетод подходит для тех случаев, когда кандидатов много, но мы не можем быть уверены в том, что среди них есть нужный. Задача заключается в том, чтобы заставить машину сказать “я не знаю”. Для этого используется следующий алгоритм.\nВыбирается часть признаков (например 0.5), на основании которых для спорного текста вычисляются его “ближайшие соседи” в корпусе; для этого может использоваться любая мера расстояния, например Delta, косинусное сходство или др. Потом берется еще одна случайная выборка признаков, снова ищется сосед и так k раз (обычно 100).\nАвтор считается установленным, если атрибуция одному автору превышает некий установленный порог; значение этого порога устанавливается в зависимости от того, какова цена ошибки, то есть что для нас важнее – точность, precision (доля объектов, названными классификатором положительными и при этом действительно являющимися положительными) или полнота, recall (доля объектов положительного класса из всех объектов положительного класса). В качестве подмоги можно использовать функцию imposters.optimize().\nНо подробнее об этом речь пойдет в разделе про машинное обучение.\n\n\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#samplesize.penalize",
    "href": "stylo.html#samplesize.penalize",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.5 samplesize.penalize()",
    "text": "16.5 samplesize.penalize()\nОдна из известных проблем стилометрии связана с тем, что любые метрики плохо работают на небольших отрывках. Но какого размера должен быть текст, чтобы мы могли установить его автора?\nФункция samplesize.penalize() позволяет проверить эффективность метода на отрывках разной длины при работе с различными машинно-обучаемыми классификаторами, в том числе Delta.\nФункция извлекает из текста случайные выборки все большей и большей длины и сравнивает их с обучающей выборкой для классификации с применением разного числа mfw; по умолчанию для каждой заданной длины отрывка проводится 100 итераций. На выходе функция возвращает матрицы с указанием количества успешных классификаций для каждой длины отрывка и заданного количества mfw, а также матрицы смешения, позволяющие судить о том, между какими авторами чаще возникала путаница.\n\npenalize_result &lt;- samplesize.penalize(mfw = c(100, 200, 500), \n                    features = NULL,\n                    path = NULL, corpus.dir = \"corpus\",\n                    sample.size.coverage = seq(100, 5000, 100),\n                    sample.with.replacement = TRUE,\n                    iterations = 100, \n                    classification.method = \"delta\")\n\nФункция вернет список с показателями точность и матрицами смешения и некоторыми другими показателями. Подробнее о матрицах смешения мы будем говорить в разделе про машинное обучение, а пока просто посмотрим на то, как это выглядит.\n\nlibrary(tidyverse)\n\nhelen_confusion_mfw100 &lt;- penalize_result$confusion.matrices$Isocrates_Hel$mfw_100 |&gt; \n   as.data.frame() |&gt; \n   rownames_to_column(\"predicted\") |&gt; \n   as_tibble()\n\nhelen_confusion_mfw100\n\n\n  \n\n\n\nВытянем в длину и визуализируем.\n\nhelen_confusion_mfw100 |&gt; \n  filter(predicted == \"Isocrates\") |&gt; \n  pivot_longer(-predicted, \n               names_to = \"sample_size\", \n               values_to = \"n\") |&gt; \n  ggplot(aes(as.numeric(sample_size), n)) +\n   geom_point(color = \"steelblue\") + \n   scale_x_continuous(breaks = seq(100, 5000, 300)) +\n   labs(x = NULL)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "pmi.html#word2vec",
    "href": "pmi.html#word2vec",
    "title": "12  Векторные представления слов",
    "section": "12.4 Word2vec",
    "text": "12.4 Word2vec\nWord2vec – это полносвязаная нейросеть с одним скрытым слоем. Такое обучение называется не глубоким, а поверхностным (shallow).\n\nlibrary(word2vec)\n\ncorpus_w2v &lt;- news_tokens_pruned |&gt; \n  group_by(id) |&gt; \n  mutate(text = str_c(token, collapse = \" \")) |&gt; \n  distinct(id, text)\n\n\n# устанавливаем зерно, т.к. начальные веса устанавливаются произвольно\nset.seed(02062024) \nmodel &lt;- word2vec(x = corpus_w2v$text, \n                  type = \"skip-gram\",\n                  dim = 100,\n                  window = 10,\n                  iter = 20,\n                  hs = TRUE,\n                  min_count = 5,\n                  threads = 6)\n\nНаша модель содержит эмбеддинги для слов; посмотрим на матрицу.\n\nemb &lt;- as.matrix(model)\ndim(emb)\n\n[1] 6305  100\n\n\n\npredict(model, c(\"погода\", \"спорт\"), type = \"nearest\", top_n = 10) |&gt; \n  bind_rows()\n\n\n  \n\n\n\nПолучившуюся модель можно визуализировать, как мы это делали выше.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "consensus.html",
    "href": "consensus.html",
    "title": "17  Консенсусные деревья и сети",
    "section": "",
    "text": "17.1 Танглграммы\nПоэтому бывает необходимо сравнить разные деревья. Один из способов это сделать визуально – построить tanglegram, например, при помощи пакета dendextend.\nlibrary(stylo)\nlibrary(dendextend)\n\ndata(\"galbraith\")\ngalbraith &lt;- as.data.frame.matrix(galbraith)\ndist_mx &lt;- dist(scale(galbraith))\n\nd1 &lt;- as.dendrogram(hclust(dist_mx, method =\"average\"))  |&gt;  \n  set(\"labels_col\", value = c(\"skyblue\", \"orange\", \"grey40\"), k=3)  |&gt; \n  set(\"branches_k_color\", value = c(\"skyblue\", \"orange\", \"grey40\"), k = 3)\n\nd2 &lt;- as.dendrogram(hclust(dist_mx, method =\"ward.D2\")) |&gt; \n  set(\"labels_col\", value = c(\"skyblue\", \"orange\", \"grey40\"), k=3)  |&gt; \n  set(\"branches_k_color\", value = c(\"skyblue\", \"orange\", \"grey40\"), k = 3)\n\ndlist &lt;- dendlist(d1, d2)\n\npar(family = \"Arial Bold\")\ntanglegram(dlist, common_subtrees_color_lines = FALSE, \n           highlight_distinct_edges  = TRUE, \n           highlight_branches_lwd=FALSE, \n           margin_inner=10, \n           lwd=2, \n           axes=FALSE, \n           main_left = \"Cредняя\", \n           main_right = \"Уорд\", \n           lab.cex = 1.3)\nНа картинке видно, что книги группируются немного по-разному в зависимости от метода связи, хотя для кластеризации использовалась одна и та же матрица расстояний.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#танглграммы",
    "href": "consensus.html#танглграммы",
    "title": "17  Консенсусные деревья и сети",
    "section": "",
    "text": "число предикторов (например, наиболее частотных слов для разных произведений);\nрасстояние, которое используется для попарных сравнений (евклидово, косинусное, др.)\nметод связи (метод полной связи, метод средней связи, метод Уорда и др.);",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусные-деревья-в-stylo",
    "href": "consensus.html#консенсусные-деревья-в-stylo",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.2 Консенсусные деревья в stylo",
    "text": "17.2 Консенсусные деревья в stylo\nКонсенсусное дерево позволяет “обобщить” произвольное число дендрограмм. В stylo за консенсусные деревья отвечает метод BCT (Bootstrap Consensus Tree), к которому можно обратиться через GUI (но здесь мы показываем решение без него).\n\nbct_result &lt;- stylo(gui = FALSE, \n                    frequencies = galbraith,\n                    analysis.type = \"BCT\",\n                    mfw.min = 100,\n                    mfw.max = 500,\n                    mfw.incr = 100,\n                    distance.measure = \"wurzburg\",\n                    write.png.file = FALSE,\n                    consensus.strength = 0.5,\n                    plot.custom.width = 8, \n                    plot.custom.height = 8\n                    )\n\n\nРаботать через GUI удобно, но есть нюансы. Во-первых, не получится кастомизировать внешний вид дерева, а, во-вторых, в Stylo реализована достаточно специфическая процедура бутстрепа (повторных выборок).\nВот что пишут разработчики:\n\nUnder the FEATURES tab, users can define the minutes of the MFW division and sampling procedure, using the increment, the minimum and maximum parameters. For minimum = 100, maximum = 3000, and increment = 50, stylo will run subsequent analyses for the following frequency bands: 100 MFW, 50–150 MFW, 100–200 MFW, …, 2900–2950 MFW, 2950–3000 MFW.\n\nДля консенсуса нужно много деревьев, и Stylo будет строить эти деревья в заданном интервале. Это значит, что последние деревья будут построены уже не на основе самой частотной лексики, т.е. скорее всего на них отразится тематика текстов, входящих в корпус.\nВ некоторых случаях это работает неплохо, но, возможно, у вас есть другие идеи для консенсуса. Разные расстояния. Разные методы кластеризации. Случайные выборки из первых двух сотен слов или еще что-то. Тогда придется самим строить сразу множество деревьев.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#целый-лес-c-purrr",
    "href": "consensus.html#целый-лес-c-purrr",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.3 Целый лес c purrr",
    "text": "17.3 Целый лес c purrr\nЕсли изучить изнанку функции stylo(), которая вызывает GUI в одноименном пакете, то можно заметить, что за консенсусное дерево там отвечает пакет для работы с филогенетическими данными под названием Ape 🦍\nЧто делает машина, когда вы заказываете у нее консенсусное дерево? Принимает на входе матрицу с 1 … n столбцами, в которых хранится частотность для слов. Потом отбирает первые сколько-то слов (скажем, сотню или сколько скажете), считает расстояние, строит на основе матрицы расстояний дерево, складывает его в корзинку. Потом берет следующую сотню слов, считает расстояние, строит дерево, складывает в корзинку… Ну вы поняли. Получается целый лес.\nЗвучит как итерация, а такие задачи в R решаются при помощи цикла for или пакета purrr. Функции map() из пакета purrr надо вручить другую функцию – у нас это будет пользовательская get_tree(). Она берет случайные 100 столбцов в таблице с частотностями galbraith из пакета Stylо, считает расстояние городских кварталов между документами и строит дерево.\n\n\n\n\n\n\nВопрос\n\n\n\nКак называется метод, использующий расстояние городских кварталов на стандартизированных показателях частотности? Ответ найдете в предыдущем уроке.\n\n\n\nlibrary(ape)\n\nget_tree &lt;- function(df) {\n  X &lt;- df[ , sample(3000, replace = F, size = 100)]\n  # стандартизация\n  distmx &lt;- dist(scale(X), method = \"manhattan\")\n  tr &lt;- as.phylo(hclust(distmx))\n  tr\n}\n\nЗапускаем функцию несколько раз при помощи map(), получаем список деревьев. Если хочется на них посмотреть по отдельности, то функцией walk() печатаем сразу несколько деревьев одной строчкой кода 🧙🪄\n\nlibrary(purrr)\nset.seed(123)\n\n\ntrees_result &lt;- map(1:100, ~get_tree(galbraith))\n\n# отдельные деревья\npar(mfrow = c(2, 2), mar = c(1,1,1,1))\nwalk(trees_result[1:4], plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nПодумайте, как можно модифицировать функцию, чтобы посчитать косинусное расстояние? Ответ найдете чуть ниже.\n\n\nТак можно построить и 100, и 1000 деревьев. Но сравнивать их вручную мы не будем, а вместо этого посчитаем консенсус. Но сначала разберемся, что это такое.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусное-дерево-c-ape-простой-пример",
    "href": "consensus.html#консенсусное-дерево-c-ape-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.4 Консенсусное дерево c ape: простой пример",
    "text": "17.4 Консенсусное дерево c ape: простой пример\nДопустим, у нас есть три дерева. Создадим их с использованием формата Ньюика, т.е. просто-напросто комбинации скобок и запятых.\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,2),(3,4));\")\n\n\npar(mfrow = c(1, 3), mar = c(5,1,5,1), cex = 1)\nwalk(list(tr1, tr2, tr3), plot.phylo, tip.color = 2)\n\n\n\n\n\n\n\n\nКластеры 1-2, 3-4 встречаются в двух деревьях, остальные лишь в одном. Задача — найти наиболее устойчивые кластеры методом простого большинства. Для этого считаем консенсус, причем аргумент p указывает, что кластер должен быть представлен не менее, чем в половине деревьев. Также уточняем, что наши деревья укоренены.\n\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = TRUE)\n\nЗначение p не может быть меньше 0.5, потому что конфликтующие сплиты не могут быть представлены вместе в одном дереве.\nТеперь изобразим консенсус в виде дерева; дополнительно для узлов укажем силу консенсуса (2/3 = 0.67):\n\npar(mfrow = c(1,1), mar = c(5,5,5,5))\nplot.phylo(cons, tip.color = 2)\nnodelabels(round(cons$node.label[3],2), 7, frame = \"c\", cex = 0.7)\nnodelabels(round(cons$node.label[2],2), 6, bg = \"yellow\")\n\n\n\n\n\n\n\n\nЭто очень простое консенсусное дерево, построенное по методу простого большинства.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусное-дерево-c-ape-galbraith",
    "href": "consensus.html#консенсусное-дерево-c-ape-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.5 Консенсусное дерево c  ape: galbraith",
    "text": "17.5 Консенсусное дерево c  ape: galbraith\nТеперь попробуем сделать такое же дерево для текстовых данных.\n\nlibrary(tidyverse)\nlibrary(paletteer)\n# добавим красоты \npal &lt;- paletteer_d(\"ghibli::PonyoMedium\")[c(1:4,6)]\n\ncons &lt;- consensus(trees_result, p = 0.5, rooted = FALSE)\n\n# назначаем авторам цвета\ncols &lt;- tibble(author = str_remove(cons$tip.label, \"_.+\")) |&gt; \n  mutate(color = case_when(author == \"coben\"  ~ pal[1],\n                           author == \"galbraith\" ~ pal[2],\n                           author == \"lewis\"  ~ pal[3],\n                           author == \"rowling\" ~ pal[4],\n                           author == \"tolkien\"  ~ pal[5]))\n \n# строим дерево\npar(mar = c(0,0,0,0))\nplot.phylo(cons, \n           type = \"fan\", \n           use.edge.length = FALSE,\n           edge.width = 1.5, \n           node.color = \"grey30\",\n           font = 1, \n           no.margin = TRUE, \n           label.offset = 0.1,\n           direction = \"rightwards\", \n           plot = TRUE, \n           lab4ut = \"a\",\n           node.depth = 1, \n           tip.color = cols$color)\n\n# подписываем узлы\nnodelabels(text=sprintf(\"%.2f\", cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"#E8C4A2FF\",\n           cex = 0.5, \n           )",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусная-сеть-c-phangorn-простой-пример",
    "href": "consensus.html#консенсусная-сеть-c-phangorn-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.6 Консенсусная сеть c phangorn: простой пример",
    "text": "17.6 Консенсусная сеть c phangorn: простой пример\nУ консенсусного дерева есть одно очевидное ограничение: оно плохо передает конфликтующие сигналы. Допустим, у нас есть три неукоренённых дерева.\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,4),(2,3));\")\n\npar(mfrow = c(1, 3), mar = c(0,0,0,0), cex=1)\nwalk(list(tr1, tr2, tr3), \n     plot.phylo, \n     tip.color=2, \n     type=\"unrooted\",\n     label.offset = 0.1)\n\n\n\n\n\n\n\n\nКонсенсусное дерево в таком случае никак не поможет: оно не допускает значений p &lt; 0.5. Проверьте сами: код ниже вернет садовые вилы 🔱\n\npar(mfrow = c(1,1))\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = F)\nplot.phylo(cons)\n\nnodelabels(text=sprintf(\"%.2f\", cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"#E8C4A2FF\",\n           cex = 0.5, \n           )\n\n\n\n\n\n\n\n\nВ таких случаях на помощь приходит консенсусная сеть. Построим сеть с использованием пакета phangorn. На входе отдаем объект класса multiPhylo, это по сути просто три дерева в одном “букете”.\n\nlibrary(phangorn)\nlibrary(TreeTools)\nmph &lt;- as.multiPhylo(list(tr1, tr2, tr3))\n\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\nclass(cons.nw)\n\n[1] \"networx\" \"phylo\"  \n\n\nОбъект cons.nw относится к классу networx. Его можно изобразить как в двух, так и в трех измерениях. Вот 2D.\n\nset.seed(16092024)\npar(mar = c(0,0,0,0))\nplot(cons.nw, type = \"2D\")\n\n\n\n\n\n\n\n\nА вот 3D.\n\nlibrary(rgl) \nplot(cons.nw, \"3D\")\n# create animated gif file \nmovie3d(spin3d(axis=c(0,1,0), rpm=3), \n        duration=10, \n        dir = \".\",  \n        type = \"gif\")\n\n\nТеперь попробуем понять, что это значит (иллюстрация и объяснение отсюда).\n\nРассмотрим неукорененные деревья в середине: их внутренние ветви определяют расщепления (splits), а именно 12|34, 13|24 и 14|23, которые явно не могут наблюдаться в одном дереве и, следовательно, все они несовместимы. Сеть в левом верхнем углу представляет одновременно два первых дерева с прямоугольником, символизирующим две внутренние ветви. Чтобы представить все три расщепления, нам нужен куб, как показано справа.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусная-сеть-c-phangorn-galbraith",
    "href": "consensus.html#консенсусная-сеть-c-phangorn-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.7 Консенсусная сеть c phangorn: galbraith",
    "text": "17.7 Консенсусная сеть c phangorn: galbraith\nИтак, у нас есть 13 деревьев для данных galbraith.\n\n# вычисляем консенсус\nmph &lt;- as.multiPhylo(trees_result)\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\n\nПридется немного поколдовать, чтобы раскрасить сеть.\n\nlibrary(tidyverse)\ncons.nw$col &lt;- str_remove_all(cons.nw$tip.label, \"_.+\")\n\ncol_tbl &lt;- tibble(label = unique(cons.nw$col),\n                  col = pal)\n\ncolor_group &lt;- tibble(label = cons.nw$col) |&gt; \n  left_join(col_tbl)\n\nJoining with `by = join_by(label)`\n\ncons.nw$col &lt;- color_group$col\n\n\n# рисуем\nset.seed(16092024)\npar(mar = c(0,0,0,0))\nplot(cons.nw, type = \"2D\", \n     direction = \"axial\",\n     tip.color = cons.nw$col,\n     edge.color = \"grey30\",\n     edge.width = 1,)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#neighbornet-galbraith",
    "href": "consensus.html#neighbornet-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.10 neighborNet(): galbraith",
    "text": "17.10 neighborNet(): galbraith\nТеперь применим алгоритм к реальным данным.\n\npar(mar = c(0,0,0,0), cex = 0.8)\nnnet &lt;- neighborNet(dist_mx)\n\npal &lt;- pal_d3()(5)\n\n\n# назначаем авторам цвета\ncols &lt;- tibble(author = str_remove(nnet$tip.label, \"_.+\")) |&gt; \n  mutate(color = case_when(author == \"coben\"  ~ pal[1],\n                           author == \"galbraith\" ~ pal[2],\n                           author == \"lewis\"  ~ pal[3],\n                           author == \"rowling\" ~ pal[4],\n                           author == \"tolkien\"  ~ pal[5]))\n \n\nplot(nnet, \n     direction = \"axial\",\n     edge.color = \"grey30\",\n     use.edge.length = TRUE, # попробуйте FALSE\n     edge.width = 1,\n     tip.color = cols$color)\n\n\n\n\n\n\n\n\nВ статье “Untangling Our Past: Languages, Trees, Splits and Networks” создатели алгоритма NeighborNet объясняют, как правильно интерпретировать подобный граф на примере дерева германских языков.\n\nКонфликтующие сигналы передаются за счет “ретикулярной структуры” (квадратиков, проще говоря). Там, где конфликта нет, мы видим дерево.\nКаждый сплит представлен несколькими параллельными линиями, и если эти параллели удалить, то граф распадется на два. Чем длиннее ребро, тем “весомее” сплит.\nНа графе видно, что креольский язык сранан-тонго обладает сходством и с английским, и с нидерландским (граф можно разрезать по зеленым линиям двояко).\nБолее слабый конфликтующий сигнал прослеживается между немецким, нидерландским и фламандским, с одной стороны, и пенсильванским немецким, с другой (синий разрез).\nРассмотренные филогенетические методы (консенсусные сети, консенсусные деревья и neighborNet) ничего не говорят о происхождении одного текста от другого. Филограмма, полученная дистанционными методами, не отражает эволюционный процесс, а показывает степень дивергенции таксонов.\nЭто значит, что модель NeighborNet не делает никаких допущений о происхождении, однако в каком-то смысле она вполне способна показывать то, что называют «конфликтующими сигналами». В биологии это рекомбинация, гибридизация и т.п., а в гуманитарных науках — жанровые и диалектные особенности, отношения подражания, заимствования и все то, что способно влиять на результат классификации текстов, помимо авторства.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#neighbornet-простой-пример",
    "href": "consensus.html#neighbornet-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.9 neighborNet(): простой пример",
    "text": "17.9 neighborNet(): простой пример\nЕще один алгоритм для визуализации неоднозначных филогенетических отношений в R назвается neighborNet. Он подходит для тех случаев, когда мы подозреваем нарушения в древовидной структуре (в генетике это может быть горизонтальный перенос генов, а в литературе – например, отношения подражания или т.п.).\nКлючевое различие по сравнению с consensusNet() заключается в том, что neighborNet() строит сеть непосредственно из данных, а не на основе набора деревьев.\nNeighborNet работает в два шага:\n\nСначала строит круговую раскладку для таксонов таким образом, чтобы минимизировать расстояния между парами кластеров, каждый из которых включает в себя 1 или 2 таксона.\nПотом считает веса для сплитов. На этом этапе некоторые ребра удаляются, а другие вытягиваются сообразно весам. Чем длиннее ребро, тем больше вес сплита.\n\nРассмотрим это на простом примере. Представьте, что у нас есть следующая матрица расстояний.\n\nmx &lt;- matrix(data = c(0, 0.07, 0.12, 0.12, 0.07, 0, 0.13, 0.09, 0.12, 0.13, 0, 0.06, 0.12, 0.09, 0.06, 0), nrow = 4)\ncolnames(mx) &lt;- c(\"a\", \"b\", \"c\", \"d\")\nrownames(mx) &lt;- colnames(mx)\nmx\n\n     a    b    c    d\na 0.00 0.07 0.12 0.12\nb 0.07 0.00 0.13 0.09\nc 0.12 0.13 0.00 0.06\nd 0.12 0.09 0.06 0.00\n\nD &lt;- as.dist(mx)\nD\n\n     a    b    c\nb 0.07          \nc 0.12 0.13     \nd 0.12 0.09 0.06\n\n\nОт матрицы расстояний можно перейти к длине ребер. Для нашей простой матрицы длина горизонтальных ребер, например, считается по формуле:\n\\(1/2 (max(D[a,d]+D[b,c], D[a,c]+D[b,d])-D[a,b] – D[d,c])\\)\n\\(1/2 (max(0.12+0.13, 0.12+0.09) – 0.07 – 0.06) = 0.06\\)\n\nnnet &lt;- neighborNet(D)\npar(mar = c(0,0,0,0))\nplot(nnet, show.edge.label = T, \n     edge.label = nnet$edge.length, \n     edge.color = \"grey\", \n     col.edge.label = \"navy\")\n\n\n\n\n\n\n\n\nАналогичным образом считается длина вертикальных ребер. Формула сработает максимум для четырех таксонов, для более сложных структур понадобится метод наименьших квадратов. Все вычисления делает функция neighborNet из пакета phangorn.\nЕсли аргументу edge.label оставить значение по умолчанию, то на картинке увидите номер сплита.\n\npar(mar = c(0,0,0,0))\nplot(nnet, show.edge.label = T, \n     edge.color = \"grey\", \n     col.edge.label = \"firebrick\")\n\n\n\n\n\n\n\n\nУ каждого сплита есть свой вес (рассчитанный методом наименьших квадратов). Его можно достать из объекта nnet.\n\nw = attr(nnet$splits, \"weights\")\nw\n\n[1] 0.01 0.03 0.03 0.02 0.06 0.02\n\n\nЭто можно понять так: чтобы попасть из пунка b в пункт d, нужно сложить веса для сплитов 4, 5 и 1:\n\nw[4] + w[5] + w[1]\n\n[1] 0.09\n\n\nЭто вернет нам 0.09. Сверяемся с матрицей расстояний — все верно!\nСплит — это разбиение совокупности таксонов на два непустых множества. Посмотрим, какие сплиты возможны для 4 таксонов из нашего примера:\n\nas.matrix(nnet$splits)\n\n     d c a b\n[1,] 1 0 0 0\n[2,] 0 1 0 0\n[3,] 0 0 1 0\n[4,] 0 0 0 1\n[5,] 1 1 0 0\n[6,] 1 0 0 1\n\n\nПервые четыре сплита довольно заурядны: мы просто откусываем по одному углу от нашего прямоугольника. Пятый сплит делит прямоугольник поперек, а шестой — вдоль. Дальше алгоритм для каждого сплита считает, какие пары таксонов оказались с разных сторон сплита. На основе матрицы сплитов А и исходной матрицы расстояний D рассчитывается длина ребра таким образом, чтобы кратчайшие пути между таксонами были максимально приближены к исходной матрице расстояний.\nКак уже говорилось, для 4-х таксонов соответствие может быть полным. Это легко проверить, достав атрибут RSS (Residual Sum of Squares, остаточная сумма квадратов) из объекта nnet, который мы создали.\n\nround(attr(nnet$splits, \"RSS\"), 3)\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусная-сеть-stylo",
    "href": "consensus.html#консенсусная-сеть-stylo",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.10 Консенсусная сеть stylo",
    "text": "17.10 Консенсусная сеть stylo\nПакет stylo не создает сетей как таковых, однако он генерирует таблицы ребер/узлов (или только ребер), используя два алгоритма Эдера для установления связей между узлами. Таблицу можно загрузить в Gephi (https://gephi.org) или прочитать в R (что мы сделаем дальше). Чтобы получить такую таблицу, вызовите функцию stylo() с аргументом network=TRUE и, по желанию, с некоторыми другими аргументами.\n\nstylo(network = TRUE, \n      frequencies = galbraith, \n      network.type=\"undirected\",\n      network.tables=\"both\",\n      linked.neighbors=3,\n      edge.weights=\"linear\",\n      gui=FALSE)\n\nТеперь в рабочей директории должны были появиться два файла .csv.\n\nmy_csv &lt;- list.files(pattern = \"csv\")\nmy_csv\n\n[1] \"book_CA_100_MFWs_Culled_0__Classic Delta_EDGES.csv\"\n[2] \"book_CA_100_MFWs_Culled_0__Classic Delta_NODES.csv\"\n\n\n\ngalbraith_edges &lt;- read_csv(my_csv[1])\ngalbraith_edges\n\n\n  \n\n\ngalbraith_nodes &lt;- read_csv(my_csv[2])\ngalbraith_nodes\n\n\n  \n\n\n\nСоединим две таблицы.\n\nnet_data &lt;- galbraith_edges |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Source == Id)) |&gt; \n  select(-Source) |&gt; \n  rename(Source = Label) |&gt; \n  relocate(Source, .before = Target) |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Target == Id)) |&gt; \n  select(-Target) |&gt; \n  rename(Target = Label) |&gt; \n  relocate(Target, .after = Source) |&gt; \n  select(Source, Target, Weight)\n\nnet_data\n\n\n  \n\n\n\n\nlibrary(igraph)\n\ngalbraith_graph &lt;- graph_from_data_frame(net_data, directed = FALSE)\ngalbraith_graph\n\nIGRAPH 1a39d85 UN-- 26 104 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 1a39d85 (vertex names):\n [1] coben_breaker  --coben_dropshot       coben_breaker  --coben_fadeaway      \n [3] coben_breaker  --coben_falsemove      coben_breaker  --coben_dropshot      \n [5] coben_dropshot --coben_fadeaway       coben_dropshot --coben_falsemove     \n [7] coben_breaker  --coben_fadeaway       coben_dropshot --coben_fadeaway      \n [9] coben_fadeaway --coben_falsemove      coben_breaker  --coben_falsemove     \n[11] coben_dropshot --coben_falsemove      coben_fadeaway --coben_falsemove     \n[13] coben_falsemove--coben_goneforgood    coben_falsemove--coben_nosecondchance\n[15] coben_falsemove--coben_tellnoone      coben_falsemove--coben_goneforgood   \n+ ... omitted several edges\n\n\n\nlibrary(ggraph)\n\ncols &lt;- gryffindor_theme_colors[-c(1:2)]\n\nggraph(galbraith_graph, layout = \"auto\") +\n  geom_edge_link(color = cols[1]) +\n  geom_node_point(color = cols[1]) +\n  geom_node_label(aes(label = name), vjust = 1, hjust = 1, \n                 color = cols[5]) +\n  labs(x = NULL, y = NULL) +\n  theme_gryffindor()\n\n\n\n\n\n\n\n\nУ нас получились три не связанные между собой подсети. О том, как работать с такого рода объектами в R, и как их интерпретировать, мы поговорим уже в следующий раз 🧙‍♂️.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#plot.phylo-простой-пример",
    "href": "consensus.html#plot.phylo-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.4 plot.phylo(): простой пример",
    "text": "17.4 plot.phylo(): простой пример\nДопустим, у нас есть три дерева. Создадим их с использованием формата Ньюика, т.е. просто-напросто комбинации скобок и запятых.\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,2),(3,4));\")\n\n\npar(mfrow = c(1, 3), mar = c(5,1,5,1), cex = 1)\nwalk(list(tr1, tr2, tr3), plot.phylo, tip.color = \"firebrick\", font = 2, edge.width = 1.5)\n\n\n\n\n\n\n\n\nКластеры 1-2, 3-4 встречаются в двух деревьях, остальные лишь в одном. Задача — найти наиболее устойчивые кластеры методом простого большинства. Для этого считаем консенсус, причем аргумент p указывает, что кластер должен быть представлен не менее, чем в половине деревьев. Также уточняем, что наши деревья укоренены.\n\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = TRUE)\n\nЗначение p не может быть меньше 0.5, потому что конфликтующие сплиты не могут быть представлены вместе в одном дереве.\nТеперь изобразим консенсус в виде дерева; дополнительно для узлов укажем силу консенсуса (2/3 = 0.67). Обратите внимание, как менять форму и цвет меток.\n\npar(mfrow = c(1,1), mar = c(5,5,5,5))\nplot.phylo(cons, tip.color = \"firebrick\", \n           edge.width = 1.5, font = 2)\nnodelabels(round(cons$node.label[3],2), 7, \n           frame = \"c\", cex = 0.7)\nnodelabels(round(cons$node.label[2],2), 6, \n           bg = \"darkolivegreen\", col = \"white\")\n\n\n\n\n\n\n\n\nЭто очень простое консенсусное дерево, построенное по методу простого большинства.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#plot.phylo-galbraith",
    "href": "consensus.html#plot.phylo-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.5 plot.phylo(): galbraith",
    "text": "17.5 plot.phylo(): galbraith\nТеперь попробуем сделать такое же дерево для текcтовых данных. Для выбора палитры обратимся к пакету {ggsci} (виньетка).\n\nlibrary(tidyverse)\nlibrary(ggsci) \n\n# добавим красоты \ncols &lt;- pal_igv()(5)\n\ncons &lt;- consensus(trees_result, p = 0.5, rooted = FALSE)\n\n# назначаем авторам цвета\ncols &lt;- tibble(author = str_remove(cons$tip.label, \"_.+\")) |&gt; \n  mutate(color = case_when(author == \"coben\"  ~ cols[1],\n                           author == \"galbraith\" ~ cols[2],\n                           author == \"lewis\"  ~ cols[3],\n                           author == \"rowling\" ~ cols[4],\n                           author == \"tolkien\"  ~ cols[5]))\n \n# строим дерево\npar(mar = c(0,0,0,0))\nplot.phylo(cons, \n           type = \"fan\", \n           use.edge.length = TRUE,\n           edge.width = 1.5, \n           node.color = \"grey30\",\n           font = 2, \n           no.margin = TRUE, \n           label.offset = 0.1,\n           direction = \"rightwards\", \n           plot = TRUE, \n           lab4ut = \"a\",\n           node.depth = 1, \n           tip.color = cols$color,\n           cex = 1.2)\n\n# подписываем узлы\nnodelabels(text=sprintf(\"%.2f\", cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"white\",\n           cex = 1, \n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nО чем вам говорит это дерево? Поменяйте тип дерева с fan на что-то иное.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#consensusnet-простой-пример",
    "href": "consensus.html#consensusnet-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.6 consensusNet(): простой пример",
    "text": "17.6 consensusNet(): простой пример\nУ консенсусного дерева есть одно очевидное ограничение: оно плохо передает конфликтующие сигналы. Допустим, у нас есть три неукоренённых дерева.\n\nlibrary(ape)\nlibrary(purrr)\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,4),(2,3));\")\n\n# Настраиваем область графика\npar(mfrow = c(1, 3), \n    mar = c(2, 2, 2, 2), \n    oma = c(1, 1, 1, 1))\n\n# Функция для рисования с увеличенными границами\nplot_tree_with_space &lt;- function(tree) {\n  # Используем в вашем случае больший отступ\n  plot.phylo(tree, \n             tip.color = \"firebrick\",  \n             font = 2,\n             edge.width = 1.5,\n             type = \"unrooted\",\n             label.offset = 0.5,\n             cex = 1,\n             # Добавляем параметр, дающий больше места\n             x.lim = c(-2, 2),  # Увеличенные границы по X\n             y.lim = c(-2, 2))  # Увеличенные границы по Y\n}\n\n# Применяем функцию к каждому дереву\ninvisible(lapply(list(tr1, tr2, tr3), plot_tree_with_space))\n\n\n\n\n\n\n\n\nКонсенсусное дерево в таком случае никак не поможет: оно не допускает значений p &lt; 0.5. Проверьте сами: код ниже вернет садовые вилы 🔱\n\npar(mfrow = c(1,1))\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = F)\nplot.phylo(cons, tip.color = \"firebrick\", \n           font =2, label.offset = 0.1)\n\nnodelabels(text=as.character(cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"darkolivegreen\",\n           col = \"white\"\n           )\n\n\n\n\n\n\n\n\nВ таких случаях на помощь приходит консенсусная сеть. Построим сеть с использованием пакета phangorn. На входе отдаем объект класса multiPhylo, это по сути просто три дерева в одном “букете”.\n\nlibrary(phangorn)\nlibrary(TreeTools)\nmph &lt;- as.multiPhylo(list(tr1, tr2, tr3))\n\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\nclass(cons.nw)\n\n[1] \"networx\" \"phylo\"  \n\n\nОбъект cons.nw относится к классу networx. Его можно изобразить как в двух, так и в трех измерениях. Вот 2D.\n\nset.seed(16092024)\npar(mar = c(0,0,0,0))\nplot(cons.nw, type = \"2D\", \n     tip.color = \"firebrick\", font = 2)\n\n\n\n\n\n\n\n\nА вот 3D.\n\nlibrary(rgl) \nplot(cons.nw, \"3D\")\n# create animated gif file \nmovie3d(spin3d(axis=c(0,1,0), rpm=3), \n        duration=10, \n        dir = \".\",  \n        type = \"gif\")\n\n\nТеперь попробуем понять, что это значит (иллюстрация и объяснение отсюда).\n\nРассмотрим неукорененные деревья в середине: их внутренние ветви определяют расщепления (splits), а именно 12|34, 13|24 и 14|23, которые явно не могут наблюдаться в одном дереве и, следовательно, все они несовместимы. Сеть в левом верхнем углу представляет одновременно два первых дерева с прямоугольником, символизирующим две внутренние ветви. Чтобы представить все три расщепления, нам нужен куб, как показано справа.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#consensusnet-galbraith",
    "href": "consensus.html#consensusnet-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.7 consensusNet(): galbraith",
    "text": "17.7 consensusNet(): galbraith\nИтак, у нас есть сто деревьев для данных galbraith.\n\n# вычисляем консенсус\nmph &lt;- as.multiPhylo(trees_result)\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\n\nПридется немного поколдовать, чтобы раскрасить сеть.\n\nlibrary(tidyverse)\ncons.nw$author &lt;- str_remove_all(cons.nw$tip.label, \"_.+\")\n\ncol_tbl &lt;- tibble(label = unique(cons.nw$author),\n                  col = pal_d3()(5))\n\ncolor_group &lt;- tibble(label = cons.nw$author) |&gt; \n  left_join(col_tbl)\n\nJoining with `by = join_by(label)`\n\ncons.nw$col &lt;- color_group$col\n\n\nset.seed(04032024)\npar(mar = c(0,0,0,0), oma = c(0,0,0,0), cex = 1.2)\nplot(cons.nw, type = \"2D\", \n     direction = \"axial\",\n     use.edge.length = FALSE,\n     font = 2,\n     tip.color = cons.nw$col,\n     edge.color = \"grey30\",\n     edge.width = 1.2, \n     label.offset = 0.1)\n\n\n\n\n\n\n\n\nТаким образом, consensusNet() строит консенсусную сеть на основе набора деревьев: это позволяет визуализировать степень поддержки различных связей, найденных в наборе деревьев. Подход полезен для выявления областей неопределенности в филогенетических отношениях, когда несколько разных деревьев одинаково хорошо соответствуют данным.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#сетевой-анализ-в-stylo",
    "href": "consensus.html#сетевой-анализ-в-stylo",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.11 Сетевой анализ в stylo",
    "text": "17.11 Сетевой анализ в stylo\nПакет stylo не создает сетей как таковых, однако он генерирует таблицы ребер/узлов (или только ребер), используя два алгоритма Эдера для установления связей между узлами. Таблицу можно загрузить в Gephi (https://gephi.org) или прочитать в R (что мы сделаем дальше). Чтобы получить такую таблицу, вызовите функцию stylo() с аргументом network=TRUE и, по желанию, с некоторыми другими аргументами.\n\nstylo(network = TRUE, \n      frequencies = galbraith, \n      network.type=\"undirected\",\n      network.tables=\"both\",\n      linked.neighbors=3,\n      edge.weights=\"linear\",\n      gui=FALSE)\n\nТеперь в рабочей директории должны были появиться два файла .csv.\n\nmy_csv &lt;- list.files(pattern = \"csv\")\nmy_csv\n\n[1] \"book_CA_100_MFWs_Culled_0__Classic Delta_EDGES.csv\"\n[2] \"book_CA_100_MFWs_Culled_0__Classic Delta_NODES.csv\"\n\n\n\ngalbraith_edges &lt;- read_csv(my_csv[1])\ngalbraith_edges\n\n\n  \n\n\ngalbraith_nodes &lt;- read_csv(my_csv[2])\ngalbraith_nodes\n\n\n  \n\n\n\nСоединим две таблицы.\n\nnet_data &lt;- galbraith_edges |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Source == Id)) |&gt; \n  select(-Source) |&gt; \n  rename(Source = Label) |&gt; \n  relocate(Source, .before = Target) |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Target == Id)) |&gt; \n  select(-Target) |&gt; \n  rename(Target = Label) |&gt; \n  relocate(Target, .after = Source) |&gt; \n  select(Source, Target, Weight)\n\nnet_data\n\n\n  \n\n\n\n\nlibrary(igraph)\n\ngalbraith_graph &lt;- graph_from_data_frame(net_data, directed = FALSE)\ngalbraith_graph\n\nIGRAPH aa6e9d6 UN-- 26 104 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from aa6e9d6 (vertex names):\n [1] coben_breaker  --coben_dropshot       coben_breaker  --coben_fadeaway      \n [3] coben_breaker  --coben_falsemove      coben_breaker  --coben_dropshot      \n [5] coben_dropshot --coben_fadeaway       coben_dropshot --coben_falsemove     \n [7] coben_breaker  --coben_fadeaway       coben_dropshot --coben_fadeaway      \n [9] coben_fadeaway --coben_falsemove      coben_breaker  --coben_falsemove     \n[11] coben_dropshot --coben_falsemove      coben_fadeaway --coben_falsemove     \n[13] coben_falsemove--coben_goneforgood    coben_falsemove--coben_nosecondchance\n[15] coben_falsemove--coben_tellnoone      coben_falsemove--coben_goneforgood   \n+ ... omitted several edges\n\n\n\nlibrary(ggraph)\n\n# нормализация весов\nweights &lt;- (E(galbraith_graph)$Weight - min(E(galbraith_graph)$Weight)) / (max(E(galbraith_graph)$Weight) - min(E(galbraith_graph)$Weight))\nE(galbraith_graph)$Weight &lt;- weights\n\n\n# атрибут с именем автора\nlabels = str_remove(V(galbraith_graph)$name, \"_.+$\")\nV(galbraith_graph)$label &lt;- labels\n\n# граф\nggraph(galbraith_graph, layout = \"kk\") +\n  geom_edge_link(aes(alpha = Weight), \n                 linewidth = 1.1,\n                 show.legend = FALSE, \n                 color = \"grey70\") +\n  geom_node_point(aes(color = label),\n                  size = 3, shape = 21, \n                  fill = \"white\", \n                  show.legend = FALSE) +\n  geom_node_label(aes(label = name, color = label), \n                 vjust = -1, cex = 2,\n                 show.legend = FALSE) +\n  labs(x = NULL, y = NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nУ нас получились три не связанные между собой подсети. О том, как работать с такого рода объектами в R, и как их интерпретировать, мы поговорим уже в следующий раз 🧙‍♂️.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "igraph.html",
    "href": "igraph.html",
    "title": "18  Сетевые данные в igraph",
    "section": "",
    "text": "18.1 Создание графа",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#создание-графа",
    "href": "igraph.html#создание-графа",
    "title": "18  Сетевые данные в igraph",
    "section": "",
    "text": "18.1.1 Функция make_graph()\nigraph предлагает много способов создания графа. Наиболее распространенный способ создания графа - make_graph(), который строит сеть на основе указанных ребер. Например, чтобы создать граф с 10 узлами (пронумерованными от 1 до 10) и двумя ребрами, соединяющими узлы 1-2 и 1-5:\n\ng &lt;- make_graph(edges = c(1, 2, 1, 5), n = 10, directed = FALSE)\n\nplot(g)\n\n\n\n\n\n\n\n\nТакже можно использовать буквальную запись с помощью формульной нотации igraph. Выражения состоят из имен вершин и операторов ребер. Оператор ребра - это последовательность символов - и +, где первый используется для ребер, а второй - для стрелок. Ребра могут быть произвольно длинными, то есть вы можете использовать столько символов -, сколько нужно для “рисования” их. Если все операторы ребер состоят только из символов -, тогда граф будет ненаправленным, в то время как хотя бы один символ + подразумевает направленный граф:\n\ng &lt;- make_graph(~ 1--2, 1--5, 3, 4, 5, 6, 7, 8, 9, 10)\n\nМы можем напечатать граф, чтобы получить сводку его узлов и ребер:\n\ng\n\nIGRAPH 64c3c01 UN-- 10 2 -- \n+ attr: name (v/c)\n+ edges from 64c3c01 (vertex names):\n[1] 1--2 1--5\n\n\nЭто означает: ненаправленный граф с 10 вершинами и 2 ребрами.\nНаправленность также можно узнать при помощи специальной функции.\n\nis_directed(g)\n\n[1] FALSE\n\n\n\ng &lt;- graph_from_literal(Sam-+Mary, Sam-+Tom, Mary++Tom)\ng\n\nIGRAPH c0db80d DN-- 3 4 -- \n+ attr: name (v/c)\n+ edges from c0db80d (vertex names):\n[1] Sam -&gt;Mary Sam -&gt;Tom  Mary-&gt;Tom  Tom -&gt;Mary\n\nplot(g)\n\n\n\n\n\n\n\n\nФункция make_graph() также может создавать некоторые графы по названию. Например, вы можете создать граф, представляющий социальную сеть клуба каратэ Захарии, который показывает дружбу между 34 членами клуба каратэ в университете США в 1970-х годах:\n\ng &lt;- make_graph(\"Zachary\")\n\nВызовите документацию к функции, чтобы узнать, какие еще графы можно создать таким способом.\nВот еще несколько возможностей.\n\ng.full = make_full_graph(7)\ng.ring = make_ring(7)\ng.tree = make_tree(7, children = 2, mode=\"undirected\")\ng.star = make_star(7, mode = \"undirected\")\n\npar(mfrow = c(2,2), mai = rep(0.2, 4))\nplot(g.full)\nplot(g.ring)\nplot(g.tree)\nplot(g.star)\n\n\n\n\n\n\n\n\n\n\n18.1.2 Cоциоматрица\nЕще один способ – социоматрица, т.е. матрица, хранящая информацию о сети. Ее можно создать вручную.\n\nnetmat1 &lt;- rbind(c(0,1,1,0,0),\n                c(0,0,1,1,0),\n                c(0,1,0,0,0),\n                c(0,0,0,0,0),\n                c(0,0,1,0,0))\nrownames(netmat1) &lt;- letters[1:5]\ncolnames(netmat1) &lt;- letters[1:5]\n\ng &lt;- graph_from_adjacency_matrix(netmat1)\nplot(g)\n\n\n\n\n\n\n\nclass(g)\n\n[1] \"igraph\"\n\nsummary(g)\n\nIGRAPH 89619a9 DN-- 5 6 -- \n+ attr: name (v/c)\n\n\n\n\n18.1.3 Список ребер\nТакже матрицу можно построить при помощи списка ребер. Списки ребер меньше по размеру, и собирать сетевые данные в таком формате проще.\n\nnetmat2 &lt;- rbind(c(1,2),\n                 c(1,3),\n                 c(2,3),\n                 c(2,4),\n                 c(3,2),\n                 c(5,3))\ng &lt;- graph_from_edgelist(netmat2)\nV(g)$name &lt;- letters[1:5]\nplot(g)\n\n\n\n\n\n\n\nsummary(g)\n\nIGRAPH 4ecc601 DN-- 5 6 -- \n+ attr: name (v/c)\n\n\n\n\n18.1.4 Таблица\nВоспользуемся датасетом, подготовленным Б.В. Ореховым и опубликованном на сайте Пушкинского дома, “Словарь русских писателей XVIII века: сеть персоналий”.\n\nДатасет представляет собой осмысленные в терминах сетевого анализа междустатейные ссылки в Словаре русских писателей XVIII века (1988–2010. Вып. 1–3). Узлами сети выступают посвященные персоналиям статьи словаря, а ребрами — ссылки на другие статьи в том же словаре.\n\n\nurl &lt;- \"https://dataverse.pushdom.ru/api/access/datafile/3646\"\ndownload.file(url, destfile = \"Persons_EDGES.csv\")\n\n\nlibrary(readr)\n# у вас будет другой путь\nwriters_data &lt;- read_tsv(file = \"../files/Persons_EDGES.csv\")\n\nwriters_data\n\n\n  \n\n\n\nЭту таблицу можно преобразовать в сеть несколькими способами. Можно использовать функцию graph_from_edgelist(), которая ожидает на входе матрицу с двумя столбцами или же создать граф напрямую из датафрейма.\n\nlibrary(dplyr)\n\nwriters &lt;- writers_data |&gt; \n              select(-Type) |&gt; \n              graph_from_data_frame()\n\nwriters\n\nIGRAPH 1eac482 DN-- 780 4440 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 1eac482 (vertex names):\n [1] Н.И.Ахвердов -&gt;П.И.Богданович    А.Д.Байбаков -&gt;А.А.Барсов       \n [3] А.Д.Кантемир -&gt;А.К.Барсов        А.Д.Кантемир -&gt;С.С.Волчков      \n [5] А.Д.Кантемир -&gt;И.И.Ильинский     А.Д.Кантемир -&gt;Ф.Кролик         \n [7] А.Д.Кантемир -&gt;М.В.Ломоносов     А.Д.Кантемир -&gt;Е.Прокопович     \n [9] А.Д.Кантемир -&gt;А.П.Сумароков     А.Д.Кантемир -&gt;В.К.Тредиаковский\n[11] П.М.Карабанов-&gt;А.А.Барсов        П.М.Карабанов-&gt;И.И.Виноградов   \n[13] П.М.Карабанов-&gt;Д.П.Горчаков      П.М.Карабанов-&gt;Г.Р.Державин     \n[15] П.М.Карабанов-&gt;С.Е.Десницкий     П.М.Карабанов-&gt;И.И.Дмитриев     \n+ ... omitted several edges\n\n\nОписание позволяет понять, что граф является направленным (D), а его узлы имеют имена (N). Всего в графе 780 вершин и 4440 связей.\n\nДемонстрационная версия интерактивного приложения, построенного на сетевых данных, размещена здесь.\n\nРазумеется, таблицу не обязательно импортировать, но можно создать самим. Например, на основе совместной встречаемости слов, которую вы уже умеете считать.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#вершины-и-ребра",
    "href": "igraph.html#вершины-и-ребра",
    "title": "18  Сетевые данные в igraph",
    "section": "18.2 Вершины и ребра",
    "text": "18.2 Вершины и ребра\nСамая главная характеристика сети – это ее размер. Размер – это количество участников (members), которые называются узлами (nodes), вершинами (vertices) или акторами.\n\nV(writers)\n\n+ 780/780 vertices, named, from 1eac482:\n  [1] Н.И.Ахвердов                 А.Д.Байбаков                \n  [3] А.Д.Кантемир                 П.М.Карабанов               \n  [5] Н.Карандашов                 Ф.В.Каржавин                \n  [7] Н.Г.Карин                    П.Кохановский               \n  [9] Н.А.Краевич                  П.Крайский                  \n [11] В.И.Крамаренков              И.Краснопольский            \n [13] Н.С.Краснопольский           С.П.Крашенинников           \n [15] П.Н.Крекшин                  И.Кременецкий               \n [17] В.В.Крестинин                Ф.В.Кречетов                \n [19] И.Кречетовский               Г.А.Криновский              \n+ ... omitted several vertices\n\n\n\nvcount(writers)\n\n[1] 780\n\n\nУзнать число ребер и характер связей можно так\n\nE(writers)\n\n+ 4440/4440 edges from 1eac482 (vertex names):\n [1] Н.И.Ахвердов -&gt;П.И.Богданович    А.Д.Байбаков -&gt;А.А.Барсов       \n [3] А.Д.Кантемир -&gt;А.К.Барсов        А.Д.Кантемир -&gt;С.С.Волчков      \n [5] А.Д.Кантемир -&gt;И.И.Ильинский     А.Д.Кантемир -&gt;Ф.Кролик         \n [7] А.Д.Кантемир -&gt;М.В.Ломоносов     А.Д.Кантемир -&gt;Е.Прокопович     \n [9] А.Д.Кантемир -&gt;А.П.Сумароков     А.Д.Кантемир -&gt;В.К.Тредиаковский\n[11] П.М.Карабанов-&gt;А.А.Барсов        П.М.Карабанов-&gt;И.И.Виноградов   \n[13] П.М.Карабанов-&gt;Д.П.Горчаков      П.М.Карабанов-&gt;Г.Р.Державин     \n[15] П.М.Карабанов-&gt;С.Е.Десницкий     П.М.Карабанов-&gt;И.И.Дмитриев     \n[17] П.М.Карабанов-&gt;ЕкатеринаII       П.М.Карабанов-&gt;Н.М.Карамзин     \n[19] П.М.Карабанов-&gt;П.Г.Левшин        П.М.Карабанов-&gt;А.А.Нартов       \n+ ... omitted several edges\n\necount(writers)\n\n[1] 4440",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#плотность",
    "href": "igraph.html#плотность",
    "title": "18  Сетевые данные в igraph",
    "section": "18.3 Плотность",
    "text": "18.3 Плотность\nЕще одна важная характеристика сети – это ее плотность.\n\nedge_density(writers)\n\n[1] 0.007307199\n\n\nПлотность – это доля имеющихся связей по отношению к максимально возможному количеству связей. Формула плотности будет отличаться для направленных и ненаправленных сетей.\n\n\nНаправленный граф\n\\(\\frac{L}{k\\times(k - 1)}\\)\n\nНенаправленный граф\n\\(\\frac{2L}{k\\times(k-1)}\\)\n\n\nЗдесь \\(k\\times(k-1)\\) – это максимально возможное число связей, а k - число акторов.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#компоненты",
    "href": "igraph.html#компоненты",
    "title": "18  Сетевые данные в igraph",
    "section": "18.4 Компоненты",
    "text": "18.4 Компоненты\nКомпонента сети – это подгруппа, где все акторы связаны между собой прямо или косвенно.\n\ncomponents(writers)$no\n\n[1] 3\n\ncomponents(writers)$csize\n\n[1] 776   2   2\n\n\nОбратим внимание: в нашем графе 4 писателя, которые не связаны с главной компонентой.\n\nwhich(components(writers)$membership !=1)\n\nИ.В.Паузе И.Выродов    Э.Глюк А.Выродов \n      154       727       760       779 \n\n\nИнтересная компания (точнее, две компании) из XVIII в. Иоганн-Вернер Паузе был переводчиком Эразма Роттердамского в начале XVIII века, и его также обычно причисляют к создателям «Зерцала». Братья Иван Выродов и Андрей Выродов – выпускники Московского благородного пансиона (ныне МГУ), а Эрнст Глюк – один из переводчиков Библии на русский язык.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#диаметр",
    "href": "igraph.html#диаметр",
    "title": "18  Сетевые данные в igraph",
    "section": "18.5 Диаметр",
    "text": "18.5 Диаметр\nДиаметр сети – количество шагов, которые нужно пройти, чтобы попасть из узла А в узел B; для сетей с несколькими компонентами учитывается та, что больше. Сначала вычисляются кратчайшие пути (геодезическое расстояние) между каждой парой узлов, затем из них берется максимальный.\n\nlgc &lt;- largest_component(writers)\ndiameter(lgc, directed = TRUE)\n\n[1] 11\n\n\n\nget_diameter(lgc)\n\n+ 12/776 vertices, named, from d64ec00:\n [1] Н.Карандашов         И.Г.Бакмейстер       М.М.Щербатов        \n [4] Н.И.Новиков          П.И.Богданович       Н.Ф.Эмин            \n [7] Н.Р.Судовщиков       А.М.Ченыхаев         И.В.Нехачин         \n[10] В.Д.Голицын          М.И.Прокудин-Горский Ф.П.Печерин         \n\n\n\nfarthest_vertices(lgc)\n\n$vertices\n+ 2/776 vertices, named, from d64ec00:\n[1] Н.Карандашов Ф.П.Печерин \n\n$distance\n[1] 11\n\n\nПосмотрим на кратчайшие пути.\n\nshortest_paths(lgc, from = \"Н.Карандашов\", to = \"Ф.П.Печерин\")\n\n$vpath\n$vpath[[1]]\n+ 12/776 vertices, named, from d64ec00:\n [1] Н.Карандашов         И.Г.Бакмейстер       М.М.Щербатов        \n [4] Н.И.Новиков          П.И.Богданович       Н.Ф.Эмин            \n [7] Н.Р.Судовщиков       А.М.Ченыхаев         И.В.Нехачин         \n[10] В.Д.Голицын          М.И.Прокудин-Горский Ф.П.Печерин         \n\n\n$epath\nNULL\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\n\nПочему Николай Карандашов оказался так далеко от Федора Печерина, нам решительно не известно.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#транзитивность",
    "href": "igraph.html#транзитивность",
    "title": "18  Сетевые данные в igraph",
    "section": "18.6 Транзитивность",
    "text": "18.6 Транзитивность\nКоэффициент кластеризации, или транзитивность, отражает тенденцию к созданию закрытых треугольников, т.е. к замыканию. Транзитивность определяется как доля закрытых треугольников по отношению к общему количеству открытых и закрытых треугольников.\n\ntransitivity(writers)\n\n[1] 0.127146",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#атрибуты-вершин",
    "href": "igraph.html#атрибуты-вершин",
    "title": "18  Сетевые данные в igraph",
    "section": "18.7 Атрибуты вершин",
    "text": "18.7 Атрибуты вершин\nВ датасете “Словарь…” в качестве атрибута вершины хранятся данные об имени автора:\n\nnames &lt;-vertex_attr(writers)$name\nnames[1:12]\n\n [1] \"Н.И.Ахвердов\"     \"А.Д.Байбаков\"     \"А.Д.Кантемир\"     \"П.М.Карабанов\"   \n [5] \"Н.Карандашов\"     \"Ф.В.Каржавин\"     \"Н.Г.Карин\"        \"П.Кохановский\"   \n [9] \"Н.А.Краевич\"      \"П.Крайский\"       \"В.И.Крамаренков\"  \"И.Краснопольский\"",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#фильтрация-на-основе-значений-атрибутов-вершин",
    "href": "igraph.html#фильтрация-на-основе-значений-атрибутов-вершин",
    "title": "18  Сетевые данные в igraph",
    "section": "18.8 Фильтрация на основе значений атрибутов вершин",
    "text": "18.8 Фильтрация на основе значений атрибутов вершин\nАтрибуты вершин можно использовать для того, чтобы задать новую подсеть для анализа.\n\nvert &lt;- which(vertex_attr(writers)$name == \"М.С.Пахомов\")\n\np &lt;- induced_subgraph(writers, vids = vert)\n\np\n\nIGRAPH 11b20f4 DN-- 1 0 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 11b20f4 (vertex names):\n\n\n\nПАХОМОВ Матвей Сергеевич [1745—1792], преподаватель Смольного ин-та, совм. с И. И. Сидоровским перевел с греч. языка «Разговоры Лукиана Самосатского» (1775—1784. Ч. 1—3), «Творения велемудрого Платона» (1780—1785. Ч. 1—3; с кратким изложением содержания перед текстом каждого рассуждения), «Павсания, или Павсаниево описание Еллады, то есть Греции» (1788—1789. Ч. 1—3) и «Землеописание» Страбона (последний перевод остался неизданным).\n\nМы отобрали всего один узел, что не очень интересно. Вот его соседи.\n\nneighbors(writers, \"М.С.Пахомов\")\n\n+ 3/780 vertices, named, from 1eac482:\n[1] И.И.Сидоровский      И.Ф.Янковичдемириево А.А.Барсов          \n\n\n\nСИДОРОВСКИЙ Иван Иванович [1748-1795], преподаватель Смольного ин-та. Совместно с М. С. Пахомовым перевел с греч. языка: «Разговоры Лукиана Самосатского» (1775—1784. Ч. 1—3), «Творения велемудрого Платона» (1780—1785. Ч. 1—3), «Павсаний, или Павсаниево описание Еллады, то есть Греции» (1788—1789. Ч. 1—3). Ч. 3 сочинений Платона и Ч. 3 «Разговоров…» переведены одним С. Также С. перевел с греч. сб. «Поучительных разных слов и бесед Св. Иоанна Златоустого» (1787—1791. Ч. 1—2), в котором мн. из произведений Златоуста были впервые представлены на рус. языке, и «Деяния церковные и гражданские от Рождества Христова до половины XV столетия, собранные Георгием Кедрином с продолжением других» (1794. Ч. 1—3). В конце жизни С. начал переводить «Толкование св. Кирилла Александрийского на 12 Малых Пророков».\n\n\np2 &lt;- induced_subgraph(writers, vids = c(vert, neighbors(writers, \"М.С.Пахомов\")))\n\n\nplot(p2)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#функция-make_ego_graph",
    "href": "igraph.html#функция-make_ego_graph",
    "title": "18  Сетевые данные в igraph",
    "section": "18.9 Функция make_ego_graph()",
    "text": "18.9 Функция make_ego_graph()\nMake_ego_graph() создает (под)графы из всех соседей заданных вершин. Аргумент o можно мыслить как число “рукопожатий”: порядок 0 - это всегда сама v, порядок 1 - это v плюс ее ближайшие соседи, порядок 2 - это порядок 1 плюс ближайшие соседи вершин из порядка 1 и т.д. Попробуем найти “друзей друзей” М.С. Пахомова.\n\np3 &lt;- make_ego_graph(\n  writers,\n  order = 2,\n  nodes = \"М.С.Пахомов\",\n  mode = \"all\"\n)[[1]]\n\n# функция позволит распечатать все ребра\n# print_all(p3)\n\np3\n\nIGRAPH 5312832 DN-- 48 266 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 5312832 (vertex names):\n [1] А.Д.Байбаков -&gt;А.А.Барсов           П.М.Карабанов-&gt;Н.М.Карамзин        \n [3] П.М.Карабанов-&gt;А.А.Барсов           П.М.Карабанов-&gt;ЕкатеринаII         \n [5] И.И.Мелиссино-&gt;Н.И.Новиков          И.И.Мелиссино-&gt;И.И.Бецкой          \n [7] И.И.Мелиссино-&gt;М.Н.Муравьев         И.И.Мелиссино-&gt;А.А.Барсов          \n [9] И.И.Мелиссино-&gt;ЕкатеринаII          И.И.Мелиссино-&gt;И.И.Шувалов         \n[11] И.И.Мелиссино-&gt;М.В.Ломоносов        И.И.Мелиссино-&gt;А.П.Сумароков       \n[13] Ф.И.Миллер   -&gt;Н.И.Новиков          Ф.И.Миллер   -&gt;Н.Н.Поповский       \n[15] Ф.И.Миллер   -&gt;Н.Н.Бантыш-Каменский Ф.И.Миллер   -&gt;В.Н.Татищев         \n+ ... omitted several edges\n\n\nЧтобы изобразить такой граф, придется немного поправить настройки.\n\npar(mar = rep(0,4), cex = 0.7)\nlayout_p3 &lt;- layout_with_kk(p3)\n\nplot(p3, vertex.size=6, \n     edge.arrow.size = 0.5, \n     vertex.label.dist = 1,\n     edge.curved = 0.2,\n     edge.color = \"grey80\",\n     vertex.color = \"plum\",\n     layout = layout_p3)\n\n\n\n\n\n\n\n\nМ.С. Пахомов связан с Екатериной II через И.Ф. Янковича де Мириево, камерпажа императрицы, впоследствии (при Павле I) – генерал-майора и участника походов против Наполеона в 1805 и 1807 г., отличившегося в сражении под Аустерлицем.\nЭто можно подтвердить и другим, уже известным нам способом:\n\nshortest_paths(writers, from = \"М.С.Пахомов\", to = \"ЕкатеринаII\")$vpath[[1]]\n\n+ 3/780 vertices, named, from 1eac482:\n[1] М.С.Пахомов          И.Ф.Янковичдемириево ЕкатеринаII         \n\n\nУзнать размер ego-графа можно при помощи специальной функции.\n\n# размер подграфа\nego_size(writers,\n  order = 2,\n  nodes = \"М.С.Пахомов\",\n  mode = \"all\")\n\n[1] 48\n\n\nПосмотрим, как растет размер сети при увеличении порядка.\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following objects are masked from 'package:igraph':\n\n    compose, simplify\n\nf &lt;- function(x) {\n  ego_size(writers, \n  order = x,\n  nodes = \"М.С.Пахомов\", \n  mode = \"all\")\n}\n\norder &lt;- 1:10\nout &lt;- map_dbl(order, f)\nplot(order, out, \"o\")",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#атрибуты-ребер",
    "href": "igraph.html#атрибуты-ребер",
    "title": "18  Сетевые данные в igraph",
    "section": "18.10 Атрибуты ребер",
    "text": "18.10 Атрибуты ребер\nУ ребер в данных “Словаря…” есть атрибут, но он везде одинаковый.\n\noptions(max.print=35)\n\nedge_attr(writers)\n\n$Weight\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [ reached getOption(\"max.print\") -- omitted 4405 entries ]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#преобразование-направленной-сети-в-ненаправленную",
    "href": "igraph.html#преобразование-направленной-сети-в-ненаправленную",
    "title": "18  Сетевые данные в igraph",
    "section": "18.11 Преобразование направленной сети в ненаправленную",
    "text": "18.11 Преобразование направленной сети в ненаправленную\nНаправленный граф бывает необходимо преобразовать в неправленный. Возьмем подграф “соседей” М.С. Пахомова и создадим симметричную сеть, оставив только те связи, где ссылки взаимны.\n\np4 &lt;- as.undirected(p3, mode = \"mutual\")\n\nWarning: `as.undirected()` was deprecated in igraph 2.1.0.\nℹ Please use `as_undirected()` instead.\n\np4\n\nIGRAPH 712f520 UN-- 48 58 -- \n+ attr: name (v/c)\n+ edges from 712f520 (vertex names):\n [1] И.И.Мелиссино       --Н.И.Новиков         \n [2] Н.И.Новиков         --М.И.Багрянский      \n [3] Ф.И.Миллер          --Н.Н.Бантыш-Каменский\n [4] Н.И.Новиков         --Н.Н.Бантыш-Каменский\n [5] Н.Н.Поповский       --Ф.Я.Яремский        \n [6] М.С.Пахомов         --И.И.Сидоровский     \n [7] И.И.Мелиссино       --М.Н.Муравьев        \n [8] Н.И.Новиков         --М.Н.Муравьев        \n+ ... omitted several edges\n\n\nВ новой сети по-прежнему 48 узлов, но количество связей стало меньше (58 вместо 266).\n\npar(mar = rep(0,4), cex = 0.7)\nplot(p4, vertex.size=6, \n     edge.arrow.size = 0.5, \n     vertex.label.dist = 1,\n     edge.curved = 0.2,\n     edge.color = \"grey80\",\n     vertex.color = \"plum\",\n     layout = layout_p3)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#удаление-изолированных-узлов",
    "href": "igraph.html#удаление-изолированных-узлов",
    "title": "18  Сетевые данные в igraph",
    "section": "18.12 Удаление изолированных узлов",
    "text": "18.12 Удаление изолированных узлов\nПосле симметризации некоторые узлы оказались изолированы. Удалим их. Для этого сначала необходимо найти узлы, степень которых равна 0.\n\nd &lt;- unname(degree(p4))\np4 &lt;- set_vertex_attr(p4, name = \"degree\", value = d)\n\nПроверим.\n\nvertex_attr(p4)$degree\n\n [1]  1  0  3  2  7  0  0  2  0  0  0  8  2  3  1  0  1  0  1  2  0  0  0  0  6\n[26]  6  0  4  0  4  0  9  1  0  0  0  9  1  7  0  1  1  3  9  0 11  2  9\n\n\n\np5 &lt;- delete_vertices(p4, vertex_attr(p4)$degree == 0)\np5\n\nIGRAPH e9f5e4f UN-- 28 58 -- \n+ attr: name (v/c), degree (v/n)\n+ edges from e9f5e4f (vertex names):\n [1] И.И.Мелиссино       --Н.И.Новиков         \n [2] Н.И.Новиков         --М.И.Багрянский      \n [3] Ф.И.Миллер          --Н.Н.Бантыш-Каменский\n [4] Н.И.Новиков         --Н.Н.Бантыш-Каменский\n [5] Н.Н.Поповский       --Ф.Я.Яремский        \n [6] М.С.Пахомов         --И.И.Сидоровский     \n [7] И.И.Мелиссино       --М.Н.Муравьев        \n [8] Н.И.Новиков         --М.Н.Муравьев        \n+ ... omitted several edges\n\n\n\npar(mar = rep(0,4), cex = 0.7)\nlayout_p5 &lt;- layout_with_kk(p5)\n\nplot(p5, vertex.size=6, \n     edge.arrow.size = 0.6, \n     vertex.label.dist = 1,\n     edge.curved = 0.2,\n     edge.color = \"grey80\",\n     vertex.color = \"plum\",\n     layout = layout_p5)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#сравнение-графов",
    "href": "igraph.html#сравнение-графов",
    "title": "18  Сетевые данные в igraph",
    "section": "18.13 Сравнение графов",
    "text": "18.13 Сравнение графов\nУбедимся, что графы отличаются.\n\nidentical_graphs(p4, p5)\n\n[1] FALSE\n\n\nДва графа называются изоморфными, если у них одинаковое число вершин (обозначим его n) и вершины каждого из них можно занумеровать так числами от 1 до n, что в первом графе две вершины соединены ребром тогда и только тогда, когда вершины с такими же номерами во втором графе соединены.\nПроверим подграфы на изоморфность.\n\nisomorphic(p4, p5)\n\n[1] FALSE\n\n\nТеперь произвольным образом переименуем узлы в p5, удалим один из атрибутов вершины (атрибуты ребер “потерялись” при симметризации) и снова проверим “двойника” на изоморфность.\n\np6 &lt;- p5\nV(p6)$name &lt;- c(letters, \"aa\", \"bb\")\np6 &lt;- delete_vertex_attr(p6, \"degree\")\n\np6\n\nIGRAPH e9f5e4f UN-- 28 58 -- \n+ attr: name (v/c)\n+ edges from e9f5e4f (vertex names):\n [1] b--d  d--g  c--h  d--h  f--i  e--k  b--m  d--m  f--m  d--n  g--n  h--n \n[13] m--n  a--q  e--q  f--q  l--q  m--q  d--s  l--s  o--s  p--s  q--s  f--u \n[25] p--u  r--u  s--u  q--v  f--w  s--x  c--y  f--y  n--y  o--y  q--y  u--y \n[37] x--y  f--z  j--z  m--z  n--z  o--z  p--z  s--z  u--z  x--z  y--z  q--aa\n[49] t--aa b--bb d--bb f--bb o--bb p--bb s--bb u--bb y--bb z--bb\n\n\nСнова сравним графы двумя способами.\n\nidentical_graphs(p5, p6)\n\n[1] FALSE\n\n\n\nisomorphic(p5, p6)\n\n[1] TRUE\n\n\n\n\n\n\nЛюк, Дуглас. 2017. Анализ сетей (графов) в среде R: Руководство пользователя. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "plot.html#диаграмма-рассеяния-с-geom_point",
    "href": "plot.html#диаграмма-рассеяния-с-geom_point",
    "title": "3  Визуализации",
    "section": "3.3 Диаграмма рассеяния с geom_point()",
    "text": "3.3 Диаграмма рассеяния с geom_point()\nФункция ggplot() имеет два основных аргумента: data и mapping. Аргумент mapping задает эстетические атрибуты геометрических объектов. Обычно используется в виде mapping = aes(x, y), где aes() означает aesthetics.\nПод “эстетикой” подразумеваются графические атрибуты, такие как размер, форма или цвет. Вы не увидите их на графике, пока не добавите какие-нибудь “геомы” – геометрические объекты (точки, линии, столбики и т.п.). Эти объекты могут слоями накладываться друг на друга (Wickham и Grolemund 2016).\nДиаграмма рассеяния, которая подходит для отражения связи между двумя переменными, делается при помощи geom_point(). Попробуем настройки по умолчанию.\n\nnoveltm |&gt; \n  ggplot(aes(inferreddate, n_words)) + \n  geom_point()\n\n\n\n\n\n\n\n\nУпс. Точек очень много, и они накладываются друг на друга, так как число слов – дискретная величина. Поступим так же, как Моретти, который отразил на графике среднее для каждого года.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#среднее-со-stat_summary",
    "href": "plot.html#среднее-со-stat_summary",
    "title": "3  Визуализации",
    "section": "3.4 Среднее со stat_summary()",
    "text": "3.4 Среднее со stat_summary()\nДля этого у нас есть два пути. Первый: обобщить данные при помощи group_by() и summarise(), как мы делали в прошлом уроке. Второй: воспользоваться возможностями stat_summary() в самом ggplot2.\n\nnoveltm |&gt; \n  filter(!is.na(n_words)) |&gt; \n  ggplot(aes(inferreddate, n_words)) +\n  geom_point(color = \"grey80\") +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"steelblue\")\n\n\n\n\n\n\n\n\nОставим только среднее и добавим линию тренда, а также уберем подпись оси X.\n\nnoveltm |&gt; \n  filter(!is.na(n_words)) |&gt; \n  ggplot(aes(inferreddate, n_words)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"steelblue\") +\n  geom_smooth(color = \"tomato\") +\n  labs(x = NULL)\n\n\n\n\n\n\n\n\nНисходящая тенденция, о которой писал Моретти, хорошо прослеживается. Но, возможно, она характерна не для всех стран?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#просто-украшательство",
    "href": "plot.html#просто-украшательство",
    "title": "3  Визуализации",
    "section": "3.10 Просто украшательство",
    "text": "3.10 Просто украшательство\nПоскольку нас интересует доля женщин, логично поменять группы местами.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  # меняем местами группы\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  coord_flip() +\n  # разные мелочи\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nТакже поменяем порядок, в котором идут декады (от меньшей к большей).\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  # меняем порядок лет\n  scale_x_reverse() +\n  coord_flip() +\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nУбавим цвет в мужской части диаграммы и добавим заголовки.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           # обводим столбики \n           color = \"darkred\", \n           # убираем легенду\n           show.legend = FALSE) +\n  scale_x_reverse() +\n  # беремся за палитру\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  # меняем цвет и шрифт текста\n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"))\n\n\n\n\n\n\n\n\nСтоит подвинуть заголовок и убрать просветы между столбцами.\n\n# почти ничего нового!\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           color = \"darkred\", \n           show.legend = FALSE,\n           # столбик во всю ширину\n           width = 10\n) +\n  # добавляем делений на оси\n  scale_x_reverse(breaks = seq(1800, 2000, 10)) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"),\n        # выравниваем заголовок\n        plot.title.position = \"plot\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#подписи-с-geom_text",
    "href": "plot.html#подписи-с-geom_text",
    "title": "3  Визуализации",
    "section": "3.11 Подписи с geom_text()",
    "text": "3.11 Подписи с geom_text()\nФункции geom_text() можно передать таблицу, которую мы сделали выше и которая хранит сведения о доле женщин по декадам. Обратите внимание: у геомов могут быть разные данные!\n\nlabel_data &lt;- noveltm_new_prop |&gt; \n                          filter(gender == \"f\")\n\n# тут все старое\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           color = \"darkred\", \n           show.legend = FALSE,\n           width = 10\n) +\n  scale_x_reverse(breaks = seq(1800, 2000, 10)) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(\n    text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n    axis.text = element_text(color = \"darkred\"),\n    plot.title.position = \"plot\"\n    ) + \n  # тут чуть-чуть нового\n  geom_text(data = label_data, \n            aes(label = round(share, 2),\n                y = share),\n            family = \"serif\", \n            hjust = 1.2, \n            color = \"darkred\")\n\n\n\n\n\n\n\n\nОтличная работа! Все сестры Бронте вами гордятся.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "ggraph.html",
    "href": "ggraph.html",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "",
    "text": "19.1 Дизайн узлов\nДля визуализации используем библиотеку ggraph. Минимум необходимых усилий уже даст нам что-то осмысленное, но это только начало.\nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point() +\n  geom_node_text(aes(label = name)) +\n  theme_graph() \n\nUsing \"sugiyama\" as default layout\nПри работе с узлами мы можем закодировать несколько переменных при помощи размера, цвета или, например, формы. Здесь мы ограничимся двумя способами: при помощи размера отразим степень узла (количество связей с другими участниками), а при помощи цвета – гендер.\nДля этого сначала считаем степень узлов; как это делать, мы обсуждали в предыдущем уроке.\nd &lt;- as.numeric(degree(tudors_g))\nV(tudors_g)$degree &lt;- d\ntudors_g\n\nIGRAPH 2b5cd7f DN-- 25 35 -- \n+ attr: name (v/c), degree (v/n), relationship (e/c)\n+ edges from 2b5cd7f (vertex names):\n [1] Henry VII          -&gt;Elizabeth of York     \n [2] Arthur Tudor       -&gt;Catharine of Aragon   \n [3] Henry VIII         -&gt;Catharine of Aragon   \n [4] Henry VIII         -&gt;Anne Boleyn           \n [5] Henry VIII         -&gt;Jane Seymour          \n [6] Henry VIII         -&gt;Anne of Cleves        \n [7] Henry VIII         -&gt;Katherine Howard      \n [8] Henry VIII         -&gt;Catherine Parr        \n+ ... omitted several edges\nТеперь в код выше вносим несколько изменений.\nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree)) +\n  geom_node_text(aes(label = name)) +\n  theme_graph() \n\nUsing \"sugiyama\" as default layout",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#добавление-атрибутов-узлов",
    "href": "ggraph.html#добавление-атрибутов-узлов",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.2 Добавление атрибутов узлов",
    "text": "19.2 Добавление атрибутов узлов\nДанных о гендере в датасете нет, но их несложно добавить.\n\ngender_tbl &lt;- tibble(name = V(tudors_g)$name) |&gt; \n  mutate(gender = case_when(\n    str_detect(name, \"(Margaret|Mary|Elizabeth|[CK]ath[ae]rine|Anne|Jane)\") ~ \"f\",\n    .default = \"m\"))\n\ngender_tbl\n\n\n  \n\n\n\n\nV(tudors_g)$gender &lt;- gender_tbl$gender\n\nvertex_attr(tudors_g)\n\n$name\n [1] \"Henry VII\"              \"Arthur Tudor\"           \"Henry VIII\"            \n [4] \"Margaret Tudor\"         \"Mary Tudor\"             \"James V\"               \n [7] \"Mary Queen of Scots\"    \"Mary I\"                 \"James VI/I\"            \n[10] \"Elizabeth I\"            \"Edward VI\"              \"Elizabeth of York\"     \n[13] \"Catharine of Aragon\"    \"Anne Boleyn\"            \"Jane Seymour\"          \n[16] \"Anne of Cleves\"         \"Katherine Howard\"       \"Catherine Parr\"        \n[19] \"James IV\"               \"Louis XII\"              \"Charles Duke of Suffok\"\n[22] \"Mary of Guise\"          \"Frances II of France\"   \"Henry Lord Darnley\"    \n[25] \"Philip II\"             \n\n$degree\n [1]  5  3 11  4  4  4  5  3  2  2  2  5  3  2  2  1  1  1  2  1  1  2  1  2  1\n\n$gender\n [1] \"m\" \"m\" \"m\" \"f\" \"f\" \"m\" \"f\" \"f\" \"m\" \"f\" \"m\" \"f\" \"f\" \"f\" \"f\" \"f\" \"f\" \"f\" \"m\"\n[20] \"m\" \"m\" \"f\" \"m\" \"m\" \"m\"\n\n\nГендер можно закодировать цветом.\n\nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree, color = gender)) +\n  geom_node_text(aes(label = name)) +\n  theme_graph(base_family = \"sans\") \n\nUsing \"sugiyama\" as default layout\n\n\n\n\n\n\n\n\n\nПоменяем цветовую шкалу уже известным способом.\n\nlibrary(paletteer)\n# двухцветная палитра\ncols &lt;- paletteer_d(\"suffrager::classic\")\n  \nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, # это кружки с заливкой\n                  color = \"black\"\n                  ) +\n  geom_node_text(aes(label = name)) +\n  # убираем лишнюю легенду\n  scale_size(guide = 'none') +\n  scale_fill_manual(values = cols) +\n  theme_graph(base_family = \"sans\") \n\nUsing \"sugiyama\" as default layout\n\n\n\n\n\n\n\n\n\nТеперь подумаем над укладкой.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#укладка-сети",
    "href": "ggraph.html#укладка-сети",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.3 Укладка сети",
    "text": "19.3 Укладка сети\nГрафическое представление одной и той же сети будет зависеть от выбранного способа укладки.\nПри построении графиков сетей стремятся следовать следующим принципам:\n\nминимизировать пересечения ребер;\nмаксимизировать симметричность укладки узлов;\nминимизировать изменчивость длины ребер;\nмаксимизировать угол между ребрами, когда они пересекают или соединяют узлы;\nминимизировать общее пространство для вывода сети.\n\n\nДля автоматического построения укладок разработано большое количество методов. В пакете igraph для каждого есть особая функция; вот некоторые из них:\n\nlayout_randomly()\n\nlayout_in_circle()\n\nlayout_on_sphere()\nlayout_with_drl() (Distributed Recursive Layout)\nlayout_with_fr() (Fruchterman-Reingold)\nlayout_with_dh() (Davidson-Harel)\nlayout_with_kk() (Kamada-Kawai)\nlayout_with_lgl() (Large Graph Layout)\nlayout_as_tree() (Reingold-Tilford)\nlayout_nicely()\n\nПакет ggraph позволяет выбрать укладку, не вызывая отдельно функцию:\n\nlibrary(gridExtra)\n\nlayouts &lt;- c(\"dh\", \"graphopt\", \"fr\", \"kk\")\n\nplot_graph &lt;- function(layout) {\n  g &lt;- ggraph(tudors_g, layout = layout) +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  show.legend = FALSE,\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  labs(title = layout)\n  \n  return(g)\n}\n\ng_list &lt;- map(layouts, plot_graph)\n\ngrid.arrange(grobs = g_list, nrow = 2)\n\n\n\n\n\n\n\n\nПодробнее см. здесь.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#дизайн-ребер",
    "href": "ggraph.html#дизайн-ребер",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.4 Дизайн ребер",
    "text": "19.4 Дизайн ребер\nНаш граф носит направленный характер, а значит мы можем отразить и направленность, и характер связей. Кодируем атрибут relationship, например, типом линии.\n\nset.seed(21092024)\n# добавляем итерации для укладки\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # вот тут вносим изменения\n  geom_edge_link(aes(edge_linetype = relationship),\n                 # меняем цвет линии\n                 color = \"grey50\",\n                 # меняем тип линии\n                 edge_width = 1.2) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  # перемещаем легенду\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nМожно заменить линии на стрелки.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_link(color = \"grey50\",\n                 # стрелка\n                 arrow = arrow(angle = 30, \n                               length = unit(0.25, \"cm\"),\n                               ends = \"last\", \n                               type = \"closed\"),\n                 # небольшой отступ от кружка\n                 end_cap = circle(1.5, \"mm\")\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nИли придать им изогнутости и раскрасить.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # вот тут изменения\n  geom_edge_arc(aes(color = relationship),\n                 # как сильно изгибать\n                 strength = 0.2,\n                 arrow = arrow(angle = 30, \n                               length = unit(0.2, \"cm\"),\n                               # от родителей к детям, а не наоборот\n                               ends = \"first\", \n                               type = \"closed\"),\n                 # тут тоже меняем \n                 start_cap = circle(1.5, \"mm\")\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  # цветовая шкала для ребер\n  scale_edge_color_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#подписи",
    "href": "ggraph.html#подписи",
    "title": "19  Графический дизайн сетей в ggraph",
    "section": "19.5 Подписи",
    "text": "19.5 Подписи\nЕсли мы просто вернем подписи, то они будут не очень читаемы, даже на нашем (очень небольшом) датасете.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # тип линии вместо цвета, убираем стрелку\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  # чуть подвинем\n  geom_node_text(aes(label = name), nudge_y = 0.5) +\n  scale_fill_manual(values = cols) +\n  # тип линии для ребер\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nОдно из решений может выглядеть так.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  # изменения тут\n  geom_node_label(aes(label = name, \n                      fill = gender),\n                  color = \"white\"\n                  ) +\n  scale_fill_manual(values = cols) +\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nПри желании можно заменить подписи на портреты или любую другую картинку.\n\nlibrary(ggimage)\nqueen &lt;- c(\"./images/queen.png\")\nking &lt;- c(\"./images/king.png\")\n\ngender_tbl &lt;- gender_tbl |&gt; \n  mutate(image = case_when(gender == \"m\" ~ king,\n                           gender == \"f\" ~ queen))\n\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                color = \"grey50\",\n                strength = 0.2\n  ) + \n  # изменения тут\n  geom_image(aes(x = x, \n                 y = y,\n                 image = gender_tbl$image),\n             size = 0.1)+\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nЕсли бы в наших данных были сведения о годе рождения, то мы могли бы их тоже учесть на графе, но пока оставим как есть.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей в `ggraph`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#подписи-с-geom_node_label",
    "href": "ggraph.html#подписи-с-geom_node_label",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.5 Подписи с geom_node_label()",
    "text": "19.5 Подписи с geom_node_label()\nЕсли мы просто вернем подписи, то они будут не очень читаемы, даже на нашем (очень небольшом) датасете.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # тип линии вместо цвета, убираем стрелку\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  # чуть подвинем\n  geom_node_text(aes(label = name), nudge_y = 0.5) +\n  scale_fill_manual(values = cols) +\n  # тип линии для ребер\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nОдно из решений может выглядеть так.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  # изменения тут\n  geom_node_label(aes(label = name, \n                      fill = gender),\n                  color = \"white\"\n                  ) +\n  scale_fill_manual(values = cols) +\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#картинки-с-geom_image",
    "href": "ggraph.html#картинки-с-geom_image",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.6 Картинки с geom_image()",
    "text": "19.6 Картинки с geom_image()\nПри желании можно заменить подписи на портреты или любую другую картинку.\n\nlibrary(ggimage)\nqueen &lt;- c(\"./images/queen.png\")\nking &lt;- c(\"./images/king.png\")\n\ngender_tbl &lt;- gender_tbl |&gt; \n  mutate(image = case_when(gender == \"m\" ~ king,\n                           gender == \"f\" ~ queen))\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                color = \"grey50\",\n                strength = 0.2\n  ) + \n  # изменения тут\n  geom_image(aes(x = x, \n                 y = y,\n                 image = gender_tbl$image),\n             size = 0.1)+\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nЕсли бы в наших данных были сведения о годе рождения, то мы могли бы их тоже учесть на графе, но пока оставим как есть.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#интерактивный-граф",
    "href": "ggraph.html#интерактивный-граф",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.7 Интерактивный граф",
    "text": "19.7 Интерактивный граф\nЧтобы добавить интерактивности, придется выйти за пределы ggraph. Пакет networkD3 требует на входе датафрейм.\n\n# install.packages(\"networkD3\")\nlibrary(networkD3)\nsimpleNetwork(tudors)\n\n\n\n\n\nЕще один вариант. Сначала трансформируем igraph в объект visNetwork. Цвета, если мы хотим на них повлиять, можно поменять вручную.\n\ncolors &lt;- ifelse(V(tudors_g)$gender==\"f\", cols[1], cols[2])\n\nV(tudors_g)$color &lt;- colors\n\n\n#install.packages(\"visNetwork\")\nlibrary(visNetwork)\ndata &lt;- toVisNetworkData(tudors_g)\n\n\ntudors_3d &lt;- visNetwork(nodes = data$nodes, \n                             edges = data$edges, \n                             color = data$nodes$color,\n                             width = \"100%\", \n                             height = 600)\n\nНастраиваем и сохраняем граф.\n\nvisOptions(tudors_3d, \n           highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE), \n           nodesIdSelection = FALSE)  |&gt; \n  visPhysics(maxVelocity = 20, stabilization = FALSE)  |&gt;  \n  visInteraction(dragNodes = TRUE)  |&gt; \n  # удалите эту строку, если хотите видеть граф во вьюере\n  visSave(file = \"tudors.html\")\n\nEt voilà. Все наши Тюдоры как живые.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "dracor.html",
    "href": "dracor.html",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "",
    "text": "20.1 О корпусе Dracor\nDraCor — сокращение от drama corpora — это собрание размеченных по стандарту TEI драматических текстов. Здесь есть пьесы на французском, немецком, испанском, русском, итальянском, шведском, португальском (только Кальдерон) и английском (только Шекспир), а также совсем небольшие коллекции эльзасских, татарских и башкирских пьес.\nДва крупных корпуса пьес в составе собрания — немецкий и русский — были собраны и поддерживаются создателями проекта DraCor. Остальные корпуса были взяты из сторонних проектов, а затем адаптированы для совместимости с функционалом DraCor. Подробнее об этом можно прочитать здесь.\nНа сайте проекта “Системный Блокъ” можно прочитать серию материалов о том, как возможности Dracor используются в литературоведении:",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#о-корпусе-dracor",
    "href": "dracor.html#о-корпусе-dracor",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "",
    "text": "о “Ревизоре”;\nо плотности сетей в трагедии и комедии;\nо “зоне смерти” в “Гамлете”.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#начало-работы-с-dracor",
    "href": "dracor.html#начало-работы-с-dracor",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.2 Начало работы с Dracor",
    "text": "20.2 Начало работы с Dracor\n\n# remotes::install_github(\"Pozdniakov/rdracor\")\nlibrary(rdracor)\nlibrary(tidyverse)\nlibrary(igraph)\n\nget_dracor_meta()  |&gt; \n  summary()\n\nDraCor hosts 21 corpora comprising 4312 plays.\n\nThe last updated corpus was Polish Drama Corpus (2025-01-19 11:56:45.9).\n\n\nИзвлекаем метаданные.\n\nmeta &lt;- get_dracor_meta()  |&gt; \n  select(name, title, plays)\n\nmeta\n\n\n  \n\n\n\n\nmeta  |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\nrus &lt;- get_dracor(\"rus\")\nsummary(rus)\n\n212 plays in Russian Drama Corpus   \nCorpus id: rus, repository: https://github.com/dracor-org/rusdracor \nDescription: Edited by Frank Fischer and Daniil Skorinkin. Features more than 200 Russian plays from the 1740s to the 1940s. For a corpus description and full credits please see the [README on GitHub](https://github.com/dracor-org/rusdracor).\nWritten years (range): 1747–1940    \nPremiere years (range): 1750–1992   \nYears of the first printing (range): 1747–1986\n\n\n\nrus &lt;- as_tibble(rus)\nrus\n\n\n  \n\n\n\nТут хранится очень много всего: размер сети, плотность сети и т.д. Вот так, например, выглядят самые длинные пьесы в корпусе:\n\nrus  |&gt; \n  arrange(-wordCountText)  |&gt; \n  select(firstAuthorName, title, wordCountText)\n\n\n  \n\n\n\nА так – самые густонаселенные.\n\nrus  |&gt; \n  arrange(-size) |&gt; \n  select(firstAuthorName, title, size)\n\n\n  \n\n\n\nПодробнее см. презентацию Ивана Позднякова, разработчика DraCor Shiny App (https://shiny.dracor.org/).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#сети-dracor",
    "href": "dracor.html#сети-dracor",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.3 Сети Dracor",
    "text": "20.3 Сети Dracor\nИзвлекаем граф для “Бориса Годунова”.\n\ngodunov &lt;- get_net_cooccur_igraph(play = \"pushkin-boris-godunov\", corpus = \"rus\")\n\ngodunov\n\nIGRAPH 19c706a UNW- 79 327 -- \n+ attr: name (v/c), isGroup (v/l), gender (v/c), numOfScenes (v/n),\n| numOfSpeechActs (v/n), numOfWords (v/n), degree (v/n), weightedDegree\n| (v/n), closeness (v/n), betweenness (v/n), eigenvector (v/n),\n| wikidataId (v/c), weight (e/n)\n+ edges from 19c706a (vertex names):\n[1] Воротынский--Шуйский                   \n[2] Воротынский--Борис                     \n[3] Воротынский--Бояре                     \n[4] Шуйский    --Борис                     \n[5] Шуйский    --Бояре                     \n+ ... omitted several edges\n\n\nПомимо уже знакомых нам атрибутов вроде имени (name (v/c)), гендера (gender (v/c)) и степени (degree(v/n)), мы видим здесь много новой информации. Некоторые атрибуты интуитивно понятны: является ли персонаж групповым (isGroup (v/l)); в каком часле явлений он участвует (numOfScenes (v/n)); сколько у него реплик (numOfSpeechActs (v/n)) и слов (numOfWords (v/n)).\nАтрибут wikidataId (v/c) представлен лишь для исторических лиц, например, для самого Годунова.\n\ntibble(name = V(godunov)$name,\n       isGroup = V(godunov)$isGroup,\n       numOfScenes = V(godunov)$numOfScenes,\n       numOfSpeechActs = V(godunov)$numOfSpeechActs,\n       numOfWords = V(godunov)$numOfWords) |&gt; \n  arrange(-numOfScenes)\n\n\n  \n\n\n\nЭта информация заботливо собрана создателями Dracor’а, но при желании ее можно проверить: функция get_text_df() дает возможность извлечь текст пьесы в виде датафрейма. Убедимся, например, что Григорий появляется в 8 сценах.\n\ngodunov_df &lt;- get_text_df(play = \"pushkin-boris-godunov\", corpus = \"rus\")\ngodunov_df\n\n\n  \n\n\n\n\ngodunov_df |&gt; \n  filter(who == \"grigorij_dimitrij_lzhedimitrij_samozvanets\") |&gt; \n  count(scene_id)\n\n\n  \n\n\n\nНам осталось разобраться с такими атрибутами, как weightedDegree (v/n), closeness (v/n), betweenness (v/n), eigenvector (v/n), weight (e/n). Начнем с последнего.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#анализ-узлов-и-ребер",
    "href": "dracor.html#анализ-узлов-и-ребер",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.4 Анализ узлов и ребер",
    "text": "20.4 Анализ узлов и ребер\n\n20.4.1 Вес ребра\nВеса ребер, как следует из технической документации к пакету, хранят информацию о том, сколько раз персонажи вместе появляются на сцене.\n\nE(godunov)[weight &gt; 1] \n\n+ 9/327 edges from 19c706a (vertex names):\n[1] Воротынский--Шуйский        Шуйский    --Борис         \n[3] Борис      --Бояре          Борис      --Феодор        \n[5] Борис      --Басманов       Григорий   --Гаврила Пушкин\n[7] Григорий   --Курбский       Григорий   --Ляхи          \n[9] Ксения     --Феодор        \n\n\nЭти сведения можно извлечь и при помощи специальной функции.\n\ncooc_dracor &lt;- get_net_cooccur_edges(play = \"pushkin-boris-godunov\", corpus = \"rus\")\n\ncooc_dracor |&gt; \n  filter(Weight &gt; 1) |&gt; \n  arrange(-Weight)\n\n\n  \n\n\n\n\n\n20.4.2 Взвешенная центральность\nВажность (prominence) участника (актора, вершины, узла) определяется его положением внутри сети. Применительно к ненаправленным сетям говорят о центральности (центральный актор вовлечен в наибольшее количество связей, прямых или косвенных), а применительно к направленным – о престиже. Престижный актор характеризуется большим количеством входящих связей.\nМы уже умеем считать центральность по степени (degree centrality), которая определяется количеством связей: чем больше прямых связей, тем более важным является узел.\n\ndegrees &lt;- degree(godunov)\nsort(degrees, decreasing = T)[1:10]\n\n         Борис          Народ       Григорий         Феодор          Бояре \n            29             26             25             23             21 \n      Басманов         Ксения        Шуйский Гаврила Пушкин           Ляхи \n            15             14             13             11             10 \n\n# проверка\n# V(godunov)$degree == degrees\n\nС понятием веса ребра тесно связана взвешенная центральность по степени (weightedDegree(v/n)). В отличие от простой центральности, она учитывает вес связанных с узлом ребер.\n\nwDegree &lt;- strength(godunov)\n\n# проверка\ntibble(wd_old = V(godunov)$weightedDegree,\n       wd_new = wDegree) |&gt; \n  arrange(-wd_old)\n\n\n  \n\n\n\nВот так, например, считается взвешенная центральность для Бориса Годунова:\n\n# ребра, связанные с Годуновым\nidx &lt;- incident(godunov, \"Борис\")\n\n# суммарный вес ребер\nsum(E(godunov)[idx]$weight)\n\n[1] 35\n\n\nНа графе веса ребер можно отразить за счет толщины и (или) прозрачности линии, а взвешенную центральность - за счет размера узла\n\nlibrary(ggraph)\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"nbapalettes::hawks_statement\")\n\nset.seed(22092024)\nggraph(godunov, layout = \"kk\", maxiter = 500) + \n  # здесь кодируем вес ребер\n  geom_edge_link(aes(alpha = weight),\n                 color = cols[3],\n                 width = 0.8,\n                 show.legend = FALSE) +\n  # здесь взвешенная центральность\n  geom_node_point(aes(size = weightedDegree),\n                  color = cols[2],\n                  show.legend = FALSE) + \n  # обратите внимание на фильтр!\n  geom_node_text(aes(filter = (weightedDegree &gt; 10 | name %in% c(\"Курбский\", \"Воротынский\")),\n                     label = name),\n                 color = cols[1],\n                 repel = TRUE) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n20.4.3 Центральность по близости\nЦентральность по близости (closeness centrality) говорит о том, насколько близко узел расположен к другим узлам сети. Центральность по близости – это величина, обратная сумме расстояний от узла i до всех остальных узлов сети.\n\\[\\frac{1}{\\sum_{i\\neq v}d_{vi}}\\]\n\nОбратите внимание: в Dracor все метрики рассчитывались с использованием Python-пакета networkX. Имплементации расчетов сетевых метрик могут не совпадать.\n\n\nclose_new &lt;- closeness(godunov, \n                     mode = \"all\",\n                     normalized = TRUE)\n\n# похожие значения хранятся как атрибуты узлов\ntibble(name = V(godunov)$name,\n       close_old = V(godunov)$closeness,\n       close_new = close_new) |&gt; \narrange(-close_old)\n\n\n  \n\n\n\n\nВ пьесах эта метрика может означать, напрямую ли взаимодействуют с этим персонажем или нет. Например, в пьесе А. Н. Островского «Лес» персонаж Аксюша имеет невысокую взвешенную степень, но наибольшую степень близости. По сюжету, она находится в зависимом положении, в первую очередь от Гурмыжской (которая имеет наибольшую взвешенную степень), и это может означать что остальные персонажи взаимодействуют с ней напрямую, так как могут себе это позволить. – Источник.\n\nНа графе закодируем этот атрибут цветом; температурную шкалу установим вручную.\n\nset.seed(22092024)\nggraph(godunov, layout = \"kk\", maxiter = 500) + \n  geom_edge_link(aes(alpha = weight),\n                 color = cols[3],\n                 width = 0.8,\n                 show.legend = FALSE) +\n  geom_node_point(aes(size = weightedDegree,\n                      # тут новое\n                      color = closeness),\n                  show.legend = FALSE) + \n  geom_node_text(aes(filter = (weightedDegree &gt; 10 | name %in% c(\"Курбский\", \"Воротынский\")),\n                     label = name),\n                 color = cols[3],\n                 repel = TRUE) +\n  # градиентная шкала для closeness\n  scale_color_gradient(low = \"plum\", high = \"purple\") +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n20.4.4 Центральность по посредничеству\nЦентральность по посредничеству (betweenness centrality) характеризует, насколько важную роль данный узел играет на пути “между” парами других узлов сети.\n\nbetweenness_new &lt;- betweenness(godunov, \n                               directed = FALSE,\n                               normalized = TRUE)\n\ntibble(name = V(godunov)$name,\n       b_old = round(V(godunov)$betweenness, 4),\n       b_new = round(betweenness_new, 4)) |&gt; \n  arrange(-b_old)\n\n\n  \n\n\n\n\nХороший пример персонажа с высокой степенью посредничества в корпусе русской драмы — второстепенный персонаж Гаврила Пушкин из пьесы «Борис Годунов» А.С. Пушкина. …По сюжету, он является связующим персонажем между приближёнными Бориса и Григорием. При прочтении легко не заметить важность этого персонажа, однако на визуализации сети пьесы хорошо видно, что Гаврила связывает два кластера — персонажей в Москве и в Польше. – Источник.\n\n\n\n20.4.5 Центральность по собственному вектору\nСтепень влиятельности (eigenvector centrality) показывает важность персонажа, учитывая влиятельность персонажей, с которыми взаимодействует данный персонаж. В пьесах эта метрика позволяет разделить действующих лиц на «центральных» и «периферийных».\n\nПерсонажи более значимы, если они взаимодействуют с персонажами важнее себя, и теряют свою значимость при контакте с менее важными действующими лицами. – Источник.\n\nЧтобы посчитать eigenvector centrality, необходимо преобразовать граф в матрицу смежности (социоматрицу), в которой единицами отмечено наличие рёбер между персонажами, а нулями – их отсутствие. Вместо единиц в матрице могут быть указаны веса рёбер; в таком случае матрица будет взвешенной.\nУ таких матриц есть собственные векторы, то есть такие векторы, произведение которых на матрицу эквивалентно произведению числа на этот вектор.\n\\[\\lambda \\cdot C_e = A \\cdot C_e,\\] где\n\n\\(А\\) — это матрица смежности;\n\\(λ\\) — действительное число;\n\\(С_e\\) — собственный вектор матрицы \\(А\\).\n\nЭлементы вектора \\(C_e\\) являются степенями влиятельности для каждой вершины. Это можно переписать для отдельных вершин так:\n\\[C_e(v_i)=\\frac{1}{\\lambda}\\sum_{j=1}^{n}a_{ij} C_e(v_j),\\] где\n\n\\(С_E(v_i)\\) — это степень влиятельности вершины \\(v_i\\);\n\\(a_ij\\) — элемент матрицы \\(A\\), расположенный в i-й строке и j-м столбце.\n\nВ такой записи видно, что на степень влиятельности вершины \\(v_i\\) влияют значения всех остальных вершин. Алгоритм расчета eigenvector centrality в последних версиях igraph немного отличается.\n\neigen_new &lt;- eigen_centrality(godunov, scale = FALSE)$vector\n\nWarning: The `scale` argument of `eigen_centrality()` always as if TRUE as of igraph\n2.1.1.\nℹ Normalization is always performed\n\ntibble(name = V(godunov)$name,\n       eigen_old = V(godunov)$eigenvector,\n       eigen_new = eigen_new) |&gt; \n  arrange(-eigen_old)\n\n\n  \n\n\n\nТеперь мы понимаем смысл всех атрибутов в графах Dracor, и можем перейти к характеристике графа в целом.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#централизация",
    "href": "dracor.html#централизация",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.5 Централизация",
    "text": "20.5 Централизация\nРассмотрим два крайних случая: круговой граф и звездчатый граф.\n\nstar_g &lt;- make_star(5, mode = \"undirected\") \ncircle_g &lt;- make_ring(5)\n\npar(mfrow = c(1, 2))\nplot(circle_g, vertex.color=2)\nplot(star_g, vertex.color=3)\n\n\n\n\n\n\n\n\nВ случае звездчатого графа централизация максимальна, а для отдельных узлов наблюдается разброс центральности.\n\ncentr_clo(star_g)\n\n$res\n[1] 1.0000000 0.5714286 0.5714286 0.5714286 0.5714286\n\n$centralization\n[1] 1\n\n$theoretical_max\n[1] 1.714286\n\n\nВо втором случае наборот – разброса нет, а для графа в целом централизация минимальна.\n\ncentr_clo(circle_g)\n\n$res\n[1] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667\n\n$centralization\n[1] 0\n\n$theoretical_max\n[1] 1.714286\n\n\nРасчитаем централизацию для графа “Годунова”.\n\ncentr_clo(godunov)$centralization\n\n[1] 0.3050424\n\n\nНо в наших данных она уже рассчитана.\n\nsummary(godunov)\n\nrus: pushkin-boris-godunov - co-ocurence network summary    \nПушкин, Александр Сергеевич: Борис Годунов (1831)   \n    \n         Size: 79 (9 FEMALES, 69 MALES, 1 UNKNOWN)  \n      Density: 0.11 \n       Degree:  \n         - Maximum: 29 (Борис)  \n     Distance:  \n         - Maximum (Diameter): 7    \n         - Average: 3.45    \n   Clustering:  \n         - Global: 0.65 \n         - Average local: 0.92  \n     Cohesion: 1    \nAssortativity: -0.06",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#точки-сочленения",
    "href": "dracor.html#точки-сочленения",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.6 Точки сочленения",
    "text": "20.6 Точки сочленения\nТочка сочленения – это узел, при удалении которого увеличивается число компонент связности. Таким образом, они соединяют разные части сети. При их удалении акторы (узлы, вершины) не могут взаимодействовать друг с другом.\n\narticulation_points(godunov)\n\n+ 7/79 vertices, named, from 19c706a:\n[1] Народ          Патриарх       Григорий       Марина         Гаврила Пушкин\n[6] Борис          Шуйский       \n\n\nТочки сочленения тесто связаны с центральностью по посредничеству.\n\nV(godunov)[betweenness &gt; 0.025]\n\n+ 10/79 vertices, named, from 19c706a:\n [1] Шуйский        Народ          Борис          Бояре          Григорий      \n [6] Патриарх       Феодор         Гаврила Пушкин Марина         Басманов",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#клики",
    "href": "dracor.html#клики",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.7 Клики",
    "text": "20.7 Клики\nМногие сети состоят из относительно плотных подгрупп, которые соединены между собой менее крепкими связями. Один из способов взглянуть на подгруппы сети заключается в исследовании социальной сплочености (cohesion). Сплоченные подгруппы - это множество акторов, которые объединены между собой посредством многочисленных, сильных и прямых связей.\nКлика – один из самых простых типов сплоченных подгрупп; это максимально полный подграф, т.е. подмножество узлов со всеми возможными связями между ними. Вопреки своему названию, функция clique_num() возвращает размер наибольшей клики:\n\nclique_num(godunov)\n\n[1] 11\n\n\nНа самом деле таких клик даже три. Узнаем, кто туда входит.\n\ncliques(godunov, min=11)\n\n[[1]]\n+ 11/79 vertices, named, from 19c706a:\n [1] Народ                   Ксения                  Феодор                 \n [4] Нищий                   Стража                  Один из народа (Кремль)\n [7] Другой (Кремль)         Один из народа (Кремль) Другой (Кремль)        \n[10] Третий (Кремль)         Мосальский             \n\n[[2]]\n+ 11/79 vertices, named, from 19c706a:\n [1] Борис     Бояре     Феодор    Басманов  Боярин    Один      Другой   \n [8] Третий    Четвертый Пятый     Шестой   \n\n[[3]]\n+ 11/79 vertices, named, from 19c706a:\n [1] Народ                                          \n [2] Борис                                          \n [3] Бояре                                          \n [4] Один из народа (Площадь перед собором в Москве)\n [5] Другой (Площадь перед собором в Москве)        \n [6] Третий (Площадь перед собором в Москве)        \n [7] Четвертый (Площадь перед собором в Москве)     \n [8] Мальчишки                                      \n [9] Старуха                                        \n[10] Юродивый                                       \n+ ... omitted several vertices\n\n\nИли, что то же самое:\n\nlargest_cliques(godunov)\n\nНо клика – это очень строгое определение сплоченной группы. Например, чтобы подграф, состоящий из 11 вершин, считался кликой, нужно, чтобы между ними было проведено \\((11 \\times 10) / 2 = 21\\) связей. Если хотя бы одно ребро отсутствует, то условие не выполняется. Такие клики просто очень редко встречаются.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#k-ядра",
    "href": "dracor.html#k-ядра",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.8 K-ядра",
    "text": "20.8 K-ядра\nПопулярным определением социальной сплоченности является k-ядро (k-core). Это максимальный подграф, в котором каждая вершина связана минимум с k другими вершинами этого же подграфа. K-ядра имеют множество преимуществ:\n\nони вложены друг в друга (каждый участник 4-ядра является также участником 3-ядра и т.д.);\nони не перекрываются;\nих легко определить.\n\n\nВыражение 6-ядро читают как “ядро степени 6”.\n\nЯдро степени k+1 является подграфом ядра степени k. Любой узел в ядре степени k имеет степень либо k, либо выше. При этом coreness узла определяется по ядру с наибольшей степенью, к которому они принадлежат.\n\nДля определения k-ядерной структуры используется функция graph.coreness():\n\ncores_godunov &lt;- coreness(godunov)\nhead(cores_godunov)\n\n             Воротынский                  Шуйский   Один (Красная площадь) \n                       3                        5                        4 \nДругой (Красная площадь) Третий (Красная площадь)                    Народ \n                       4                        4                       10 \n\n\nПосчитаем количество вершин в ядрах.\n\ntable(cores_godunov)\n\ncores_godunov\n 1  2  3  4  5  7 10 \n 2  6  1  7 11 23 29 \n\n\nДля лучшей интерпретации k-ядерной структуры мы можем графически изобразить сеть, используя информацию о множестве k-ядер. Для начала добавим информацию о цвете к атрибутам узлов.\n\nV(godunov)$core &lt;- cores_godunov\n\n# убедимся, что добавился новый атрибут\nnames(vertex_attr(godunov))\n\n [1] \"name\"            \"isGroup\"         \"gender\"          \"numOfScenes\"    \n [5] \"numOfSpeechActs\" \"numOfWords\"      \"degree\"          \"weightedDegree\" \n [9] \"closeness\"       \"betweenness\"     \"eigenvector\"     \"wikidataId\"     \n[13] \"core\"           \n\n\n\nset.seed(22092024)\nggraph(godunov, layout = \"kk\", maxiter = 500) + \n  geom_edge_link(color = cols[3],\n                 alpha = 0.3,\n                 width = 0.6) +\n  # тут  новое\n  geom_node_point(aes(color = as.factor(core)),\n                  size = 3, \n                  show.legend = TRUE) + \n  # новый фильтр\n  geom_node_text(aes(filter = degree &gt; 10,\n                     label = name),\n                 color = cols[3],\n                 repel = TRUE) +\n  scale_color_brewer(\"k-ядра\", type = \"qual\") +\n  theme_void()\n\n\n\n\n\n\n\n\nЧтобы глубже исследовать подгруппы, последовательно удаляют k-ядра более низкой степени. Для этого можно воспользоваться функцией induced_subgraph().\n\ngodunov5_10 &lt;- induced_subgraph(godunov, vids=V(godunov)[core &gt; 4])\n\nggraph(godunov5_10, layout = \"kk\", maxiter = 500) + \n  geom_edge_link(color = cols[3],\n                 alpha = 0.3,\n                 width = 0.6) +\n  geom_node_point(aes(color = as.factor(core)),\n                  size = 3, \n                  show.legend = TRUE) + \n  geom_node_text(aes(filter = degree &gt; 10,\n                     label = name),\n                 color = cols[3],\n                 repel = TRUE) +\n  scale_color_brewer(\"k-ядра\", type = \"qual\") +\n  theme_void()\n\n\n\n\n\n\n\n\nПри интерпретации важно помнить, что ядра являются вложенными. Чем выше степень ядра, тем больше узлы связаны между собой.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#модулярность",
    "href": "dracor.html#модулярность",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.9 Модулярность",
    "text": "20.9 Модулярность\nМодулярность — одна из мер структуры сетей или графов. Мера была разработана для измерения силы разбиения сети на модули (называемые группами, кластерами или сообществами). Сети с высокой модулярностью имеют плотные связи между узлами внутри модулей, но слабые связи между узлами в различных модулях.\n\nМодулярность равна доле рёбер от общего числа рёбер, которые попадают в данные группы, минус ожидаемая доля рёбер, которые попали бы в те же группы, если бы они были распределены случайно.\nЕсли все узлы принадлежат к одному классу, то модулярность равна нулю. Если разбиение на классы хорошее, то модулярность должна быть высокая. Мы можем проверить, хорошо ли группируются персонажи по гендеру. Для этого перекодируем гендер, так как функция modularity() принимает числовую переменную в качестве аргумента. Значение гендера 3 (unknown) в пьесе имеют групповые персонажи.\n\nV(godunov)$gender\n\n [1] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"UNKNOWN\" \"MALE\"   \n [8] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[15] \"MALE\"    \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[22] \"MALE\"    \"FEMALE\"  \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[29] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[36] \"FEMALE\"  \"MALE\"    \"FEMALE\"  \"FEMALE\"  \"FEMALE\"  \"MALE\"    \"MALE\"   \n[43] \"MALE\"    \"MALE\"    \"MALE\"    \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"   \n[50] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[57] \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[64] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[71] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[78] \"MALE\"    \"MALE\"   \n\n## male = 1\nidx &lt;- V(godunov)$gender==\"MALE\"\nV(godunov)$gender[idx] &lt;- 1\n\n## female = 2\nidx &lt;- V(godunov)$gender==\"FEMALE\"\nV(godunov)$gender[idx] &lt;- 2\n\n## unknown = 3\nidx &lt;- V(godunov)$gender==\"UNKNOWN\"\nV(godunov)$gender[idx] &lt;- 3\n\n\ngender &lt;- as.numeric(V(godunov)$gender)\nmodularity(godunov, gender)\n\n[1] 0.01434597\n\n\n\nПри выделении сообществ в большинстве случаев наша задача – максимизировать модулярность.\n\nОчевидно, что гендер не лучшим образом описывает деление персонажей “Годунова” на группы. Поищем другие сообщества.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#алгоритмы-обнаружения-сообществ",
    "href": "dracor.html#алгоритмы-обнаружения-сообществ",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.10 Алгоритмы обнаружения сообществ",
    "text": "20.10 Алгоритмы обнаружения сообществ\nВ пакете igraph реализовано множество алгоритмов обнаружения сообществ. Обычной практикой является применение нескольких алгоритмов и сравнение результатов.\n\nУ нас ненаправленная взвешенная сеть. Применим алгоритм “случайного блуждания”.\n\ncw &lt;- cluster_walktrap(godunov)\nmembership(cw) |&gt; head()\n\n             Воротынский                  Шуйский   Один (Красная площадь) \n                       2                        2                        9 \nДругой (Красная площадь) Третий (Красная площадь)                    Народ \n                       9                        9                        2 \n\n\n\npar(mar = rep(0, 4))\nplot(cw, godunov)\n\n\n\n\n\n\n\n\nЗначение модулярности достаточно высокое (уж точно лучше, чем гендер).\n\nmodularity(cw)\n\n[1] 0.5639771\n\n\nПоищем другое разбиение.\n\ncsg &lt;- cluster_spinglass(godunov)\nmembership(csg) |&gt; head()\n\n             Воротынский                  Шуйский   Один (Красная площадь) \n                       6                        6                        2 \nДругой (Красная площадь) Третий (Красная площадь)                    Народ \n                       2                        2                        2 \n\n\n\npar(mar = rep(0, 4))\nplot(csg, godunov)\n\n\n\n\n\n\n\n\nПоказатели модулярности чуть выше, чем для предыдущего разбиения.\n\nmodularity(csg)\n\n[1] 0.6128681\n\n\nТакже используем алгоритм под названием “главный собственный вектор”.\n\ncev &lt;- cluster_leading_eigen(godunov)\nmodularity(cev)\n\n[1] 0.617586\n\n\n\npar(mar = rep(0, 4))\nplot(cev, godunov)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "21  Пространственные данные в R",
    "section": "",
    "text": "21.1 Данные: римские амфитеатры\nДанные для этого урока происходят из пакета cawd (Collected Ancient World Data), который, в свою очередь, опирается на следующие ресурсы:\nМы заберем из пакета датафрейм с римскими амфитеатрами (подробнее о нем можно прочитать здесь) и карту Римской империи на 200 г. н.э. (в формате sp, который представляет собой немного устаревший, но легко конвертируемый формат хранения пространственных данных в R).\n#devtools::install_github(\"sfsheath/cawd\")\nlibrary(cawd)\nlibrary(sp)\nclass(awmc.roman.empire.200.sp)\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\nОбъект sp имеет свой метод plot().\npar(mai=c(0,0,0,0))\nplot(awmc.roman.empire.200.sp)\nДля начала выберем нужные столбцы из датафрейма с данными об амфитеатрах.\nlibrary(tidyverse)\n\nramphs &lt;- cawd::ramphs |&gt; \n  dplyr::select(label, longitude, latitude, capacity, type, prov.type)\n\nramphs",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#пакет-tmap",
    "href": "maps.html#пакет-tmap",
    "title": "21  Пространственные данные в R",
    "section": "21.3 Пакет tmap",
    "text": "21.3 Пакет tmap\nЕсть множество пакетов для работы с пространственными данными в R; мы начнем с одного из наиболее простого и интуитивно понятного tmap.\n\n# install.packages(\"tmap\")\n# install.packages(\"tmaptools\")\nlibrary(tmap)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntmap_style(\"white\") # default\n\nstyle set to \"white\" (tmap default)\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\"\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\n\n\ntm_shape(roman_map) +\n  tm_fill(fill = \"magenta\") +\n  tm_borders(col = \"white\") \n\n\n\n\n\n\n\n\nПакет tmap предлагает хороший выбор стилей для оформления карты.\n\ntmap_style(\"classic\")\n\nstyle set to \"classic\"\n\n\nother available styles are: \"white\" (tmap default), \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\ntm_shape(roman_map) +\n  tm_fill() +\n  tm_borders() \n\n\n\n\n\n\n\n\nВручную можно добавить, например, компасс, координатную сетку и шкалу масштаба.\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_graticules() +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scalebar(\n    breaks = c(0, 500, 1000, 1500),\n    text.size = 1, \n    position = c(\"left\", \"bottom\")) \n\nScale bar set for latitude km and will be different at the top and bottom of the map.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#точки",
    "href": "maps.html#точки",
    "title": "21  Пространственные данные в R",
    "section": "21.3 Точки",
    "text": "21.3 Точки\n\nlibrary(tidyverse)\n\nramphs &lt;- cawd::ramphs |&gt; \n  dplyr::select(label, longitude, latitude, capacity, prov.type)\n\nramphs\n\n\n  \n\n\n\n\namph_points &lt;- ramphs |&gt; \n   st_as_sf(coords = c(\"longitude\", \"latitude\"))\n\namph_points\n\n\n  \n\n\n\n\ntmap_style(\"classic\")\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(type = \"8star\", \n             position = c(\"right\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), \n               text.size = 1, \n               position = c(\"left\", \"bottom\")) +\n  tm_shape(amph_points) +\n  tm_bubbles(size = \"capacity\", \n             alpha = 0.8, \n             scale = 1,\n             col = \"prov.type\", \n             palette = c(\"red\", \"blue\", \"green\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE\n            )\n\n\n\n\n\n\n\n\nhttps://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(type = \"8star\", \n             position = c(\"right\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), \n               text.size = 1, \n               position = c(\"left\", \"bottom\")) +\n  tm_shape(amph_points) +\n  tm_bubbles(size = \"capacity\", \n             alpha = 0.8, \n             scale = 1,\n             col = \"prov.type\", \n             palette = c(\"red\", \"blue\", \"green\")\n             ) +\n  # фильтр для названий\n  tm_shape(amph_points |&gt; filter(capacity &gt; 30000 )) +\n  # текст\n  tm_text(\"label\", size = 0.8) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE\n            )\n\nWarning: Currect projection of shape amph_points unknown. Long-lat (WGS84) is\nassumed.\n\n\nWarning: Currect projection of shape filter(amph_points, capacity &gt; 30000)\nunknown. Long-lat (WGS84) is assumed.\n\n\nScale bar set for latitude km and will be different at the top and bottom of the map.\n\n\n\n\n\n\n\n\n\nhttps://r-tmap.github.io/tmap/",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#заливка",
    "href": "maps.html#заливка",
    "title": "21  Пространственные данные в R",
    "section": "21.4 Заливка",
    "text": "21.4 Заливка\nhttps://r-spatial.org/book/07-Introsf.html Посчитать число точек в многограннике\nhttps://epsg.io/ почему это важно см. lovelace\n\nlibrary(sf)\n#st_crs(roman_map)\nst_crs(amph_points)\n\nCoordinate Reference System: NA\n\n\n\nst_intersects(roman_map, amph_points)\n\nError in st_geos_binop(\"intersects\", x, y, sparse = sparse, prepared = prepared, : st_crs(x) == st_crs(y) is not TRUE\n\n\nЧтобы исправить, необходимо назначить координатные системы.\n\n# если нужно трансформировать\nroman_map &lt;- st_transform(roman_map, 4326)\n\n# если нужно назначить\namph_points &lt;- st_set_crs(amph_points, 4326)\n\n# пересечения\ninter &lt;- st_intersects(roman_map, amph_points)\n\n\ninter\n\nSparse geometry binary predicate list of length 112, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 45, 46, 69, 78, 80, 88, 108, 109, 110, 151, ...\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: (empty)\n 6: (empty)\n 7: 3, 21, 38, 39, 47, 74, 89, 114, 115, 124, ...\n 8: (empty)\n 9: (empty)\n 10: (empty)\n\n\n\n# добавляем данные \nroman_map$count &lt;- lengths(inter)\n\n\ntm_shape(roman_map) +\n  tm_polygons(col = \"count\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#ландшафт",
    "href": "maps.html#ландшафт",
    "title": "21  Пространственные данные в R",
    "section": "21.5 Ландшафт",
    "text": "21.5 Ландшафт\nhttps://rdrr.io/cran/tmaptools/man/bb.html\n\nlibrary(tmaptools)\nbb_region = bb(roman_map)\nbb_region\n\n    xmin     ymin     xmax     ymax \n-9.48732 22.89549 43.10774 55.10117 \n\n\nhttps://docs.stadiamaps.com/map-styles/stamen-terrain/ (почему не гугл: нужна регистрация с картой!)\n\ntmap_mode(\"view\")\n\ntm_basemap(\"https://tiles.stadiamaps.com/tiles/stamen_terrain_background/{z}/{x}/{y}{r}.png\") +\n  tm_shape(roman_map) +\n  tm_polygons(alpha = 0.5, col = \"count\")",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#плотность-2d",
    "href": "maps.html#плотность-2d",
    "title": "21  Пространственные данные в R",
    "section": "21.6 Плотность 2D",
    "text": "21.6 Плотность 2D\nhttp://sebastianheath.com/cawd/inst/rmarkdown/amphitheater-heatmaps.html\n\nggplot() +\n  geom_sf(data = roman_map) +\n  geom_point(ramphs, \n             mapping = aes(longitude, latitude),\n             color = \"steelblue\", \n             alpha = 0.5)  +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_point(ramphs, color = \"steelblue\", alpha = 0.5,\n             mapping = aes(longitude, latitude)) +\n  geom_density2d(data = ramphs, \n                 mapping = aes(longitude, latitude, \n                               color = after_stat(level)),\n                 size = 1, alpha = 0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#сотовая-диаграмма",
    "href": "maps.html#сотовая-диаграмма",
    "title": "21  Пространственные данные в R",
    "section": "21.7 Сотовая диаграмма",
    "text": "21.7 Сотовая диаграмма\n\ng &lt;- ggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_hex(data = ramphs,\n                 mapping = aes(longitude, latitude),\n           bins = 25,\n           color = \"royalblue\")  +\n  theme_bw() +\n  scale_fill_continuous(trans = \"reverse\") \n\ng\n\n\n\n\n\n\n\n\nhttps://info5940.infosci.cornell.edu/notes/geoviz/raster-maps-with-ggmap/",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#plotly",
    "href": "maps.html#plotly",
    "title": "21  Пространственные данные в R",
    "section": "21.8 Plotly",
    "text": "21.8 Plotly\nhttps://www.paulamoraga.com/book-spatial/making-maps-with-r.html\n\nlibrary(plotly)\nggplotly(g)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#данные-римские-дороги",
    "href": "maps.html#данные-римские-дороги",
    "title": "21  Пространственные данные в R",
    "section": "21.10 Данные: римские дороги",
    "text": "21.10 Данные: римские дороги\n\nroman_roads &lt;- cawd::darmc.roman.roads.major.sp |&gt; \n  st_as_sf()\n\nroman_roads\n\n\n  \n\n\n\n\nggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_sf(data = roman_roads,\n          color = \"steelblue\",\n          alpha = 0.5) +\n  geom_tile()\n\n\n\n\n\n\n\n\nДобавим подложку.\n\nlibrary(ggspatial)\n# Reproject to EPSG:3857 (Web Mercator)\nroman_map_3857 &lt;- st_transform(roman_map, 3857)\nroman_roads_3857 &lt;- st_transform(roman_roads, 3857)\n\nggplot() +\n  # Add spatial tile background\n  annotation_map_tile(\n    type = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/${z}/${y}/${x}.jpg\",\n    zoomin = -1) +\n\n  # Add your spatial features\n  geom_sf(data = roman_map_3857, \n          fill = \"wheat\", alpha = 0.4) +\n  geom_sf(data = roman_roads_3857, \n          color = \"darkblue\", alpha = 0.8, size = 1) +\n\n  # Set coordinate system to Web Mercator to match tiles\n  coord_sf(crs = st_crs(3857)) \n\nZoom: 3",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#географическая-сеть",
    "href": "maps.html#географическая-сеть",
    "title": "21  Пространственные данные в R",
    "section": "21.10 Географическая сеть",
    "text": "21.10 Географическая сеть\nhttps://book.archnetworks.net/visualization https://agricolamz.github.io/daR4hs/7_working_with_geodata.html\n\norbis_coord &lt;- orbis_e |&gt; \n  left_join(orbis_n, by = join_by(source == id)) |&gt; \n  mutate(source = label, .before = target) |&gt; \n  select(-label) |&gt; \n  # сначала пишется широта, потом долгота\n  # например Рим 42 с.ш. 12 в.д., здесь долгота в y\n  # но на карте горизонталь - это долгота \n  rename(x1 = y, y1 = x) |&gt; \n  left_join(orbis_n, by = join_by(target == id)) |&gt; \n  mutate(target = label, .after = source) |&gt; \n  select(-label) |&gt; \n  rename(x2 = y, y2 = x) |&gt;\n  # пуповины к центру мира\n  filter(x1 != 0, y1 !=0, x2 != 0, y2 != 0)\n\n\nworld &lt;- map_data(\"world\") \n\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"wheat\", color = \"grey\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = \"steelblue\", alpha = 0.5) +\n  geom_point(data = orbis_coord |&gt; \n               filter(source == \"Roma\"), \n             color = \"tomato\",\n             aes(x1, y1)) +\n  coord_map(xlim = c(-10, 50),\n            ylim = c(23, 54)) +\n  geom_segment(data = orbis_coord, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type))\n\n\n\n\n\n\n\n\nПараллельные линии https://rpubs.com/BrendanKnapp/GeospatialNetworkPlotting\nМожно просто удалить часть городов (восточнее Берениса)\n\norbis_coord_pruned &lt;-  orbis_coord |&gt; \n  filter(y1 &gt; 28 & y2 &gt; 28)\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"basetheme::brutal\")\n\n\npar(mar = rep(0,4))\nset.seed(24092024)\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"white\", color = \"wheat\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = cols[1], alpha = 0.5) +\n  geom_segment(data = orbis_coord_pruned, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type)) +\n  geom_label(data = orbis_coord |&gt; \n               filter(source %in% c(\"Roma\", \"Alexandria\", \"Carthago\", \"Sirmium\", \"Corinthus\", \"Antiochia\", \"Londinium\", \"Tarraco\", \"Augusta Taurinorum\", \"Jerusalem\")),\n             aes(x1, y1, label = source),\n             color = cols[5], \n             label.size = 0.15,\n             fontface = \"bold\") +\n  coord_map(xlim = c(-10, 45),\n            ylim = c(26, 54)) +\n  labs(x = NULL, y = NULL, \n       title = \"Транспортное сообщение в Римской империи\",\n       subtitle = \"Данные проекта Orbis\") +\n  theme_bw(base_family = \"serif\") +\n  theme(legend.position=\"bottom\", \n        legend.box = \"horizontal\",\n        panel.background = element_rect(fill = \"aliceblue\"),\n        text = element_text(color = cols[5])) +\n  scale_color_manual(\"тип\", values = sample(cols, 10))\n\n\n\n\n\n\n\n\nВсе очень красиво, но есть одно но: все границы стран – современные… 🤡.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#leaflet",
    "href": "maps.html#leaflet",
    "title": "21  Пространственные данные в R",
    "section": "21.7 Leaflet",
    "text": "21.7 Leaflet\nУдобный способ создания интерактивных карт предлагает также пакет Leaflet. Вызовем фон:\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n  addTiles() \n\n\n\n\n\nCcылка на галерею подложек (подсмотрена в курсе Георгия Мороза).\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n  addProviderTiles(\"Esri.WorldImagery\") \n\n\n\n\n\nНекоторые подложки потребуют аутентификации. Для этого надо зарегистрироваться на сайте https://stadiamaps.com/ (это бесплатно), создать в личном кабинете Property и прописать доменное имя для карты. Например, акварельная подложка при публикации в Сети требует аутентификации. Многие другие работают без нее (и почти все – локально).\n\n# цветовая палитра\nramphs$type &lt;- factor(ramphs$type)\nfactpal &lt;- colorFactor(palette = c(\"#DE7424FF\", \"#F5CA37FF\", \"#AD8D26FF\", \"#496849FF\", \"#654783FF\"),\n                       ramphs$type)\nramphs |&gt; \n  leaflet() |&gt; \n  addProviderTiles(\"Stadia.StamenWatercolor\") |&gt; \n  addCircles(lng = ~longitude,\n             lat = ~latitude,\n             color = ~factpal(type),\n             opacity = 0.7,\n             popup = ~paste0(\n               label, \n               \"&lt;/br&gt;\", \n               capacity)\n             )  |&gt; \n  addLegend(pal = factpal, \n            values = ~type)\n\n\n\n\n\nЗаменим кружки на маркеры и сгруппируем их. Наложим это все на снимок из космоса (ок, это просто демо, с освещением у них было не очень).\n\nramphs |&gt; \n  leaflet() |&gt; \n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\") |&gt; \n  addMarkers(lng = ~longitude,\n             lat = ~latitude,\n             popup = ~paste0(\n               label, \n               \"&lt;/br&gt;\", \n               capacity),\n             clusterOptions = markerClusterOptions()\n  ) \n\n\n\n\n\nЗаменим маркеры на изображения амфитеатров.\n\nmy_icon &lt;- makeIcon(\n  iconUrl = \"./images/amphitheatre.png\",\n  iconWidth = 31*215/230,\n  iconHeight = 31, \n  iconAnchorY = 16,\n  iconAnchorX = 31*215/230/2\n)\n\nramphs |&gt; \n  leaflet() |&gt; \n  addProviderTiles(\"Esri.WorldTerrain\") |&gt; \n  addMarkers(icon = ~my_icon, \n             clusterOptions = markerClusterOptions())\n\nAssuming \"longitude\" and \"latitude\" are longitude and latitude, respectively",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#анимация",
    "href": "maps.html#анимация",
    "title": "21  Пространственные данные в R",
    "section": "21.12 Анимация",
    "text": "21.12 Анимация",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "iterate.html#векторизованные-вычисления",
    "href": "iterate.html#векторизованные-вычисления",
    "title": "4  Циклы, условия, функции",
    "section": "4.2 Векторизованные вычисления",
    "text": "4.2 Векторизованные вычисления\nХорошая новость: многие функции в R уже векторизованы, и если необходимо применить функцию к каждому элементу вектора, в большинстве случаев достаточно просто вызвать функцию. Например, у нас есть символьный вектор, и мы хотим узнать количество символов в каждом слове.\n\nhomer &lt;- c(\"в\", \"мысли\", \"ему\", \"то\", \"вложила\", \"богиня\", \"державная\", \"гера\")\n\nДля каждого компонента вектора необходимо выполнить одну итерацию цикла, в нашем случае – применить функцию nchar(). В некоторых языках программирования это делается как-то так:\n\nfor(i in homer) print(nchar(i))\n\n[1] 1\n[1] 5\n[1] 3\n[1] 2\n[1] 7\n[1] 6\n[1] 9\n[1] 4\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВ циклах часто используется буква i. Но никакой особой магии в ней нет, имя переменной можно изменить.\n\n\nМы написали цикл for, который считает количество букв для каждого слова в векторе. Как видно, все сработало. Но в R это избыточно, потому что nchar() уже векторизована:\n\nnchar(homer)\n\n[1] 1 5 3 2 7 6 9 4\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nЭто относится не только ко многим встроенным функциям R, но и к операторам. x + 4 в действительности представляет собой +(x, 4). Ключевую роль здесь играет переработка данных, о которой мы говорили в первом уроке: короткий вектор повторяется до тех пор, пока его длина не сравняется с длиной более длинного вектора.\n\n\nЛишний цикл может замедлить вычисления. Проверим.\n\nlibrary(tictoc)\n\n# способ первый\ntic()\nfor(i in homer) print(nchar(i))\ntoc()\n# 0.075 sec elapsed\n\n# способ второй \ntic()\nnchar(homer)\ntoc()\n# 0.017 sec elapsed\n\n\nОдин из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми.\n— Нормат Мэтлофф\n\nДля работы со списками циклы тоже чаще всего избыточны. Для случаев, когда надо применить какую-то функцию ко всем элементам списка, в базовом R для используются функционалы семейства _apply(), а в tidyverse их с успехом заменяет семейство map_() из пакета {purrr}.\n\n\n\n\n\n\nЗадание\n\n\n\nОпционально: пройдите урок 10 lapply and sapply и урок 11 vapply and tapply из курса R Programming в swirl.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#семейство-_apply",
    "href": "iterate.html#семейство-_apply",
    "title": "4  Циклы, условия, функции",
    "section": "4.2 Семейство _apply()",
    "text": "4.2 Семейство _apply()\nДля работы со списками циклы тоже чаще всего избыточны. Снова воспользуемся списком печенек из коллекции rcorpora.\n\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\n\ntic()\nfor (i in 1:length(my_list)) print(length(my_list[[i]]))\n\n[1] 1\n[1] 35\n[1] 20\n\ntoc()\n\n0.001 sec elapsed\n\n\nНо в базовом R для таких случаев существуют функционалы lapply() и sapply(). Они принимают на входе список и функцию и применяют функцию к каждому элементу списка. Получается быстрее:\n\ntic()\nlapply(my_list, length)\n\n$description\n[1] 1\n\n$breads\n[1] 35\n\n$pastries\n[1] 20\n\ntoc()\n\n0.001 sec elapsed\n\n\nФункция sapply() упростит результат до вектора (s означает “simplify”):\n\ntic()\nsapply(my_list, length)\n\ndescription      breads    pastries \n          1          35          20 \n\ntoc()\n\n0.001 sec elapsed\n\n\nПоскольку датафрейм – это двумерный аналог списка, то и здесь можно заменить цикл на _apply(). Сравните.\n\ndf &lt;- data.frame(author=c(\"Joe\",\"Jane\"), year=c(1801,1901), reprints=c(TRUE,FALSE))\n\n## цикл \ntic()\nfor (i in seq_along(df)) {\n print(class(df[,i]))\n}\n\n[1] \"character\"\n[1] \"numeric\"\n[1] \"logical\"\n\ntoc()\n\n0.002 sec elapsed\n\n## sapply\ntic()\nsapply(df, class)\n\n     author        year    reprints \n\"character\"   \"numeric\"   \"logical\" \n\ntoc()\n\n0 sec elapsed\n\n\nЕсть еще vapply(), tapply() и mapply(), но и про них мы не будем много говорить, потому что все их с успехом заменяет семейство map_() из пакета purrr в tidyverse.\n\n\n\n\n\n\nЗадание\n\n\n\nПройдите урок 10 lapply and sapply и урок 11 vapply and tapply из курса R Programming в swirl.\n\n\nТем не менее, перед освоением семейства map_() стоит потренироваться работать с обычными циклами, особенно если вам не приходилось иметь с ними дела (например, на Python). Несмотря на все недостатки, цикл for интуитивно понятен и часто проще начинать именно с него.\n\n\n\n\n\n\nЗадание\n\n\n\nПревратите детскую потешку “Ted in the Bed” в функцию. Обобщите до любого числа спящих.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#синтаксис-функций",
    "href": "iterate.html#синтаксис-функций",
    "title": "4  Циклы, условия, функции",
    "section": "4.3 Синтаксис функций",
    "text": "4.3 Синтаксис функций\nФункция и код – не одно и то же. Чтобы стать функцией, кусок кода должен получить имя. Но зачем давать имя коду, который и так работает?\nВот три причины, которые приводит Хадли Уикхем:\n\nу функции есть выразительное имя, которое облегчает понимание кода;\nпри изменении требований необходимо обновлять код только в одном месте, а не во многих;\nменьше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом)\n\n\nWriting good functions is a lifetime journey.\n— Hadley Wickham\n\nМашине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена!\nДалее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть).\nНаписание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию.\nНапишем функцию, которая будет переводить градусы по Фаренгейту в градусы по Цельсию.\n\nfahrenheit_to_celsius &lt;- function(fahrenheit){ \n  celsius = (fahrenheit - 32) / 1.8\n  return(round(celsius))\n}\n\nfahrenheit_to_celsius(451)\n\n[1] 233\n\n\nВнутри нашей функции есть переменная celsius, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции.\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию, которая ищет совпадения в двух символьных векторах и возвращает совпавшие элементы.\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗагрузите библиотеку swirl, выберите курс R Programming и пройдите из него урок 9 Functions.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДля просмотра исходного кода любой функции необходимо…\n\n\n\n\n\n\nвызвать help к функции\n\n\nнабрать имя функции без аргументов и без скобок\n\n\nединственный способ — найти код функции в репозитории на GitHub\n\n\nиспользовать специальную функцию для просмотра кода",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#условия-внутри-функций",
    "href": "iterate.html#условия-внутри-функций",
    "title": "4  Циклы, условия, функции",
    "section": "4.6 Условия внутри функций",
    "text": "4.6 Условия внутри функций\nФункция может принимать произвольное число аргументов. Доработаем наш код:\n\nconvert_temperature &lt;- function(x, mode = \"f_to_c\"){ \n  if(mode == \"f_to_c\") {\n    celsius = round((x - 32) / 1.8)\n    return(paste(celsius, \"градусов по Цельсию\"))\n  } else if (mode == \"c_to_f\") {\n    fahrenheit = round(x * 1.8 + 32)\n    return(paste(fahrenheit, \"градусов по Фаренгейту\"))\n  }\n}\n\nconvert_temperature(84)\n\n[1] \"29 градусов по Цельсию\"\n\nconvert_temperature(29, mode = \"c_to_f\")\n\n[1] \"84 градусов по Фаренгейту\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#сообщения-и-условия-остановки",
    "href": "iterate.html#сообщения-и-условия-остановки",
    "title": "4  Циклы, условия, функции",
    "section": "4.7 Сообщения и условия остановки",
    "text": "4.7 Сообщения и условия остановки\nЧасто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении.\n\nconvert_temperature &lt;- function(x, mode = \"f_to_c\"){\n  if(!is.numeric(x)) stop(\"non-numeric input\")\n  \n  message(\"Please, wait...\")\n  if(mode == \"f_to_c\") {\n    celsius = round((x - 32) / 1.8)\n    return(paste(celsius, \"градусов по Цельсию\"))\n  } else if (mode == \"c_to_f\") {\n    fahrenheit = round(x * 1.8 + 32)\n    return(paste(fahrenheit, \"градусов по Фаренгейту\"))\n  }\n}\n\nconvert_temperature(\"двадцать пять\")\n\nError in convert_temperature(\"двадцать пять\"): non-numeric input\n\nconvert_temperature(78)\n\nPlease, wait...\n\n\n[1] \"26 градусов по Цельсию\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#switch",
    "href": "iterate.html#switch",
    "title": "4  Циклы, условия, функции",
    "section": "4.8 switch()",
    "text": "4.8 switch()\nСлишком много условий в теле функции могут сделать ее нечитаемой. Для таких случаев подойдет switch().\n\nconvert_temperature &lt;- function(x, mode = \"f_to_c\"){\n  if(!is.numeric(x)) stop(\"wrong input\")\n  \n  switch(mode,\n         f_to_c = round((x - 32) / 1.8) |&gt; \n           paste(\"градусов по Цельсию\"),\n         c_to_f = round(x * 1.8 + 32) |&gt; \n           paste(\"градусов по Фаренгейту\"),\n         stop(\"unknown mode\")\n  )\n}\n\nconvert_temperature(78, mode = \"c_to_k\")\n\nError in convert_temperature(78, mode = \"c_to_k\"): unknown mode\n\nconvert_temperature(78, mode = \"f_to_c\")\n\n[1] \"26 градусов по Цельсию\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#map",
    "href": "iterate.html#map",
    "title": "4  Циклы, условия, функции",
    "section": "4.10 map()",
    "text": "4.10 map()\nВоспользуемся возможностями purrr, чтобы исследовать датасет starwars из пакета dplyr. Для начала узнаем число отсутствующих значений в каждом столбце. Косая черта (\\) указывает на то, что мы используем анонимную функцию\n\nlibrary(tidyverse)\nstarwars &lt;- starwars\nmap_int(starwars, \\(x) sum(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year \n         0          6         28          5          0          0         44 \n       sex     gender  homeworld    species      films   vehicles  starships \n         4          4         10          4          0          0          0 \n\n\nОбратите внимание, что map_int, как и map_dbl возвращает именованный вектор. Чтобы избавиться от имен, можно использовать unname().\n\nИспользуйте map_int и n_distinct, чтобы узнать число уникальных наблюдений в каждом столбце.\n\nЕсли функция принимает дополнительные аргументы, их можно задать после названия функции. В таком случае для каждого вызова функции будет использовано это значение аргумента. В примере ниже это аргумент na.rm.\n\nstarwars |&gt; \n  # выбираем все столбцы, где хранятся числовые значения\n  select_if(is.numeric) |&gt; \n  map(mean, na.rm = TRUE)\n\n$height\n[1] 174.6049\n\n$mass\n[1] 97.31186\n\n$birth_year\n[1] 87.56512\n\n\nПри вызове map_df есть дополнительная возможность сохранить названия столбцов, используя аргумент .id:\n\nstarwars |&gt; \n  map_df(~data.frame(unique_values = n_distinct(.x),\n                     col_class = class(.x)),\n         .id = \"variable\"\n         )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#map2",
    "href": "iterate.html#map2",
    "title": "4  Циклы, условия, функции",
    "section": "4.11 map2()",
    "text": "4.11 map2()\nЕсли необходимо несколько раз вызывать одну и ту же функцию с двумя аргументами, используется функция map2().\n\nvar1 &lt;- seq(10, 50, 10)\nvar2 &lt;- seq(1, 5, 1)\n\n# формула\nmap2(var1, var2, ~.x+.y)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 22\n\n[[3]]\n[1] 33\n\n[[4]]\n[1] 44\n\n[[5]]\n[1] 55\n\n\nАргументы, которые меняются при каждом вызове, пишутся до функции или формулы; аргументы, которые остаются неизменны, – после. Это можно представить так (источник):\n\nВо всех случаеях, когда у функции больше двух аргументов, используется pmap().\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Advanced R Programming\") и пройдите из него урок 3 Functional Programming with purrr.\n\n\nНесколько вопросов для самопроверки.\n\n\n\n\n\n\nВопрос\n\n\n\nФункции-предикаты (predicate functions) возвращают TRUE или FALSE. Выберите из списка все функции-предикаты.\n\n\n\n\nevery()\n\n\nsome()\n\n\nnone()\n\n\nhas_element()\n\n\nis.factor()\n\n\nkeep()\n\n\ndiscard()\n\n\nis.numeric()\n\n\ndetect()\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие из функций ниже принимают в качестве аргумента функции-предикаты?\n\n\n\n\nevery()\n\n\nsome()\n\n\nnone()\n\n\nhas_element()\n\n\nis.factor()\n\n\nkeep()\n\n\ndiscard()\n\n\nis.numeric()\n\n\ndetect()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#функционалы-в-анализе-данных",
    "href": "iterate.html#функционалы-в-анализе-данных",
    "title": "4  Циклы, условия, функции",
    "section": "4.12 Функционалы в анализе данных",
    "text": "4.12 Функционалы в анализе данных\nДатасет “Гарри Поттер” представляет собой набор файлов .csv, содержащих метаданные о ресурсах из коллекций Британской библиотеки, связанных с Гарри Поттером, . Первоначально он был выпущен к 20-летию публикации книги «Гарри Поттер и философский камень» 26 июня 2017 года и с тех пор ежегодно обновлялся. Всего в датасете пять файлов, каждый из которых содержит разное представление данных.\nДатасет до 2023 г. был доступен на сайте Британской библиотеки (https://www.bl.uk/); в репозитории курса сохранена его копия. Скачаем архив.\n\nmy_url &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/HP.zip\"\ndownload.file(url = my_url, destfile = \"../files/HP.zip\")\n\nПосле этого переходим в директорию с архивом и распаковываем его.\n\nunzip(\"../files/HP.zip\")\n\nСохраним список всех файлов с расширением .csv, используя подходящую функцию из base R.\n\nmy_files &lt;- list.files(\"../files/HP\", pattern = \".csv\", full.names = TRUE)\nmy_files\n\n[1] \"../files/HP/classification.csv\" \"../files/HP/names.csv\"         \n[3] \"../files/HP/records.csv\"        \"../files/HP/titles.csv\"        \n[5] \"../files/HP/topics.csv\"        \n\n\nТеперь задействуем функционалы.\n\nФункционалы – это функции, которые используют в качестве аргументов другие функции.\n\nДля того, чтобы прочесть все файлы одним вызовом функции, используем map(). В качестве аргументов передаем список файлов, функцию read_csv() и аргумент этой функции col_types.\n\n# чтение файлов \nHP &lt;- map(my_files, read_csv, col_types = cols())\n\nОбъект HP – это список. В нем пять элементов, так как на входе у нас было пять файлов. Для удобства назначаем имена элементам списка.\n\nmy_files_short &lt;- list.files(\"../files/HP\", pattern = \".csv\")\nnames(HP) &lt;- my_files_short\n\n\nПопробуем выяснить, какие столбцы есть во всех пяти таблицах. Для этого подойдет функция reduce() из того же purrr. Она принимает на входе вектор (или список) и функцию и применяет функцию последовательно к каждой паре значений.\n\n\n\nИсточник.\n\n\n\n\nHP |&gt; \n  map(colnames) |&gt; \n  # это тоже функционал\n  reduce(intersect)\n\n [1] \"Dewey classification\"       \"BL record ID\"              \n [3] \"Type of resource\"           \"Content type\"              \n [5] \"Material type\"              \"BNB number\"                \n [7] \"ISBN\"                       \"ISSN\"                      \n [9] \"Name\"                       \"Dates associated with name\"\n[11] \"Type of name\"               \"Role\"                      \n[13] \"Title\"                      \"Series title\"              \n[15] \"Number within series\"       \"Country of publication\"    \n[17] \"Place of publication\"       \"Publisher\"                 \n[19] \"Date of publication\"        \"Edition\"                   \n[21] \"Physical description\"       \"BL shelfmark\"              \n[23] \"Genre\"                      \"Languages\"                 \n[25] \"Notes\"                     \n\n\nЕще одна неочевидная возможность функции reduce - объединение нескольких таблиц в одну одним вызовом. Например, так:\n\nHP_joined &lt;- HP |&gt; \n  reduce(left_join)\n\nHP_joined\n\n\n  \n\n\n\nО других возможностях пакета purrr мы поговорим в следующем уроке, а пока почистим данные и построить несколько разведывательных графиков.\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(Languages, into = c(\"language\", NA), sep = \";\") |&gt;\n  mutate(language = str_squish(language)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(language)) |&gt; \n  group_by(year, language) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = language)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nТакже построим облако слов. Для этого заберем первое слово в каждом ряду из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./images/hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#разведывательные-графики",
    "href": "iterate.html#разведывательные-графики",
    "title": "4  Циклы, условия, функции",
    "section": "4.13 Разведывательные графики",
    "text": "4.13 Разведывательные графики\nТеперь можно почистить данные и построить несколько разведывательных графиков.\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(Languages, into = c(\"language\", NA), sep = \";\") |&gt;\n  mutate(language = str_squish(language)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(language)) |&gt; \n  group_by(year, language) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = language)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nВ качестве небольшого бонуса к этому уроку построим облако слов. Для этого заберем первое слово в каждом ряду из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./images/hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#пакет-purrr",
    "href": "iterate.html#пакет-purrr",
    "title": "4  Циклы, условия, функции",
    "section": "4.3 Пакет {purrr}",
    "text": "4.3 Пакет {purrr}\nРазработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham и Grolemund 2016).\n\nYou should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work.\n— Hadley Wickham & Garrett Grolemund\n\nВ семействе функций map_ из этого пакета всего 23 вариации. Вот основные из них:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nВсе они принимают на входе данные и функцию (или формулу), которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_int() – целочисленный вектор, и т.д.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#ленивые-вычисления",
    "href": "iterate.html#ленивые-вычисления",
    "title": "4  Циклы, условия, функции",
    "section": "4.4 Ленивые вычисления",
    "text": "4.4 Ленивые вычисления\nВычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет.\n\nfahrenheit_to_celsius &lt;- function(fahrenheit, your_name = \"locusclassicus\"){ \n  celsius = (fahrenheit - 32) / 1.8\n  return(round(celsius))\n}\n\nfahrenheit_to_celsius(451)\n\n[1] 233\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию awesome_plot, которая будет принимать в качестве аргументов два вектора, трансформировать их в тиббл и строить диаграмму рассеяния при помощи ggplot(). Задайте цвет и прозрачность точек.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "import.html#пакет-jsonlite",
    "href": "import.html#пакет-jsonlite",
    "title": "5  Импорт",
    "section": "5.2 Пакет jsonlite",
    "text": "5.2 Пакет jsonlite\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\n\nlibrary(jsonlite)\n\npath &lt;- \"../files/TBBT.json\"\ntbbt &lt;- read_json(path)\n\nФункция read_json() вернула нам список со следующими элементами:\n\nsummary(tbbt)\n\n                          Length Class  Mode     \nname                        1    -none- character\nseason_count                1    -none- character\nepisodes_count_total        1    -none- character\nepisodes_count_per_season  12    -none- list     \ncasting                    11    -none- list     \nepisode_list              280    -none- list     \nreferences                  1    -none- list",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#от-списка-к-таблице",
    "href": "import.html#от-списка-к-таблице",
    "title": "5  Импорт",
    "section": "5.3 От списка к таблице",
    "text": "5.3 От списка к таблице\nВыборочно преобразуем список в тиббл. Функция transpose() берет список списков и выворачивает его наизнанку: вместо списка, в котором для каждого из персонажей указан актер и первое появление, мы получаем три списка: с персонажами, актерами и эпизодами. На месте отсутствующих значений ставится NULL.\n\nlibrary(tidyverse)\n\ncast_tbl &lt;- tbbt$casting |&gt; \n  transpose() |&gt; \n  map(as.character) |&gt; \n  as_tibble()\n\ncast_tbl\n\n\n  \n\n\n\nПроделаем то же самое для списка эпизодов, но другим способом. Функция pluck() представляет собой аналог [[, который можно использовать в пайпе. Она позволяет эффективно индексировать многоуровневые списки. Поскольку списков много, мы используем ее в сочетании с map_chr().\n\nepisodes_tbl &lt;- tibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\"))\n\nepisodes_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСамостоятельно создайте тиббл, в котором будет храниться количество серий для каждого сезона.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "maps.html#данные-римские-амфитеатры",
    "href": "maps.html#данные-римские-амфитеатры",
    "title": "21  Пространственные данные в R",
    "section": "",
    "text": "Digital Atlas of the Roman Empire;\nAncient World Mapping Center;\nГеопространственная сетевая модель Римской империи Orbis.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#simple-features",
    "href": "maps.html#simple-features",
    "title": "21  Пространственные данные в R",
    "section": "21.2 Simple Features",
    "text": "21.2 Simple Features\nСовременный формат хранения векторных геоданных называется Simple Features. Основное отличие объектов sf от объектов sp в том, что данные хранятся в виде датафрейма со списком-колонкой для хранения геометрии (линии, точки или полигона). Эта колонка называется sfc (simple features geometry column), а сама геометрия внутри нее – sfg (simple feature geometry).\n\nТо, что объекты типа Simple Features реализованы в виде самых обычных фреймов данных, означает, что любая операция, применимая к фрейму данных, будет также применима к объекту типа sf. Это очень важная особенность объектов типа sf, которой сильно не хватало в экосистеме исторического пакета sp. – Источник.\n\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nroman_map &lt;- sf::st_as_sf(awmc.roman.empire.200.sp)\nroman_map\n\n\n  \n\n\n# Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n# Simple feature collection with 112 features and 8 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -9.48732 ymin: 22.89549 xmax: 43.10774 ymax: 55.10117\n# Geodetic CRS:  +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\n# First 10 features:\n#   OBJECTID         AREA  PERIMETER NEWDIO_ NEWDIO_ID ID Shape_Leng\n# 0        1 19.612702708 35.3870861       2         0  0 35.1149460\n# 1        2  0.080670307  1.2122280       3         0  0  1.2122280\n\nПосмотрим внимательно на это описание.\n\nbounding box: прямоугольная рамка, которая задает границы карты; здесь координаты по оси x соответствуют долготе, а по оси y - широте. Будьте внимательны, потому что мы пишем обычно сначала широту, а потом долготу. Но по аналогии с алгеброй x определяет сдвиг вправо-влево (долготу), в то время как y - вверх-вниз (широту). Получается, что перед нами кусочек северного полушария, в основном к востоку от нулевого меридиана.\n\n\nlibrary(tmaptools)\nbb(roman_map)\n\n    xmin     ymin     xmax     ymax \n-9.48732 22.89549 43.10774 55.10117 \n\n\nGeodetic CRS:  +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 – это определение геодезической системы координат (Geodetic CRS):\n\n+proj=longlat - указывает, что используется географическая система координат (широта и долгота).\n+datum=WGS84 - определяет геодезическую основу, в данном случае это Всемирная геодезическая система 1984 года (World Geodetic System 1984).\n+ellps=WGS84 - указывает, что используется эллипсоид, соответствующий системе WGS84.\n+towgs84=0,0,0 - определяет параметры трансформации между используемым эллипсоидом и эллипсоидом системы WGS84. Значения “0,0,0” означают, что никаких трансформаций не требуется, так как данные уже находятся в системе WGS84.\n\nПомимо географической системы координат, которые используют сферическую или эллипсоидальную поверхность Земли, бывают проекционные (плоские) системы, которые используют плоскую (двумерную) поверхность. Кроме того, они используют другие единицы измерения: не градусы широты и долготы, а линейные единицы (например, метры).\nФункция st_is_valid() проверяет, является ли заданная пространственная геометрия (например, точка, линия, многоугольник) топологически корректной. В нашем случае есть одна ошибка.\n\nst_is_valid(roman_map)\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE\n\n\nНадо починить, иначе дальше будет ошибка.\n\nroman_map &lt;- st_make_valid(roman_map)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#tm_bubbles",
    "href": "maps.html#tm_bubbles",
    "title": "21  Пространственные данные в R",
    "section": "21.4 tm_bubbles()",
    "text": "21.4 tm_bubbles()\nТепрь нанесем на карту отдельные амфитеатры в виде точек. Для этого преобразуем датафрейм в объект sf. Обратите внимание, что геометрии здесь другие (точки).\n\namph_points &lt;- ramphs |&gt; \n   st_as_sf(coords = c(\"longitude\", \"latitude\"))\n\namph_points\n\n\n  \n\n\n\n\ntmap_style(\"classic\")\n\nstyle set to \"classic\"\n\n\nother available styles are: \"white\" (tmap default), \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(\n    type = \"8star\",\n    position = c(\"right\", \"top\")\n  ) +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_shape(amph_points) +\n  tm_bubbles(\n    size = \"capacity\", \n    size.scale = tm_scale_continuous(values.scale = 1),  \n    fill = \"prov.type\", \n    fill.scale = tm_scale(values = c(\"red\", \"blue\", \"green\")),\n    fill_alpha = 0.8  \n  ) +\n  tm_layout(\n    legend.position = c(\"right\", \"bottom\"),\n    legend.frame = TRUE\n  )\n\nScale bar set for latitude km and will be different at the top and bottom of the map.\n\n\n\n\n\n\n\n\n\nКак и в ggplot, разные геометрии могут использовать разные данные. В нашем случае – отфильтрованный список названий.\n\ntmap_mode(\"plot\")  # Убедимся, что мы в режиме рисования\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scalebar(breaks = c(0, 100, 200), text.size = 1, position = c(\"left\", \"bottom\")) +\n\n  # Слой с пузырями\n  tm_shape(amph_points) +\n  tm_bubbles(\n    size = \"capacity\",\n    size.scale = tm_scale_continuous(values = c(0.5, 2)), \n    fill = \"prov.type\", \n    fill.scale = tm_scale(values = c(\"red\", \"blue\", \"green\")),\n    fill_alpha = 0.8\n  ) +\n\n  # Слой с подписями для амфитеатров с capacity &gt; 30000\n  tm_shape(amph_points |&gt; filter(capacity &gt; 30000)) +\n  tm_text(\n    text = \"label\",\n    options = opt_tm_text(point.label = TRUE)\n    ) +\n\n  # Настройка легенды\n  tm_layout(\n    legend.position = c(\"right\", \"bottom\"),\n    legend.frame = TRUE\n  )\n\nScale bar set for latitude km and will be different at the top and bottom of the map.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#пересечения-между-геометриями",
    "href": "maps.html#пересечения-между-геометриями",
    "title": "21  Пространственные данные в R",
    "section": "21.5 Пересечения между геометриями",
    "text": "21.5 Пересечения между геометриями\nМы можем посчитать, сколько точек приходится на один многогранник. Но для этого координатные системы должны совпадать. Сейчас у точек нет никакой CRS, в чем легко убедиться.\n\nst_crs(amph_points)\n\nCoordinate Reference System: NA\n\n\n\nst_intersects(roman_map, amph_points)\n\nError in st_geos_binop(\"intersects\", x, y, sparse = sparse, prepared = prepared, : st_crs(x) == st_crs(y) is not TRUE\n\n\nЧтобы избавиться от ошибки, необходимо назначить или трансформировать координатные системы. Четыре цифры ниже представляют собой код EPSG (European Petroleum Survey Group). Это один из способов задания (хранения) пространственной привязки. EPSG:4326 соответствует WGS84, а Web Mercator – EPSG:3857.\n\n# если нужно трансформировать\nroman_map &lt;- st_transform(roman_map, 4326)\n\n# если нужно назначить\namph_points &lt;- st_set_crs(amph_points, 4326)\n\nСнова уточним пересечения.\n\n# пересечения\ninter &lt;- st_intersects(roman_map, amph_points)\n\ninter\n\nSparse geometry binary predicate list of length 112, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 45, 46, 69, 78, 80, 88, 108, 109, 110, 151, ...\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: (empty)\n 6: (empty)\n 7: 3, 21, 38, 39, 47, 74, 89, 114, 115, 124, ...\n 8: (empty)\n 9: (empty)\n 10: (empty)\n\n\nДобавим новый столбец в датафрейм с картой.\n\nroman_map$count &lt;- lengths(inter)\n\nТеперь его можно использовать для выбора цвета заливки.\n\ntm_shape(roman_map) +\n  tm_polygons(fill = \"count\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#tm_basemap",
    "href": "maps.html#tm_basemap",
    "title": "21  Пространственные данные в R",
    "section": "21.6 tm_basemap()",
    "text": "21.6 tm_basemap()\nПарящая в вакууме империя не очень радует глаз; в таком случае стоит добавить растровое изображение ландшафта. Пока это доступно только для динамической карты, поэтому переключаемся в режим “view” (ниже представлен скриншот).\nБудьте внимательны, совмещая исторические карты с современными! Убедитесь, что вы не показываете походы Цезаря в современную Швейцарию, как это произошло, например, здесь.\n\n# Устанавливаем режим просмотра (интерактивная карта)\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n# Строим интерактивную карту\ntm_shape(amph_points) +\n  tm_basemap(\"Stadia.StamenTerrainBackground\") +\n  tm_symbols(\n    size = \"capacity\",     \n    fill = \"white\",\n    col = \"steelblue\", \n    fill_alpha = 0.8\n  )",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#возможности-ggplot2",
    "href": "maps.html#возможности-ggplot2",
    "title": "21  Пространственные данные в R",
    "section": "21.8 Возможности ggplot2",
    "text": "21.8 Возможности ggplot2\nДля статичных карт можно использовать привычный ggplot(), как показано, например, здесь.\n\nggplot() +\n  geom_sf(data = roman_map) +\n  geom_point(ramphs, \n             mapping = aes(longitude, latitude),\n             color = \"steelblue\", \n             alpha = 0.5)  +\n  theme_bw()\n\n\n\n\n\n\n\n\nЕсли точек много, то может быть уместней представить на карте плотность их распределения.\n\nggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_point(ramphs, color = \"steelblue\", alpha = 0.5,\n             mapping = aes(longitude, latitude)) +\n  geom_density2d(data = ramphs, \n                 mapping = aes(longitude, latitude, \n                               color = after_stat(level)),\n                 linewidth = 1, alpha = 0.5)\n\n\n\n\n\n\n\n\nЕще один способ отразить области скопления точек – сотовая диаграмма. На такой диаграмме координатная плоскость разбивается на гексагоны, которые закрашиваются в соответствии с градиентом плотности попавших в них точек.\n\ng &lt;- ggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_hex(data = ramphs,\n                 mapping = aes(longitude, latitude),\n           bins = 25,\n           color = \"royalblue\")  +\n  theme_bw() +\n  scale_fill_continuous(trans = \"reverse\") \n\ng",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#пакет-plotly",
    "href": "maps.html#пакет-plotly",
    "title": "21  Пространственные данные в R",
    "section": "21.9 Пакет plotly",
    "text": "21.9 Пакет plotly\nПакет plotly позволяет добавить интерактивности на карту.\n\nlibrary(plotly)\nggplotly(g)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#сеть-на-карте",
    "href": "maps.html#сеть-на-карте",
    "title": "21  Пространственные данные в R",
    "section": "21.11 Сеть на карте",
    "text": "21.11 Сеть на карте\nКооринаты узлов – это широта и долгота (главное не перепутать).\n\norbis_coord &lt;- orbis_e |&gt; \n  left_join(orbis_n, by = join_by(source == id)) |&gt; \n  mutate(source = label, .before = target) |&gt; \n  select(-label) |&gt; \n  # координаты начала ребра\n  rename(x1 = y, y1 = x) |&gt; \n  left_join(orbis_n, by = join_by(target == id)) |&gt; \n  mutate(target = label, .after = source) |&gt; \n  select(-label) |&gt; \n  # координаты конца ребра\n  rename(x2 = y, y2 = x) |&gt;\n  # отрезаем пуповины к центру мира\n  filter(x1 != 0, y1 !=0, x2 != 0, y2 != 0)\n\n\n# для простоты пока берем современную карту\nworld &lt;- map_data(\"world\") \n\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"wheat\", color = \"grey\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = \"steelblue\", alpha = 0.5) +\n  coord_map(xlim = c(-10, 50),\n            ylim = c(23, 54)) +\n  geom_segment(data = orbis_coord, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type))\n\n\n\n\n\n\n\n\nПараллельные линии создают шум в нижней правой четверти; есть несколько способов от этого избавиться, но мы пока просто отрежем часть городов (восточнее Берениса).\n\norbis_coord_pruned &lt;-  orbis_coord |&gt; \n  filter(y1 &gt; 28 & y2 &gt; 28)\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"basetheme::brutal\")\n\n\n\npar(mar = rep(0,4))\nset.seed(24092024)\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"white\", color = \"wheat\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = cols[1], alpha = 0.5) +\n  geom_segment(data = orbis_coord_pruned, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type)) +\n  geom_label(data = orbis_coord |&gt; \n               filter(source %in% c(\"Roma\", \"Alexandria\", \"Carthago\", \"Sirmium\", \"Corinthus\", \"Antiochia\", \"Londinium\", \"Tarraco\", \"Augusta Taurinorum\", \"Jerusalem\")),\n             aes(x1, y1, label = source),\n             color = cols[5], \n             label.size = 0.15,\n             fontface = \"bold\") +\n  coord_map(xlim = c(-10, 45),\n            ylim = c(26, 54)) +\n  labs(x = NULL, y = NULL, \n       title = \"Транспортное сообщение в Римской империи\",\n       subtitle = \"Данные проекта Orbis\") +\n  theme_bw(base_family = \"serif\") +\n  theme(legend.position=\"bottom\", \n        legend.box = \"horizontal\",\n        panel.background = element_rect(fill = \"aliceblue\"),\n        text = element_text(color = cols[5])) +\n  scale_color_manual(\"тип\", values = sample(cols, 10))",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "22  Регрессионный анализ",
    "section": "",
    "text": "22.1 Данные: Оксфордская керамика\nВ этом уроке мы познакомимся с методами простой и множественой линейной регрессии.\nДанные для этого урока основаны на нескольких публикациях Яна Ходдера, который проанализировал пространственное распределение поздней романо-британской керамики, произведенной в Оксфорде, в статьях 1974 г. “The Distribution of Two Types of Romano-British coarse pottery” и “A Regression Analysis of Some Trade and Marketing Patterns”. Датасет доступен в пакете archdata, содержащем и другие наборы данных для археологов.\nlibrary(archdata)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(equatiomatic)\nlibrary(broom)\nlibrary(gt)\ndata(\"OxfordPots\")\nOxfordPots\nДатафрейм содержит 30 наблюдений по следующим 7 переменным:",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#простая-линейная-регрессия",
    "href": "regression.html#простая-линейная-регрессия",
    "title": "22  Регрессионный анализ",
    "section": "22.2 Простая линейная регрессия",
    "text": "22.2 Простая линейная регрессия\nПростая линейная регрессия – метод, который позволяет предсказывать количественный отклик переменной y на основе единственной независимой переменной x. Случайная величина, которая используется для целей предсказания, называется предиктором. Величина, значения которой предсказываются, называется переменной отклика. Здесь мы освоим лишь самые азы, подробнее стоит посмотреть соответствующие уроки курса “Introduction to Modern Statistics (2e)”.\n\n22.2.1 Линейная функция\nЧтобы разобраться с регрессией, надо вспомнить, что такое линейная зависимость:\n\\[y\\approx\\beta_o + \\beta_1x\\]\nВ этом уравнении \\(\\beta_o\\) и \\(\\beta_1\\) - это константы, известные как свободный член и угол наклона линейной модели. Совокупно их называют коэффициентами, или параметрами, модели. Геометрически первый из них определяет точку пересечения оси y (intercept), а второй – угол наклона (slope).\nПосмотрите внимательно на линии на примере и подумайте, чем они отличаются.\n\n\n\n22.2.2 Ошибка прогноза\nНа практике линейная зависимость в чистом виде почти не встречается: всегда есть небольшая ошибка прогноза (\\(\\epsilon\\)):\n\\[y\\approx\\beta_o + \\beta_1x + \\epsilon\\]\n\n\n\n\n\n\n\n\n\nПри создании регрессионной модели наша задача заключается в том, чтобы на основе доступных наблюдений подобрать коэффициенты \\(\\beta_0\\) и \\(\\beta_1\\) таким образом, чтобы минимизировать ошибку.\n\\[\\sum(y_i- \\hat y)^2 = \\sum\\epsilon^2\\]\nЧтобы подчеркнуть, что речь идет лишь об оценке, над бетой ставится “крышечка”:\n\\[\\hat y \\approx \\hat\\beta_o + \\hat\\beta_1x\\]\n\n\n22.2.3 Простая регрессия с lm()\nПосмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ggthemes::wsj_rgby\")\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nЧем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон.\nИ наклон, и точку пересечения с осью y определяет функция lm.\n\nfit &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nparameters(fit)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#данные-оксфордская-керамика",
    "href": "regression.html#данные-оксфордская-керамика",
    "title": "22  Регрессионный анализ",
    "section": "",
    "text": "место;\nпроцент оксфордской керамики;\nрасстояние до Оксфорда в милях;\nпроцент гончарных изделий из Нью-Фореста;\nрасстояние до Нью-Фореста;\nплощадь обнесенного стеной города;\nналичие водного транспортного сообщения.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#метод-наименьших-квадратов",
    "href": "regression.html#метод-наименьших-квадратов",
    "title": "22  Простая линейная регрессия",
    "section": "22.3 Метод наименьших квадратов",
    "text": "22.3 Метод наименьших квадратов\nПосмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ltc::trio2\")\n\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[3], se = FALSE) +\n  geom_point(color = cols[1], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nЧем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон. И наклон, и точка пересечения с осью y определяет функция lm, и мы можем их узнать.\n\nfit &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nsummary(fit)\n\n\nCall:\nlm(formula = OxfordPct ~ OxfordDst, data = OxfordPots)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.388  -4.003  -1.005   3.844  10.757 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20.87665    2.23356   9.347 4.18e-10 ***\nOxfordDst   -0.12290    0.02989  -4.112 0.000311 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.601 on 28 degrees of freedom\nMultiple R-squared:  0.3765,    Adjusted R-squared:  0.3542 \nF-statistic: 16.91 on 1 and 28 DF,  p-value: 0.0003111",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#коэффициенты-модели",
    "href": "regression.html#коэффициенты-модели",
    "title": "22  Простая линейная регрессия",
    "section": "22.5 Коэффициенты модели",
    "text": "22.5 Коэффициенты модели\nФункция summary() возвращает много всего, но пока нас интересуют только коэффициенты. Их можно достать из подогнанной модели так:\n\ncoefficients(fit)\n\n(Intercept)   OxfordDst \n 20.8766508  -0.1229049 \n\n\nЭто значит, что наши данные описываются функцией (где \\(\\epsilon\\) - это ошибка):\n\\[OxfordPct = 20.88 - 0.12 \\times OxfordDst + \\epsilon\\] Интуитивно понятно, что коэффициент \\(\\beta_1\\) связан с ковариацией (мерой совместной изменчивости двух величин). Действительно, он рассчитывается по формуле:\n\\[\\beta_1=\\frac{Cov(x,y)}{Var(x)}\\] Проверяем.\n\nx &lt;- OxfordPots$OxfordDst\ny &lt;- OxfordPots$OxfordPct\n\nbeta_1&lt;- cov(x, y) / var(x)\nbeta_1\n\n[1] -0.1229049\n\n\nЗная \\(\\beta_1\\), можно вычислить \\(\\beta_0\\) по формуле:\n\\[\\beta_0=\\bar y - \\beta_1 \\bar x\\]\nСнова проверим.\n\nbeta_0 = mean(y) - beta_1 * mean(x)\nbeta_0\n\n[1] 20.87665",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#стандартные-ошибки-коэффициентов",
    "href": "regression.html#стандартные-ошибки-коэффициентов",
    "title": "22  Простая линейная регрессия",
    "section": "22.6 Стандартные ошибки коэффициентов",
    "text": "22.6 Стандартные ошибки коэффициентов\nДля обоих коэффициентов приведена стандартная ошибка и t-статистика.\n\nsummary(fit)$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 20.8766508 2.23355676  9.346819 4.184111e-10\nOxfordDst   -0.1229049 0.02989016 -4.111887 3.110772e-04\n\n\nСтандартная ошибка для \\(\\beta_0\\) рассчитывается по формуле:\n\\[SE(\\beta_0)=\\sqrt{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}} \\times \\sqrt{\\frac{1}{n}+\\frac{\\bar x^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\]\nПервый множитель в этой формуле – это дисперсия остатков модели. Чем она больше, тем больше неопределенность. На второй множитель влияет как размер выборки, так и разброс независимой переменной x: чем больше размер выборки n, тем меньше \\(\\frac{1}{n}\\) и чем больше \\(Σ(x - \\bar x)^2\\), тем меньше весь множитель. Посчитаем вручую и сравним с результатом, который возвращает команда summary(fit).\n\nx_bar &lt;- mean(x)\n\nmult1 &lt;- sqrt(sum(fit$residuals^2) / 28)\nmult2 &lt;- sqrt(1/30 + ( x_bar^2 / sum((x - x_bar)^2)))\n\nmult1 * mult2\n\n[1] 2.233557\n\n\nСтандартная ошибка для \\(\\beta_1\\) рассчитывается по формуле:\n\\[SE(b_1)=\\sqrt{\\frac{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\] Большая дисперсия остатков (в числителе) будет приводить к увеличению ошибки, а размах \\(x_i\\) – к уменьшению; интуитивно это объясняется тем, что в таком случае у нас больше информации для оценивания угла наклона. Снова перепроверим.\n\nmult1 / sqrt(sum((x - x_bar)^2))\n\n[1] 0.02989016\n\n\nФункция geom_smooth добавляет стандартную ошибку коэффициента наклона на график в виде серой полосы, которая означает, что с вероятностью 95% (значение по умолчанию, которое можно поменять) истинное значение отклика находится в этой зоне (predicted ± 1.96 * se). В статистике это называется доверительный интервал.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], \n              se = TRUE, level = 0.95) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#невязки",
    "href": "regression.html#невязки",
    "title": "22  Простая линейная регрессия",
    "section": "22.8 Невязки",
    "text": "22.8 Невязки\nКак правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые невязки) должны быть минимальны.\nНевязка – это разница между прогнозируемым и фактическим значениями отклика: \\((y_i- \\hat y)\\). На графике ниже невязки обозначены пунктиром.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  geom_segment(aes(xend = OxfordDst,\n                   yend = predict(fit)), \n               linetype = 2, \n               color = cols[1]) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nМы можем убедиться в том, что невязки (fit$residuals) представляют собой разницу между фактическим (OxfordPots$OxfordPct) и предсказанным значением (fit$fitted.value). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями.\n\nall.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)\n\n[1] TRUE\n\n\nЕсли модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.\n\nmean(fit$residuals)\n\n[1] 3.256654e-16\n\ncov(fit$residuals, OxfordPots$OxfordDst)\n\n[1] -3.920236e-15\n\n\nКроме того, полезно проверить остатки на нормальность. Это можно сделать при помощи специального теста или визуально.\n\nshapiro.test(residuals(fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.97648, p-value = 0.7262\n\n\nВысокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально.\nТакже проведем три визуальных теста.\n\nlibrary(gridExtra)\n\ng1 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  mutate(residuals_st = scale(residuals)) |&gt; \n  ggplot(aes(sample = residuals_st)) +\n  stat_qq(color = cols[3], \n          size = 2, alpha = 0.8) + \n  stat_qq_line(color = cols[2]) +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\ng2 &lt;- tibble(residuals = residuals(fit),\n       fitted = predict(fit)) |&gt; \n  ggplot(aes(fitted, residuals)) +\n  geom_point(color = cols[3], \n             size = 2, alpha = 0.8) + \n  theme_minimal()\n\ng3 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  ggplot(aes(residuals)) +\n  geom_histogram(fill = cols[3], color = \"white\") + \n  theme_minimal()\n\ngrid.arrange(g1, g2, g3, nrow = 1)\n\n\n\n\n\n\n\n\nВидно, что самые низкие значения остатков немного выбиваются из общей картины. Отрицательные остатки означают, что соответствующие значения \\(\\hat y\\) завышены. К этому мы еще вернемся.\nВы можете также использовать базовую plot(), передав ей подогнанную модель в качестве аргумента.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №1 “Introduction”, №2 “Residuals”, №3 “Least Squares Estimation”, №4 “Residual Variation”. Один из этих уроков потребует установить пакет manipulate, позволяющий взаимодействовать с графиком в интерактивном режиме.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#оценка-модели",
    "href": "regression.html#оценка-модели",
    "title": "22  Регрессионный анализ",
    "section": "22.4 Оценка модели",
    "text": "22.4 Оценка модели\nОбщая оценка модели проводится при помощи функции performance() из пакета {easystats} или базовой summary().\n\nperformance(fit)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\n\n22.4.1 RSE, MSE, RMSE\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\nПоскольку наши оценки могут быть как завышенными, так и заниженными, значения ошибок возводятся в квадрат и суммируются по всем точкам данных. Узнаем сумму квадратов остатков (RSS = Residual sum of squares), которая считается по формуле:\n\\[RSS = \\sum_{i=n}^n(y_i- \\hat y_i)^2\\]\n\nrss &lt;- sum(fit$residuals^2)\nrss\n\n[1] 878.439\n\n\nЗная это число, определяем среднеквадратичную ошибку (MSE = Mean square error), корень из среднеквадратичной ошибки (RMSE), а также стандартную ошибку остатков (RSE = Residual standard error).\n\nmse &lt;- rss / length(fit$residuals)\nmse\n\n[1] 29.2813\n\nrmse &lt;- sqrt(mse)\nrmse\n\n[1] 5.41122\n\n\n\nrse &lt;-  sqrt(rss / fit$df.residual)\nrse\n\n[1] 5.601145\n\n\n\n\n22.4.2 \\(R^2\\)\nRSE – это мера несоответствия модели данным. Но поскольку она выражается в тех же единицах измерения, что и y, то не всегда бывает ясно, какая RSE является хорошей. Коэффициент детерминации \\(R^2\\) представляет собой альтернативную меру соответствия. Этот показатель принимает форму доли – доли объясненной дисперсии, в связи с чем он всегда изменяется от 0 до 1 и не зависит от шкалы измерения.\n\\[R^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] Здесь \\(TSS = \\sum(y_i - \\bar y)^2\\), то есть общая сумма квадратов.\nTSS является мерой общей дисперсии отклика Y, и о ней можно думать как о степени изменчивости, присущей отклику до выполнения регрессионного анализа. В то же время RSS выражает степень изменчивости, которая осталась необъясненной после построения регрессионной модели. Следовательно, TSS - RSS выражает количество дисперсии отклика, объясненное (“изъятое”) после выполнения регрессионного анализа, а \\(R^2\\) – долю дисперсии Y, объясненную при помощи X. Статистика \\(R^2\\), близкая к 1, означает, что значительная доля изменчивости отклика была объяснена регрессионной моделью (Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани 2017, 82).\n\ntss &lt;- sum((y - mean(y))^2)\ntss\n\n[1] 1408.878\n\n1 - rss / tss\n\n[1] 0.3764977\n\n\nСнова сравним с результатом, который нам вернула модель.\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\nДля простой линейной регрессии статистика \\(R^2\\) совпадает с квадратом коэффициента корреляции.\n\ncor(x, y)^2\n\n[1] 0.3764977\n\n\n\n\n22.4.3 Анализ остатков\nКак правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые невязки) должны быть минимальны.\nНевязка – это разница между прогнозируемым и фактическим значениями отклика: \\((y_i- \\hat y)\\). На графике ниже невязки обозначены пунктиром.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  geom_segment(aes(xend = OxfordDst,\n                   yend = predict(fit)), \n               linetype = 2, \n               color = cols[1]) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nПерепроверим.\n\nМы можем убедиться в том, что невязки (fit$residuals) представляют собой разницу между фактическим (OxfordPots$OxfordPct) и предсказанным значением (fit$fitted.value). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями.\n\nall.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)\n\n[1] TRUE\n\n\n\nЕсли модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.\n\nmean(fit$residuals) |&gt; \n  round(2)\n\n[1] 0\n\ncov(fit$residuals, OxfordPots$OxfordDst) |&gt; \n  round(2)\n\n[1] 0\n\n\nКроме того, полезно проверить остатки на нормальность и гомоскедастичность (равномерность дисперсии остатков). Это можно сделать при помощи специального теста или визуально.\n\nshapiro.test(residuals(fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.97648, p-value = 0.7262\n\n\nВысокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально.\nТакже проведем визуальные тесты.\n\ncheck_model(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №1 “Introduction”, №2 “Residuals”, №3 “Least Squares Estimation”, №4 “Residual Variation”.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#множественная-регрессия",
    "href": "regression.html#множественная-регрессия",
    "title": "22  Регрессионный анализ",
    "section": "22.7 Множественная регрессия",
    "text": "22.7 Множественная регрессия\nМножественная регрессия подходит для тех случаев, где на переменную отклика могут влиять несколько предикторов. Допустим, что в случае с долей оксфордской керамики это не только расстояние от Оксфорда, но и близость крупных городских центров, вокруг которых выстраивались торговые взаимодействия.\nВ общем виде множественная регрессионная модель имеет форму:\n\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2+ ... \\beta_px_p + \\epsilon\\]\n\n22.7.1 Модель с двумя предикторами\nПодгоним вторую модель и посмотрим, дает ли нам что-то добавление второго предиктора.\n\nfit2 &lt;- lm(OxfordPct ~ OxfordDst + WalledArea, data = OxfordPots)\n\n\n\n\n\n\n\n\n\nModel with 1 Predictor\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel with 2 Predictors\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n133.022\n135.522\n137.201\n0.548\n0.498\n4.748\n5.129\n\n\n\n\n\n\n\nНа первый взгляд, все хорошо: RSE уменьшилась, а доля объясненной дисперсии увеличилась.\n\ncompare_performance(fit_null, fit, fit2) |&gt; \n  plot()\n\n\n\n\n\n\n\n\nОднако p-value для второго предиктора (0.69) указывает на то, что он не является статистически значимым.\n\nparameters(fit2)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n24.103\n2.748\n0.950\n18.330\n29.875\n8.772\n18.000\n0.000\n\n\nOxfordDst\n−0.147\n0.031\n0.950\n−0.213\n−0.081\n−4.668\n18.000\n0.000\n\n\nWalledArea\n−0.005\n0.014\n0.950\n−0.034\n0.023\n−0.397\n18.000\n0.696\n\n\n\n\n\n\n\nЭто может означать, что связи между площадью обнесенного стеной города и числом оксфордских горшков на самом деле нет.\n\nКстати, к похожему выводу пришел и Ян Ходдер в упомянутых исследованиях: торговля грубой керамикой, данные о которой содержит наш датасет, меньше зависит от городов, чем торговля более изысканными товарами. Одним словом, горшки везде нужны, и в городе, и в деревне.\n\nПочему же мы видим увеличение \\(R^2\\)? Дело в том, что этот показатель всегда возрастает при добавлении в модель дополнительных переменных, даже если эти переменные очень слабо связаны с откликом. Поэтому важнейшая задача при обучении модели связана с отбором информативных переменных. В противном случае велик риск переобучить модель.\n\n\n22.7.2 Мнимые переменные\nДля построения модели можно использовать не только количественные, но и качественные предикторы. Если качественный предиктор имеет только два уровня (например, мужской и женский пол), то он превращается в фиктивную переменную, принимающую значения 1 или 0. В нашем датасете в таком виде хранятся сведения о наличии водного сообщения между Оксфордом и местом обнаружения керамических осколков.\n\nfit3 &lt;- lm(OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)\nparameters(fit3)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n14.505\n1.276\n0.950\n11.887\n17.123\n11.367\n27.000\n0.000\n\n\nOxfordDst\n−0.084\n0.015\n0.950\n−0.115\n−0.053\n−5.525\n27.000\n0.000\n\n\nWaterTrans\n10.250\n1.074\n0.950\n8.047\n12.454\n9.545\n27.000\n0.000\n\n\n\n\n\n\n\nОбратите внимание, что угловой коэффициент для WaterTrans представляет собой положительное число: если водный путь есть, линия регрессии не так резко уходит вниз по мере удаления от Оксфорда.\nОчевидно, что наличие водного пути – важный предиктор, что можно подтвердить графически.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ggthemes::wsj_rgby\")\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct, \n             color = as.factor(WaterTrans), \n             group = as.factor(WaterTrans))) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_manual(\"WaterTrans\", values = cols[4:3]) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nСравним эффективность моделей при помощи функции compare_performance(). Добавим и вторую модель тоже, хотя мы помним, что она содержит статистически незначимый предиктор.\n\n\n\n\n\n\n\n\nName\nModel\nR2\nR2_adjusted\nRMSE\nSigma\nAIC_wt\nAICc_wt\nBIC_wt\nPerformance_Score\n\n\n\n\nfit2\nlm\n0.55\n0.50\n4.75\n5.13\n1.00\n1.00\n1.00\n0.74\n\n\nfit3\nlm\n0.86\n0.85\n2.59\n2.73\n0.00\n0.00\n0.00\n0.57\n\n\nfit\nlm\n0.38\n0.35\n5.41\n5.60\n0.00\n0.00\n0.00\n0.22\n\n\nfit_null\nlm\n0.00\n0.00\n6.85\n6.97\n0.00\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\ncompare_performance(fit_null, fit, fit2, fit3) |&gt; \n  plot()",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#мнимые-переменные",
    "href": "regression.html#мнимые-переменные",
    "title": "22  Простая линейная регрессия",
    "section": "22.9 Мнимые переменные",
    "text": "22.9 Мнимые переменные",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#полиномиальная-регрессия",
    "href": "regression.html#полиномиальная-регрессия",
    "title": "22  Простая линейная регрессия",
    "section": "22.10 Полиномиальная регрессия",
    "text": "22.10 Полиномиальная регрессия",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#проблема-переобучения",
    "href": "regression.html#проблема-переобучения",
    "title": "22  Простая линейная регрессия",
    "section": "22.11 Проблема переобучения",
    "text": "22.11 Проблема переобучения",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#трансформация-данных",
    "href": "regression.html#трансформация-данных",
    "title": "22  Простая линейная регрессия",
    "section": "22.12 Трансформация данных",
    "text": "22.12 Трансформация данных\nЗадачи для",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#сравнение-моделей-с-anova",
    "href": "regression.html#сравнение-моделей-с-anova",
    "title": "22  Простая линейная регрессия",
    "section": "22.13 Сравнение моделей с ANOVA",
    "text": "22.13 Сравнение моделей с ANOVA",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#ошибка-прогноза",
    "href": "regression.html#ошибка-прогноза",
    "title": "22  Простая линейная регрессия",
    "section": "22.2 Ошибка прогноза",
    "text": "22.2 Ошибка прогноза\nНа практике линейная зависимость в чистом виде почти не встречается: всегда есть небольшая ошибка прогноза (\\(\\epsilon\\)):\n\\[y\\approx\\beta_o + \\beta_1x + \\epsilon\\]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nПри создании регрессионной модели наша задача заключается в том, чтобы на основе доступных наблюдений подобрать коэффициенты \\(\\beta_0\\) и \\(\\beta_1\\) таким образом, чтобы минимизировать ошибку.\n\\[\\sum(y_i- \\hat y)^2 = \\sum\\epsilon^2\\]\nЧтобы подчеркнуть, что речь идет лишь об оценке, над бетой ставится “крышечка”:\n\\[\\hat y \\approx \\hat\\beta_o + \\hat\\beta_1x\\]",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#простая-регрессия-с-lm",
    "href": "regression.html#простая-регрессия-с-lm",
    "title": "22  Простая линейная регрессия",
    "section": "22.4 Простая регрессия с lm()",
    "text": "22.4 Простая регрессия с lm()\nПосмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ggthemes::wsj_rgby\")\n\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nЧем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон.\nИ наклон, и точка пересечения с осью y определяет функция lm.\n\nfit &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nsummary(fit)\n\n\nCall:\nlm(formula = OxfordPct ~ OxfordDst, data = OxfordPots)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.388  -4.003  -1.005   3.844  10.757 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20.87665    2.23356   9.347 4.18e-10 ***\nOxfordDst   -0.12290    0.02989  -4.112 0.000311 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.601 on 28 degrees of freedom\nMultiple R-squared:  0.3765,    Adjusted R-squared:  0.3542 \nF-statistic: 16.91 on 1 and 28 DF,  p-value: 0.0003111",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#уровень-значимости",
    "href": "regression.html#уровень-значимости",
    "title": "22  Простая линейная регрессия",
    "section": "22.7 Уровень значимости",
    "text": "22.7 Уровень значимости\n\nlibrary(broom)\ntidy(fit)\n\n\n  \n\n\n\nСтолбец statistic, как легко убедиться, содержит результат деления коэффицентов на стандартную ошибку; а p.value (уровень значимости) указывает, какова вероятность случайно получить такое значение. В нашем случае – почти 0, что говорит о том, что доля оксфордской керамики на участке действительно зависит от расстояния.\n\ntidy(fit) |&gt; \n  transmute(t_stat = estimate / std.error) |&gt; \n  mutate(p_val = 2*pt(abs(t_stat), 28, lower.tail = FALSE))\n\n\n  \n\n\n\nРезультат, возвращаемый функцией pt(), умножается на два, т.к. используется двусторонний t-test. Буква p в названии означает функцию распределения вероятностей (probability), а t – распределение Стьюдента для заданного числа степеней свободы (28 в нашем случае).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#предсказания-с-predict",
    "href": "regression.html#предсказания-с-predict",
    "title": "22  Регрессионный анализ",
    "section": "22.6 Предсказания с predict()",
    "text": "22.6 Предсказания с predict()\nПредсказанные значения можно извлечь при помощи predict(). Это почти то же самое, что fit$fitted.values. Разница в том, что функции predict() можно передать новые данные. Узнаем, какую долю оксфордской керамики наша модель ожидает обнаружить на расстоянии ровно 100 миль от Оксфорда.\n\nnewdata &lt;- data.frame(OxfordDst = 100)\npredict(fit, newdata)\n\n       1 \n8.586158 \n\n\nПод капотом функция predict() подставляет подогнанные значения коэффициентов:\n\nfit$coefficients[[1]] + fit$coefficients[[2]] * 100\n\n[1] 8.586158",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#оценка-модели-c-r2",
    "href": "regression.html#оценка-модели-c-r2",
    "title": "22  Простая линейная регрессия",
    "section": "22.11 Оценка модели c \\(R^2\\)",
    "text": "22.11 Оценка модели c \\(R^2\\)\nRSE – это мера несоответствия модели данным. Но поскольку она выражается в тех же единицах измерения, что и y, то не всегда бывает ясно, какая RSE является хорошей. Коэффициент детерминации \\(R^2\\) представляет собой альтернативную меру соответствия. Этот показатель принимает форму доли – доли объясненной дисперсии, в связи с чем он всегда изменяется от 0 до 1 и не зависит от шкалы измерения.\n\\[R^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] Здесь \\(TSS = \\sum(y_i - \\bar y)^2\\), то есть общая сумма квадратов.\nTSS является мерой общей дисперсии отклика Y, и о ней можно думать как о степени изменчивости, присущей отклику до выполнения регрессионного анализа. В то же время RSS выражает степень изменчивости, которая осталась необъясненной после построения регрессионной модели. Следовательно, TSS - RSS выражает количество дисперсии отклика, объясненное (“изъятое”) после выполнения регрессионного анализа, а \\(R^2\\) – долю дисперсии Y, объясненную при помощи X. Статистика \\(R^2\\), близкая к 1, означает, что значительная доля изменчивости отклика была объяснена регрессионной моделью (Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани 2020, 82).\n\ntss &lt;- sum((y - mean(y))^2)\ntss\n\n[1] 1408.878\n\n1 - rss / tss\n\n[1] 0.3764977\n\n\nСнова сравним с результатом, который нам вернула модель.\n\nsummary(fit)$r.squared\n\n[1] 0.3764977\n\n\nДля простой линейной регрессии статистика \\(R^2\\) совпадает с квадратом коэффициента корреляции.\n\ncor(x, y)^2\n\n[1] 0.3764977",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#стандартная-ошибка-остатков-rse",
    "href": "regression.html#стандартная-ошибка-остатков-rse",
    "title": "22  Простая линейная регрессия",
    "section": "22.10 Стандартная ошибка остатков (RSE)",
    "text": "22.10 Стандартная ошибка остатков (RSE)\nПоскольку наши оценки могут быть как завышенными, так и заниженными, значения ошибок возводятся в квадрат и суммируются по всем точкам данных. Узнаем сумму квадратов остатков (RSS = Residual sum of squares), которая считается по формуле:\n\\[RSS = \\sum_{i=n}^n(y_i- \\hat y_i)^2\\]\n\nrss = sum(fit$residuals^2)\nrss\n\n[1] 878.439\n\n\nЗная это число, определяем среднюю (MSE = Mean square error) и стандартную ошибку остатков (RSE = Residual standard error). Грубо говоря, это средняя величина отклонения отклика от регрессионной линии. Заметьте, что вместо 30 делим на 28 (n - 2); это делается, чтобы избежать смещения данных.\n\nmse &lt;- rss / ( length(fit$residuals) - 2)\nmse\n\n[1] 31.37282\n\nrse &lt;- sqrt(mse) \nrse\n\n[1] 5.601145\n\n\nСравним с тем, что нам вернула команда summary(fit).\n\nsummary(fit)$sigma\n\n[1] 5.601145",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#нулевая-модель",
    "href": "regression.html#нулевая-модель",
    "title": "22  Простая линейная регрессия",
    "section": "22.12 Нулевая модель",
    "text": "22.12 Нулевая модель\nВажно знать, что следующие два вызова возвращают одинаковые модели.\n\nfit1 &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nfit2 &lt;- lm(OxfordPct ~ 1 + OxfordDst, data = OxfordPots)\n\n\nfit1$coef == fit2$coef\n\n(Intercept)   OxfordDst \n       TRUE        TRUE \n\n\nЕдиница в вызове функции означает пересечение оси y, то есть свободный член. Это значит, что мы можем построить нулевую модель, где любому значению x будет соответствовать одно и то же (среднее) значение y.\n\nfit_null &lt;- lm(OxfordPct ~ 1, data = OxfordPots)\nsummary(fit_null)\n\n\nCall:\nlm(formula = OxfordPct ~ 1, data = OxfordPots)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2117  -5.8992  -0.9617   6.1008   9.7883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   12.712      1.273   9.989 6.77e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.97 on 29 degrees of freedom\n\n\nЕдинственный коэффициент в таком случае совпадает со средним значением y.\n\nmean(OxfordPots$OxfordPct)\n\n[1] 12.71167\n\n\nНа графике это будет выглядеть вот так.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  # обратите внимание на формулу!\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nТакая модель может быть использована для сравнения, чтобы понять, насколько мы выиграли, добавив предикторы. Подробнее о сравнении разных моделей будет сказано дальше.\n\nanova(fit_null, fit)\n\n\n  \n\n\n\n\n\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2020. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#анализ-остатков",
    "href": "regression.html#анализ-остатков",
    "title": "22  Простая линейная регрессия",
    "section": "22.8 Анализ остатков",
    "text": "22.8 Анализ остатков\nКак правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые невязки) должны быть минимальны.\nНевязка – это разница между прогнозируемым и фактическим значениями отклика: \\((y_i- \\hat y)\\). На графике ниже невязки обозначены пунктиром.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  geom_segment(aes(xend = OxfordDst,\n                   yend = predict(fit)), \n               linetype = 2, \n               color = cols[1]) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nМы можем убедиться в том, что невязки (fit$residuals) представляют собой разницу между фактическим (OxfordPots$OxfordPct) и предсказанным значением (fit$fitted.value). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями.\n\nall.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)\n\n[1] TRUE\n\n\nЕсли модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.\n\nmean(fit$residuals)\n\n[1] 3.256654e-16\n\ncov(fit$residuals, OxfordPots$OxfordDst)\n\n[1] -3.920236e-15\n\n\nКроме того, полезно проверить остатки на нормальность. Это можно сделать при помощи специального теста или визуально.\n\nshapiro.test(residuals(fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.97648, p-value = 0.7262\n\n\nВысокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально.\nТакже проведем три визуальных теста.\n\nlibrary(gridExtra)\n\ng1 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  mutate(residuals_st = scale(residuals)) |&gt; \n  ggplot(aes(sample = residuals_st)) +\n  stat_qq(color = cols[3], \n          size = 2, alpha = 0.8) + \n  stat_qq_line(color = cols[2]) +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\ng2 &lt;- tibble(residuals = residuals(fit),\n       fitted = predict(fit)) |&gt; \n  ggplot(aes(fitted, residuals)) +\n  geom_point(color = cols[3], \n             size = 2, alpha = 0.8) + \n  theme_minimal()\n\ng3 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  ggplot(aes(residuals)) +\n  geom_histogram(fill = cols[3], color = \"white\") + \n  theme_minimal()\n\ngrid.arrange(g1, g2, g3, nrow = 1)\n\n\n\n\n\n\n\n\nНа квантиль-квантильном графике и на гистограмме видно небольшой перекос в области негативных значений (это может означать, что мы чаще переоцениваем, чем недооцениваем y. К этому мы еще вернемся.\nВы можете также использовать базовую plot(), передав ей подогнанную модель в качестве аргумента.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №1 “Introduction”, №2 “Residuals”, №3 “Least Squares Estimation”, №4 “Residual Variation”. Один из этих уроков потребует установить пакет manipulate, позволяющий взаимодействовать с графиком в интерактивном режиме.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html",
    "href": "multivar.html",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "",
    "text": "23.1 Регрессионные алгоритмы\nВ машинном обучении проблемы, связанные с количественным откликом, называют проблемами регрессии, а проблемы, связанные с качественным откликом, проблемами классификации. В прошлом уроке мы познакомились с простой и множественной регрессией, но регрессионных алгоритмов великое множество. Вот лишь некоторые из них:\nКроме того, существуют методы регуляризации линейных моделей, позволяющие существенно улучшить их качество на данных большой размерности (т.е. с большим количеством предкторов). К таким алгоритмам относятся гребневая регрессия и метод лассо. О них мы поговорим в одном из следующих уроков.\nО математической стороне дела см. Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани (2017). В этом уроке мы научимся работать с различными регрессионными алгоритмами, используя библиотеку tidymodels.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#модель-с-несколькими-предикторами",
    "href": "multivar.html#модель-с-несколькими-предикторами",
    "title": "23  Множественная регрессия",
    "section": "",
    "text": "summary(fit)$r.squared\n\n[1] 0.3764977\n\nsummary(fit)$sigma\n\n[1] 5.601145\n\n\n\n\n\n\nsummary(fit2)$r.squared\n\n[1] 0.548113\n\nsummary(fit2)$sigma\n\n[1] 5.128755",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#мнимые-переменные",
    "href": "multivar.html#мнимые-переменные",
    "title": "23  Множественная регрессия",
    "section": "23.2 Мнимые переменные",
    "text": "23.2 Мнимые переменные\nДля построения модели можно использовать не только количественные, но и качественные предикторы. Если качественный предиктор имеет только два уровня (например, мужской и женский пол), то он превращается в фиктивную переменную, принимающую значения 1 или 0. В нашем датасете в таком виде хранятся сведения о наличии водного сообщения между Оксфордом и местом обнаружения керамических осколков.\n\nfit3 &lt;- lm(OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)\nsummary(fit3)\n\n\nCall:\nlm(formula = OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5730 -1.4982  0.2589  1.0286  5.5021 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.50486    1.27606  11.367 8.49e-12 ***\nOxfordDst   -0.08357    0.01513  -5.525 7.46e-06 ***\nWaterTrans  10.25020    1.07388   9.545 3.82e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.727 on 27 degrees of freedom\nMultiple R-squared:  0.8575,    Adjusted R-squared:  0.8469 \nF-statistic: 81.21 on 2 and 27 DF,  p-value: 3.785e-12\n\n\nНа то, что наша модель стала гораздо более адекватной, указывает не только возросший почти в два раза показатель \\(R^2\\), но и статистика F, о которой будет сказано ниже. Для сравнения в предыдущей модели fit этот показатель составлял 16.907611.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#полиномиальная-регрессия",
    "href": "multivar.html#полиномиальная-регрессия",
    "title": "23  Множественная регрессия",
    "section": "23.3 Полиномиальная регрессия",
    "text": "23.3 Полиномиальная регрессия",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#проблема-переобучения",
    "href": "multivar.html#проблема-переобучения",
    "title": "23  Множественная регрессия",
    "section": "23.4 Проблема переобучения",
    "text": "23.4 Проблема переобучения",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#трансформация-данных",
    "href": "multivar.html#трансформация-данных",
    "title": "23  Множественная регрессия",
    "section": "23.5 Трансформация данных",
    "text": "23.5 Трансформация данных",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#сравнение-с-нулевой-моделью",
    "href": "multivar.html#сравнение-с-нулевой-моделью",
    "title": "23  Множественная регрессия",
    "section": "23.6 Сравнение с нулевой моделью",
    "text": "23.6 Сравнение с нулевой моделью",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#сравнение-моделей",
    "href": "multivar.html#сравнение-моделей",
    "title": "23  Множественная регрессия",
    "section": "23.6 Сравнение моделей",
    "text": "23.6 Сравнение моделей\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №5 “Introduction to Multivariable Regression”, №2 “MultiVar Examples”, №3 “MultiVar Examples2”, №4 “MultiVar Examples3”, “Residuals Diagnostics and Variation”, “Variance Inflation Factors”, “Overfitting and Underfitting”, “Binary Outcomes”, “Count Outcomes”",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#что-осталось-за-кадром",
    "href": "regression.html#что-осталось-за-кадром",
    "title": "22  Регрессионный анализ",
    "section": "22.8 Что осталось за кадром",
    "text": "22.8 Что осталось за кадром\nВ этом уроке мы не рассмотрели множество аспектов регрессионного анализа: необходимость трансформации данных, учет эффектов взаимодействия переменных, использование полиномиальных моделей и др.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №5 “Introduction to Multivariable Regression”, №2 “MultiVar Examples”, №3 “MultiVar Examples2”, №4 “MultiVar Examples3”, “Residuals Diagnostics and Variation”, “Variance Inflation Factors”, “Overfitting and Underfitting”, “Binary Outcomes”, “Count Outcomes”\n\n\n\n\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "multivar.html#библиотека-tidymodels",
    "href": "multivar.html#библиотека-tidymodels",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.2 Библиотека tidymodels",
    "text": "23.2 Библиотека tidymodels\nБиблиотека tidymodels позволяет обучать модели и оценивать их эффективность с использованием принципов опрятных данных. Она представляет собой набор пакетов R, которые разработаны для работы с машинным обучением и являются частью более широкой экосистемы tidyverse.\nВот некоторые из ключевых пакетов, входящих в состав tidymodels:\n\nparsnip - универсальный интерфейс для различных моделей машинного обучения, который упрощает переключение между разными типами моделей;\nrecipes - фреймворк для создания и управления “рецептами” предварительной обработки данных перед тренировкой модели;\nrsample - инструменты для разделения данных на обучающую и тестовую выборки, а также для кросс-валидации;\ntune - функции для оптимизации гиперпараметров моделей машинного обучения;\nyardstick - инструменты для оценки производительности моделей;\nworkflow позволяет объединить различные компоненты модели в единый объект: препроцессинг данных, модель машинного обучения, настройку гиперпараметров.\n\nМы также будем использовать пакет textrecipes, который представляет собой аналог recipes для текстовых данных.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#данные",
    "href": "multivar.html#данные",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.3 Данные",
    "text": "23.3 Данные\nДатасет для этого урока хранит данные о названиях, рейтингах, жанре, цене и числе отзывов на некоторые книги с Amazon. Мы попробуем построить регресионную модель, которая будет предсказывать цену книги.\n\nbooks  &lt;- readxl::read_xlsx(\"../files/AmazonBooks.xlsx\")\nbooks\n\n\n  \n\n\n\nДанные не очень опрятны, и прежде всего их надо тайдифицировать.\n\ncolnames(books) &lt;- tolower(colnames(books))\nbooks &lt;- books |&gt; \n  rename(rating = `user rating`)\n\nНа графике ниже видно, что сильной корреляции между количественными переменными не прослеживается, так что задача перед нами стоит незаурядная. Посмотрим, что можно сделать в такой ситуации.\n\nbooks |&gt; \n  select_if(is.numeric) |&gt; \n  cor() |&gt; \n  corrplot::corrplot(method = \"ellipse\")\n\n\n\n\n\n\n\n\nМы видим, что количественные предикторы объясняют лишь ничтожную долю дисперсии (чуть более информативен жанр).\n\nsummary(lm(price ~ reviews + year + rating + genre, data  = books))\n\n\nCall:\nlm(formula = price ~ reviews + year + rating + genre, data = books)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.472  -5.050  -1.841   2.307  89.686 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       8.987e+02  2.734e+02   3.287  0.00107 ** \nreviews           7.779e-07  3.181e-05   0.024  0.98050    \nyear             -4.324e-01  1.370e-01  -3.156  0.00168 ** \nrating           -3.655e+00  1.933e+00  -1.891  0.05909 .  \ngenreNon Fiction  3.920e+00  8.669e-01   4.522 7.41e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.16 on 595 degrees of freedom\nMultiple R-squared:  0.06903,   Adjusted R-squared:  0.06277 \nF-statistic: 11.03 on 4 and 595 DF,  p-value: 1.235e-08\n\n\nПосмотрим, можно ли как-то улучшить этот результат. Но сначала оценим визуально связь между ценой, с одной стороны, и годом и жанром, с другой.\n\ng1 &lt;- books |&gt; \n  ggplot(aes(year, price, color = genre, group = genre)) + \n  geom_jitter(show.legend = FALSE, alpha = 0.7) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\ng2 &lt;- books |&gt; \n  ggplot(aes(genre, price, color = genre)) + \n  geom_boxplot() + \n  theme_minimal()\n\ngridExtra::grid.arrange(g1, g2, nrow = 1)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#обучающая-и-контрольная-выборка",
    "href": "multivar.html#обучающая-и-контрольная-выборка",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.4 Обучающая и контрольная выборка",
    "text": "23.4 Обучающая и контрольная выборка\nВы уже знаете, при обучении модели мы стремимся к минимизации среднеквадратичной ошибки (MSE), однако в большинстве случаев нас интересует не то, как метод работает на обучающих данных, а то, как он покажет себя на контрольных данных. Чтобы избежать переобучения, очень важно в самом начале разделить доступные наблюдения на две группы.\n\nbooks_split &lt;- books |&gt; \n  initial_split()\n\nbooks_train &lt;- training(books_split)\nbooks_test &lt;- testing(books_split)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#определение-модели",
    "href": "multivar.html#определение-модели",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.5 Определение модели",
    "text": "23.5 Определение модели\nОпределение модели включает следующие шаги:\n\nуказывается тип модели на основе ее математической структуры (например, линейная регрессия, случайный лес, KNN и т. д.);\nуказывается механизм для подгонки модели – чаще всего это программный пакет, который должен быть использован, например glmnet. Это самостоятельные модели, и parsnip обеспечивает согласованные интерфейсы, используя их в качестве движков для моделирования.\nпри необходимости объявляется режим модели. Режим отражает тип прогнозируемого результата. Для числовых результатов режимом является регрессия, для качественных - классификация. Если алгоритм модели может работать только с одним типом результатов прогнозирования, например, линейной регрессией, режим уже задан.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#дизайн-переменных",
    "href": "multivar.html#дизайн-переменных",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.8 Дизайн переменных",
    "text": "23.8 Дизайн переменных\nТеперь нам нужен препроцессор. За него отвечает пакет recipes. Если вы не уверены, какие шаги необходимы на этом этапе, можно заглянуть в шпаргалку. В случае с линейной регрессией это может быть логарифмическая трансформация, нормализация, отсев переменных с нулевой дисперсией (zero variance), добавление (impute) недостающих значений или удаление переменных, которые коррелируют с другими переменными.\nВот так выглядит наш первый рецепт. Обратите внимание, что формула записывается так же, как мы это делали ранее внутри функции lm().\n\nbooks_rec &lt;- recipe(price ~ year + genre + name, \n                    data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_normalize(year) |&gt; \n  step_tokenize(name)  |&gt; \n  step_tokenfilter(name, max_tokens = 1000)  |&gt; \n  step_tfidf(name) \n\nПри желании можно посмотреть на результат предобработки.\n\nprep(books_rec, books_train) |&gt; \n  bake(new_data = NULL) |&gt; \n  head(5) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nyear\nprice\ngenre_Non.Fiction\ntfidf_name_1\ntfidf_name_1,111\ntfidf_name_10\ntfidf_name_100\ntfidf_name_11\ntfidf_name_12\ntfidf_name_13\ntfidf_name_140\ntfidf_name_150\ntfidf_name_16\ntfidf_name_17\ntfidf_name_1936\ntfidf_name_1984\ntfidf_name_2\ntfidf_name_2.0\ntfidf_name_20\ntfidf_name_2016\ntfidf_name_22\ntfidf_name_3\ntfidf_name_30\ntfidf_name_300\ntfidf_name_4\ntfidf_name_451\ntfidf_name_49\ntfidf_name_5\ntfidf_name_5,000\ntfidf_name_500\ntfidf_name_5000\ntfidf_name_52\ntfidf_name_5th\ntfidf_name_6\ntfidf_name_60\ntfidf_name_63\ntfidf_name_6th\ntfidf_name_7\ntfidf_name_700\ntfidf_name_8\ntfidf_name_a\ntfidf_name_about\ntfidf_name_absurd\ntfidf_name_according\ntfidf_name_account\ntfidf_name_achieving\ntfidf_name_acid\ntfidf_name_activity\ntfidf_name_adult\ntfidf_name_adults\ntfidf_name_advanced\ntfidf_name_adventures\ntfidf_name_adversity\ntfidf_name_afterlife\ntfidf_name_aftermath\ntfidf_name_again\ntfidf_name_against\ntfidf_name_ages\ntfidf_name_agreements\ntfidf_name_ahead\ntfidf_name_alaska\ntfidf_name_alex\ntfidf_name_alexander\ntfidf_name_all\ntfidf_name_allâ\ntfidf_name_allegiant\ntfidf_name_almost\ntfidf_name_alphabet\ntfidf_name_am\ntfidf_name_amateur\ntfidf_name_amazing\ntfidf_name_america\ntfidf_name_american\ntfidf_name_americans\ntfidf_name_an\ntfidf_name_ancient\ntfidf_name_and\ntfidf_name_animal\ntfidf_name_animals\ntfidf_name_answers\ntfidf_name_antidote\ntfidf_name_antiracist\ntfidf_name_apologizing\ntfidf_name_approach\ntfidf_name_are\ntfidf_name_arguing\ntfidf_name_art\ntfidf_name_as\ntfidf_name_asians\ntfidf_name_assassination\ntfidf_name_assault\ntfidf_name_association\ntfidf_name_astounding\ntfidf_name_astrophysics\ntfidf_name_at\ntfidf_name_athena\ntfidf_name_attitude\ntfidf_name_audacious\ntfidf_name_autobiography\ntfidf_name_awesome\ntfidf_name_azkaban\ntfidf_name_b\ntfidf_name_baby\ntfidf_name_back\ntfidf_name_badass\ntfidf_name_ball\ntfidf_name_ballad\ntfidf_name_barefoot\ntfidf_name_basketball\ntfidf_name_battling\ntfidf_name_be\ntfidf_name_bear\ntfidf_name_beautiful\ntfidf_name_beck's\ntfidf_name_become\ntfidf_name_becoming\ntfidf_name_bed\ntfidf_name_bee\ntfidf_name_beginner's\ntfidf_name_beginners\ntfidf_name_being\ntfidf_name_believing\ntfidf_name_belly\ntfidf_name_berlin\ntfidf_name_between\ntfidf_name_big\ntfidf_name_bill\ntfidf_name_billionaires\ntfidf_name_bin\ntfidf_name_birthday\ntfidf_name_blitz\ntfidf_name_blood\ntfidf_name_blue\ntfidf_name_boat\ntfidf_name_body\ntfidf_name_bombers\ntfidf_name_book\ntfidf_name_books\ntfidf_name_boom\ntfidf_name_born\ntfidf_name_bossypants\ntfidf_name_box\ntfidf_name_boxed\ntfidf_name_boy\ntfidf_name_boy's\ntfidf_name_boys\ntfidf_name_brain\ntfidf_name_brain's\ntfidf_name_brave\ntfidf_name_brawl\ntfidf_name_bree\ntfidf_name_bringing\ntfidf_name_brink\ntfidf_name_broke\ntfidf_name_brothers\ntfidf_name_brown\ntfidf_name_brush\ntfidf_name_building\ntfidf_name_business\ntfidf_name_buy\ntfidf_name_by\ntfidf_name_cabin\ntfidf_name_cakes\ntfidf_name_called\ntfidf_name_calligraphy\ntfidf_name_calling\ntfidf_name_camelot\ntfidf_name_can\ntfidf_name_can't\ntfidf_name_cannot\ntfidf_name_capital\ntfidf_name_captain\ntfidf_name_carbs\ntfidf_name_case\ntfidf_name_caste\ntfidf_name_casual\ntfidf_name_cat\ntfidf_name_catch\ntfidf_name_catching\ntfidf_name_caterpillar\ntfidf_name_cats\ntfidf_name_cauldron\ntfidf_name_cause\ntfidf_name_celebrations\ntfidf_name_century\ntfidf_name_chamber\ntfidf_name_champion\ntfidf_name_change\ntfidf_name_changed\ntfidf_name_changing\ntfidf_name_chaos\ntfidf_name_chicka\ntfidf_name_child\ntfidf_name_china\ntfidf_name_choose\ntfidf_name_christian\ntfidf_name_chronicles\ntfidf_name_churchill\ntfidf_name_ck\ntfidf_name_clash\ntfidf_name_class\ntfidf_name_classic\ntfidf_name_classics\ntfidf_name_cleanse\ntfidf_name_clintons\ntfidf_name_club\ntfidf_name_collected\ntfidf_name_college\ntfidf_name_color\ntfidf_name_coloring\ntfidf_name_colors\ntfidf_name_columbus's\ntfidf_name_comes\ntfidf_name_comfort\ntfidf_name_comments\ntfidf_name_common\ntfidf_name_companies\ntfidf_name_complete\ntfidf_name_comprehensive\ntfidf_name_conducted\ntfidf_name_confession\ntfidf_name_confident\ntfidf_name_confidential\ntfidf_name_confronts\ntfidf_name_conservative\ntfidf_name_constitution\ntfidf_name_construction\ntfidf_name_contessa\ntfidf_name_control\ntfidf_name_cookbook\ntfidf_name_cookbooks\ntfidf_name_cooker\ntfidf_name_cooking\ntfidf_name_cooks\ntfidf_name_cooling\ntfidf_name_coping\ntfidf_name_counterintuitive\ntfidf_name_courage\ntfidf_name_crawdads\ntfidf_name_crayons\ntfidf_name_crazy\ntfidf_name_create\ntfidf_name_created\ntfidf_name_creating\ntfidf_name_creative\ntfidf_name_creator\ntfidf_name_crest\ntfidf_name_crime\ntfidf_name_crisis\ntfidf_name_cross\ntfidf_name_crows\ntfidf_name_culinary\ntfidf_name_cultivate\ntfidf_name_culture\ntfidf_name_cups\ntfidf_name_cursed\ntfidf_name_cutting\ntfidf_name_cycle\ntfidf_name_da\ntfidf_name_dad\ntfidf_name_daily\ntfidf_name_dance\ntfidf_name_dangerous\ntfidf_name_dangers\ntfidf_name_danny\ntfidf_name_dare\ntfidf_name_daring\ntfidf_name_darker\ntfidf_name_david\ntfidf_name_day\ntfidf_name_days\ntfidf_name_dead\ntfidf_name_dear\ntfidf_name_death\ntfidf_name_decades\ntfidf_name_decision\ntfidf_name_deckled\ntfidf_name_decluttering\ntfidf_name_defiance\ntfidf_name_definitive\ntfidf_name_defy\ntfidf_name_delicious\ntfidf_name_deluxe\ntfidf_name_designed\ntfidf_name_designs\ntfidf_name_devotional\ntfidf_name_diagnostic\ntfidf_name_diary\ntfidf_name_die\ntfidf_name_diet\ntfidf_name_dietâ\ntfidf_name_difference\ntfidf_name_difficult\ntfidf_name_dinnertime\ntfidf_name_dirt\ntfidf_name_disappointments\ntfidf_name_discontents\ntfidf_name_discovery\ntfidf_name_disease\ntfidf_name_disorders\ntfidf_name_divergent\ntfidf_name_divine\ntfidf_name_do\ntfidf_name_doctor's\ntfidf_name_documents\ntfidf_name_dog\ntfidf_name_don't\ntfidf_name_donkey\ntfidf_name_doomsday\ntfidf_name_doubting\ntfidf_name_dover\ntfidf_name_dragon\ntfidf_name_dragons\ntfidf_name_drive\ntfidf_name_dsm\ntfidf_name_dukan\ntfidf_name_dungeons\ntfidf_name_during\ntfidf_name_dysfunctions\ntfidf_name_earth\ntfidf_name_easy\ntfidf_name_eat\ntfidf_name_eater's\ntfidf_name_eclipse\ntfidf_name_edge\ntfidf_name_edition\ntfidf_name_educated\ntfidf_name_effective\ntfidf_name_electric\ntfidf_name_elegance\ntfidf_name_elegy\ntfidf_name_elements\ntfidf_name_elephants\ntfidf_name_embracing\ntfidf_name_enchanted\ntfidf_name_end\ntfidf_name_english\ntfidf_name_enjoying\ntfidf_name_enough\ntfidf_name_epic\ntfidf_name_essential\ntfidf_name_eternity\ntfidf_name_ever\ntfidf_name_every\ntfidf_name_everyday\ntfidf_name_everything\ntfidf_name_everywhere\ntfidf_name_exceptional\ntfidf_name_expect\ntfidf_name_expecting\ntfidf_name_f\ntfidf_name_fable\ntfidf_name_face\ntfidf_name_facing\ntfidf_name_facts\ntfidf_name_fahrenheit\ntfidf_name_faith\ntfidf_name_families\ntfidf_name_family\ntfidf_name_fast\ntfidf_name_fat\ntfidf_name_fate\ntfidf_name_fault\ntfidf_name_fear\ntfidf_name_feast\ntfidf_name_featuring\ntfidf_name_feel\ntfidf_name_feeling\ntfidf_name_fetch\ntfidf_name_fever\ntfidf_name_fey\ntfidf_name_fiction\ntfidf_name_fifty\ntfidf_name_find\ntfidf_name_finding\ntfidf_name_fire\ntfidf_name_fires\ntfidf_name_first\ntfidf_name_firsthand\ntfidf_name_fish\ntfidf_name_five\ntfidf_name_flap\ntfidf_name_flawed\ntfidf_name_fleas\ntfidf_name_floral\ntfidf_name_flowers\ntfidf_name_food\ntfidf_name_foods\ntfidf_name_foolproof\ntfidf_name_fools\ntfidf_name_for\ntfidf_name_forâ\ntfidf_name_forest\ntfidf_name_forever\ntfidf_name_found\ntfidf_name_four\ntfidf_name_fox\ntfidf_name_freakonomics\ntfidf_name_free\ntfidf_name_freed\ntfidf_name_freedom\ntfidf_name_freezer\ntfidf_name_french\ntfidf_name_friends\ntfidf_name_from\ntfidf_name_frontier\ntfidf_name_frozen\ntfidf_name_full\ntfidf_name_fully\ntfidf_name_fun\ntfidf_name_gain\ntfidf_name_game\ntfidf_name_games\ntfidf_name_gatsby\ntfidf_name_general\ntfidf_name_gentleman\ntfidf_name_geographic\ntfidf_name_george\ntfidf_name_get\ntfidf_name_giants\ntfidf_name_gifts\ntfidf_name_giraffes\ntfidf_name_girl\ntfidf_name_girls\ntfidf_name_give\ntfidf_name_giving\ntfidf_name_glenn\ntfidf_name_global\ntfidf_name_glory\ntfidf_name_go\ntfidf_name_goals\ntfidf_name_goblet\ntfidf_name_god\ntfidf_name_going\ntfidf_name_gold\ntfidf_name_golden\ntfidf_name_goldfinch\ntfidf_name_goliath\ntfidf_name_gone\ntfidf_name_good\ntfidf_name_goodnight\ntfidf_name_government\ntfidf_name_grades\ntfidf_name_grain\ntfidf_name_gratitude\ntfidf_name_great\ntfidf_name_greatly\ntfidf_name_greatness\ntfidf_name_green\ntfidf_name_greenlights\ntfidf_name_grey\ntfidf_name_grime\ntfidf_name_guernsey\ntfidf_name_guide\ntfidf_name_guided\ntfidf_name_gut\ntfidf_name_guy\ntfidf_name_habit\ntfidf_name_habits\ntfidf_name_half\ntfidf_name_hamilton\ntfidf_name_hand\ntfidf_name_handbook\ntfidf_name_hands\ntfidf_name_happened\ntfidf_name_hard\ntfidf_name_hardcover\ntfidf_name_harry\ntfidf_name_haul\ntfidf_name_have\ntfidf_name_haven\ntfidf_name_hayek\ntfidf_name_head\ntfidf_name_heal\ntfidf_name_healing\ntfidf_name_health\ntfidf_name_healthy\ntfidf_name_healthyâ\ntfidf_name_heat\ntfidf_name_heaven\ntfidf_name_hedgehog\ntfidf_name_hell\ntfidf_name_help\ntfidf_name_henna\ntfidf_name_henrietta\ntfidf_name_her\ntfidf_name_hero\ntfidf_name_heroes\ntfidf_name_hidden\ntfidf_name_higher\ntfidf_name_highly\ntfidf_name_hillbilly\ntfidf_name_his\ntfidf_name_historia\ntfidf_name_history\ntfidf_name_holidays\ntfidf_name_homebody\ntfidf_name_honey\ntfidf_name_hornet's\ntfidf_name_horse\ntfidf_name_hour\ntfidf_name_house\ntfidf_name_how\ntfidf_name_howard\ntfidf_name_human\ntfidf_name_humanity\ntfidf_name_humans\ntfidf_name_hunger\ntfidf_name_hungry\ntfidf_name_hurricane\ntfidf_name_hurry\ntfidf_name_hurt\ntfidf_name_hyperbole\ntfidf_name_hypothetical\ntfidf_name_hyrule\ntfidf_name_i\ntfidf_name_ice\ntfidf_name_icons\ntfidf_name_ideas\ntfidf_name_idiots\ntfidf_name_if\ntfidf_name_ii\ntfidf_name_ii's\ntfidf_name_illustrated\ntfidf_name_imagination\ntfidf_name_immortal\ntfidf_name_implications\ntfidf_name_in\ntfidf_name_inches\ntfidf_name_incredible\ntfidf_name_inferno\ntfidf_name_inflatable\ntfidf_name_influence\ntfidf_name_inheritance\ntfidf_name_inky\ntfidf_name_inside\ntfidf_name_inspirational\ntfidf_name_inspired\ntfidf_name_instant\ntfidf_name_insurance\ntfidf_name_intimate\ntfidf_name_into\ntfidf_name_introverts\ntfidf_name_is\ntfidf_name_it\ntfidf_name_it's\ntfidf_name_iv\ntfidf_name_jackson\ntfidf_name_japan\ntfidf_name_japanese\ntfidf_name_jesus\ntfidf_name_jobs\ntfidf_name_jokes\ntfidf_name_jon\ntfidf_name_journal\ntfidf_name_journey\ntfidf_name_joy\ntfidf_name_joyland\ntfidf_name_k\ntfidf_name_kane\ntfidf_name_keep\ntfidf_name_keeps\ntfidf_name_kennedy\ntfidf_name_keto\ntfidf_name_ketogenic\ntfidf_name_key\ntfidf_name_kicked\ntfidf_name_kid\ntfidf_name_kids\ntfidf_name_kill\ntfidf_name_killed\ntfidf_name_killers\ntfidf_name_killing\ntfidf_name_kings\ntfidf_name_kissed\ntfidf_name_kitchen\ntfidf_name_kitties\ntfidf_name_knickerbocker\ntfidf_name_knock\ntfidf_name_know\ntfidf_name_knowledge\ntfidf_name_lacks\ntfidf_name_laden\ntfidf_name_land\ntfidf_name_languages\ntfidf_name_last\ntfidf_name_lasts\ntfidf_name_laugh\ntfidf_name_lead\ntfidf_name_leadership\ntfidf_name_lean\ntfidf_name_leap\ntfidf_name_learn\ntfidf_name_leave\ntfidf_name_lectin\ntfidf_name_lecture\ntfidf_name_legend\ntfidf_name_lego\ntfidf_name_leonardo\ntfidf_name_lessons\ntfidf_name_lethal\ntfidf_name_lettering\ntfidf_name_liberty\ntfidf_name_lies\ntfidf_name_life\ntfidf_name_lifestyle\ntfidf_name_lifetime\ntfidf_name_lift\ntfidf_name_light\ntfidf_name_like\ntfidf_name_lincoln\ntfidf_name_literary\ntfidf_name_litigators\ntfidf_name_little\ntfidf_name_live\ntfidf_name_lived\ntfidf_name_living\ntfidf_name_long\ntfidf_name_looking\ntfidf_name_lord\ntfidf_name_lose\ntfidf_name_losing\ntfidf_name_loss\ntfidf_name_lost\ntfidf_name_loud\ntfidf_name_love\ntfidf_name_loyalty\ntfidf_name_luck\ntfidf_name_machine\ntfidf_name_magic\ntfidf_name_magical\ntfidf_name_magnolia\ntfidf_name_make\ntfidf_name_man\ntfidf_name_man's\ntfidf_name_mandalas\ntfidf_name_manifesto\ntfidf_name_manual\ntfidf_name_mark\ntfidf_name_martian\ntfidf_name_master\ntfidf_name_mastering\ntfidf_name_matter\ntfidf_name_matters\ntfidf_name_maybe\ntfidf_name_mayhem\ntfidf_name_maze\ntfidf_name_mccain\ntfidf_name_me\ntfidf_name_meals\ntfidf_name_meant\ntfidf_name_mechanisms\ntfidf_name_medicine\ntfidf_name_meditation\ntfidf_name_meltdown\ntfidf_name_memoir\ntfidf_name_mental\ntfidf_name_mermaid\ntfidf_name_midnight\ntfidf_name_military\ntfidf_name_milk\ntfidf_name_millennium\ntfidf_name_mind\ntfidf_name_mindfulness\ntfidf_name_minds\ntfidf_name_mindset\ntfidf_name_minute\ntfidf_name_miracles\ntfidf_name_misfits\ntfidf_name_missing\ntfidf_name_mission\ntfidf_name_mockingbird\ntfidf_name_mockingjay\ntfidf_name_modern\ntfidf_name_mole\ntfidf_name_mon\ntfidf_name_moon\ntfidf_name_more\ntfidf_name_mortal\ntfidf_name_moscow\ntfidf_name_most\ntfidf_name_mother\ntfidf_name_motivates\ntfidf_name_much\ntfidf_name_my\ntfidf_name_national\ntfidf_name_navy\ntfidf_name_nba\ntfidf_name_need\ntfidf_name_neptune\ntfidf_name_nest\ntfidf_name_neurosurgeon's\ntfidf_name_never\ntfidf_name_new\ntfidf_name_night\ntfidf_name_nightingale\ntfidf_name_nine\ntfidf_name_no\ntfidf_name_not\ntfidf_name_nouveau\ntfidf_name_novel\ntfidf_name_novella\ntfidf_name_now\ntfidf_name_numbers\ntfidf_name_nutrient\ntfidf_name_nutrition\ntfidf_name_o'reilly's\ntfidf_name_obama\ntfidf_name_odds\ntfidf_name_of\ntfidf_name_off\ntfidf_name_official\ntfidf_name_oh\ntfidf_name_olympian\ntfidf_name_olympians\ntfidf_name_olympics\ntfidf_name_olympus\ntfidf_name_on\ntfidf_name_one\ntfidf_name_oprah's\ntfidf_name_option\ntfidf_name_or\ntfidf_name_organizing\ntfidf_name_origins\ntfidf_name_orphan\ntfidf_name_osama\ntfidf_name_other\ntfidf_name_others\ntfidf_name_our\ntfidf_name_out\ntfidf_name_outliers\ntfidf_name_ove\ntfidf_name_over\ntfidf_name_overwhelmed\ntfidf_name_owls\ntfidf_name_p\ntfidf_name_p.s\ntfidf_name_pacific\ntfidf_name_paine\ntfidf_name_paint\ntfidf_name_paisleyâ\ntfidf_name_palin\ntfidf_name_paperback\ntfidf_name_paradox\ntfidf_name_parent\ntfidf_name_parts\ntfidf_name_passions\ntfidf_name_pastimes\ntfidf_name_path\ntfidf_name_patient\ntfidf_name_patriot's\ntfidf_name_patriotic\ntfidf_name_patriots\ntfidf_name_patrol\ntfidf_name_patton\ntfidf_name_paw\ntfidf_name_peace\ntfidf_name_peel\ntfidf_name_people\ntfidf_name_percy\ntfidf_name_performers\ntfidf_name_person\ntfidf_name_personal\ntfidf_name_pictures\ntfidf_name_pie\ntfidf_name_pilgrims\ntfidf_name_pioneer\ntfidf_name_places\ntfidf_name_plan\ntfidf_name_planet\ntfidf_name_plant\ntfidf_name_played\ntfidf_name_player\ntfidf_name_player's\ntfidf_name_point\ntfidf_name_points\ntfidf_name_pokã\ntfidf_name_politics\ntfidf_name_portrait\ntfidf_name_pot\ntfidf_name_potato\ntfidf_name_potter\ntfidf_name_potty\ntfidf_name_pounds\ntfidf_name_pout\ntfidf_name_power\ntfidf_name_powerful\ntfidf_name_practical\ntfidf_name_prayer\ntfidf_name_pre\ntfidf_name_preschool\ntfidf_name_presence\ntfidf_name_presents\ntfidf_name_presidency\ntfidf_name_president\ntfidf_name_press\ntfidf_name_pressure\ntfidf_name_preventing\ntfidf_name_printsâ\ntfidf_name_prisoner\ntfidf_name_prize\ntfidf_name_program\ntfidf_name_promised\ntfidf_name_promote\ntfidf_name_proof\ntfidf_name_prostitutes\ntfidf_name_psychological\ntfidf_name_psychology\ntfidf_name_publication\ntfidf_name_pulitzer\ntfidf_name_punishment\ntfidf_name_puppy\ntfidf_name_put\ntfidf_name_pyramid\ntfidf_name_quest\ntfidf_name_questions\ntfidf_name_quiet\ntfidf_name_quit\ntfidf_name_quotes\ntfidf_name_race\ntfidf_name_racing\ntfidf_name_racketeer\ntfidf_name_rain\ntfidf_name_rapid\ntfidf_name_readingâ\ntfidf_name_ready\ntfidf_name_reagan\ntfidf_name_real\ntfidf_name_reasons\ntfidf_name_recipes\ntfidf_name_reckoning\ntfidf_name_red\ntfidf_name_redemption\ntfidf_name_references\ntfidf_name_rehearsal\ntfidf_name_relentless\ntfidf_name_resilience\ntfidf_name_restore\ntfidf_name_results\ntfidf_name_reusable\ntfidf_name_revere\ntfidf_name_revised\ntfidf_name_revolution\ntfidf_name_revolutionary\ntfidf_name_rich\ntfidf_name_riddles\ntfidf_name_right\ntfidf_name_ring\ntfidf_name_rising\ntfidf_name_road\ntfidf_name_rogue\ntfidf_name_rolls\ntfidf_name_room\ntfidf_name_routines\ntfidf_name_rules\ntfidf_name_ruling\ntfidf_name_run\ntfidf_name_runner\ntfidf_name_rush\ntfidf_name_sacred\ntfidf_name_saga\ntfidf_name_salt\ntfidf_name_sarah's\ntfidf_name_sat\ntfidf_name_save\ntfidf_name_saved\ntfidf_name_says\ntfidf_name_school\ntfidf_name_scientific\ntfidf_name_score\ntfidf_name_script\ntfidf_name_scripture\ntfidf_name_scrumptious\ntfidf_name_seal\ntfidf_name_second\ntfidf_name_secret\ntfidf_name_secrets\ntfidf_name_see\ntfidf_name_selfish\ntfidf_name_sense\ntfidf_name_serfdom\ntfidf_name_series\ntfidf_name_serious\ntfidf_name_serpent's\ntfidf_name_sesame\ntfidf_name_set\ntfidf_name_sex\ntfidf_name_sh\ntfidf_name_shack\ntfidf_name_shades\ntfidf_name_shadow\ntfidf_name_shame\ntfidf_name_shocking\ntfidf_name_short\ntfidf_name_should\ntfidf_name_silent\ntfidf_name_silly\ntfidf_name_simple\ntfidf_name_site\ntfidf_name_smart\ntfidf_name_sniper\ntfidf_name_solution\ntfidf_name_some\ntfidf_name_sookie\ntfidf_name_soul\ntfidf_name_stackhouse\ntfidf_name_stars\ntfidf_name_start\ntfidf_name_states\ntfidf_name_statistical\ntfidf_name_step\ntfidf_name_steps\ntfidf_name_sticker\ntfidf_name_stone\ntfidf_name_stop\ntfidf_name_storm\ntfidf_name_story\ntfidf_name_strange\ntfidf_name_street\ntfidf_name_strengthsfinder\ntfidf_name_study\ntfidf_name_subtle\ntfidf_name_success\ntfidf_name_sugar\ntfidf_name_sun\ntfidf_name_surprising\ntfidf_name_survival\ntfidf_name_swords\ntfidf_name_talking\ntfidf_name_tea\ntfidf_name_team\ntfidf_name_techniques\ntfidf_name_than\ntfidf_name_that\ntfidf_name_the\ntfidf_name_things\ntfidf_name_thirteen\ntfidf_name_this\ntfidf_name_thousand\ntfidf_name_three\ntfidf_name_thrones\ntfidf_name_thug\ntfidf_name_tidying\ntfidf_name_time\ntfidf_name_to\ntfidf_name_toddlers\ntfidf_name_toltec\ntfidf_name_total\ntfidf_name_tragedy\ntfidf_name_train\ntfidf_name_trauma\ntfidf_name_travel\ntfidf_name_trilogy\ntfidf_name_true\ntfidf_name_trust\ntfidf_name_truth\ntfidf_name_twilight\ntfidf_name_two\ntfidf_name_unbroken\ntfidf_name_underpants\ntfidf_name_unexpected\ntfidf_name_united\ntfidf_name_unlock\ntfidf_name_up\ntfidf_name_us\ntfidf_name_very\ntfidf_name_vol\ntfidf_name_volume\ntfidf_name_war\ntfidf_name_washington's\ntfidf_name_way\ntfidf_name_we\ntfidf_name_week\ntfidf_name_weight\ntfidf_name_what\ntfidf_name_wheat\ntfidf_name_wheel\ntfidf_name_when\ntfidf_name_where\ntfidf_name_white\ntfidf_name_who\ntfidf_name_whole30\ntfidf_name_why\ntfidf_name_wild\ntfidf_name_will\ntfidf_name_wimpy\ntfidf_name_win\ntfidf_name_wisdom\ntfidf_name_with\ntfidf_name_woman\ntfidf_name_women\ntfidf_name_wonder\ntfidf_name_wonderful\ntfidf_name_wonky\ntfidf_name_words\ntfidf_name_workbook\ntfidf_name_world\ntfidf_name_year\ntfidf_name_york\ntfidf_name_you\ntfidf_name_you'll\ntfidf_name_your\ntfidf_name_zoo\n\n\n\n\n1.0031296\n5\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.5012066\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.2687051\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.4643081\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0.5012066\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.5574755\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6022817\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6022817\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0.6022817\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1124394\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n0.7141358\n8\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1065468\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.2146242\n0\n0.1119850\n0\n0\n0\n0.0000000\n0\n0\n0\n0.2529642\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2819287\n0\n0\n0\n0\n0.2957377\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2957377\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2957377\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2146242\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.194396\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.2457391\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2819287\n0\n0\n0\n0\n0\n0\n0.2457391\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1188463\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.1840274\n0\n0.2070697\n0\n\n\n-1.3088208\n11\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6111467\n0\n0\n0\n0\n0\n0\n0\n0.3433987\n0\n0.1791759\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.501728\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.3931826\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.5420535\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.4731803\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1901541\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.5420535\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.501728\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n1.0031296\n15\n1\n0\n0\n0\n0\n0\n0.7639334\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.4292484\n0\n0.0000000\n0\n0\n0\n0.7639334\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.7639334\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.3022932\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.388792\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6775669\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2376926\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n-1.5978146\n4\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.4870709\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.8730668\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.8730668\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.5469488\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.5616894\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.7167543\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n\n\n\n\n\nДобавляем препроцессор в воркфлоу.\n\nsvm_wflow &lt;- svm_wflow |&gt; \n  add_recipe(books_rec)\n\nsvm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Support Vector Machine Model Specification (regression)\n\nComputational engine: LiblineaR",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#подгонка-модели",
    "href": "multivar.html#подгонка-модели",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.9 Подгонка модели",
    "text": "23.9 Подгонка модели\nПодгоним модель на обучающих данных.\n\nsvm_fit &lt;- svm_wflow |&gt;\n  fit(data = books_train)\n\nПакет broom позволяет тайдифицировать модель. Посмотрим на слова, которые приводят к “удорожанию” книг. Видно, что в начале списка – слова, связанные с научными публикациями, что не лишено смысла.\n\nsvm_fit |&gt; \n  tidy() |&gt; \n  arrange(-estimate) |&gt; \n  head(6) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\ntfidf_name_5th\n22.13462\n\n\ntfidf_name_diagnostic\n22.13462\n\n\ntfidf_name_disorders\n22.13462\n\n\ntfidf_name_dsm\n22.13462\n\n\ntfidf_name_mental\n22.13462\n\n\ntfidf_name_statistical\n22.13462\n\n\n\n\n\n\n\nОценим модель на контрольных данных.\n\npred_data &lt;- tibble(truth = books_test$price,\n                    estimate = predict(svm_fit, books_test)$.pred)\n\nbooks_metrics &lt;- metric_set(rmse, rsq, mae)\n\nbooks_metrics(pred_data, truth = truth,  estimate = estimate) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n8.0599806\n\n\nrsq\nstandard\n0.2178193\n\n\nmae\nstandard\n3.9472913",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#повторные-выборки",
    "href": "multivar.html#повторные-выборки",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.10 Повторные выборки",
    "text": "23.10 Повторные выборки\nЧтобы не распечатывать каждый раз тестовые данные (в идеале мы их используем один, максимум два раза!), задействуется ряд методов, позволяющих оценить ошибку путем исключения части обучающих наблюдений из процесса подгонки модели и последующего применения этой модели к исключенным наблюдениям.\n\n\n\nИсточник.\n\n\nВ пакете rsample из библиотеки tidymodels реализованы, среди прочего, следующие методы повторных выборок для оценки производительности моделей машинного обучения:\n\nМетод проверочной выборки – набор наблюдений делится на обучающую и проверочную, или удержанную, выборку (validation set): для этого используется initial_validation_split().\nK-кратная перекрестная проверка – наблюдения разбиваются на k групп примерно одинакового размера, первый блок служит в качестве проверочной выборки, а модель подгоняется по остальным k-1 блокам; процедура повторяется k раз: функция vfold_cv().\nПерекрестная проверка Монте-Карло – в отличие от предыдущего метода, создается множество случайных разбиений данных на обучающую и тестовую выборки: функция mc_cv().\nБутстреп – отбор наблюдений выполняется с возвращением, т.е. одно и то же наблюдение может встречаться несколько раз: функция bootstraps().\nПерекрестная проверка по отдельным наблюдениям (leave-one-out сross-validation): одно наблюдение используется в качестве контрольного, а остальные составляют обучающую выборку; модель подгоняется по n-1 наблюдениям, что повторяется n раз: функция loo_cv().\n\nЭти методы повторных выборок позволяют получить надежные оценки производительности моделей машинного обучения, избегая переобучения и обеспечивая репрезентативность тестовых выборок.\n\nset.seed(05102024)\nbooks_folds &lt;- vfold_cv(books_train, v = 10) \n\nset.seed(05102024)\nsvm_rs &lt;- fit_resamples(\n  svm_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nТеперь соберем метрики и убедимся, что предыдущая оценка на контрольных данных была слишком оптимистичной. Однако результат не так уж плох: во всяком случае мы смогли добиться заметного улучшения по сравнению с нулевой моделью.\n\ncollect_metrics(svm_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.7160268\n10\n0.69827622\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4673747\n10\n0.09020819\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nsvm_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") + \n  theme_minimal() +\n  coord_cartesian(xlim = c(0,50), ylim = c(0,50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#нулевая-модель",
    "href": "multivar.html#нулевая-модель",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.11 Нулевая модель",
    "text": "23.11 Нулевая модель\nКстати, проверим, какой результат даст нулевая модель.\n\nnull_reg &lt;- null_model() |&gt; \n  set_engine(\"parsnip\") |&gt; \n  set_mode(\"regression\")\n\nnull_wflow &lt;- workflow() |&gt; \n    add_model(null_reg) |&gt; \n    add_recipe(books_rec)\n\nnull_rs &lt;- fit_resamples(\n  null_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\n\n\n\n\ncollect_metrics(null_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n10.38916\n10\n1.161447\nPreprocessor1_Model1\n\n\nrsq\nstandard\nNaN\n0\nNA\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\\(R^2\\) в таком случае должен быть NaN.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#пакет-textrecipes",
    "href": "multivar.html#пакет-textrecipes",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.10 Пакет textrecipes",
    "text": "23.10 Пакет textrecipes\nОбновим препроцессор, добавив новые переменные в качестве предикторов.\n\nbooks_rec &lt;- recipe(price ~ rating + reviews + year + genre + name, \n                    data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_tokenize(name)  |&gt; \n  step_tokenfilter(name, max_tokens = 1e3)  |&gt; \n  step_tfidf(name)\n\n\nlm_wflow &lt;- lm_wflow |&gt; \n  update_recipe(books_rec)\n\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_dummy()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nset.seed(05102024)\nlm_rs &lt;- fit_resamples(\n  lm_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\n\n\n\n\ncollect_metrics(lm_rs)\n\n\n  \n\n\n\nСущественного улучшения нет: попробуем другой движок.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#регуляризованные-модели",
    "href": "multivar.html#регуляризованные-модели",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.11 Регуляризованные модели",
    "text": "23.11 Регуляризованные модели\n\n\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#регрессионные-алгоритмы",
    "href": "multivar.html#регрессионные-алгоритмы",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "",
    "text": "полиномиальная регрессия: расширение линейной регрессии, позволяющее учитывать нелинейные зависимости.\nлогистическая регрессия: используется для прогнозирования категориальных (бинарных) откликов.\nрегрессия на опорных векторах (SVM): ищет гиперплоскость, позволяющую минимизировать ошибку в многомерном пространстве.\nдеревья регрессии: строят иерархическую древовидную модель, последовательно разбивая данные на подгруппы.\nслучайный лес: комбинирует предсказания множества деревьев для повышения точности и устойчивости.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#регрессия-на-опорных-векторах",
    "href": "multivar.html#регрессия-на-опорных-векторах",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.6 Регрессия на опорных векторах",
    "text": "23.6 Регрессия на опорных векторах\nSupport Vector Regression — это метод машинного обучения, основанный на идеях метода опорных векторов (SVM), но адаптированный к задаче регрессии, а не классификации (о чем см. следующий урок).\nВместо поиска разделяющей гиперплоскости между классами (как в классификации), SVR старается найти функцию, которая:\n\nигнорирует небольшие отклонения внутри некоторого допустимого порога ε (эпсилон),\nакцентирует внимание на точках, которые лежат вне этой “трубы”, — это и есть опорные векторы.\n\nВ этом заключается отличие от обычной регрессии, которая старается проложить прямую, наиболее близкую ко всем точкам и “наказывает” любое отклонение.\nSVR тоже строит линию (или кривую в случае нелинейного ядра), но с другим подходом. Она “довольна”, если предсказание находится в пределах допустимой ошибки ε (эпсилон) от настоящего значения.\nSVR концентрируется только на тех точках, что выходят за эту “зону безразличия” или лежат на ее границе — они называются опорными векторами. Именно они определяют форму и положение модели. Остальные точки (в пределах ε) никак не влияют на модель.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#случайный-лес",
    "href": "multivar.html#случайный-лес",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.11 Случайный лес",
    "text": "23.11 Случайный лес\nУточним, какие движки доступны для случайных лесов.\n\nshow_engines(\"rand_forest\")\n\n\n  \n\n\n\nСоздадим спецификацию модели. Деревья используются как в задачах классификации, так и в задачах регрессии, поэтому задействуем функцию set_mode().\n\nrf_spec &lt;- rand_forest(trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\nrf_wflow &lt;- workflow() |&gt; \n  add_model(rf_spec) |&gt; \n  add_recipe(books_rec)\n\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nОбучение займет чуть больше времени.\n\nrf_rs &lt;- fit_resamples(\n  rf_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nМы видим, что среднеквадратическая ошибка уменьшилась, а доля объясненной дисперсии выросла.\n\ncollect_metrics(rf_rs)\n\n\n  \n\n\n\nТем не менее на графике можно заметить нечто странное: наша модель систематически переоценивает низкие значения и недооценивает высокие. Это связано с тем, что случайные леса не очень подходят для работы с разреженными данными (Hvitfeldt и Silge 2022).\n\nrf_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#многослойный-перцептрон",
    "href": "multivar.html#многослойный-перцептрон",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.12 Многослойный перцептрон",
    "text": "23.12 Многослойный перцептрон\nТакже попробуем построить регрессию с использованием нейросетевой модели.\n\nnnet_spec &lt;- \n  mlp(epochs = 1000, \n      hidden_units = 1, \n      penalty = 0.01, \n      learn_rate = 0.01) |&gt; \n  set_engine(\"brulee\") |&gt; \n  set_mode(\"regression\")\n\n\nnnet_wflow &lt;- workflow() |&gt; \n  add_model(nnet_spec) |&gt; \n  add_recipe(books_rec)\n\nnnet_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  hidden_units = 1\n  penalty = 0.01\n  epochs = 1000\n  learn_rate = 0.01\n\nComputational engine: brulee \n\n\nНа этом этапе может быть предложено установить дополнительные пакеты; соглашаемся.\n\nnnet_rs &lt;- fit_resamples(\n  nnet_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\ncollect_metrics(nnet_rs)\n\n\n  \n\n\n\nНейросеть с одним скрытым слоем в нашем случае показала себя хуже, чем опорные векторы. Напомним, что нам удалось добиться увеличения \\(R^2\\) до 0.369. Возможно, получится немного улучшить этот результат.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#удаление-стопслов",
    "href": "multivar.html#удаление-стопслов",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.16 Удаление стопслов",
    "text": "23.16 Удаление стопслов\nИзменим рецепт приготовления данных.\n\nstopwords_rec &lt;- function(stopwords_name) {\n  recipe(price ~ year + genre + name, data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_normalize(year) |&gt; \n  step_tokenize(name)  |&gt; \n  step_stopwords(name, stopword_source = stopwords_name) |&gt; \n  step_tokenfilter(name, max_tokens = 1000)  |&gt; \n  step_tfidf(name) \n}\n\nСоздадим воркфлоу.\n\nsvm_wflow &lt;- workflow() |&gt; \n  add_model(svm_spec)\n\nИ снова проведем перекрестную проверку, на этот раз с разными списками стоп-слов. На этом шаге команда вернет предупреждения о том, что число слов меньше 1000, это нормально, т.к. после удаления стопслов токенов стало меньше.\n\nset.seed(123)\nsnowball_rs &lt;- fit_resamples(\n  svm_wflow |&gt;  add_recipe(stopwords_rec(\"snowball\")),\n  books_folds\n)\n\nset.seed(234)\nsmart_rs &lt;- fit_resamples(\n  svm_wflow |&gt; add_recipe(stopwords_rec(\"smart\")),\n  books_folds\n)\n\nset.seed(345)\nstopwords_iso_rs &lt;- fit_resamples(\n  svm_wflow |&gt; add_recipe(stopwords_rec(\"stopwords-iso\")),\n  books_folds\n)\n\n\ncollect_metrics(smart_rs)  |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.881703\n10\n0.39913002\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.442588\n10\n0.08075147\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics(snowball_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.6361664\n10\n0.41158411\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4582287\n10\n0.08018042\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics((stopwords_iso_rs)) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n8.0260370\n10\n0.35141531\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4388275\n10\n0.07788576\nPreprocessor1_Model1\n\n\n\n\n\n\n\nВ нашем случае удаление стоп-слов положительного эффекта не имело.\n\nword_counts &lt;- tibble(name = c(\"snowball\", \"smart\", \"stopwords-iso\")) %&gt;%\n  mutate(words = map_int(name, ~length(stopwords::stopwords(source = .))))\n\nlist(snowball = snowball_rs,\n     smart = smart_rs,\n     `stopwords-iso` = stopwords_iso_rs)  |&gt; \n  map_dfr(show_best, metric = \"rmse\", .id = \"name\")  |&gt; \n  left_join(word_counts, by = \"name\")  |&gt; \n  mutate(name = paste0(name, \" (\", words, \" words)\"),\n         name = fct_reorder(name, words))  |&gt; \n  ggplot(aes(name, mean, color = name)) +\n  geom_crossbar(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.6) +\n  geom_point(size = 3, alpha = 0.8) +\n  theme(legend.position = \"none\") + \n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#настройки-числа-n-grams",
    "href": "multivar.html#настройки-числа-n-grams",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.17 Настройки числа n-grams",
    "text": "23.17 Настройки числа n-grams\n\nngram_rec &lt;- function(ngram_options) {\n  recipe(price ~ year + genre + name, data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_normalize(year) |&gt; \n  step_tokenize(name, token = \"ngrams\", options = ngram_options)  |&gt; \n  step_tokenfilter(name, max_tokens = 1000)  |&gt; \n  step_tfidf(name) \n}\n\n\nfit_ngram &lt;- function(ngram_options) {\n  fit_resamples(\n    svm_wflow  |&gt; \n    add_recipe(ngram_rec(ngram_options)),\n    books_folds\n  )\n}\n\n\nset.seed(123)\nunigram_rs &lt;- fit_ngram(list(n = 1))\n\nset.seed(234)\nbigram_rs &lt;- fit_ngram(list(n = 2, n_min = 1))\n\nset.seed(345)\ntrigram_rs &lt;- fit_ngram(list(n = 3, n_min = 1))\n\n\ncollect_metrics(unigram_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.7160268\n10\n0.69827622\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4673747\n10\n0.09020819\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics(bigram_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.2108600\n10\n0.38677343\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4771493\n10\n0.07009008\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics(trigram_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.479379\n10\n0.44739644\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.467884\n10\n0.06368306\nPreprocessor1_Model1\n\n\n\n\n\n\n\nТаким образом, униграмы дают лучший результат:\n\nlist(`1` = unigram_rs,\n     `1 and 2` = bigram_rs,\n     `1, 2, and 3` = trigram_rs) |&gt; \n  map_dfr(collect_metrics, .id = \"name\")  |&gt; \n  filter(.metric == \"rmse\")  |&gt; \n  ggplot(aes(name, mean, color = name)) +\n  geom_crossbar(aes(ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.6) +\n  geom_point(size = 3, alpha = 0.8) +\n  theme(legend.position = \"none\") +\n  labs(\n    y = \"RMSE\"\n  ) + \n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#лучшая-модель-и-оценка",
    "href": "multivar.html#лучшая-модель-и-оценка",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.18 Лучшая модель и оценка",
    "text": "23.18 Лучшая модель и оценка\n\nsvm_fit &lt;- svm_wflow |&gt;\n  add_recipe(books_rec) |&gt; \n  fit(data = books_test)\n\nWarning: max_tokens was set to 1000, but only 519 was available and selected.\n\nsvm_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\n$TypeDetail\n[1] \"L2-regularized L2-loss support vector regression primal (L2R_L2LOSS_SVR)\"\n\n$Type\n[1] 11\n\n$W\n          year genre_Non.Fiction tfidf_name_1 tfidf_name_10 tfidf_name_100\n[1,] -1.520238          2.066713    -4.793447     -2.403936      -1.802735\n     tfidf_name_11 tfidf_name_14 tfidf_name_15 tfidf_name_1936 tfidf_name_2.0\n[1,]      3.607565   -0.01058476    -0.4253752      -0.2957425      0.8642611\n     tfidf_name_3 tfidf_name_4 tfidf_name_451 tfidf_name_49 tfidf_name_5\n[1,]    -1.996153    -1.894541      -0.349303       1.02874    -1.376993\n     tfidf_name_500 tfidf_name_6 tfidf_name_6th tfidf_name_7 tfidf_name_a\n[1,]      0.3061039   -0.8640108        7.62841     2.724701    -2.022854\n     tfidf_name_about tfidf_name_acid tfidf_name_act tfidf_name_adult\n[1,]        0.8368777        1.897636      0.4972617        -1.490207\n     tfidf_name_adults tfidf_name_advanced tfidf_name_after\n[1,]       -0.09387077           0.3061039       -0.2481804\n     tfidf_name_afterlife tfidf_name_ages tfidf_name_agreements tfidf_name_air\n[1,]           -0.9194414      -0.5591052              -1.30218      0.5562863\n     tfidf_name_alchemist tfidf_name_all tfidf_name_alphabet tfidf_name_am\n[1,]             9.643137       1.518092          -0.5591052    0.05458998\n     tfidf_name_amazing tfidf_name_america's tfidf_name_american\n[1,]          -1.391954            0.0276968            5.288851\n     tfidf_name_americans tfidf_name_an tfidf_name_ancient tfidf_name_and\n[1,]           -0.2957425     0.4403215          0.0276968     -0.9599414\n     tfidf_name_angie's tfidf_name_animal tfidf_name_animals tfidf_name_are\n[1,]         -0.1783316        -0.2104393          -1.843032      0.4204923\n     tfidf_name_art tfidf_name_as tfidf_name_association tfidf_name_astounding\n[1,]     0.09081542    -0.3419302                7.62841            -0.4254951\n     tfidf_name_at tfidf_name_atomic tfidf_name_back tfidf_name_bad\n[1,]     0.0524675       -0.03391317       -3.710758    -0.03391317\n     tfidf_name_balance tfidf_name_ball tfidf_name_barefoot tfidf_name_be\n[1,]         -0.1783316     -0.01058476            1.437301     0.4204923\n     tfidf_name_bear tfidf_name_beasts tfidf_name_beautiful tfidf_name_become\n[1,]       -1.160322          1.833383           0.05458998         0.4204923\n     tfidf_name_becomes tfidf_name_beginners tfidf_name_believing\n[1,]          0.5562863            0.3061039            0.4204923\n     tfidf_name_berlin tfidf_name_better tfidf_name_between tfidf_name_big\n[1,]          1.278907          1.080442        -0.05331562     -0.5591052\n     tfidf_name_bill tfidf_name_birth tfidf_name_blood tfidf_name_boat\n[1,]       0.3222009         1.167256       -0.8230085      -0.2957425\n     tfidf_name_book tfidf_name_books tfidf_name_boxed tfidf_name_boy's\n[1,]       -9.466271        -3.582869         4.565776       -0.4254951\n     tfidf_name_boys tfidf_name_brave tfidf_name_brawl tfidf_name_break\n[1,]      -0.2957425       0.05458998       -0.8640108      -0.03391317\n     tfidf_name_breaking tfidf_name_breath tfidf_name_brigance tfidf_name_brown\n[1,]           -3.582468         0.5562863            1.301934        -1.160322\n     tfidf_name_build tfidf_name_bundo tfidf_name_burn tfidf_name_by\n\n...\nand 238 more lines.\n\n\nВзглянем на остатки. Для этого пригодится уже знакомая функция augment() из пакета broom.\n\nsvm_res &lt;- augment(svm_fit, new_data = books_test) |&gt; \n  mutate(res = price - .pred) |&gt; \n  select(price, .pred, res)\n\nsvm_res |&gt; \n  head(6) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nprice\n.pred\nres\n\n\n\n\n8\n8.429922\n-0.429922461\n\n\n5\n5.602691\n-0.602691055\n\n\n17\n16.178382\n0.821618312\n\n\n4\n5.445476\n-1.445476436\n\n\n6\n6.176515\n-0.176515259\n\n\n6\n6.001730\n-0.001730247\n\n\n\n\n\n\n\n\nlibrary(gridExtra)\n\ng1 &lt;- svm_res |&gt; \n  mutate(res = price - .pred) |&gt; \n  ggplot(aes(res)) +\n  geom_histogram(fill = \"steelblue\", color  = \"white\") +\n  theme_minimal()\n\ng2 &lt;- svm_res |&gt; \n  ggplot(aes(price, .pred)) +\n  geom_jitter(color = \"steelblue\", alpha = 0.7) +\n  geom_abline(linetype = 2, color = \"grey80\", linewidth = 2) +\n  theme_minimal()\n\ngrid.arrange(g1, g2, nrow = 1)\n\n\n\n\n\n\n\n\nСоберем метрики.\n\nbooks_metrics &lt;- metric_set(rmse, rsq, mae)\nbooks_metrics(svm_res, truth = price,  estimate = .pred)\n\n\n  \n\n\n\nТакже посмотрим, какие слова больше всего связаны с увеличением и с уменьшением цены.\n\nsvm_fit |&gt; \n  tidy() |&gt; \n  filter(term != \"year\") |&gt; \n  filter(!str_detect(term, \"genre\")) |&gt; \n  mutate(sign = case_when(estimate &gt; 0 ~ \"дороже\",\n                          .default = \"дешевле\"),\n         estimate = abs(estimate), \n         term = str_remove_all(term, \"tfidf_name_\")) |&gt; \n  group_by(sign) |&gt; \n  top_n(20, estimate) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x = estimate, y = fct_reorder(term, estimate),\n             fill = sign)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(expand = c(0,0)) +\n  facet_wrap(~sign, scales = \"free\") +\n  labs(y = NULL, \n       title = \"Связь слов с ценой книг\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nЛюбопытно: судя по нашему датасету, конституция США раздается на Амазоне бесплатно.\n\n\n\n\nHvitfeldt, Emil, и Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. Taylor; Francis.\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#градиентные-бустинговые-деревья",
    "href": "multivar.html#градиентные-бустинговые-деревья",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.15 Градиентные бустинговые деревья",
    "text": "23.15 Градиентные бустинговые деревья\nТакже попробуем построить регрессию с использованием градиентных бустинговых деревьев. В 2023 г. эта техника показала хорошие результаты в эксперименте по датировке греческих документальных папирусов.\n\nxgb_spec &lt;- \n  boost_tree(mtry = 50, trees = 1000)  |&gt; \n  set_engine(\"xgboost\")  |&gt; \n  set_mode(\"regression\")\n\n\nxgb_wflow &lt;- workflow() |&gt; \n  add_model(xgb_spec) |&gt; \n  add_recipe(books_rec)\n\nxgb_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 50\n  trees = 1000\n\nComputational engine: xgboost \n\n\nПроводим перекрестную проверку.\n\nxgb_rs &lt;- fit_resamples(\n  xgb_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\ncollect_metrics(xgb_rs)  |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.4865979\n10\n0.52910597\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4834618\n10\n0.07396304\nPreprocessor1_Model1\n\n\n\n\n\n\n\nМетрики неплохие! Но если взглянуть на остатки, можно увидеть что-то вроде буквы S.\n\nrf_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "plot.html#информативный-дизайн",
    "href": "plot.html#информативный-дизайн",
    "title": "3  Визуализации",
    "section": "3.10 Информативный дизайн",
    "text": "3.10 Информативный дизайн\nПоскольку нас интересует доля женщин, логично поменять группы местами.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  # меняем местами группы\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  coord_flip() +\n  # разные мелочи\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nТакже поменяем порядок, в котором идут декады (от меньшей к большей).\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  # меняем порядок лет\n  scale_x_reverse() +\n  coord_flip() +\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nУбавим цвет в мужской части диаграммы и добавим заголовки.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           # обводим столбики \n           color = \"darkred\", \n           # убираем легенду\n           show.legend = FALSE) +\n  scale_x_reverse() +\n  # беремся за палитру\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_minimal() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  # меняем цвет и шрифт текста\n  theme(\n    text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n    axis.text = element_text(color = \"darkred\")\n    )\n\n\n\n\n\n\n\n\nСтоит подвинуть заголовок и убрать просветы между столбцами.\n\n# почти ничего нового!\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           color = \"darkred\", \n           show.legend = FALSE,\n           # столбик во всю ширину\n           width = 10\n) +\n  # добавляем делений на оси\n  scale_x_reverse(breaks = seq(1800, 2000, 10)) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(\n    text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n    axis.text = element_text(color = \"darkred\"),\n    # выравниваем заголовок\n    plot.title.position = \"plot\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПостройте несколько графиков с использованием датасета starwars из пакета dplyr. Используйте тему “Звездных войн” из ThemePark.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "lda.html#подготовка-данных",
    "href": "lda.html#подготовка-данных",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.3 Подготовка данных",
    "text": "14.3 Подготовка данных\nЧтобы понять возможности алгоритма, мы попробуем передать ему тот же новостной архив (ссылка для скачивания). На новостях сразу видно адекватность модели; но это не значит, что применение LDA ограничено подобными сюжетами. Этот метод с успехом применяется, например, в историко-научных или литературоведческих исследованиях. Он хорошо подходит, если необходимо на основе журнального архива описать развитие некоторой области знания. Но сейчас нам подойдет пример попроще 👶\n\nlibrary(tidyverse)\nload(\"../data/news_tokens_pruned.Rdata\")\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПоскольку LDA – вероятностная модель, то на входе она принимает целые числа. В самом деле, не имеет смысла говорить о том, что некое распределение породило 0.5 слов или того меньше. Поэтому мы считаем абсолютную, а не относительную встречаемость – и не tf_idf.\n\nnews_counts &lt;- news_tokens_pruned |&gt; \n  count(token, id)\n\nnews_counts",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "llm.html",
    "href": "llm.html",
    "title": "29  Работа с LLM",
    "section": "",
    "text": "29.1 Что такое LLM?\nБольшие языковые модели — это алгоритмы искусственного интеллекта, обученные на огромных объемах текстовых данных. Они могут генерировать, анализировать и преобразовывать текст, что делает их ценным инструментом для цифровых гуманитарных наук.\nПрименение в филологии:\nНекоторые ограничения:\nВозможное решение:\nlibrary(ellmer)\nlibrary(ollamar)\nlibrary(xml2)\nlibrary(tidyverse)\nlibrary(diffobj)\nМы будем работать с пакетом {ellmer} https://ellmer.tidyverse.org/, разработанным для удобного взаимодействия с большими языковыми моделями (LLM) через различные API.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#пакет-ellmer",
    "href": "llm.html#пакет-ellmer",
    "title": "24  Работа с LLM",
    "section": "24.2 Пакет {ellmer}",
    "text": "24.2 Пакет {ellmer}\nПакет {ellmer} дает больше возможностей для работы с моделями как через API, так и локально. Повторим наш вопрос, но на этот раз добавим системный промпт.\nСистемный промпт - это специальная инструкция, которая передается языковой модели (LLM) для задания контекста и ограничений при генерации текста. Он включать в себя следующие элементы:\n\nОписание роли: четкое определение того, какую роль должен взять на себя LLM при генерации текста. Например, “Ты научный помощник, который помогает объяснять сложные концепции простым языком”.\nРуководящие принципы: набор правил или инструкций, которым должен следовать LLM, такие как “Всегда будь вежливым и профессиональным” или “Не генерируй контент, который может быть вредным или незаконным”.\nТематические ограничения: Определение тем, о которых LLM может/не может генерировать текст, например, “Отвечай только на вопросы, касающиеся истории и философии, избегай политических тем”.\nСтилистические указания: Рекомендации по использованию определенного стиля, тона, длины ответов и других характеристик генерируемого текста.\n\nСистемный промпт играет ключевую роль в настройке поведения и вывода LLM для конкретной задачи или контекста.\n\nlibrary(ellmer)\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = \"Reply in poetic form.\")\n. &lt;- chat$chat(\"Who is Socrates?\", echo = FALSE)\nchat\n\n&lt;Chat turns=3 tokens=13/308&gt;\n── system ──────────────────────────────────────────────────────────────────────\nReply in poetic form.\n── user ────────────────────────────────────────────────────────────────────────\nWho is Socrates?\n── assistant ───────────────────────────────────────────────────────────────────\n&lt;think&gt; Alright, so the user asked for a response to \"Who is Socrates?\" written\nin poetic form. I need to create a poem that reflects on Socrates but keeps it\nin a poetic way.\n\nFirst, I should think about what makes Socrates unique. He's known for his\ndialogues but also associated with Socratic methods. Maybe focus on the method\nor his approach to discovery.\n\nI want the poem to have a flowing tone and maybe some imagery related to light\nand wisdom. Maybe compare him to an ancient mind that is never old.\n\nAlso, I should touch on how he taught people how to learn from himself rather\nthan others. Emphasize the continuous journey or the search for truth through\nquestioning.\n\nLet me consider how to structure it: possibly a few stanzas with each line\nbuilding up the theme. Maybe start by introducing him, then describe his method\nof discovery, and end on wisdom or the search within himself.\n\nI need to avoid technical terms and keep it poetic without losing the essence.\nUse metaphors that relate to light, growth, and discovery.\n\nOkay, let's draft some lines. &lt;/think&gt;\n\nIn shadowed minds where truth unfolds Socrates stands at his highest place, A\nmind ancient, eternal, unchanging, Guided by the voice of wisdom.\n\nHe led with him a world of light, Where logic and reason met, His reign of\ndiscovery never fails.  From youth to old, he walked like an oar, In seeking\ntruth never ends.\n\n\nЗа расходом токенов полезно следить, если обращаетесь к модели по подписке. В нашем случае никаких ограничений нет.\n\ntoken_usage()",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#дизайн-промпта",
    "href": "llm.html#дизайн-промпта",
    "title": "24  Работа с LLM",
    "section": "24.3 Дизайн промпта",
    "text": "24.3 Дизайн промпта\nНебольшой вопрос можно сохранить в окружение.\n\nquestion &lt;- \"Who is Socrates?\"\n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\")\nchat$chat(question)\n\nНо в большинстве случаев вы будете создавать развесистые системные и пользовательские промпты, которые лучше сразу сохранять в markdown-документе.\n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = \"You are Cicero. \")\nchat$chat(question)\n\n&lt;think&gt;\nOkay, so I'm trying to learn about who Julius Caesar was. From what I remember \nin history class, he was a important figure during the Roman Empire, and his \nname seems to be tied to Caesar's Leadership Academy where educated people were\nencouraged. So, I think he was pretty influential.\n\nBut when I try to dive deeper, I feel like there's more to him. I guess it all \nstarted with him killing Caesar in a fight at the Battle of Actium. That sounds\nlike a big deal and maybe some kind of conspiracy theory now, but I don't want \nto get bogged down by that part.\n\nNow, on the political side, he was a great ruler who expanded his territory \ninto the Mediterranean, which includes areas like Libya, Sicily, and Tyrconia \nin Italy. He did this with some pretty brutal methods, like decimating the \npeople of his domain, right? I think he went into positions that were seen as \nimportant for power, but some people might have felt manipulated or overthrown \nthat way.\n\nThen there's Caesar's Leadership Academy, which is supposed to promote \neducation and give more middle-class people a voice. That sounds interesting \nbecause it contrasts with the rise of materialism in Rome during that time. \nWhen they implemented the ideas at the Academy, maybe some of them didn't like \nhow it affected higher classes.\n\nI'm also trying to remember his actions after the fall of Rome, like he went \nback to his family lands and became one of France's kings. There are images \nthat make him look really noble and generous, but I'm not sure if he was as \nkind as he seemed today or just a character design by someone for the story.\n\nI want to know more about the legacy of Julius Caesar. He was a key figure in \nRoman history, especially regarding the expansion of their empire into the \nMediterranean. His method of governance might have had bad consequences both \npersonally and for others. Now I think about how this relates to modern times \nor other historical contexts. Maybe someone studied him from a modern \nviewpoint, but I don't know much beyond that.\n\nI'm still curious about his personal beliefs. Was he really more of an \nidealist, or was there something else I'm not seeing? Did he have any issues \nwith religious or philosophical views at the time? It would be interesting to \nfind out because it could change how we view his actions and their impact on \nsociety.\n\nAnother thing is, when he went as a soldier in history, he might have had \nspecific motivations. Like fighting Caesar's forces and wanting to protect Rome\nfrom another king. That makes sense now that I think about it, but I didn't \nrealize there was such detail before.\n\nSo, putting this all together, Julius Caesar was a complex figure with both \nhistorical significance and personal details that are often exaggerated for \nstory purposes. His actions were part of his character, but they also had \nlarger implications for the political system and his influence on subsequent \nleaders in Rome.\n&lt;/think&gt;\n\nJulius Caesar (580 BC – 643 BC) was a pivotal figure in the history of the \nRoman Empire, marked by both historical significance and exaggerated personal \ndetails. His reign of control over Italy began when he led a group of soldiers \nwho fought off a powerful opponent at the Battle of Actium. This event is often\ncited as a source of conspiracy belief among later centuries.\n\nAs an emperor of Rome, Caesar expanded his territories into the Mediterranean, \nincluding areas such as Libya and Sicili, and gained widespread approval from \nthe educated class. This influence on Roman society and culture may have \ncontributed to his later role in the formation of Caesar's Leadership Academy, \nwhere the academy was intended to promote education and give middle-class \nindividuals more influence in political matters.\n\nAfter the fall of Rome, Caesius returned to Italy under Alexander, becoming one\nof France's kings. While images of him appear as noble figures due to design \nchoices, his personality and actions also reflect personal beliefs that were \noften misunderstood during his time. He is known for his generous approach and \nactive participation in both military and leadership activities.\n\nCaesar's contributions to Roman history are significant, particularly in the \ncontext of expanding Italy to include parts of the Mediterranean, which had \nmixed effects on political structure. His reign is remembered for his bravery \nand adaptability as an emperor but also for his controversial actions that may \nhave led to personal conflicts within the empire.\n\nIn summary, Julius Caesar was a complex figure whose history was both rich and \nnuanced. His reign in Rome demonstrated strategic dominance, yet his personal \nactions also revealed underlying complexities tied to beliefs and institutions \nof the time.\n\n\nВот резюме рекомендаций по написанию промпта для моделей на основе предоставленной информации:\n\nИспользуйте markdown для оформления промптов, чтобы сделать их более читабельными как для LLM, так и для людей.\nХраните каждый промпт в отдельном файле с информативным названием, например, “prompt-extract-metadata.md”.\nВедите контроль версий промптов с помощью Git для отслеживания изменений. 4. Используйте функцию ellmer::chat() для интеграции динамических данных в промпт.\nСоздавайте набор примеров для регулярной проверки промпта.\nПри генерации кода используйте как системный промпт (для общего поведения), так и пользовательский промпт (для конкретного вопроса).\nБудьте максимально конкретными в требованиях к выходному коду, используя примеры и подробные описания. 8. Если модель не использует новые языковые возможности, предоставляйте ей примеры, чтобы научить этому.\nПри извлечении структурированных данных используйте многошотовые промпты с примерами входных и выходных данных.\nВключайте в выходные данные исходные входные данные для облегчения сопоставления.\n\nОсновная идея - создавать подробные, структурированные промпты, которые точно направляют модель на желаемый результат, и использовать лучшие практики для управления и итерации промптов.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#установка-ollama",
    "href": "llm.html#установка-ollama",
    "title": "24  Работа с LLM",
    "section": "",
    "text": "resp &lt;- generate(\"deepseek-r1:1.5b\", \n                 system = \"Reply in one sentence.\",\n                 prompt = \"Who is Socrates?\") \n\nresp_process(resp, \"text\")  |&gt; \n  cat()",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#дизайн-промтпа",
    "href": "llm.html#дизайн-промтпа",
    "title": "25  Работа с LLM",
    "section": "25.2 Дизайн промтпа",
    "text": "25.2 Дизайн промтпа\nВ коде выше мы добавили системный промпт. Системный промпт - это специальная инструкция, которая передается языковой модели (LLM) для задания контекста и ограничений при генерации текста. Он включать в себя следующие элементы:\n\nОписание роли: четкое определение того, какую роль должен взять на себя LLM при генерации текста. Например, “Ты научный помощник, который помогает объяснять сложные концепции простым языком”.\nРуководящие принципы: набор правил или инструкций, которым должен следовать LLM, такие как “Всегда будь вежливым и профессиональным” или “Не генерируй контент, который может быть вредным или незаконным”.\nТематические ограничения: Определение тем, о которых LLM может/не может генерировать текст, например, “Отвечай только на вопросы, касающиеся истории и философии, избегай политических тем”.\nСтилистические указания: Рекомендации по использованию определенного стиля, тона, длины ответов и других характеристик генерируемого текста.\n\nСистемный промпт играет ключевую роль в настройке поведения и вывода LLM для конкретной задачи или контекста.\nИ системный, пользовательский промпт можно хранить в окружении, если они небольшие.\n\nquestion &lt;- \"Who is Socrates?\"\n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\")\nchat$chat(question)\n\nНо в большинстве случаев вы будете создавать развесистые промпты, которые лучше сразу сохранять в markdown-документе, причем под контролем версий, чтобы можно было вернуться к прежним версиям. Давайте таким файлам информативные названия, например prompt-extract-metadata.md. В этом случае код выглядит как-то так.\n\nquestion &lt;- readLines(\"user_prompt.Rmd\", warn = FALSE) \n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = readLines(\"system_prompt.Rmd\", warn = FALSE))\n\nchat$chat(question)\n\nПри написании промпта будьте максимально конкретными в требованиях, используя примеры и подробные описания. Используйте синтаксис markdown – LLM его понимают.\nДалее рассмотрим несколько задач, которые можно решать при помощи LLM.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#распознавание-изображений",
    "href": "llm.html#распознавание-изображений",
    "title": "29  Работа с LLM",
    "section": "29.5 Распознавание изображений",
    "text": "29.5 Распознавание изображений\nНекоторые модели принимают на входе изображения.\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Ты дружелюбный ассистент, который отвечает по-русски.\",\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"qwen/qwen2.5-vl-32b-instruct:free\"\n)\n\nchat$chat(\n  content_image_url(\"https://www.r-project.org/Rlogo.png\"),\n  \"Что на изображении?\"\n)\n\nПопробуйте использовать модель для распознавания. Запишите ответ в переменную, если планируете его дальше использовать.\n\n\nres &lt;- chat$chat(\n  content_image_file(\"processed.png\"),\n  \"Распознай текст на изображении. \"\n)\n\n# Текст на изображении:\n# \n# &gt; \n# &gt; Просим огласить следующий факт: Бывшавший в 1894 г. из Вятской губ. \n# Бауман (ветеринарный врач, служивший в Саратовском земстве в 95–96 гг.)\n# удачно скрылся от преследований шпионов, попав случайно в совершенно \n# незнакомую местность, село Хлебное, Задонского уезда, Воронежской \n# губернии. Измученный голодом и продолжительной дорогой пьяный, он \n# резонно решил обратиться за содействием к самому интеллигентному \n# представителю деревни, к земскому врачу Валериану Валерьевичу.\n# &gt; \n# &gt;",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#пересказ-текста",
    "href": "llm.html#пересказ-текста",
    "title": "25  Работа с LLM",
    "section": "25.7 Пересказ текста",
    "text": "25.7 Пересказ текста",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#добавление-разметки",
    "href": "llm.html#добавление-разметки",
    "title": "25  Работа с LLM",
    "section": "25.8 Добавление разметки",
    "text": "25.8 Добавление разметки",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#анализ-тональности",
    "href": "llm.html#анализ-тональности",
    "title": "25  Работа с LLM",
    "section": "25.6 Анализ тональности",
    "text": "25.6 Анализ тональности",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#инструменты",
    "href": "llm.html#инструменты",
    "title": "25  Работа с LLM",
    "section": "25.11 Инструменты",
    "text": "25.11 Инструменты",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#структурированные-данные",
    "href": "llm.html#структурированные-данные",
    "title": "25  Работа с LLM",
    "section": "25.6 Структурированные данные",
    "text": "25.6 Структурированные данные",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "24  Бинарная классификация",
    "section": "",
    "text": "24.1 Записки “Федералиста”\nВ предыдущих двух уроках мы познакомились с регрессией, а в этом поговорим о классификации. Алгоритмов классификации в МО великое множество, в этом уроке мы рассмотрим два из них: линейно-дискриминантный анализ и наивный Байес, а также научимся подбирать гиперпараметры модели.\nВ 1963 году два американских статистика, Фредерик Мостеллер и Дэвид Уоллес, опубликовали статью «Inference in an Authorship Problem», в которой они успешно разрешили вопрос о том, кто написал 12 спорных памфлетов из «Записок федералиста» — сборника статей в поддержку утверждения Конституции США (кон. XVIII в.).\nКандидатами в авторы 12 спорных памфлетов были Джеймс Мэдисон (четвертый президент США) и Александр Гамильтон (соратник Джорджа Вашингтона, основоположник американской экономической системы). Гамильтон и Мэдисон писали в схожей ораторской манере, и в некоторых отношениях были практически стилистическими «близнецами». Однако статистикам удалось найти способ их различить.\nВ распоряжении статистиков были методы традиционной фишеровской статистики (дискриминантный анализ, предложенный в 1936 г.), но они впервые решили дополнить его байесовскими методами, что можно считать рождением алгоритма, известного сегодня в МО под именем Наивный Байес. Кроме того, Мостеллер и Уоллес впервые показали, что для решения вопроса об авторстве важны наиболее частотные слова, употребление которых человек почти не контролирует. Впоследствии это наблюдение легло в основу метода, предложенного Берроузом.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "llm.html#эмбеддинги",
    "href": "llm.html#эмбеддинги",
    "title": "25  Работа с LLM",
    "section": "25.9 Эмбеддинги",
    "text": "25.9 Эмбеддинги",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#классификация",
    "href": "llm.html#классификация",
    "title": "25  Работа с LLM",
    "section": "25.10 Классификация",
    "text": "25.10 Классификация",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#системный-промпт",
    "href": "llm.html#системный-промпт",
    "title": "25  Работа с LLM",
    "section": "25.2 Системный промпт",
    "text": "25.2 Системный промпт\nСистемный промпт - это специальная инструкция, которая передается языковой модели (LLM) для задания контекста и ограничений при генерации текста. Он включать в себя следующие элементы:\n\nОписание роли: четкое определение того, какую роль должен взять на себя LLM при генерации текста. Например, “Ты научный помощник, который помогает объяснять сложные концепции простым языком”.\nРуководящие принципы: набор правил или инструкций, которым должен следовать LLM, такие как “Всегда будь вежливым и профессиональным” или “Не генерируй контент, который может быть вредным или незаконным”.\nТематические ограничения: Определение тем, о которых LLM может/не может генерировать текст, например, “Отвечай только на вопросы, касающиеся истории и философии, избегай политических тем”.\nСтилистические указания: Рекомендации по использованию определенного стиля, тона, длины ответов и других характеристик генерируемого текста.\n\nСистемный промпт играет ключевую роль в настройке поведения и вывода LLM для конкретной задачи или контекста.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#пользовательский-промпт-стратегии",
    "href": "llm.html#пользовательский-промпт-стратегии",
    "title": "25  Работа с LLM",
    "section": "25.3 Пользовательский промпт: стратегии",
    "text": "25.3 Пользовательский промпт: стратегии\nСтратегии промптирования представляют собой различные подходы к формулировке запроса к модели, которые помогают получить более точные, релевантные и обоснованные ответы. Рассмотрим основные из них:\n\nСтратегия прямых запросов (Zero-shot prompting). При таком подходе пользователь формулирует запрос без дополнительных примеров, полагаясь на то, что модель уже обладает достаточными знаниями для ответа. Это минималистичный вариант, когда инструкция передается в виде одного сообщения без демонстрации образцов ответов.\nOne-shot prompting представляет собой метод, при котором вместе с запросом пользователю предоставляется ровно один пример, показывающий, каким должен быть желаемый ответ. Этот подход занимает промежуточное положение между zero-shot prompting (когда примеров нет вовсе) и few-shot prompting (когда примеров несколько). Использование одного примера помогает модели лучше понять формат и стиль нужного ответа, минимизируя объем вводной информации, но при этом обеспечивая достаточную направленность для получения релевантного результата. Такой метод полезен, когда в задаче достаточно однозначного примера, демонстрирующего специфику ответа, без необходимости загромождать запрос большим количеством примеров.\nМетод с примерами (Few-shot prompting). В этом случае вместе с основным запросом в промпт включается несколько примеров, то есть пара «вопрос-ответ», показывающих требуемый формат или стиль ответа. Такой подход помогает модели лучше понять, какую информацию и в каком виде она должна предоставить, особенно если задача сложная или специфическая.\nМногошаговое рассуждение (Chain-of-thought prompting). Здесь пользователь побуждает модель не просто выдавать ответ, а проходить через промежуточные этапы рассуждения. Инструкция может требовать развернутого описания логических шагов, что позволяет получить более обоснованный и прозрачный результат. Такой метод особенно полезен при решении сложных задач, требующих рассуждений или математических выкладок.\nРолевое (или контекстное) промптирование. При этом подходе пользователь задаёт модели конкретную роль или контекст, в котором она должна действовать, например, «представь, что ты эксперт в экономике» или «ответь так, как будто ты историк». Задание роли помогает сместить акценты и получить ответы, адаптированные к определённой области знаний или стиля общения.\nИтеративное уточнение запроса. Иногда первоначальный запрос может быть недостаточно точным или содержательным. В таких случаях используется метод пошагового уточнения, когда после получения первого ответа пользователь задаёт дополнительные вопросы или корректирует исходный запрос, добиваясь уточнения, расширения или сужения информации.\nИспользование самокритического подхода. Некоторые стратегии подразумевают, что модель не только генерирует ответ, но и сама анализирует его корректность, выявляет возможные ошибки и при необходимости пересматривает свой вывод. Это может включать подсказки типа «подумай ещё раз» или дополнительные указания для оценки достоверности результата.\n\nКаждая из этих стратегий имеет свои преимущества в зависимости от задачи, требуемой точности и специфики данных. Выбор подхода может существенно повлиять на качество итогового ответа, поэтому часто оптимизируют промпт, комбинируя несколько методов: от указания роли до добавления примеров и поэтапного рассуждения.\n\n25.3.1 Zero-shot\n\nlibrary(tibble)\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Answer with just the correct category.\",\n  \"user\",   \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\"\n)\nquery(q)\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Negative\n\n\n\n25.3.2 One-shot\nСтруктура включает системную подсказку, за которой следует запрос пользователя с примером текста и вопросом классификации, пример классификации от ассистента и затем ещё один запрос пользователя с новым текстом для классификации.\n\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Answer with just the correct category.\",\n  \"user\", \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Negative\",\n  \"user\", \"text: What an excellent father you have, girls!”\\ncategories: positive, neutral, negative\"\n)\nquery(q)\n# \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Category: Positive.\n\nПопросим вернуть результат в формате JSON.\n\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Provide the following information: category, confidence, and the word that is most important for your coding decision.\",\n  \"user\", \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"{'Category':'Negative','Confidence':'100%','Important':'compassion'}\",\n  \"user\", \"text: What an excellent father you have, girls!\\ncategories: positive, neutral, negative\"\n)\nanswer &lt;- query(q)\n# \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# {'Category':'Positive', 'Confidence':80,'Important':'father'}\n\nИспользуйте pluck(answer, \"message\", \"content\"), чтобы извлечь ответ.\n\n\n25.3.3 Few-shot\nДобавим другие примеры.\n\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Provide the following information: category, confidence, and the word that is most important for your coding decision.\",\n  \"user\", \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Negative\",\n  \"user\", \"text: What an excellent father you have, girls!\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Positive\",\n  \"user\", \"text: The rest of the evening was spent in conjecturing how soon he would return Mr. Bennet’s visit\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Neutral\",\n  \"user\", \"text: An invitation to dinner was soon afterwards dispatched\\ncategories: positive, neutral, negative\"\n)\nanswer &lt;- query(q)\n# \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Category: Positive\n\nLLM иногда что-то от себя додумывают и находят хорошее и плохое там, где его нет.\n\n\n25.3.4 Chain-of-Thought\nПопросим модель немного подумать.\n\nq_thought &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. \",\n  \"user\",  \"text: An invitation to dinner was soon afterwards dispatched\\n What sentiment (positive, neutral, negative) would you assign? Provide some thoughts.\"\n)\noutput_thought &lt;- query(q_thought, output = \"text\")\n\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# I would assign a positive sentiment to the text.\n# \n# The word \"soon\" implies a sense of haste or urgency, which suggests\n# that the invitation is being made in response to an event or situation\n# where time was of the essence. The fact that it's soon afterwards\n# dispatched implies that the recipient has been waiting for some time\n# and is eager to accept the invitation.\n# \n# Additionally, the use of the word \"was\" suggests a sense of certainty\n# and clarity about the timing of the invitation, which adds to its\n# positive connotation.\n# \n# Overall, the text has a friendly and inviting tone, suggesting that the\n# host is enthusiastic about sharing their dinner plans with the\n# recipient.\n\nТеперь создадим дополнительный шаг в рассуждении.\n\nq_thought &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. \",\n  \"user\",  \"text: An invitation to dinner was soon afterwards dispatched\\n What sentiment (positive, neutral, negative) would you assign? Provide some thoughts.\",\n  \"assistant\", output_thought,\n  \"user\",   \"Now answer with just the correct category (positive, neutral, or negative)\"\n)\nresps &lt;- query(q)\n\n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Category: Positive\n\nПожалуй, сестры Беннет могли бы согласиться, что в приглашении на ужин есть что-то хорошее.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#функция-make_query",
    "href": "llm.html#функция-make_query",
    "title": "25  Работа с LLM",
    "section": "25.4 Функция make_query()",
    "text": "25.4 Функция make_query()\nФункция make_query() предназначена для упрощения создания структурированного запроса для классификации текста, чтобы вам не приходилось самостоятельно создавать tibble и запоминать специфическую структуру.\nКомпоненты:\n\ntext: новый текст для аннотирования.\nprompt: вопрос, содержащий категории для аннотирования.\ntemplate: определяет структуру сообщений пользователя. Шаблон может включать заполнители, например, {text}, {prefix} и для динамического форматирования входных данных.\nsystem: системный промпт (необязательно).\nprefix: cтрока, добавляемая в начало запросов пользователя (необязательно).\nsuffix: cтрока, добавляемая в конец запросов пользователя (необязательно).\nexamples: Предыдущие примеры, состоящие из сообщений пользователя и ответов ассистента (для обучения с одним или несколькими примерами) (необязательно).\n\nИспользование без подсказок.\n\n# Call the make_query function\nq_zs &lt;- make_query(\n  template = \"{text}\\n{prompt}\",\n  text = \"You have no compassion for my poor nerves.\",\n  prompt = \"Categories: positive, neutral, negative\",\n  system = \"You assign texts into categories. Answer with just the correct category.\",\n)\n\n# Print the query\nprint(q_zs)\n\nquery(q_zs)\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Negative\n\nДобавляем один пример.\n\nexamples_os &lt;- tibble::tribble(\n  ~text, ~answer,\n  \"You have no compassion for my poor nerves\", \"negative\"\n)\n\nq_os &lt;- make_query(\n  text = \"She is the most beautiful creature I ever beheld!\",\n  template = \"{text}\\n{prompt}\",\n  prompt = \"Categories: positive, neutral, negative\",\n  system = \"You assign texts into categories. Answer with just the correct category.\",\n  example = examples_os,\n)\n\nquery(q_os)\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ────────────────────────────────\n# positive\n\nАналогично можно добавить другие примеры.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#запускаем-в-производство",
    "href": "llm.html#запускаем-в-производство",
    "title": "25  Работа с LLM",
    "section": "25.5 Запускаем в производство",
    "text": "25.5 Запускаем в производство\nНа практике вы, вероятно, едва ли будете аннотировать только один текст, разве что для тестирования. Обычно необходимо разметить коллекцию текстов, поэтому разберемся, как это делается. Для начала создаем таблицу с данными для анализа.\n\nmovie_reviews &lt;- tibble::tibble(\n  review_id = 1:5,\n  review = c(\"A stunning visual spectacle with a gripping storyline.\",\n             \"The plot was predictable, but the acting was superb.\",\n             \"An overrated film with underwhelming performances.\",\n             \"A beautiful tale of love and adventure, beautifully shot.\",\n             \"The movie lacked depth, but the special effects were incredible.\")\n)\n\nmovie_reviews\n\nТеперь сформируем запрос.\n\nqueries &lt;- make_query(\n  text = movie_reviews$review,\n  prompt = \"Categories: positive, neutral, negative\",\n  template = \"{prefix}{text}\\n{prompt}\",\n  system = \"Classify the sentiment of the movie review. Answer with just the correct category.\",\n  prefix = \"Text to classify: \"\n)\n\nЭто создает список таблиц с запросами в одном формате. Все они содержат один и тот же prompt, системное сообщение и prefix, но каждый включает разный текст, взятый из ранее созданной таблицы. Функция query принимает списки запросов, поэтому мы можем получить аннотации, просто используя:\n\n# Process and annotate the movie reviews\nmovie_reviews$annotation &lt;- query(queries, screen = FALSE, output = \"text\")\n\nmovie_reviews\n# A tibble: 5 × 3\n#   review_id review                                                           annotation\n#       &lt;int&gt; &lt;chr&gt;                                                            &lt;chr&gt;     \n# 1         1 A stunning visual spectacle with a gripping storyline.           Positive  \n# 2         2 The plot was predictable, but the acting was superb.             Positive  \n# 3         3 An overrated film with underwhelming performances.               Negative  \n# 4         4 A beautiful tale of love and adventure, beautifully shot.        Positive  \n# 5         5 The movie lacked depth, but the special effects were incredible. Positive  \n\nЭто занимает немного больше времени, чем классическое контролируемое машинное обучение или даже классификация с использованием трансформеров. Однако преимущество в том, что инструкции можно давать на простом английском языке, моделям для достижения удивительно хороших результатов требуется очень мало примеров, а лучшие модели, такие как llama3.2, часто справляются с более сложными категориями, чем другие методы.\nНо в большинстве случаев вы будете создавать развесистые промпты, которые лучше сразу сохранять в markdown-документе, причем под контролем версий, чтобы можно было вернуться к прежним версиям. Давайте таким файлам информативные названия, например prompt-extract-metadata.md. В этом случае код выглядит как-то так.\n\nquestion &lt;- readLines(\"user_prompt.Rmd\", warn = FALSE) \n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = readLines(\"system_prompt.Rmd\", warn = FALSE))\n\nchat$chat(question)\n\nПри написании промпта будьте максимально конкретными в требованиях, используя примеры и подробные описания. Используйте синтаксис markdown – LLM его понимают.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "classification.html#записки-федералиста",
    "href": "classification.html#записки-федералиста",
    "title": "24  Бинарная классификация",
    "section": "",
    "text": "Александр Гамильтон\n\n\n\n\n\n\n\nДжеймс Мэдисон",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "classification.html#наивный-байес",
    "href": "classification.html#наивный-байес",
    "title": "24  Бинарная классификация",
    "section": "24.4 Наивный Байес",
    "text": "24.4 Наивный Байес\nЕще один алгоритм, который часто используется в задачах классификации, называется “наивный Байес”.\n\n24.4.1 Теорема Байеса\nТеорема Байеса позволяет оценить вероятность одного события на основе вероятности другого собыитя. Математически теорема Байеса выглядит так:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nЗдесь:\n\n\\(P(A|B)\\) - вероятность события A при условии, что произошло событие B (апостериорная вероятность); она рассчитывается с учетом того, как часто события А и В происходят вместе и того, как часто вообще происходит B. Например: какова вероятность того, что письмо, содержащее слово “наследство”, является спамом?\n\\(P(B|A)\\) - вероятность события B при условии, что произошло событие A (правдоподобие). Например: какова вероятность встретить слово “наследство” в спаме?\n\\(P(A)\\) - вероятность события A (априорная вероятность). Например: какова вероятность получить спам?\n\\(P(B)\\) - вероятность события B (маргинальное правдоподобие). Например: как часто вообще встречается слово “наследство”?\n\nТеорема Байеса широко применяется в задачах классификации в машинном обучении. Например, в наивном байесовском классификаторе, который использует теорему Байеса для вычисления вероятности принадлежности объекта к тому или иному классу.\nЧтобы лучше понять теорему, решите несколько задач.\n\n\n\n\n\n\nЗадание\n\n\n\nВ кофейне подают два вида кофе: латте и капучино. 70% клиентов выбирают латте, а 30% — капучино. Известно, что 20% клиентов, выбравших латте, добавляют сахар, а среди выбравших капучино сахар добавляют 40%. Клиент добавил сахар в кофе. Какова вероятность, что он выбрал латте? Ответ запишите в процентах c округлением до двух сотых.\n\n\nДано:\n\n\\(P(L)=0.7\\) (вероятность выбора латте).\n\\(P(C)=0.3\\) (вероятность выбора капучино).\n\\(P(S|L)=0.2\\) (вероятность добавления сахара для латте).\n\\(P(S∣C)=0.4P\\) (вероятность добавления сахара для капучино).\n\nНеобходимо найти \\(P(L∣S)\\) — вероятность того, что клиент выбрал латте при условии, что он добавил сахар. Полная вероятность добавления сахара считается так: \\(P(S) = P(S∣L)⋅P(L)+ P(S∣C)⋅P(C)\\).\nОтвет: \n\n\n\n\n\n\nЗадание\n\n\n\nВ приюте для животных есть коты двух пород: мейн-куны и британские короткошерстные. 70% котов — мейн-куны, а 30% — британские короткошерстные. Известно, что 15% мейн-кунов имеют зеленые глаза, а у британских короткошерстных котов зеленые глаза встречаются у 40%. Посетитель приюта случайным образом выбирает кота с зелеными глазами. Какова вероятность, что это мейн-кун? Ответ запишите в процентах c округлением до двух сотых.\n\n\nДано:\n\\(P(M)=0.7\\) (вероятность выбрать мейн-куна).\n\\(P(B)=0.3\\) (вероятность выбрать британского короткошерстного).\n\\(P(G∣M)=0.15\\) (вероятность зеленых глаз для мейн-куна).\n\\(P(G∣B)=0.4\\) (вероятность зеленых глаз для британского короткошерстного).\nНеобходимо найти: \\(P(M∣G)\\) — вероятность того, что кот — мейн-кун при условии, что у него зеленые глаза.\nОтвет: .\n\n\n\n\n\n\nЗадание\n\n\n\nВ библиотеке есть книги двух жанров: детективы и фантастика. 60% книг — детективы, а 40% — фантастика. Известно, что 10% детективов имеют красную обложку, а у фантастики красная обложка у 25% книг. Читатель случайным образом выбирает книгу с красной обложкой. Какова вероятность, что это детектив? Ответ запишите в процентах c округлением до двух сотых.\n\n\nОтвет: .\nМожете придумать свою задачу?\n\n\n24.4.2 Применение теоремы в МО\nНаивный байесовский классификатор называется “наивным” из-за ключевого допущения, которое он делает в своей работе: предположение о независимости признаков.\nНаивный Байес предполагает, что признаки (предикторы) объекта, который нужно классифицировать, являются статистически независимыми друг от друга, то есть значение одного признака не зависит от значений других признаков.\nЭто “наивное” предположение значительно упрощает вычисления, необходимые для применения теоремы Байеса. Вместо того, чтобы вычислять сложную совместную вероятность всех признаков, наивный Байес разбивает это на произведение вероятностей отдельных признаков.\n\nЗнаменатель будет для всех групп одинаков, поэтому:\n\nХотя это предположение редко выполняется в реальных данных, наивный Байес часто демонстрирует неожиданно хорошую производительность. Таким образом, “наивность” этого классификатора относится именно к этому упрощающему предположению.\nЧто если в обучающем корпусе слово в каком-то классе не встречается? Чтобы все вероятности не обнулились, применяют критерий Лапласа, то есть добавляют ко всем значениям в таблице небольшое число.\n\n\n24.4.3 Препроцессор и модель\n\n# предсказываем автора по всем переменным\nnb_rec &lt;- recipe(author ~ ., data = data_train) \n\n# выбираем модель\nnb_spec &lt;- naive_Bayes(Laplace = tune(),\n                       smoothness = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"naivebayes\")\n\nnb_spec\n\nNaive Bayes Model Specification (classification)\n\nMain Arguments:\n  smoothness = tune()\n  Laplace = tune()\n\nComputational engine: naivebayes \n\n\nСогласно документации, меньшие значения smoothness приводят к более гибким, адаптивным границам между классами. Другими словами, smoothness - это параметр, с помощью которого можно контролировать гибкость границ классификации, определяемых наивным байесовским классификатором. Низкие значения smoothness позволяют модели более точно подстраиваться под обучающие данные, но могут также приводить к переобучению. Высокие значения сглаживают границы и делают модель более устойчивой, но менее точной на обучающих данных.\n\n\n24.4.4 Выбор гиперпараметров\n\nnb_param &lt;- extract_parameter_set_dials(nb_spec)\nnb_param\n\n\n  \n\n\n\nТеперь добавим модель и препроцессор в воркфлоу.\n\n# workflow \nnb_wflow &lt;- workflow() |&gt; \n  add_model(nb_spec) |&gt; \n  add_recipe(nb_rec)\n\nnb_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nMain Arguments:\n  smoothness = tune()\n  Laplace = tune()\n\nComputational engine: naivebayes \n\n\n\nnb_tune &lt;- nb_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = nb_param |&gt; grid_regular(levels = 5),\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_resamples(save_pred = TRUE)\n  )\n\nnb_tune \n\n\n  \n\n\n\n\ncollect_metrics(nb_tune)\n\n\n  \n\n\n\n\nautoplot(nb_tune)\n\n\n\n\n\n\n\n\n\nshow_best(nb_tune, n = 1)\n\nWarning in show_best(nb_tune, n = 1): No value of `metric` was given;\n\"accuracy\" will be used.\n\n\n\n  \n\n\n\n\nnb_best &lt;- select_best(nb_tune, metric = \"accuracy\")\nnb_best\n\n\n  \n\n\n\n\n\n24.4.5 Матрица смешения\nНапомним, что в обучающих данных всего 52 текста, из них 40 принадлежит Гамильтону, а 12 – Мэдисону.\n\nconf_mat_resampled(nb_tune, tidy = FALSE, parameters = nb_best) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"#eaeff6\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\")) +\n  ggtitle(\"NB, 70 признаков\")\n\n\n\n\n\n\n\n\n\n\n24.4.6 ROC-кривая\n\nnb_predictions &lt;- nb_tune |&gt; \n  collect_predictions(parameters = nb_best) |&gt; \n  mutate_if(is.numeric, round, 3)\n\nnb_predictions\n\n\n  \n\n\n\n\nnb_predictions |&gt; \n  roc_curve(author, .pred_Hamilton) |&gt; \n  autoplot() \n\n\n\n\n\n\n\n\nРезультат чуть хуже, чем дает LDA, так что применять этот алгоритм к спорным текстам мы не будем.\nДля сравнения различных методов бывает полезно вывести на одном графике несколько моделей.\n\nlda_predictions |&gt; \n  roc_curve(author, .pred_Hamilton) |&gt; \n  mutate(model = \"LDA\") |&gt; \n  bind_rows(nb_predictions |&gt; \n              roc_curve(author, .pred_Hamilton) |&gt; \n              mutate(model = \"NB\")) |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "classification.html#линейно-дискриминантный-анализ",
    "href": "classification.html#линейно-дискриминантный-анализ",
    "title": "24  Бинарная классификация",
    "section": "24.3 Линейно-дискриминантный анализ",
    "text": "24.3 Линейно-дискриминантный анализ\nВ дискриминантном анализе (например, линейном — LDA) мы хотим:\n\nперевести объекты из пространства признаков в новое пространство,\nв котором группы (классы) лучше всего разделены.\n\nЭто достигается путём создания новых переменных — дискриминантных функций. Каждая из них — это линейная комбинация исходных признаков, то есть новая «ось» или направление в признаковом пространстве.\nНапример, если у нас есть 3 признака (var₁, var₂, var₃), мы можем создать новую ось:\n  DF₁ = –0.5 × var₁ + 1.2 × var₂ + 0.85 × var₃\nЭто и есть «новая ось», вдоль которой мы будем оценивать, хорошо ли разделяются классы. Если всё правильно, точки из разных групп на этой оси будут располагаться как можно дальше друг от друга.\nТеперь — как мы эту ось находим. Идея LDA: найти такую прямую (новое направление / ось), на которой проекции точек из двух групп лежат как можно дальше друг от друга, но внутри группы — как можно плотнее. Это называется максимизация межклассовой дисперсии и минимизация внутриклассовой.\nС математической точки зрения, мы ищем вектор w (то есть направление новой оси), который максимизирует критерий:\n  J(w) = (межклассовая дисперсия) / (внутриклассовая дисперсия)\nВ простом случае двух классов:\n  J(w) = \\(\\frac{(\\bar{x_1}-\\bar{x_2})^2}{s^2_1+s^2_2}\\)\nгде:\n\nμ₁ и μ₂ — средние проекции объектов классов на вектор w,\nσ₁² и σ₂² — дисперсии проекций в каждом классе.\n\nНаша задача — найти такой вектор w, который максимально разделяет средние значения разных классов и минимально разносит точки одного класса.\n\n\n\nИсточник.\n\n\nЕсли признаков не два, а, скажем, 100 (как бывает в задаче с текстами, генами и т. п.), то алгоритм строит до K–1 дискриминантных функций (ось), где K — число классов. Например:\n\nДля 2 классов → 1 ось (DF₁),\nДля 3 классов → 2 оси (DF₁ и DF₂),\nИ т. д.\n\nВ итоге мы можем визуализировать данные в новом пространстве.\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВажно: чем это отличается от PCA (главных компонент)?\n\nPCA — выбирает оси с максимальной общей дисперсией, но не учитывает классы.\nLDA — выбирает оси, которые максимально разделяют известные классы (использует метки классов).\n\nТаким образом, LDA работает как «обученный» метод (supervised), в отличие от PCA.\n\n\n\n24.3.1 Препроцессор и модель\nМы будем использовать регуляризованный LDA. Он применяется в тех случаях, когда число признаков (features) в данных превышает число наблюдений, а также когда в наборе данных присутствует сильная мультиколлинеарность между признаками.\n\n# предсказываем автора по всем переменным\nlda_rec &lt;- recipe(author ~ ., data = data_train) \n\n# выбираем модель\nlda_spec &lt;- discrim_linear(regularization_method = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"sparsediscrim\")\n\nlda_spec\n\nLinear Discriminant Model Specification (classification)\n\nMain Arguments:\n  regularization_method = tune()\n\nComputational engine: sparsediscrim \n\n\n\n\n24.3.2 Выбор гиперпараметров\nМетод регуляризации мы подберем при помощи настройки.\n\nlda_param &lt;- extract_parameter_set_dials(lda_spec)\nlda_param\n\n\n  \n\n\n\nСоздадим сетку гиперпараметров.\n\nlda_grid &lt;- lda_param |&gt; \n  grid_regular()\n\nlda_grid\n\n\n  \n\n\n\nТеперь добавим модель и препроцессор в воркфлоу.\n\n# workflow \nlda_wflow &lt;- workflow() |&gt; \n  add_model(lda_spec) |&gt; \n  add_recipe(lda_rec)\n\nlda_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Discriminant Model Specification (classification)\n\nMain Arguments:\n  regularization_method = tune()\n\nComputational engine: sparsediscrim \n\n\n\nlda_tune &lt;- lda_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = lda_grid,\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_resamples(save_pred = TRUE)\n  )\n\nlda_tune \n\n\n  \n\n\n\n\n\n24.3.3 Оценка модели: F-score\nВ этом примере мы использовали два критерия оценки: точность (т.е доля верных ответов) и F-score, также известный как F1-score или гармоническое среднее. Это комплексная метрика, которая объединяет в себе два других важных показателя эффективности модели: точность (precision) и полноту (recall).\nPrecision (точность) - это доля правильно классифицированных положительных примеров среди всех примеров, предсказанных как положительные. Recall (полнота) - это доля правильно классифицированных положительных примеров среди всех фактически положительных примеров.\nФормула для расчета F-score:\n\\[F\\text{-}score = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\\]\nГде:\n\n\\(precision = \\frac{TP}{TP + FP}\\)\n\\(recall = \\frac{TP}{TP + FN}\\)\n\nF-score находится в диапазоне от 0 до 1, и чем ближе значение к 1, тем лучше работает модель. При оценке качества поисковой системы F-score может быть более информативным, чем только точность или только полнота, поскольку учитывает оба этих аспекта.\n\ntune::collect_metrics(lda_tune)\n\n\n  \n\n\n\n\nautoplot(lda_tune) \n\n\n\n\n\n\n\n\n\nlda_best &lt;- tune::select_best(lda_tune, metric = \"accuracy\")\nlda_best\n\n\n  \n\n\n\nТаким образом, оптимальным методом является диагональная регуляризация. Цель диагональной регуляризации - избежать вырожденности ковариационной матрицы за счет добавления к ее диагональным элементам некоторой константы λ. Добавление константы λ к диагональным элементам ковариационной матрицы позволяет сделать ее невырожденной и обратимой, а это позволяет провести вычисление обратной матрицы, необходимое для LDA.\n\n\n24.3.4 ROC-кривая\nROC-кривая (англ. receiver operating characteristic, рабочая характеристика приёмника) — это график, который показывает как меняются следующие характеристики бинарного классификатора при варьировании порога отсечения.\n\nОсь Y (TPR) показывает долю правильно классифицированных положительных примеров (чувствительность, true positive rate). Изменяется от 0 до 1.\nПо оси X откладывается доля отрицательных объектов, ошибочно классифицированных как положительные (false positive rate, FPR); это значение равно 1 − специфичность. Изменяется от 0 до 1.\nДиагональная линия (y=x) - представляет случайную классификацию, когда вероятность положительного класса равна вероятности отрицательного класса.\nПлощадь под ROC-кривой (Area Under Curve, AUC) - показывает качество классификатора. Чем больше AUC (максимальное значение 1), тем лучше работает модель.\n\nОсновные интерпретации ROC-кривой:\n\nЕсли кривая расположена выше диагональной линии, это говорит о том, что модель работает лучше случайной классификации.\nЕсли кривая совпадает с диагональной линией, то модель не способна отличить положительные и отрицательные классы.\nЕсли кривая расположена ниже диагональной линии, это свидетельствует о том, что модель работает хуже случайной классификации.\n\n\nlda_predictions &lt;- lda_tune |&gt; \n  collect_predictions(parameters = lda_best) |&gt; \n  mutate_if(is.numeric, round, 3)\n\nlda_predictions\n\n\n  \n\n\n\n\nlda_predictions |&gt; \n  roc_curve(author, .pred_Hamilton) |&gt; \n  # или, для другого класса:\n  #roc_curve(author, .pred_Madison, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n24.3.5 Матрица смешения\nВзглянем на матрицу смешения. В обучающих данных всего 52 текста, из них 40 принадлежит Гамильтону, а 12 – Мэдисону.\n\nlda_param &lt;- tibble(regularization_method = \"diagonal\")\n\nconf_mat_resampled(lda_tune, tidy = FALSE, parameters = lda_param) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"#eaeff6\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\")) +\n  ggtitle(\"LDA, 70 признаков\")\n\n\n\n\n\n\n\n\n\n\n24.3.6 Окончательная настройка модели\nПрежде всего установим нужный метод регуляризации.\n\nfinal_lda_wflow &lt;- \n  lda_wflow |&gt; \n  finalize_workflow(lda_param)\n\nfinal_lda_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Discriminant Model Specification (classification)\n\nMain Arguments:\n  regularization_method = diagonal\n\nComputational engine: sparsediscrim \n\n\nИ подгоним модель.\n\nlda_fit &lt;- final_lda_wflow  |&gt; \n  fit(data_train)\n\n\n\n24.3.7 Тестовая выборка\nУ нас остались неиспользованными 14 наблюдений в тестовой выборке.\n\npred_test &lt;- predict(lda_fit, data_test, type = \"class\")\n\nЗдесь тоже 100%-я точность.\n\ntest_acc &lt;- tibble(predicted = pred_test$.pred_class, \n       expected = data_test$author, \n       value = predicted == expected)\n\nsum(test_acc$value) / nrow(test_acc)\n\n[1] 1\n\n\n\n\n24.3.8 Классификация спорных эссе\nВсе указывает на то, что в большинстве случаев Мэдисон – наиболее вероятный автор. Что касается 55-го эссе, то на его счет сомневались и Мостеллер с Уоллесом.\n\npredict(lda_fit, dispt, type = \"class\") |&gt; \n  mutate(essay = dispt$filename)",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "classification.html#подготовка-данных",
    "href": "classification.html#подготовка-данных",
    "title": "24  Бинарная классификация",
    "section": "24.2 Подготовка данных",
    "text": "24.2 Подготовка данных\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nconflicted::conflict_prefer(\"filter\", winner = \"dplyr\")\nconflicted::conflict_prefer(\"select\", winner = \"MASS\")\nlibrary(discrim)\n\nПо ссылке скачайте датасет с частотностью слов в записках ( источник). Из него мы удалим три текста предположительного двойного авторства и пять эссе Джона Джея.\n\nfed &lt;- read_csv(\"../files/fedPapers85.csv\") |&gt; \n  filter(!author %in%  c(\"HM\", \"Jay\")) \n\n# небольшой ремонт\ncolnames(fed) &lt;- make.names(colnames(fed))\n\nОтложим спорные эссе.\n\ndispt &lt;- fed |&gt; \n  filter(author == \"dispt\") \n\nessays &lt;- fed |&gt; \n  filter(author != \"dispt\") |&gt; \n  mutate(author = as.factor(author)) |&gt; \n  dplyr::select(-filename)\n\nРазобьем оставшиеся наблюдения на обучающую и проверочную выборки.\n\nset.seed(03022025)\ndata_split &lt;- essays |&gt; \n  initial_split(0.8, strata = author)\n\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\nРазобьем обучающие данные группы для перекрестной проверки.\n\nfolds &lt;- vfold_cv(data_train, strata = author, v = 10)\nfolds",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html",
    "href": "multiclass.html",
    "title": "25  Многоклассовая классификация",
    "section": "",
    "text": "25.1 Подготовка данных\nМногоклассовая классификация может использоваться для определения автора, жанра, тематики или эмоциональной тональности текста. В этом уроке мы научимся классифицировать тексты по автору, воспользовавшись учебным датасетом русской прозы.В формате zip можно забрать здесь.\nОсновные задачи этого урока:\ncorpus &lt;- load.corpus.and.parse(corpus.dir = \"../files/russian_corpus\")\nРазделим тексты на отрывки длиной 2000 слов.\ncorpus_samples &lt;- make.samples(corpus, \n                               sample.size = 2000, \n                               sampling = \"normal.sampling\",\n                               sample.overlap = 0,\n                               sampling.with.replacement = FALSE)\nПеред созданием списка слов удалим еры, которые встречаются в некоторых изданиях (“съ” и т.п.).\ncorpus_samples_clean &lt;- map(corpus_samples, \n                              function(text) str_remove(text, \"ъ$\") \n                            )",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "index.html#видео",
    "href": "index.html#видео",
    "title": "Компьютерный анализ текста",
    "section": "Видео",
    "text": "Видео\nЗаписи лекций и семинаров 2024/25 уч. г. можно найти в плейлисте по ссылке.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "multiclass.html#recipe",
    "href": "multiclass.html#recipe",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Recipe",
    "text": "25.2 Recipe\n\nlibrary(themis)\n\nbase_rec &lt;- recipe(corpus ~ ., data = data_train) |&gt; \n  step_downsample(corpus)\nbase_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor: 1001\n\n\n\n\n\n── Operations \n\n\n• Down-sampling based on: corpus\n\nnorm_rec &lt;- recipe(corpus ~ ., data = data_train)  |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_downsample(corpus)\nnorm_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor: 1001\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_predictors()\n\n\n• Down-sampling based on: corpus",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#knn",
    "href": "multiclass.html#knn",
    "title": "25  Многоклассовая классификация",
    "section": "25.3 KNN",
    "text": "25.3 KNN\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"kknn\")\n\nknn_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\n\nknn_grid &lt;- tibble(neighbors = c(1,3,5))\nknn_grid",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#svm",
    "href": "multiclass.html#svm",
    "title": "25  Многоклассовая классификация",
    "section": "25.9 SVM",
    "text": "25.9 SVM\nМетод опорных векторов (SVM) используется как в задачах регрессии, так и в задачах классификации.\nВо втором случае он пытается найти такую границу (гиперплоскость), которая максимально хорошо разделяет два класса объектов. Если упростить задачу до двух измерений, то метод ищет такую прямую, чтобы расстояние от неё до ближайших точек с каждой стороны было максимальным: классы должны быть как можно дальше от границы. Чем дальше граница от обучающих точек, тем устойчивее она к ошибкам на новых данных.\nДля этого SVM строит разделяющую прямую, которая максимально “отодвинута” от крайних точек обоих классов. Эти крайние точки, которые “касаются” границы — называются опорные векторы (support vectors).\nМаржа (англ. margin) — это расстояние от разделяющей границы до ближайших точек каждого класса. Чем больше маржа, тем увереннее разделяются классы.\nЭто проще всего пояснить при помощи графика. Обычные точки — это просто обучающие примеры. Черными отмечены как раз опорные векторы — те точки, которые оказались на краю своих классов и определили положение границы. Благодаря этим точкам SVM “знает”, где должна проходить разделяющая граница. Все “внутренние” точки не влияют на её положение.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#boost_tree",
    "href": "multiclass.html#boost_tree",
    "title": "25  Многоклассовая классификация",
    "section": "26.1 Boost_tree",
    "text": "26.1 Boost_tree\n\nbt_spec &lt;- boost_tree(trees = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"xgboost\")\n\n\nbt_grid &lt;- tibble(trees = c(100, 500, 1000))\n\n\nbt_wflow &lt;- workflow() |&gt; \n  add_model(bt_spec) |&gt; \n  add_recipe(base_rec)\n\nbt_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_downsample()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune()\n\nComputational engine: xgboost \n\n\n\nset.seed(08022025)\nbt_tune &lt;- tune_grid(\n  bt_wflow,\n  grid = bt_grid,\n  metrics = metric_set(accuracy, roc_auc),\n  folds,\n  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n)\n# → A | warning: ✖ No observations were detected in `truth` for levels: Antiphon,\n#                  Callimachus, Philostratus the Athenian, and Xenophon.\n#                ℹ Computation will proceed by ignoring those levels.\n# → B | warning: ✖ No observations were detected in `truth` for levels: Antiphon, Arrian,\n#                  Hyperides, and Philostratus the Athenian.\n#                ℹ Computation will proceed by ignoring those levels.\n# → C | warning: ✖ No observations were detected in `truth` for levels: Aeschylus, Antiphon,\n#                  Callimachus, and Hyperides.\n#                ℹ Computation will proceed by ignoring those levels.\n# → D | warning: ✖ No observations were detected in `truth` for levels: Aristophanes,\n#                  Hyperides, Isaeus, and Sophocles.\n#                ℹ Computation will proceed by ignoring those levels.\n# → E | warning: ✖ No observations were detected in `truth` for levels: Isaeus and\n#                  Sophocles.\n#                ℹ Computation will proceed by ignoring those levels.\n\n\nautoplot(bt_tune)",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#добавить-в-workflow_set",
    "href": "multiclass.html#добавить-в-workflow_set",
    "title": "25  Многоклассовая классификация",
    "section": "26.2 Добавить в workflow_set",
    "text": "26.2 Добавить в workflow_set\n\nwflow_set_final &lt;- wflow_set_fit |&gt; \n  bind_rows(as_workflow_set(bt_base = bt_tune))\n\n\nautoplot(wflow_set_final, metric = \"accuracy\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(-0.1, 1))\n\n\n\n\n\n\n\n\n\nautoplot(wflow_set_final, metric = \"roc_auc\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(0.4, 1))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#выбор-модели-и-окончательная-настройка",
    "href": "multiclass.html#выбор-модели-и-окончательная-настройка",
    "title": "25  Многоклассовая классификация",
    "section": "25.14 Выбор модели и окончательная настройка",
    "text": "25.14 Выбор модели и окончательная настройка\n\nrank_results(wflow_set_final, rank_metric = \"f_meas\")\n\n\n  \n\n\n\n\nautoplot(wflow_set_fit, id = \"tf_svm\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nbest_results &lt;- \n   wflow_set_final |&gt; \n   extract_workflow_set_result(\"tf_svm\") |&gt; \n   select_best(metric = \"accuracy\")\n\nbest_results\n\n\n  \n\n\n\nОбратите внимание: на этом этапе мы “распечатываем” тестовые данные!\n\nsvm_test_results &lt;- \n   wflow_set_final |&gt; \n   extract_workflow(\"tf_svm\") |&gt; \n   finalize_workflow(best_results) |&gt; \n   last_fit(split = data_split,\n            metrics = metric_set(accuracy, f_meas))\n\nsvm_test_results",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#оценка",
    "href": "multiclass.html#оценка",
    "title": "25  Многоклассовая классификация",
    "section": "25.15 Оценка",
    "text": "25.15 Оценка\nОцениваем эффективность на тестовых данных.\n\ncollect_metrics(svm_test_results)\n\n\n  \n\n\n\n\nsvm_test_results |&gt; \n  collect_predictions() |&gt;\n  conf_mat(truth = author, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\"),\n        axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nОтличная работа 🏆 🏆 🏆",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "consensus.html#boot.phylo-galbraith",
    "href": "consensus.html#boot.phylo-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.8 boot.phylo(): galbraith",
    "text": "17.8 boot.phylo(): galbraith\nВыше мы получили объект trees_result путем применения пользовательской функции get_tree() к данным. В пакете {phangorn}, однако, есть готовое решение для бутстрепа. Воспользуемся им и сравним результат. Заодно поменяем расстояние на косинусное и изменим алгоритм кластеризации на NJ.\n\n# функция для вычисления расстояния\ndtm_to_dist &lt;- function(data){\n  dist_mx &lt;- data |&gt; \n    scale() |&gt; \n    philentropy::distance(method = \"cosine\", \n                          use.row.names = TRUE, \n                          mute.message = TRUE) |&gt;  \n    as.dist()\n  \n  return(1 - dist_mx)\n} \n\n# матрица расстояния (все 3000 признаков)\ndist_mx &lt;- dtm_to_dist(galbraith)\n\n# кластеризация NJ\nnj &lt;- nj(dist_mx)\n\nВот так выглядит одно дерево. Пока оставим его без оформления.\n\nnj |&gt; \n  plot(type = \"unrooted\",\n       lab4ut = \"axial\")\n\n\n\n\n\n\n\n\nТеперь применяем функцию для бутстрепа. На входе она требует одно дерево, функцию для его получения, а также исходный датасет для бутстрепа. Значение аргумента trees выставляем на TRUE: это значит, что все построенные деревья будут сохраняться.\n\n# bootstrap\nFUN &lt;- function(xx) nj(dtm_to_dist(xx)) \ntree &lt;- FUN(galbraith)\nbs &lt;- boot.phylo(tree, galbraith, FUN, \n                 # сто итераций\n                 B = 100, \n                 # признаки берутся блоками по 1\n                 block = 1,\n                 rooted = FALSE, \n                 trees = TRUE)\n\n\nRunning bootstraps:       100 / 100\nCalculating bootstrap values... done.\n\n\nПосле этого строим консенсусную сеть (или консенсусное дерево, см. выше).\n\n# вычисляем консенсус\ncons.nw2 &lt;- consensusNet(bs$trees, prob = 0.3, rooted = FALSE)\n\nТеперь попробуем снова визуализировать наше консенсусное дерево.\n\nset.seed(05032024)\npar(mar = c(0,0,0,0), oma = c(0,0,0,0), cex = 1.2)\nplot(cons.nw2, type = \"2D\", \n     direction = \"axial\",\n     use.edge.length = FALSE,\n     font = 2,\n     # берем из предыдущей сети :)\n     tip.color = cons.nw$col,\n     edge.color = \"grey30\",\n     edge.width = 1.2, \n     label.offset = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nЧто произойдет с сетью, если изменить силу консенсуса? Почему? Самостоятельно постройте консенсусное дерево на основе бутстрепа.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "ggraph.html#возможности-visnetwork",
    "href": "ggraph.html#возможности-visnetwork",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.8 Возможности VisNetwork",
    "text": "19.8 Возможности VisNetwork\nПодробнее о возможностях visNetwork можно почитать здесь. Вот так можно добавить всплывающие подсказки и иконки для узлов.\n\nlibrary(visNetwork)\n\n# Создаем узлы с tooltips\nnodes &lt;- data.frame(\n  id = 1:3, \n  label = c(\"King\", \"Queen\", \"Prince\"), \n  # иконки\n  shape = \"icon\",  \n  #  tooltips для каждого узла\n  title = c(\"The ruler of the kingdom\", \n            \"The queen of the land\", \n            \"The prince in the castle\"),  \n  icon = list(\n    face = \"FontAwesome\",\n    # коды иконок FA\n    code = c(\"f118\", \"f005\", \"f183\"),  \n    size = 50,\n    color = c(\"darkred\", \"purple\", \"blue\")  # Цвета иконок\n  )\n)\n\n# связи между узлами\nedges &lt;- data.frame(from = c(1, 1), to = c(2, 3))\n\n# граф с иконками и tooltips\nvisNetwork(nodes, edges) |&gt; \n  visOptions(highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE)) |&gt; \n  addFontAwesome()\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗамените иконками изображения Тюдоров.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "multiclass.html#разведывательный-анализ",
    "href": "multiclass.html#разведывательный-анализ",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Разведывательный анализ",
    "text": "25.2 Разведывательный анализ\nВ нашем датасете есть несколько очень коротких рецензий. Негативные рецензии в целом длиннее позитивных и нейтральных.\n\nreviews |&gt; \n  mutate(n_words = str_count(review, \" \") + 1) |&gt; \n  ggplot(aes(n_words, fill = sentiment)) +\n  geom_histogram(bins = 100) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light()\n\n\n\n\n\n\n\n\nПосмотрим на число уникальных токенов в каждой из категорий.\n\nreviews_tokens &lt;- reviews |&gt; \n  mutate(id = row_number(), .before = sentiment) |&gt; \n  unnest_tokens(token, review) \n\nreviews_tokens |&gt; \n  group_by(sentiment) |&gt; \n  summarise(n = n_distinct(token))\n\n\n  \n\n\n\nБольшая часть слов встречается очень редко.\n\nreviews_tokens |&gt; \n  count(sentiment, token) |&gt; \n  ggplot(aes(n, fill = sentiment)) +\n  geom_histogram(show.legend = FALSE, bins = 1000) +\n  coord_cartesian(xlim = c(NA, 2500), ylim = c(NA, 2500)) +\n  theme_light() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\nЗдесь можно добавить пример из https://juliasilge.com/blog/nber-papers/.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#обучающая-и-тестовая-выборки",
    "href": "multiclass.html#обучающая-и-тестовая-выборки",
    "title": "25  Многоклассовая классификация",
    "section": "25.3 Обучающая и тестовая выборки",
    "text": "25.3 Обучающая и тестовая выборки\n\nset.seed(06042025)\ndata_split &lt;- corpus_top |&gt; \n  mutate(author = as.factor(author)) |&gt; \n  initial_split(strata = author)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\n\n# folds\nset.seed(06042025)\nfolds &lt;- vfold_cv(data_train, strata = author, v = 5)\nfolds",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#рецепт-для-препроцессинга",
    "href": "multiclass.html#рецепт-для-препроцессинга",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Рецепт для препроцессинга",
    "text": "25.4 Рецепт для препроцессинга\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"))\n\n# уберем повторы и упорядочим по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 380\n\n\nПодробнее о рецепте см. https://smltar.com/mlregression#firstregression\n\ntfidf_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\ntfidf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#результат-препроцессинга",
    "href": "multiclass.html#результат-препроцессинга",
    "title": "25  Многоклассовая классификация",
    "section": "25.5 Результат препроцессинга",
    "text": "25.5 Результат препроцессинга\n\nprep_train_tf &lt;- tf_rec |&gt;\n  prep(data_train) \n\ntidy(prep_train_tf)\n\n\n  \n\n\n\n\nbake_train_tf &lt;- prep_train_tf |&gt; \n  bake(new_data = NULL)\n\nbake_train_tf\n\n\n  \n\n\n\n\nprep_train_pca &lt;- pca_rec |&gt;\n  prep(data_train) \n\ntidy(prep_train_pca)\n\n\n  \n\n\n\n\nbake_train_pca &lt;- prep_train_pca |&gt; \n  bake(new_data = NULL)\n\nbake_train_pca\n\n\n  \n\n\n\nИспользуем этот шаг для разведывательного анализа.\n\nbake_train_pca |&gt; \n  ggplot(aes(PC1, PC2, color = author)) +\n  geom_point() + \n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#первая-модель-регрессия-с-регуляризацией",
    "href": "multiclass.html#первая-модель-регрессия-с-регуляризацией",
    "title": "25  Многоклассовая классификация",
    "section": "25.6 Первая модель: регрессия с регуляризацией",
    "text": "25.6 Первая модель: регрессия с регуляризацией\nКогда мы работаем с текстовыми данными и используем большое число признаков для классификации, важно избегать алгоритмов, которые плохо работают с высоким числом измерений (например, k-NN). Вместо этого лучше использовать более эффективные алгоритмы, такие как линейные модели с регуляризацией.\nДля задач классификации применяется логистическая регрессия, которая неплохо справляется с разреженными данными благодаря L1-регуляризации (Lasso) или L2-регуляризации (Ridge). В частности, лассо-регуляризация позволяет обнулять незначимые признаки, исключая их тем самым из модели.\nПоскольку в нашем датасете несколько классов, то мы применим многоклассовую логистическую регрессию.\n\nlasso_spec &lt;- multinom_reg(penalty = tune(), mixture = 1) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\n\nlasso_param &lt;- extract_parameter_set_dials(lasso_spec)\n  \nlasso_grid &lt;- lasso_param |&gt; \n  grid_regular(levels = 3)\n\nlasso_grid\n\n\n  \n\n\n\n\nlasso_wflow &lt;- workflow() |&gt; \n  add_model(lasso_spec) |&gt; \n  add_recipe(tf_rec)\n\nlasso_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nЗдесь придется немного (или много) подождать. Параллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями.\n\nlibrary(tictoc)\nlibrary(future)\n\nplan(multisession, workers = 5)\n\ntic()\nset.seed(06042025)\nlasso_tune &lt;- lasso_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = lasso_grid,\n    metrics = metric_set(accuracy, roc_auc),\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nlasso_tune \n\ntoc()\n# 12.376 sec elapsed\nplan(sequential)\n\n\nautoplot(lasso_tune)\n\n\n\n\n\n\n\n\nНаша модель уже достигла достаточно высокой точности расходимся.\n\ncollect_predictions(lasso_tune) |&gt; \n  roc_curve(truth = author, .pred_Bulgakov:.pred_Vovchok)  |&gt; \n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  coord_fixed() +\n  theme_light()\n\n\n\n\n\n\n\n\nВспомним, что все это значит:\nSensitivity (Чувствительность) = True Positive Rate (TPR): - Формула: TP/(TP+FN) - Это доля верно определенных положительных примеров среди всех положительных примеров - Показывает, насколько хорошо модель находит нужные объекты из всех существующих - Другие названия: полнота (recall), истинноположительная доля - Ось Y на ROC-кривой\n1-Specificity = False Positive Rate (FPR): - Формула: FP/(FP+TN) = 1 - TN/(FP+TN) = 1 - Specificity - Это доля неверно определенных положительных примеров среди всех отрицательных примеров - Показывает, насколько часто модель ошибочно причисляет негативные примеры к позитивным - Другие названия: ложноположительная доля - Ось X на ROC-кривой\nВ нашем контексте:\n\nsensitivity (для автора А) – это доля текстов автора А, которые правильно определены как тексты автора А,\n1-specificity (для автора А) – это доля текстов НЕ автора А, которые ошибочно определены как тексты автора А.\n\n\ncollect_metrics(lasso_tune) |&gt; \n  filter(.metric == \"accuracy\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#random-forest",
    "href": "multiclass.html#random-forest",
    "title": "25  Многоклассовая классификация",
    "section": "25.11 Random forest",
    "text": "25.11 Random forest\nМы уже знакомы с применением случайного леса в задачах регрессии, а теперь используем этот метод для классификации.\n\nrf_spec &lt;- rand_forest(\n  trees = tune()) |&gt;        \n  set_mode(\"classification\") |&gt; \n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = tune()\n\nComputational engine: ranger \n\n\nДля случайного леса создадим решетку вручную.\n\nrf_grid &lt;- tibble(trees = c(100, 200, 300))\n\nrf_grid",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#объединение-воркфлоу",
    "href": "multiclass.html#объединение-воркфлоу",
    "title": "25  Многоклассовая классификация",
    "section": "25.13 Объединение воркфлоу",
    "text": "25.13 Объединение воркфлоу\n\nwflow_set_final &lt;- wflow_set_fit |&gt; \n  bind_rows(as_workflow_set(lasso_tf = lasso_tune))\n\nwflow_set_final\n\n\n  \n\n\n\nЛучшие результаты показывает SVM. Одна из моделей лассо (с очень высоким штрафным коэффициентом) находится в самом низу.\n\nautoplot(wflow_set_final, metric = \"accuracy\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),\n            angle = 90, hjust = 1.5) \n\n\n\n\n\n\n\n\n\nautoplot(wflow_set_final, metric = \"f_meas\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),\n            angle = 90, hjust = 1.5)",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#p.s.-pca-рецепт",
    "href": "multiclass.html#p.s.-pca-рецепт",
    "title": "25  Многоклассовая классификация",
    "section": "25.13 P.S. PCA-рецепт",
    "text": "25.13 P.S. PCA-рецепт\n\npca_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 100)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\n\nprep_train_pca &lt;- pca_rec |&gt;\n  prep(data_train) \n\ntidy(prep_train_pca)\n\n\n  \n\n\n\nКак можно видеть, количество признаков уменьшилось в 10 раз.\n\nbake_train_pca &lt;- prep_train_pca |&gt; \n  bake(new_data = NULL)\n\nbake_train_pca\n\n\n  \n\n\n\n\nrf_pca_wflow &lt;- workflow() |&gt; \n  add_model(rf_spec) |&gt; \n  add_recipe(pca_rec)\n\nrf_pca_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n10 Recipe Steps\n\n• step_mutate()\n• step_mutate()\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_zv()\n• step_normalize()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = tune()\n\nComputational engine: ranger \n\n\nТеперь ждать не так много, ведь данные стали менее разреженными.\n\nplan(multisession, workers = 5)\n\ntic()\nset.seed(10032025)\nrf_pca_tune &lt;- rf_pca_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = rf_grid,\n    metrics = metric_set(accuracy),\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nrf_pca_tune\n\ntoc()\n# 31.488 sec elapsed\nplan(sequential)\n\nСнова соединим воркфлоу.\n\nwflow_set_final2 &lt;- wflow_set_final |&gt; \n  bind_rows(as_workflow_set(rf_pca = rf_pca_tune))\n\nВидно, что серьезных ухудшений при снижении размерности не произошло, а производительность улучшилась значительно.\n\nautoplot(wflow_set_final2, metric = \"accuracy\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(-0.1, 1))\n\n\n\n\n\n\n\n\nВ следующем уроке попробуем улучшить результат.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#workflow_set",
    "href": "multiclass.html#workflow_set",
    "title": "25  Многоклассовая классификация",
    "section": "25.10 Workflow_set",
    "text": "25.10 Workflow_set\nВ пакете {tidymodels} для R объект workflow_set используется для организации и управления несколькими комбинациями моделей (model specifications) и рецептов предобработки (recipes) в рамках единого набора рабочих процессов (workflows).\nWorkflow_set — это объект, который содержит множество разных рабочих процессов (workflows), каждая из которых представляет собой:\n\nопределённую модель (модельную спецификацию);\nопределённый способ предобработки данных (recipe).\n\nИначе говоря: это способ систематически и удобно перебрать (или протестировать) различные комбинации моделей и рецептов на одном наборе данных.\n\nlibrary(baguette)\nlibrary(discrim)\n\n\nAttaching package: 'discrim'\n\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n\nwflow_set &lt;- workflow_set(  \n  preproc = list(base = base_rec,\n                 pca = pca_rec,\n                 pls = pls_rec,\n                 umap = umap_rec),  \n  models = list(svm = svm_spec,\n                lasso = lasso_spec,\n                ridge = ridge_spec,\n                mlp = mlp_spec,\n                bagging = bagging_spec,\n                fda = fda_spec,\n                rda = rda_spec,\n                knn = knn_mod),  \n  cross = TRUE\n)\n\nwflow_set\n\n\n  \n\n\n\nПараллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями. Если не уверены, сколько у вас процессоров, выполните:\n\nparallel::detectCores()\n\n[1] 8\n\n\nЗдесь придется немного (или много) подождать: мы обучаем сразу 32 модели на пяти фолдах, при этом некоторые – с разными параметрами! По умолчанию количество задач (циклов обучения) на этапе кросс-валидации ограничено числом фолдов. Поэтому, сколько бы ни было ядер в системе, в каждый момент времени не может быть запущено больше v параллельных задач (это позднее можно перенастроить, но пока не будем).\nАргументы grid = 3 означает, что будет использоваться 3 различных набора/комбинации гиперпараметров для каждой модели, включенной в workflow set. Он применяется, когда мы запускаем grid search — метод перебора гиперпараметров.\n\nlibrary(future)\nplan(multisession, workers = 5)\n\ntrain_res &lt;- wflow_set |&gt; \n  workflow_map(\n    verbose = TRUE,\n    seed = 180525,\n    resamples = folds,\n    grid = 3,\n    metrics = metric_set(f_meas, accuracy),\n    control = control_resamples(save_pred = TRUE)\n  )\n# i  1 of 32 tuning:     base_svm\n# ✔  1 of 32 tuning:     base_svm (33.2s)\n# i  2 of 32 tuning:     base_lasso\n# ...\n# ✔  2 of 32 tuning:     base_lasso (23.1s)\n# i  3 of 32 tuning:     base_ridge\n# ✔  3 of 32 tuning:     base_ridge (27.2s)\n# i  4 of 32 tuning:     base_mlp\n# ...\n# ✔  4 of 32 tuning:     base_mlp (26.7s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i  5 of 32 resampling: base_bagging\n# ✔  5 of 32 resampling: base_bagging (27.6s)\n# i  6 of 32 tuning:     base_fda\n# ✔  6 of 32 tuning:     base_fda (24.2s)\n# i  7 of 32 tuning:     base_rda\n# ...\n# ✔  7 of 32 tuning:     base_rda (1m 8.9s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i  8 of 32 resampling: base_knn\n# ...\n# ✔  8 of 32 resampling: base_knn (25.3s)\n# i  9 of 32 tuning:     pca_svm\n# ✔  9 of 32 tuning:     pca_svm (24.5s)\n# i 10 of 32 tuning:     pca_lasso\n# ...\n# ✔ 10 of 32 tuning:     pca_lasso (23.5s)\n# i 11 of 32 tuning:     pca_ridge\n# ...\n# ✔ 11 of 32 tuning:     pca_ridge (23.4s)\n# i 12 of 32 tuning:     pca_mlp\n# ...\n# ✔ 12 of 32 tuning:     pca_mlp (23.7s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i 13 of 32 resampling: pca_bagging\n# ✔ 13 of 32 resampling: pca_bagging (24s)\n# i 14 of 32 tuning:     pca_fda\n# ✔ 14 of 32 tuning:     pca_fda (25.2s)\n# i 15 of 32 tuning:     pca_rda\n# ✔ 15 of 32 tuning:     pca_rda (26.9s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i 16 of 32 resampling: pca_knn\n# ...\n# ✔ 16 of 32 resampling: pca_knn (25.2s)\n# i 17 of 32 tuning:     pls_svm\n# ...\n# ✔ 17 of 32 tuning:     pls_svm (25.8s)\n# i 18 of 32 tuning:     pls_lasso\n# ...\n# ✔ 18 of 32 tuning:     pls_lasso (24.6s)\n# i 19 of 32 tuning:     pls_ridge\n# ...\n# ✔ 19 of 32 tuning:     pls_ridge (25.1s)\n# i 20 of 32 tuning:     pls_mlp\n# ...\n# ✔ 20 of 32 tuning:     pls_mlp (26.7s)\n# i 21 of 32 tuning:     pls_bagging\n# ✔ 21 of 32 tuning:     pls_bagging (26.5s)\n# i 22 of 32 tuning:     pls_fda\n# ...\n# ✔ 22 of 32 tuning:     pls_fda (23.7s)\n# i 23 of 32 tuning:     pls_rda\n# ...\n# ✔ 23 of 32 tuning:     pls_rda (25.1s)\n# i 24 of 32 tuning:     pls_knn\n# ...\n# ✔ 24 of 32 tuning:     pls_knn (26.6s)\n# i 25 of 32 tuning:     umap_svm\n# ...\n# ✔ 25 of 32 tuning:     umap_svm (58.1s)\n# i 26 of 32 tuning:     umap_lasso\n# ...\n# ✔ 26 of 32 tuning:     umap_lasso (54.7s)\n# i 27 of 32 tuning:     umap_ridge\n# ...\n# ✔ 27 of 32 tuning:     umap_ridge (52.1s)\n# i 28 of 32 tuning:     umap_mlp\n# ...\n# ✔ 28 of 32 tuning:     umap_mlp (53.7s)\n# i 29 of 32 tuning:     umap_bagging\n# ✔ 29 of 32 tuning:     umap_bagging (54.2s)\n# i 30 of 32 tuning:     umap_fda\n# ✔ 30 of 32 tuning:     umap_fda (52.7s)\n# i 31 of 32 tuning:     umap_rda\n# ...\n# ✔ 31 of 32 tuning:     umap_rda (52.4s)\n# i 32 of 32 tuning:     umap_knn\n# ...\n# ✔ 32 of 32 tuning:     umap_knn (57.3s)\n\n\nplan(sequential)\n\nВ некоторых случаях вы можете получить сообщение о том, что невозможно вычилить метрику precision (точности) для мультиклассовой классификации: в некоторых фолдах кросс-валидации модель не предсказала ни одного примера для некоторых классов. Если модель не сделала вообще не сделала ни одного предсказания (даже ошибочного), то метрика precision = TP / (TP + FP) становится неопределённой, так как знаменатель равен нулю.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#препроцессинг-tf-idf",
    "href": "multiclass.html#препроцессинг-tf-idf",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Препроцессинг: tf-idf",
    "text": "25.4 Препроцессинг: tf-idf\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"))\n\n# уберем повторы и упорядочим по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 380\n\n\nПодробнее о рецепте см. https://smltar.com/mlregression#firstregression\n\ntfidf_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\ntfidf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#препроцессинг-pca",
    "href": "multiclass.html#препроцессинг-pca",
    "title": "25  Многоклассовая классификация",
    "section": "25.5 Препроцессинг: PCA",
    "text": "25.5 Препроцессинг: PCA\n\npca_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 100)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\nНа очень большом числа признаков step_pca() может сильно замедлять вычисления, в этом случае можно попробовать step_pca_truncated() из пакета {embed}.\nТакже стоит помнить, что PCA выполняет линейное снижение размерности, что не всегда подходит. Для нелинейного подхода воспользуйтесь функцией step_umap() из того же пакета.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "dnn.html",
    "href": "dnn.html",
    "title": "26  Глубокое обучение",
    "section": "",
    "text": "26.1 Основные понятия\nВ предыдущих главах мы использовали такие алгоритмы, как регуляризованные линейные модели, машины опорных векторов и наивные байесовские модели для предсказания результатов на основе признаков, включая текстовые данные. Модели глубокого обучения решают те же задачи и преследуют те же цели, однако используются другие алгоритмы.\nГлубокое обучение – это особый раздел машинного обучения. Под “глубиной” в глубоком обучении не подразумевают более глубокое понимание, достигаемое этим подходом; идея заключается в многослойном представлении. Количество слоев, которые делится модель данных, называют глубиной модели (Шолле 2023, 33).\nСлои в модели глубокого обучения соединены в сеть, и такие модели называют нейронными сетями, хотя по сути они работают не так, как человеческий мозг. Слои могут быть соединены по-разному — такие конфигурации называют архитектурами сети.\nВ этом уроке мы познакомимся с полносвязной (dense) нейронной сетью для работы с текстом. Это одна из самых простых архитектур, и обычно такая модель не показывает наилучших результатов на текстовых данных, но с неё удобно начать, чтобы понять сам процесс построения и оценки глубоких моделей для работы с текстом. Кроме того, этот тип архитектуры может быть своеобразным мостом между методами “мешка слов”, которые мы использовали ранее, и более сложными подходами, позволяющими учитывать не только частотность слов, но также их последовательности.\nНа рисунке показана архитектура полносвязной прямой нейронной сети (feed-forward). Входные данные поступают в сеть сразу и полностью (в данном случае — полностью) соединены с первым скрытым слоем. Слой называется «скрытым», потому что не связан с внешним миром; этим занимаются только входной и выходной слои. Нейроны каждого слоя соединяются лишь со следующим слоем. Количество слоев и узлов в каждом из них может меняться; эти параметры называются гиперпараметрами и выбираются исследователем.\nПод обучением сети подразумевается поиск набора значений весов всех слоев в сети, при котором сеть будет правильно отображать образцовые входные данные в соответствующие им результаты. Первоначально весам присваиваются случайные значения, но постепенно они корректируются в нужном направлении. “Нужным” в данном случае считается такое направление, которое минимизирует функцию потерь.\nЗа корректировку отвечает оптимизатор — это алгоритм, который управляет процессом обучения нейронной сети, корректируя веса модели с целью минимизации функции ошибки (или функции потерь). Проще говоря, оптимизатор помогает найти такие значения параметров, при которых сеть даёт наилучшие предсказания. Для этого он реализует алгоритм обратного распространения ошибки (backpropagation): для каждого параметра вычисляется вклад, который он вносит в значение потерь (Шолле 2023, 93).\nЭтот вклад определяется с помощью градиентов. Градиент – это обобщение понятия производной для функций, принимающих тензоры (многомерные массивы чисел) в качестве входных данных (Шолле 2023, 83). Градиент функции f – это вектор, который указывает направление наискорейшего роста этой функции, при этом модуль градиента равен скорости изменения функции в этом направлении.\nОптимизатор обновляет веса пропорционально этим градиентам (с учетом параметра скорости обучения), что позволяет постепенно приближаться к минимуму функции потерь. Градиентный спуск (gradient descent) — это метод оптимизации, который использует вычисленные градиенты для обновления весов сети с целью минимизации функции потерь. Он корректирует веса в направлении, противоположном градиенту (т.е. в сторону уменьшения ошибки).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#данные",
    "href": "dnn.html#данные",
    "title": "26  Полносвязные нейросети",
    "section": "26.2 Данные",
    "text": "26.2 Данные\nФункция initial_validation_split() создает случайное разделение данных на три части: обучающую (training set), валидационную (validation set) и тестовую (testing set) выборки. Функции training(), validation() и testing() позволяют извлекать соответствующие подмножества данных после разбиения.\n\nset.seed(11032025)\ndata_split &lt;- reviews |&gt; \n  mutate(sentiment = as.factor(sentiment)) |&gt; \n  initial_validation_split(strata = sentiment)\ndata_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1800/600/600/3000&gt;\n\n\n\ndata_train &lt;- training(data_split)\ndata_validate &lt;- validation(data_split)\ndata_test &lt;- testing(data_split)\n\nРазделим обучающую выборку на 5 фолдов для перекрестной проверки.\n\n# folds\nset.seed(11032025)\nfolds &lt;- vfold_cv(data_train, strata = sentiment, v = 5)\nfolds",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг",
    "href": "dnn.html#препроцессинг",
    "title": "26  Полносвязные нейросети",
    "section": "26.3 Препроцессинг",
    "text": "26.3 Препроцессинг\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"))\n\n# уберем повторы и упорядочим по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\n\nМы начнем с того же рецепта, который использовали в прошлый раз. Каждая рецензия рассматривается как “мешок слов”. Число признаков снижено за счет стемминга, удаления цифр, латинских букв, а также стопслов. Снова установим максимальное значение признаков на 1000.\n\nbow_rec &lt;- recipe( ~ review, data = data_train)  |&gt;  \n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nФункция prep() вычисляет параметры всех шагов обработки, таких как токенизация, удаление стоп-слов или преобразование в bag-of-words. Функция bake() применяет подготовленный рецепт к обучающим данным.\n\n# prep and bake\nbow_prep &lt;- prep(bow_rec)\n\ntrain_bow_baked &lt;- bake(bow_prep,\n                   new_data = NULL,\n                   composition = \"matrix\")\n\n\nvalid_bow_baked &lt;- bake(bow_prep, \n                    new_data = data_validate,\n                    composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#перекодирование-меток",
    "href": "dnn.html#перекодирование-меток",
    "title": "26  Глубокое обучение",
    "section": "26.6 Перекодирование меток",
    "text": "26.6 Перекодирование меток\nOne-hot кодирование меток классов — это способ представления категориальных переменных в виде бинарных векторов.\n\nclass_train &lt;- data_train |&gt; \n  pull(class)   |&gt; \n  as.factor()  |&gt; \n  as.integer()\n\nФункция to_categorical() из пакета {keras} используется для преобразования вектора классов (представленного в виде целых чисел) в бинарную матрицу классов. Функция принимает вектор целочисленных меток классов, например, {0, 1, 2, 3}, и преобразует его в one-hot матрицу, где каждый класс кодируется бинарным вектором.\nПример:\n     [,1] [,2] [,3]\n[1,]    1    0    0  # Класс 0\n[2,]    0    1    0  # Класс 1\n[3,]    0    0    1  # Класс 2\n[4,]    0    1    0  # Класс 1\n[5,]    1    0    0  # Класс 0\nЗдесь:\n\nКаждая строка соответствует одному образцу.\nКаждый столбец – это конкретный класс.\n1 стоит в позиции индекса класса, остальное – 0.\n\nЭта функция используется в нейронных сетях, потому что выходной слой softmax ожидает one-hot представление меток классов.\n\nclass_train_onehot &lt;- to_categorical(class_train-1, num_classes = 4)\nhead(class_train_onehot)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    1    0    0    0\n[3,]    1    0    0    0\n[4,]    1    0    0    0\n[5,]    1    0    0    0\n[6,]    1    0    0    0\n\n\nТеперь проделаем то же самое для валидационного набора.\n\nclass_valid &lt;- data_validate  |&gt;  \n  pull(class)  |&gt; \n  as.factor()  |&gt; \n  as.integer()\n\nclass_valid_onehot &lt;- to_categorical(class_valid-1, num_classes = 4)\n\n\nhead(class_valid_onehot)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    1    0    0    0\n[3,]    1    0    0    0\n[4,]    1    0    0    0\n[5,]    1    0    0    0\n[6,]    1    0    0    0\n\n\n\nclass_test &lt;- data_test |&gt;  \n  pull(class)  |&gt; \n  as.factor()  |&gt; \n  as.integer()\n\nclass_test_onehot &lt;- to_categorical(class_test-1, num_classes = 4)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-bow",
    "href": "dnn.html#препроцессинг-bow",
    "title": "26  Глубокое обучение",
    "section": "26.5 Препроцессинг: BOW",
    "text": "26.5 Препроцессинг: BOW\nМы начнем с простейшей модели типа “мешок слов”. Число признаков снижено за счет стемминга, удаления цифр, а также стопслов. Установим максимальное значение признаков на 1000. Обратите внимание, что исходная формула не задаёт зависимой переменной. Это нужно для удобства преобразования в матричный формат.\n\nbow_rec &lt;- recipe( ~ description, data = data_train)  |&gt;  \n  step_mutate(description = stringr::str_remove_all(description, \"\\\\d+\")) |&gt; \n  step_tokenize(description) |&gt;\n  step_stopwords(description) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nbow_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(description, \"\\\\d+\")\n\n\n• Tokenization for: description\n\n\n• Stop word removal for: description\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: all_predictors()\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\nПрименим рецепт к обучающим данным. В функции bake() аргумент composition = \"matrix\" определяет формат возвращаемого результата. По умолчанию bake() возвращает tibble (или data.frame), где каждая строка — это наблюдение, а столбцы — признаки. Но мы планируем отдавать признаки нейросети, а она принимает матрицы на вход. Число элементов матрицы – 72 млн.\n\nbow_rec_prep &lt;- prep(bow_rec) \n\ntrain_bow_rec &lt;- bow_rec_prep |&gt; \n  bake(new_data = NULL,\n       composition = \"matrix\")\n\n\nvalid_bow_rec &lt;- bake(bow_rec_prep, \n                    new_data = data_validate,\n                    composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#dnn-bow",
    "href": "dnn.html#dnn-bow",
    "title": "26  Полносвязные нейросети",
    "section": "26.5 DNN: BOW",
    "text": "26.5 DNN: BOW\n\nbow_model &lt;- keras3::keras_model_sequential() |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 3, activation = \"softmax\")\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\nbow_model  |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\nbow_history &lt;- bow_model |&gt; \n  fit(\n    x = train_bow_baked,\n    y = sentiment_train,\n    batch_size = 100,\n    epochs = 20,\n    validation_data = list(valid_bow_baked, sentiment_valid), \n    verbose = FALSE\n  )\n\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 1\n        loss: 0.002697\nval_accuracy: 0.5617\n    val_loss: 1.828 \n\n\n\nplot(bow_history) \n\n\n\n\n\n\n\n\n\nbow_df &lt;- as.data.frame(bow_history)\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 1\n        loss: 0.002697\nval_accuracy: 0.5617\n    val_loss: 1.828",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-one-hot",
    "href": "dnn.html#препроцессинг-one-hot",
    "title": "26  Полносвязные нейросети",
    "section": "26.6 Препроцессинг: One-Hot",
    "text": "26.6 Препроцессинг: One-Hot\n\nonehot_rec &lt;- recipe( ~ review, data = data_train)  |&gt;  \n  step_tokenize(review)  |&gt;  \n  step_tokenfilter(review, max_tokens = 2000) |&gt; \n  step_sequence_onehot(review, sequence_length = 400)\n\n\n# prep and bake\nonehot_prep &lt;- prep(onehot_rec)\n\n\ntrain_onehot_baked &lt;- bake(onehot_prep,\n                           new_data = NULL,\n                           composition = \"matrix\")\n\n\nvalid_onehot_baked &lt;- bake(onehot_prep, \n                           new_data = data_validate,\n                           composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#dnn-one-hot",
    "href": "dnn.html#dnn-one-hot",
    "title": "26  Полносвязные нейросети",
    "section": "26.7 DNN: One-hot",
    "text": "26.7 DNN: One-hot\n\ndense_model &lt;- keras_model_sequential() |&gt; \n  layer_embedding(input_dim = 2001,\n                  output_dim = 64) |&gt; \n  layer_flatten() |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 3, activation = \"softmax\")\n\ndense_model\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (Embedding)             │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ flatten (Flatten)                 │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_4 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\ndense_model |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n\ndense_history &lt;- dense_model |&gt; \n  fit(\n    x = train_onehot_baked ,\n    y = sentiment_train,\n    batch_size = 100,\n    epochs = 20,\n    validation_data = list(valid_onehot_baked, sentiment_valid), \n    verbose = FALSE\n  )\n\n\nplot(dense_history)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#предсказание",
    "href": "dnn.html#предсказание",
    "title": "26  Полносвязные нейросети",
    "section": "26.8 Предсказание",
    "text": "26.8 Предсказание\n\ndense_res &lt;- predict(object = dense_model, \n                   x = valid_onehot_baked\n)\n\n19/19 - 0s - 5ms/step\n\nhead(dense_res)\n\n          [,1]        [,2]        [,3]\n[1,] 0.9899594 0.005354058 0.004686453\n[2,] 0.3758484 0.510795474 0.113356218\n[3,] 0.9771895 0.000415861 0.022394525\n[4,] 0.9713955 0.021642592 0.006962005\n[5,] 0.6321248 0.336982816 0.030892473\n[6,] 0.3736731 0.574162245 0.052164607\n\n\n\nfactor_names &lt;- tibble(levels = levels(data_train$sentiment),\n                       .pred_clas = 1:3)\n\nfactor_names\n\n\n  \n\n\n\n\npred_clas &lt;- apply(dense_res, 1, which.max)\n\nhead(pred_clas)\n\n[1] 1 2 1 1 1 2\n\n\n\ndense_res_tbl &lt;- tibble(truth = data_validate$sentiment, \n                      .pred_clas = pred_clas) |&gt; \n  left_join(factor_names) |&gt; \n  dplyr::select(-(.pred_clas)) |&gt; \n  rename(.pred_clas = levels) |&gt; \n  mutate(.pred_clas = as.factor(.pred_clas),\n         truth = as.factor(truth))\n\nmetrics(dense_res_tbl, truth = truth, estimate = .pred_clas)\n\n\n  \n\n\n\n\ndense_res_tbl |&gt; \n  group_by(truth, .pred_clas) |&gt; \n  summarise(n = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(truth, .pred_clas, fill = n)) +\n  geom_tile() +\n  geom_text(aes(label = n)) +\n  scale_fill_gradient2(low = \"#eaeff6\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\"),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  ggtitle(\"Dense Neural Network\")\n\n`summarise()` has grouped output by 'truth'. You can override using the\n`.groups` argument.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "multiclass.html#отбор-переменных",
    "href": "multiclass.html#отбор-переменных",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Отбор переменных",
    "text": "25.2 Отбор переменных\nДля построения модели берем 500 наиболее частотных слов (токенов), наименее связанных с тематикой.\n\nmfw &lt;- make.frequency.list(corpus_samples_clean)[1:500]\nsample(mfw, 20)\n\nСоставим матрицу с частотностями.\n\ncorpus_tf &lt;- stylo::make.table.of.frequencies(corpus_samples_clean, mfw) |&gt; \n  as.data.frame.matrix() |&gt; \n  rownames_to_column(\"id\") |&gt; \n  as_tibble()\n\n\ncorpus_tf\n\n\n  \n\n\n\nМы будем определять автора, поэтому разделим первый столбец на два.\n\ncorpus_tf &lt;- corpus_tf |&gt; \n  separate(id, into = c(\"author\", \"title\", NA), sep = \"_\") \ncorpus_tf\n\n\n  \n\n\n\nПосмотрим, сколько отрывков для каждого автора в корпусе.\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  arrange(n)\n\n\n  \n\n\n\nДля ускорения вычислений пока удалим авторов, у которых не так много отрывков.\n\ncorpus_top &lt;- corpus_tf |&gt; \n  add_count(author) |&gt; \n  filter(n &gt; 120) |&gt; \n  select(-n, -title) \n\n\ncorpus_top |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#препроцессинг",
    "href": "multiclass.html#препроцессинг",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Препроцессинг",
    "text": "25.4 Препроцессинг\nБольшую часть препроцессинга мы сделали в stylo, поэтому нам нужно всего несколько шагов.\n\ntf_rec &lt;- recipe(author ~ ., data = data_train) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\ntf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\nТакже создадим рецепт, в котором используем главные компоненты в качестве предикторов. В PCA максимальное число компонент равно минимуму из\n\nчисла переменных (признаков) в исходных данных;\nчисла наблюдений минус один.\n\nВ нашем случае классов 8.\n\npca_rec &lt;- recipe(author ~ ., data = data_train) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 7)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nНа очень большом числе признаков step_pca() может сильно замедлять вычисления, в этом случае можно попробовать step_pca_truncated() из пакета {embed}. Также стоит помнить, что PCA выполняет линейное снижение размерности, что подходит не для всех данных. Для нелинейного подхода воспользуйтесь функцией step_umap() из того же пакета.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#логистическая-регрессия",
    "href": "multiclass.html#логистическая-регрессия",
    "title": "25  Многоклассовая классификация",
    "section": "25.6 Логистическая регрессия",
    "text": "25.6 Логистическая регрессия\nВы уже знаете, что линейные модели используются в задачах регрессии:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\]\nгде:\n\ny — предсказание модели,\n\\(x_1, x_2, ..., x_p\\) — признаки (например, частоты слов),\n\\(\\beta_0\\) — свободный член (intercept),\n\\(\\beta_i\\) — коэффициенты модели, отражающие вклад соответствующего признака.\n\nВ задачах классификации, таких как определение темы текста, используют не просто линейную регрессию, а логистическую регрессию. Логистическая регрессия применяется для задач, где исходная переменная y категориальна (например, “спорт” или “политика”). Она предсказывает вероятность принадлежности объекта к одному из классов на основании логистической функции ( (иногда также называемой сигмоидой или логит-функцией):\n\\[p(y = 1 \\mid x) = \\frac{1}{1 + e^{-z}}\\]\nЗдесь значение p – вероятность принадлежности к положительному классу, а z – это линейная комбинация признаков:\n\\[z = β_0 + β_1x_1 + β_2x_2 + … + β_nx_n\\]\nПроведя некоторые преобразования, получаем:\n\\[\\text{logit}(p) = \\log\\left( \\frac{p}{1 - p} \\right) = z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\\] Левая часть уравнения называется “логит” (он же логарифм риска). Само по себе значение z может принимать любые значения. Однако, когда вы подставляете z в сигмоиду:\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nтогда результат всегда ограничен от 0 до 1.\nЭто значение интерпретируется как вероятность принадлежности к положительному классу. Если полученная вероятность ≥ 0.5 — модель предсказывает класс 1 (“положительный”). Если &lt; 0.5 — класс 0 (“отрицательный”). Значение 0.5 является границей между классами.\n\nТаким образом, даже если логит z может принимать любые значения от минус бесконечности до плюс бесконечности, благодаря сигмоиде результат всегда находится между 0 и 1. Это делает логистическую регрессию очень удобной для задач классификации. Однако при большом числе признаков эта модель склонна к переобучению (overfitting) — она приспосабливается слишком точно под обучающую выборку, что ухудшает её обобщающую способность.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#регуляризация-lasso-и-ridge",
    "href": "multiclass.html#регуляризация-lasso-и-ridge",
    "title": "25  Многоклассовая классификация",
    "section": "25.7 Регуляризация: Lasso и Ridge",
    "text": "25.7 Регуляризация: Lasso и Ridge\nКогда мы анализируем текстовые данные (например, классифицируем статьи по жанру, определяем тональность отзывов и т.д.), мы сталкиваемся с задачей представления текстов в числовом виде. Один из распространённых способов — построение мешка слов (bag-of-words), в котором каждый уникальный термин (слово, биграмма и пр.) — это отдельный признак. В результате для небольшого корпуса текстов может получиться десятки тысяч признаков (столбцов), большинство из которых обнулены (то есть в документе конкретное слово отсутствует). Такие данные называются разреженными (sparse), а количество признаков может значительно превышать количество наблюдений (документов).\nКогда число признаков очень велико, далеко не все алгоритмы машинного обучения работают одинаково хорошо. Некоторые, как, например, метод k-ближайших соседей (k-NN), плохо справляются с высокоразмерными пространствами. Это связано с тем, что в таких пространствах наблюдения становятся «далёкими» друг от друга, и расстояния между точками плохо отражают истинные различия между текстами. Это называют проклятием размерности (curse of dimensionality).\nВ таких случаях особенно полезны так называемые линейные модели с регуляризацией.\nЧтобы справиться с переобучением и улучшить интерпретируемость модели, используют регуляризацию — добавление штрафа за слишком большие коэффициенты β. За счет штрафа модель старается хорошо описывать данные, но при этом не сильно “разгоняться” в значениях коэффициентов.\nСуществуют два основных типа регуляризации:\n\nL1-регуляризация или Lasso (Least Absolute Shrinkage and Selection Operator),\nL2-регуляризация или Ridge.\n\nПри L2-регуляризации (Ridge Regression, гребневая регрессия) штрафом является сумма квадратов весов (здесь \\(w\\) - это вектор весов модели):\n\\[RSS + \\lambda \\sum_{j=1}^{p} w_j^2\\]\nЭтот метод уменьшает величину весов, не зануляя их. Он хорошо работает, когда все признаки важны.\nL1-регуляризация (Lasso Regression) использует как штраф сумму модулей весов:\n\\[RSS + \\lambda \\sum_{j=1}^{p} |w_j|\\] Этот метод может занулять отдельные коэффициенты, то есть по сути производит отбор признаков.\nОбъединение обеих регуляризаций называют Elastic Net. Этот метод позволяет достичь баланса между отбором признаков и сглаживанием коэффициентов.\n\\[RSS + \\lambda_1 \\| {w} \\|_1 + \\lambda_2 \\| {w} \\|_2^2\\]\nПоскольку в нашем датасете несколько классов, то мы применим многоклассовую логистическую регрессию. Пакет {tidymodels} предоставляет удобные инструменты для построения и настройки моделей с регуляризацией.\n\nlasso_spec &lt;- multinom_reg(penalty = tune(), mixture = 1) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\n\nridge_spec &lt;- multinom_reg(penalty = tune(), mixture = 0) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\nОбратите внимание на аргумент mixture:\n\nmixture = 1 задает лассо-модель;\nmixture = 0 - это гребневая регрессия;\n0 &lt; mixture &lt; 1 соответствуют Elastic Net.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#первая-модель",
    "href": "multiclass.html#первая-модель",
    "title": "25  Многоклассовая классификация",
    "section": "25.8 Первая модель",
    "text": "25.8 Первая модель\nПоскольку в нашем датасете несколько классов, то мы применим многоклассовую логистическую регрессию. Пакет {tidymodels} предоставляет удобные инструменты для построения и настройки моделей с регуляризацией.\n\nlasso_spec &lt;- multinom_reg(penalty = tune(), mixture = 1) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\nОбратите внимание на аргумент mixture:\n\nmixture = 1 задает лассо-модель;\nmixture = 0 - это гребневая регрессия;\n0 &lt; mixture &lt; 1 соответствуют Elastic Net.\n\nВыбираем лассо, чтобы отобрать наиболее значимые переменные (признаки). Гиперпараметр подберем путем настройки.\n\nlasso_param &lt;- extract_parameter_set_dials(lasso_spec)\n  \nlasso_grid &lt;- lasso_param |&gt; \n  grid_regular(levels = 3)\n\nlasso_grid\n\n\n  \n\n\n\n\nlasso_wflow &lt;- workflow() |&gt; \n  add_model(lasso_spec) |&gt; \n  add_recipe(tf_rec)\n\nlasso_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nЗдесь придется немного (или много) подождать. Параллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями.\nЕсли не уверены, сколько у вас процессоров, выполните:\n\nparallel::detectCores()\n\n[1] 8\n\n\nВ контексте обучения моделей с использованием кросс-валидации и фреймворка tidymodels в R нет смысла задействовать больше процессоров (ядер), чем количество фолдов (folds), потому что количество задач (циклов обучения) на этапе кросс-валидации ограничено числом фолдов. Поэтому, сколько бы ни было ядер в системе, в каждый момент времени не может быть запущено больше v параллельных задач.\n\n#install.packages(\"glmnet\")\nlibrary(tictoc)\nlibrary(future)\n\nplan(multisession, workers = 5)\n\ntic()\nset.seed(06042025)\nlasso_tune &lt;- lasso_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = lasso_grid,\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nlasso_tune \n\ntoc()\n# 12.376 sec elapsed\nplan(sequential)\n\n\nautoplot(lasso_tune)\n\n\n\n\n\n\n\n\nНаша модель уже достигла достаточно высокой точности расходимся.\n\ncollect_predictions(lasso_tune) |&gt; \n  roc_curve(truth = author, .pred_Bulgakov:.pred_Turgenev)  |&gt; \n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  coord_fixed() +\n  theme_light()\n\n\n\n\n\n\n\n\nВспомним, что все это значит:\nSensitivity (Чувствительность) = True Positive Rate (TPR):\n\nФормула: \\(TP/(TP+FN)\\)\nЭто доля верно определенных положительных примеров среди всех положительных примеров\nПоказывает, насколько хорошо модель находит нужные объекты из всех существующих\nДругие названия: полнота (recall), истинноположительная доля\nОсь Y на ROC-кривой\n\n1-Specificity = False Positive Rate (FPR):\n\nФормула: \\(FP/(FP+TN) = 1 - TN/(FP+TN) = 1 - Specificity\\)\nЭто доля неверно определенных положительных примеров среди всех отрицательных примеров\nПоказывает, насколько часто модель ошибочно причисляет негативные примеры к позитивным\nДругие названия: ложноположительная доля\nОсь X на ROC-кривой\n\nВ нашем контексте:\n\nsensitivity (для автора А) – это доля текстов автора А, которые правильно определены как тексты автора А,\n1-specificity (для автора А) – это доля текстов НЕ автора А, которые ошибочно определены как тексты автора А.\n\n\ncollect_metrics(lasso_tune) |&gt; \n  filter(.metric == \"f_meas\")\n\n\n  \n\n\n\n\nlasso_tune |&gt; \n  collect_predictions() |&gt;\n  conf_mat(truth = author, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#svm-в-tidymodels",
    "href": "multiclass.html#svm-в-tidymodels",
    "title": "25  Многоклассовая классификация",
    "section": "25.10 SVM в {tidymodels}",
    "text": "25.10 SVM в {tidymodels}\n\nsvm_spec &lt;- svm_linear(cost = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"LiblineaR\")\n\nsvm_spec\n\nLinear Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = tune()\n\nComputational engine: LiblineaR \n\n\nПояснение параметров:\n\ncost = tune() — здесь мы указываем, что параметр cost будет подобран автоматически (в процессе переподбора гиперпараметров с помощью tune()).\nset_mode(\"classification\") — устанавливает режим задачи как классификацию.\nset_engine(\"LiblineaR\") — указывает, что используется движок LiblineaR, реализующий SVM с линейным ядром (в пакете {tidymodels}).\n\nПараметр cost — это коэффициент штрафа за ошибки классификации. Он контролирует компромисс между количеством ошибок на обучающем наборе (т.е. насколько сильно модель стремится избежать ошибок) и шириной “маржи” — расстояния между разделительной гиперплоскостью и ближайшими точками разных классов.\nЕсли cost большое, модель старается классифицировать обучающую выборку как можно точнее: допускается меньшая ширина маржи, но это может привести к переобучению (overfitting).\nЕсли cost меньше, то модель допускает больше ошибок на обучении: маржа будет шире, это может привести к недообучению (underfitting), но лучше обобщается на новых данных.\n\nsvm_param &lt;- extract_parameter_set_dials(svm_spec)\n\nsvm_grid &lt;- svm_param |&gt; \n  grid_regular(levels = 3)\n\nsvm_grid",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multivar.html#svr-в-tidymodels",
    "href": "multivar.html#svr-в-tidymodels",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.7 SVR в tidymodels",
    "text": "23.7 SVR в tidymodels\nФункция translate() позволяет понять, как parsnip переводит пользовательский код на язык пакета.\n\nsvm_spec &lt;- svm_linear() |&gt;\n  set_engine(\"LiblineaR\") |&gt; \n  set_mode(\"regression\")\n\nsvm_spec |&gt; \n  translate()\n\nLinear Support Vector Machine Model Specification (regression)\n\nComputational engine: LiblineaR \n\nModel fit template:\nLiblineaR::LiblineaR(x = missing_arg(), y = missing_arg(), type = 11, \n    svr_eps = 0.1)\n\n\nПока это просто спецификация модели без данных и без формулы. Добавим ее к воркфлоу.\n\nsvm_wflow &lt;- workflow() |&gt; \n  add_model(svm_spec)\n\nsvm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: None\nModel: svm_linear()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Support Vector Machine Model Specification (regression)\n\nComputational engine: LiblineaR",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#деревья-решений",
    "href": "multivar.html#деревья-решений",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.12 Деревья решений",
    "text": "23.12 Деревья решений\nДеревья решений применяются как для задача регрессии, так и для задач классификации.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#случайный-лес-в-tidymodels",
    "href": "multivar.html#случайный-лес-в-tidymodels",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.14 Случайный лес в tidymodels",
    "text": "23.14 Случайный лес в tidymodels\nУточним, какие движки доступны для случайных лесов.\n\nshow_engines(\"rand_forest\")\n\n\n  \n\n\n\nСоздадим спецификацию модели. Деревья используются как в задачах классификации, так и в задачах регрессии, поэтому задействуем функцию set_mode().\n\nrf_spec &lt;- rand_forest(trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\nrf_wflow &lt;- workflow() |&gt; \n  add_model(rf_spec) |&gt; \n  add_recipe(books_rec)\n\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nОбучение займет чуть больше времени.\n\nrf_rs &lt;- fit_resamples(\n  rf_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nМы видим, что среднеквадратическая ошибка уменьшилась, а доля объясненной дисперсии выросла.\n\ncollect_metrics(rf_rs)  |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n6.5280907\n10\n0.49714066\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.5787228\n10\n0.06581642\nPreprocessor1_Model1\n\n\n\n\n\n\n\nТем не менее на графике можно заметить нечто странное: наша модель систематически переоценивает низкие значения и недооценивает высокие. Это связано с тем, что случайные леса не очень подходят для работы с разреженными данными (Hvitfeldt и Silge 2022).\n\nrf_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#деревья-решений-понятия",
    "href": "multivar.html#деревья-решений-понятия",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.12 Деревья решений: понятия",
    "text": "23.12 Деревья решений: понятия\nДеревья решений применяются как для задача регрессии, так и для задач классификации.\nРегрессионные деревья строят последовательное разбиение пространства признаков таким образом, чтобы минимизировать среднеквадратичную ошибку (MSE) в каждом из подмножеств.\nДля этого данные делятся на группы, в которых отклик (целевое значение) как можно более “однороден”. Каждое разбиение осуществляется на основе признаков (факторов), а в листьях дерева находятся средние значения отклика для соответствующей подгруппы. Вот так, например, может выглядеть предсказание расхода топлива для автомобиля (на основе датасета mtcars).\n\nДеревья легко показать графически, их легко интерпретировать, они хорошо справляются с категориальными предикторами (без создания dummy variables). Они особенно хорошо подходят для тех случаев, когда между откликом и предикторами существует нелинейная и сложная зависимость.\nНо деревья страдают от высокой дисперсии, т.е. если мы случайным образом разобьем обучающие данные на две части и построим дерево решений на основе каждой из них, полученные результаты могут оказаться довольно разными.\nЧтобы с этим справиться, используют три основных метода: бэггинг, случайный лес и бустинг.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#бэггинг-случайный-лес-бустинг",
    "href": "multivar.html#бэггинг-случайный-лес-бустинг",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.13 Бэггинг, случайный лес, бустинг",
    "text": "23.13 Бэггинг, случайный лес, бустинг\n\nБэггинг — это метод построения ансамбля моделей путем:\n\n\nповторного случайного выбора подвыборок из обучающего набора данных (бутстрэп);\nобучения на каждой из этих подвыборок дерева решений;\nобъединения (агрегации) результатов предсказаний этих моделей (для регрессии – усреднение предсказаний; для классификации: голосование).\n\nХотя бэггинг может улучшить предсказания многих методов, он особено полезен для деревьев решений.\n\nСлучайный лес – это частный случай бэггинга. Каждое дерево обучается на случайной выборке с возвращением (бутстрэп), но при построении дерева выбираются не все признаки, а случайное подмножество признаков. Это снижает корреляцию между деревьями и повышает качество ансамбля.\nБустинг работает похожим образом, но деревья строятся последовательно: каждое дерево выращивается с использованием информации по ранее выращенным деревьям. Бустинг не задействует бутстрэп, деревья обучаются на всем наборе данных. Из-за того, что деревья обучаются последовательно, его сложнее запараллелить.\n\nСлучайный лес и бустинг плохо поддаются интерпретации.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "regression.html#anova",
    "href": "regression.html#anova",
    "title": "22  Регрессионный анализ",
    "section": "22.3 ANOVA",
    "text": "22.3 ANOVA\n\nanova(fit_null, fit) |&gt; \n  export_table()\n\nRes.Df |     RSS | Df | Sum of Sq |     F |   Pr(&gt;F)\n----------------------------------------------------\n    29 | 1408.88 |    |           |       |         \n    28 |  878.44 |  1 |    530.44 | 16.91 | 3.11e-04\n\n\nФункция anova() сравнивает две вложенные линейные регрессионные модели с помощью анализа дисперсии. Цель — выяснить, добавляет ли переменная OxfordDst значительное улучшение модели по сравнению с моделью без предикторов.\n\n\n\n\n\n\n\nСтолбец\nЗначение\n\n\n\n\nRes.Df\nОстаточные степени свободы: число наблюдений минус число параметров модели.\n\n\nRSS\nResidual Sum of Squares — сумма квадратов остатков. Чем меньше, тем лучше.\n\n\nDf\nРазница в степени свободы между моделями (число добавленных предикторов).\n\n\nSum of Sq\nУлучшение, достигнутое за счёт добавленного предиктора (OxfordDst), то есть разница в RSS.\n\n\nF\nF-статистика для оценки значимости улучшения модели.\n\n\nPr(&gt;F)\np-значение: насколько вероятно наблюдать такую F-статистику случайно.\n\n\n\nRSS уменьшилась с 1408.88 до 878.44 после добавления переменной OxfordDst, значит модель улучшилась. F-статистика = 16.908, а p-value значительно ниже уровня значимости 0.05. Три звездочки (***) означают статистически значимую разницу между моделями.\nF-статистика — это статистика, которая используется для оценки качества модели в анализе дисперсии (ANOVA) и в регрессионном анализе. Она показывает, насколько хорошо модель с предикторами объясняет данные по сравнению с моделью без предикторов (или с меньшим их числом). Чем больше значение F, тем сильнее улучшение модели при добавлении переменных. Если получить такую большую F при случайных данных маловероятно (что отражает малое p-значение), то мы делаем вывод, что переменная значимо улучшает модель.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#сравнение-моделей-в-easystats",
    "href": "regression.html#сравнение-моделей-в-easystats",
    "title": "22  Регрессионный анализ",
    "section": "22.4 Сравнение моделей в {easystats}",
    "text": "22.4 Сравнение моделей в {easystats}\nДля сравнения моделей полезны следующие функции:\n\ncompare_models(fit_null, fit)\n\n\n  \n\n\n\n\ncompare_performance(fit_null, fit, rank = TRUE)\n\n\n  \n\n\n\n\ncompare_performance(fit_null, fit) |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\nlibrary(report)\ncompare_performance(fit_null, fit) |&gt; \n  report()\n\nWe compared two models; lm (R2 = 0.00, adj. R2 = 0.00, AIC = 204.62, BIC =\n207.42, RMSE = 6.85, Sigma = 6.97) and lm (R2 = 0.38, adj. R2 = 0.35, AIC =\n192.44, BIC = 196.65, RMSE = 5.41, Sigma = 5.60).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#параметры-модели",
    "href": "regression.html#параметры-модели",
    "title": "22  Регрессионный анализ",
    "section": "22.3 Параметры модели",
    "text": "22.3 Параметры модели\n\n22.3.1 Коэффициенты модели\nПервый столбец в таблице с параметрами содержит коэффициенты модели.\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00\n\n\n\n\n\n\n\nЭто значит, что наши данные описываются функцией:\n\nextract_eq(fit, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{OxfordPct}} = 20.88 - 0.12(\\operatorname{OxfordDst})\n\\]\n\n\nИнтуитивно понятно, что коэффициент \\(\\beta_1\\) связан с ковариацией (мерой совместной изменчивости двух величин). Действительно, он рассчитывается по формуле:\n\\[\\beta_1=\\frac{Cov(x,y)}{Var(x)}\\]\n\n\nПроверить.\n\n\nx &lt;- OxfordPots$OxfordDst\ny &lt;- OxfordPots$OxfordPct\n\nbeta_1&lt;- cov(x, y) / var(x)\nbeta_1\n\n[1] -0.1229049\n\n\n\nЗная \\(\\beta_1\\), можно вычислить \\(\\beta_0\\) по формуле:\n\\[\\beta_0=\\bar y - \\beta_1 \\bar x\\]\n\n\nСнова проверим.\n\n\nbeta_0 = mean(y) - beta_1 * mean(x)\nbeta_0\n\n[1] 20.87665\n\n\n\n\n\n22.3.2 Стандартные ошибки коэффициентов\nДля обоих коэффициентов приведена стандартная ошибка и t-статистика. Столбец t, как легко убедиться, содержит результат деления коэффицентов на стандартную ошибку.\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00\n\n\n\n\n\n\n\n\n\nКак рассчитываются стандартные ошибки.\n\nСтандартная ошибка для \\(\\beta_0\\) рассчитывается по формуле:\n\\[SE(\\beta_0)=\\sqrt{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}} \\times \\sqrt{\\frac{1}{n}+\\frac{\\bar x^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\]\nПервый множитель в этой формуле – это дисперсия остатков модели. Чем она больше, тем больше неопределенность. На второй множитель влияет как размер выборки, так и разброс независимой переменной x: чем больше размер выборки n, тем меньше \\(\\frac{1}{n}\\) и чем больше \\(Σ(x - \\bar x)^2\\), тем меньше второй множитель. Посчитаем вручую и сравним с результатом, который возвращает команда summary(fit).\n\nx_bar &lt;- mean(x)\n\nmult1 &lt;- sqrt(sum(fit$residuals^2) / 28)\nmult2 &lt;- sqrt(1/30 + ( x_bar^2 / sum((x - x_bar)^2)))\n\nmult1 * mult2\n\n[1] 2.233557\n\n\nСтандартная ошибка для \\(\\beta_1\\) рассчитывается по формуле:\n\\[SE(b_1)=\\sqrt{\\frac{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\]\nБольшая дисперсия остатков (в числителе) будет приводить к увеличению ошибки, а размах \\(x_i\\) – к уменьшению; интуитивно это объясняется тем, что в таком случае у нас больше информации для оценивания угла наклона. Снова перепроверим.\n\nmult1 / sqrt(sum((x - x_bar)^2))\n\n[1] 0.02989016\n\n\n\nФункция geom_smooth добавляет стандартную ошибку коэффициента наклона на график в виде серой полосы, которая означает, что с вероятностью 95% (значение по умолчанию, которое можно поменять) истинное значение отклика находится в этой зоне (predicted ± 1.95 * se). В статистике это называется доверительный интервал.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], \n              se = TRUE, level = 0.95) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n22.3.3 P-значения\nСтолбец p.value указывает, какова вероятность случайно получить такое значение. В нашем случае – почти 0, что говорит о том, что доля оксфордской керамики на участке действительно зависит от расстояния.\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00\n\n\n\n\n\n\n\n\n\nКак считается p-value.\n\n\ntidy(fit) |&gt; \n  transmute(t_stat = estimate / std.error) |&gt; \n  mutate(p_val = 2*pt(abs(t_stat), 28, lower.tail = FALSE)) |&gt; \n  export_table()\n\nt_stat |    p_val\n-----------------\n9.35   | 4.18e-10\n-4.11  | 3.11e-04\n\n\nРезультат, возвращаемый функцией pt(), умножается на два, т.к. используется двусторонний t-test. Буква p в названии означает функцию распределения вероятностей (probability), а t – распределение Стьюдента для заданного числа степеней свободы (28 в нашем случае).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#сравнение-моделей",
    "href": "regression.html#сравнение-моделей",
    "title": "22  Регрессионный анализ",
    "section": "22.5 Сравнение моделей",
    "text": "22.5 Сравнение моделей\n\n22.5.1 Нулевая модель\nВажно знать, что следующие два вызова возвращают одинаковые модели.\n\nfit1 &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nfit2 &lt;- lm(OxfordPct ~ 1 + OxfordDst, data = OxfordPots)\n\n\nfit1$coef == fit2$coef\n\n(Intercept)   OxfordDst \n       TRUE        TRUE \n\n\nЕдиница в вызове функции означает пересечение оси y, то есть свободный член. Это значит, что мы можем построить нулевую модель, где любому значению x будет соответствовать одно и то же (среднее) значение y.\n\nfit_null &lt;- lm(OxfordPct ~ 1, data = OxfordPots)\nparameters(fit_null)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n12.712\n1.273\n0.950\n10.109\n15.314\n9.989\n29.000\n0.000\n\n\n\n\n\n\n\nЕдинственный коэффициент в таком случае совпадает со средним значением y.\n\nmean(OxfordPots$OxfordPct)\n\n[1] 12.71167\n\n\nНа графике это будет выглядеть вот так.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  # обратите внимание на формулу!\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nТакая модель может быть использована для сравнения, чтобы понять, насколько мы выиграли, добавив предикторы.\n\n\n22.5.2 ANOVA\nФункция anova() сравнивает две вложенные линейные регрессионные модели с помощью анализа дисперсии. Цель — выяснить, добавляет ли переменная OxfordDst значительное улучшение модели по сравнению с моделью без предикторов.\n\nanova(fit_null, fit)\n\n\n  \n\n\n\n\n\nRes.Df |     RSS | Df | Sum of Sq |     F |   Pr(&gt;F)\n----------------------------------------------------\n    29 | 1408.88 |    |           |       |         \n    28 |  878.44 |  1 |    530.44 | 16.91 | 3.11e-04\n\n\n\n\n\n\n\n\n\nСтолбец\nЗначение\n\n\n\n\nRes.Df\nОстаточные степени свободы: число наблюдений минус число параметров модели.\n\n\nRSS\nResidual Sum of Squares — сумма квадратов остатков. Чем меньше, тем лучше.\n\n\nDf\nРазница в степени свободы между моделями (число добавленных предикторов).\n\n\nSum of Sq\nУлучшение, достигнутое за счёт добавленного предиктора (OxfordDst), то есть разница в RSS.\n\n\nF\nF-статистика для оценки значимости улучшения модели.\n\n\nPr(&gt;F)\np-значение: насколько вероятно наблюдать такую F-статистику случайно.\n\n\n\nRSS уменьшилась с 1408.88 до 878.44 после добавления переменной OxfordDst, значит модель улучшилась. F-статистика = 16.908, а p-value значительно ниже уровня значимости 0.05. Три звездочки (***) означают статистически значимую разницу между моделями.\n\n\n\n\n\n\nНа заметку\n\n\n\nF-статистика — это статистика, которая используется для оценки качества модели в анализе дисперсии (ANOVA) и в регрессионном анализе. Она показывает, насколько хорошо модель с предикторами объясняет данные по сравнению с моделью без предикторов (или с меньшим их числом). Чем больше значение F, тем сильнее улучшение модели при добавлении переменных. Если получить такую большую F при случайных данных маловероятно (что отражает малое p-значение), то мы делаем вывод, что переменная значимо улучшает модель.\n\n\n\n\n22.5.3 Сравнение с {easystats}\nДля сравнения моделей полезны следующие функции:\n\ncompare_performance(fit_null, fit, rank = TRUE)\n\n\n  \n\n\n\n\n\nName     | Model |   R2 | R2_adjusted | RMSE | Sigma |   AIC_wt |  AICc_wt\n--------------------------------------------------------------------------\nfit      |    lm | 0.38 |        0.35 | 5.41 |  5.60 |     1.00 |     1.00\nfit_null |    lm | 0.00 |        0.00 | 6.85 |  6.97 | 2.27e-03 | 2.88e-03\n\nName     |   BIC_wt | Performance_Score\n---------------------------------------\nfit      |     1.00 |                 1\nfit_null | 4.56e-03 |                 0\n\n\n\ncompare_performance(fit_null, fit) |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\nlibrary(report)\ncompare_performance(fit_null, fit) |&gt; \n  report()\n\nWe compared two models; lm (R2 = 0.00, adj. R2 = 0.00, AIC = 204.62, BIC =\n207.42, RMSE = 6.85, Sigma = 6.97) and lm (R2 = 0.38, adj. R2 = 0.35, AIC =\n192.44, BIC = 196.65, RMSE = 5.41, Sigma = 5.60).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "multiclass.html#подготовка-датасета",
    "href": "multiclass.html#подготовка-датасета",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Подготовка датасета",
    "text": "25.2 Подготовка датасета\nДля построения модели берем 500 наиболее частотных слов (токенов). Как мы увидим ниже, в этот список попали некоторые имена героев. В настоящем исследовании от них лучше избавиться, однако пока мы оставим все, как есть.\n\nmfw &lt;- make.frequency.list(corpus_samples_clean)[1:500]\n\nСоставим матрицу с частотностями.\n\ncorpus_tf &lt;- stylo::make.table.of.frequencies(corpus_samples_clean, mfw) |&gt; \n  as.data.frame.matrix() |&gt; \n  rownames_to_column(\"id\") |&gt; \n  as_tibble()\n\n\ncorpus_tf\n\n\n  \n\n\n\nМы будем определять автора, поэтому разделим первый столбец на два.\n\ncorpus_tf &lt;- corpus_tf |&gt; \n  separate(id, into = c(\"author\", \"title\", NA), sep = \"_\") \ncorpus_tf\n\n\n  \n\n\n\nПосмотрим, сколько отрывков для каждого автора в корпусе.\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  arrange(n)\n\n\n  \n\n\n\nДля ускорения вычислений пока удалим авторов, у которых не так много отрывков.\n\ncorpus_top &lt;- corpus_tf |&gt; \n  add_count(author) |&gt; \n  filter(n &gt; 120) |&gt; \n  select(-n, -title) \n\n\ncorpus_top |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#снова-о-prep-и-bake",
    "href": "multiclass.html#снова-о-prep-и-bake",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Снова о prep() и bake()",
    "text": "25.4 Снова о prep() и bake()\nБольшую часть препроцессинга мы сделали в stylo, поэтому нам нужно всего несколько шагов.\n\nbase_rec &lt;- recipe(author ~ ., data = data_train) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nbase_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\nТакже создадим рецепт, в котором используем главные компоненты в качестве предикторов. Позже число компонент можно настроить при помощи tune().\n\npca_rec &lt;- base_rec |&gt; \n  step_pca(all_predictors(), num_comp = 7)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nНа очень большом числе признаков step_pca() может сильно замедлять вычисления, в этом случае можно попробовать step_pca_truncated() из пакета {embed}. Также стоит помнить, что PCA выполняет линейное снижение размерности, что подходит не для всех данных. Для нелинейного подхода воспользуйтесь функцией step_umap() из того же пакета.\n\n\nФункция prep() обучает (подготавливает) рецепт на основе обучающего датасета. Она применяет операции, которые требуют “обучения” на данных, так что ее можно рассматривать как аналог функции fit(). Аргумент retain = TRUE в функции prep() управляет тем, будут ли сохранены предобработанные обучающие данные внутри подготовленного объекта рецепта.\n\nbase_trained &lt;- base_rec |&gt;\n  prep(data_train) \n\nbase_trained\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Training information \n\n\nTraining data contained 1816 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter removed: &lt;none&gt; | Trained\n\n\n• Centering and scaling for: и, в, не, что, на, он, с, я, как, ... | Trained\n\n\nЧто касается bake(), то это скорее аналог функции predict(): она применяет подготовленный рецепт к новым данным — например к обучающим или тестовым примерам. Она использует информацию, рассчитанную на этапе prep().\nЕсли вы вызывали prep(..., retain = TRUE), то можете использовать juice() вместо bake() для получения обработанных обучающих данных напрямую.\n\nbase_trained |&gt; \n  # или juice()\n  bake(new_data = NULL)",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#методы-снижения-размерности",
    "href": "multiclass.html#методы-снижения-размерности",
    "title": "25  Многоклассовая классификация",
    "section": "25.5 Методы снижения размерности",
    "text": "25.5 Методы снижения размерности\n\n25.5.1 PCA для разведывательного анализа\nPCA (Principal Component Analysis) — это один из основных и наиболее понятных подходов к уменьшению размерности данных. Он относится к линейным методам обучения без учителя, что означает, что для его работы не требуется информация о целевых переменных (например, метках классов). Метод создаёт новые переменные (главные компоненты) — линейные комбинации исходных признаков, которые максимизируют дисперсию данных. Первые несколько компонентов содержат основную информацию (вариативность) из всего набора признаков. Подробнее о нем см. урок 15.\n\npca_trained &lt;- pca_rec |&gt;\n  prep(data_train) \n\npca_trained |&gt; \n  juice()\n\n\n  \n\n\n\nPCA часто используется в разведывательном анализе данных (EDA — Exploratory Data Analysis), чтобы упростить структуру данных, выявить важные зависимости и визуализировать сложные многомерные данные. Вот как именно PCA применяется в EDA:\n\nУменьшение размерности для визуализации: диаграмма рассеяния помогает увидеть, есть ли скрытые кластеры, группы, выбросы или тенденции.\nОбнаружение кластеров или структуры в данных: Если после проекции на первые главные компоненты наблюдаются чётко различимые группы, это может свидетельствовать о наличии скрытой структуры или категорий.\nОбнаружение выбросов: объекты, которые лежат далеко от большинства других точек в новом пространстве, могут быть аномальными.\nОценка корреляции между признаками: в процессе анализа компонент (например, с помощью графиков нагрузок — loadings plot) можно понять, какие переменные сильно коррелируют между собой.\n\nВ нашем случае визуализация главных компонент PC1 и PC2 показывает, что распределение классов (авторов) частично перекрывается, хотя некоторые группы имеют тенденцию образовывать кластеры (например, Толстой и Достоевский). Однако большинство классов на плоскости пересекаются друг с другом, особенно в центральной части графика.\n\npca_trained |&gt; \n  juice() |&gt; \n  ggplot(aes(PC1, PC2, color = author)) +\n  geom_point() + \n  theme_light()\n\n\n\n\n\n\n\n\nПакет {learntidymodels} позволяет визуализировать нагрузки компонент.\n\n#devtools::install_github(\"tidymodels/learntidymodels\")\nlibrary(learntidymodels)\npca_trained |&gt; \n  plot_top_loadings(component_number &lt;= 4, n = 10) +\n  scale_fill_brewer(palette = \"Paired\") +\n  theme_light()\n\n\n\n\n\n\n\n\nСлова, имеющий наибольшую нагрузку в одной компоненте, являются коррелированными (“клим” и “самгин”, “она” и “сказала”). Визуализируйте компоненты 3 и 4, чтобы убедиться, что они хорошо выделяют Горького.\n\n\n25.5.2 PLS\nМетоды PLS и UMAP — это популярные техники понижения размерности в машинном обучении. Они используются для уменьшения количества признаков (переменных) в данных и извлечения наиболее важной информации, которая определяет закономерности в датасете.\nPartial Least Squares (PLS; метод частичных наименьших квадратов):\nPLS — это метод, который находит линейные комбинации исходных признаков, называемые компонентами, с учётом зависимости от отклика (целевой переменной). В отличие от PCA (главных компонент), который полностью игнорирует зависимую переменную и ищет направления максимальной дисперсии, PLS является методом обучения с учителем. Это означает, что он учитывает целевой признак при поиске новых компонент. PLS такие ищет проекции в пространстве признаков, которые одновременно объясняют вариацию и в предикторах, и в ответе, что делает его особенно полезным при построении моделей классификации или регрессии.\nВ машинном обучении метод применяется, когда имеется большое количество сильно коррелированных признаков (что может мешать моделированию). В таком случае PLS позволяет уменьшить размерность, сохранив полезную информацию для предсказаний.\nДобавим еще один шаг к обученному рецепту выше.\n\n# BiocManager::install('mixOmics')\n\npls_trained &lt;- base_trained |&gt; \n  step_pls(all_numeric_predictors(), outcome = \"author\", num_comp = 7) |&gt; \n  # дообучение\n  prep() \n\npls_trained |&gt; \n  juice() \n\n\n  \n\n\n\n\npls_trained |&gt; \n  juice() |&gt; \n  ggplot(aes(PLS1, PLS2, color = author)) +\n  geom_point() +\n  theme_light()\n\n\n\n\n\n\n\n\nНагрузки компонент выводятся аналогично тому, как мы делали выше.\n\npls_trained |&gt; \n  plot_top_loadings(component_number &lt;= 4, n = 10, type = \"pls\") +\n  scale_fill_brewer(palette = \"Paired\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n25.5.3 UMAP\nЕще один способ улучшить точность и интерпретируемость моделей, а также ускорить их обучение называется UMAP (Uniform Manifold Approximation and Projection). Это метод нелинейного понижения размерности, аналогичный t-SNE, но более быстрый. На первом этапе строится граф на основе расстояний между точками (обычно через k-ближайших соседей), который отражает топологию исходного пространства. Затем UMAP пытается разместить точки в пространстве меньшей размерности так, чтобы сохранить как можно больше свойств этого графа. Для этого используется оптимизационная функция на основе кросс-энтропии.\nВ машинном обучении используется для визуализации данных высокой размерности в 2D или 3D; может быть полезен как этап предварительной обработки перед моделированием, особенно в случаях, когда признаков много или они сильно нелинейно связаны. Важно: UMAP может применяться как без учителя, так и с учителем, но из-за стохастического характера может давать разную картину при каждом запуске и чувствителен к настройке гиперпараметров (число соседей).\n\nlibrary(embed)\n\nbase_trained |&gt; \n  step_umap(all_numeric_predictors(), outcome = \"author\", num_comp = 7) |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  ggplot(aes(UMAP1, UMAP2, color = author)) +\n  geom_point(alpha = 0.5) +\n  theme_light()\n\n\n\n\n\n\n\n\nСоздадим еще два рецепта, которые понадобятся нам при моделировании.\n\npls_rec &lt;- base_rec |&gt; \n  step_pls(all_numeric_predictors(), outcome = \"author\", num_comp = tune())\n\n\numap_rec &lt;- base_rec |&gt; \n  step_umap(all_numeric_predictors(), \n            outcome = \"author\",\n            num_comp = tune(),\n            neighbors = tune(),\n            min_dist = tune()\n  )\n\nКак мы вскоре убедимся, снижение размерности (DR) не всегда улучшает качество модели, особенно в случае таких моделей, как Random forest или “наивный Байес”, которые хорошо справляются с коллинеарными предикторами и разреженными данными.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#опорные-векторы-svm",
    "href": "multiclass.html#опорные-векторы-svm",
    "title": "25  Многоклассовая классификация",
    "section": "25.8 Опорные векторы (SVM)",
    "text": "25.8 Опорные векторы (SVM)\nМетод опорных векторов (SVM) используется как в задачах регрессии, так и в задачах классификации.\nВо втором случае он пытается найти такую границу (гиперплоскость), которая максимально хорошо разделяет два класса объектов. Если упростить задачу до двух измерений, то метод ищет такую прямую, чтобы расстояние от неё до ближайших точек с каждой стороны было максимальным: классы должны быть как можно дальше от границы. Чем дальше граница от обучающих точек, тем устойчивее она к ошибкам на новых данных.\nДля этого SVM строит разделяющую прямую, которая максимально “отодвинута” от крайних точек обоих классов. Эти крайние точки, которые “касаются” границы — называются опорные векторы (support vectors).\nМаржа (англ. margin) — это расстояние от разделяющей границы до ближайших точек каждого класса. Чем больше маржа, тем увереннее разделяются классы.\nЭто проще всего пояснить при помощи графика. Обычные точки — это просто обучающие примеры. Черными отмечены как раз опорные векторы — те точки, которые оказались на краю своих классов и определили положение границы. Благодаря этим точкам SVM “знает”, где должна проходить разделяющая граница. Все “внутренние” точки не влияют на её положение.\n\n\nsvm_spec &lt;- svm_linear(cost = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"LiblineaR\")\n\nsvm_spec\n\nLinear Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = tune()\n\nComputational engine: LiblineaR \n\n\nПояснение параметров:\n\ncost = tune() — здесь мы указываем, что параметр cost будет подобран автоматически (в процессе переподбора гиперпараметров с помощью tune()).\nset_mode(\"classification\") — устанавливает режим задачи как классификацию.\nset_engine(\"LiblineaR\") — указывает, что используется движок LiblineaR, реализующий SVM с линейным ядром (в пакете {tidymodels}).\n\nПараметр cost — это коэффициент штрафа за ошибки классификации. Он контролирует компромисс между количеством ошибок на обучающем наборе (т.е. насколько сильно модель стремится избежать ошибок) и шириной “маржи” — расстояния между разделительной гиперплоскостью и ближайшими точками разных классов.\nЕсли cost большое, модель старается классифицировать обучающую выборку как можно точнее: допускается меньшая ширина маржи, но это может привести к переобучению (overfitting).\nЕсли cost меньше, то модель допускает больше ошибок на обучении: маржа будет шире, это может привести к недообучению (underfitting), но лучше обобщается на новых данных.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#еще-несколько-моделей-для-сравнения",
    "href": "multiclass.html#еще-несколько-моделей-для-сравнения",
    "title": "25  Многоклассовая классификация",
    "section": "25.9 Еще несколько моделей для сравнения",
    "text": "25.9 Еще несколько моделей для сравнения\nОднослойная нейронная сеть - простейшая форма нейронной сети, также известная как перцептрон или логистическая регрессия с несколькими выходами.\n\nmlp_spec &lt;- mlp(hidden_units = tune(),\n                penalty = tune(),\n                epochs = tune()) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\")\n\nБэггинг деревьев решений – упомянутый ранее ансамблевый метод; строит множество решающих деревьев на бутстреп-выборках, а результат — среднее (для регрессии) или голосование (для классификации).\n\nbagging_spec &lt;- bag_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"classification\")\n\nFlexible Discriminant Analysis (FDA) – расширение линейного дискриминантного анализа (LDA), где границы между классами аппроксимируются при помощи нелинейных моделей (например, сплайнов).\n\nfda_spec &lt;- discrim_flexible(prod_degree = tune()) |&gt; \n  set_engine(\"earth\")\n\nRegularized Discriminant Analysis (RDA) - rомпромисс между линейным (LDA) и квадратичным дискриминантным анализом (QDA) с добавлением регуляризации.\n\nrda_spec &lt;- discrim_regularized(frac_common_cov = tune(), \n                                frac_identity = tune())  |&gt; \n  set_engine('klaR')\n\nМетод ближайших соседей (K-Nearest Neighbors — KNN) - классификация (или регрессия) объекта производится на основе меток (или значений) K ближайших к нему объектов из обучающей выборки.\n\n#devtools::install_github(\"KlausVigo/kknn\")\n\nknn_mod &lt;- nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#оценка-и-выбор-модели",
    "href": "multiclass.html#оценка-и-выбор-модели",
    "title": "25  Многоклассовая классификация",
    "section": "25.11 Оценка и выбор модели",
    "text": "25.11 Оценка и выбор модели\nХорошо видно, что снижение размерности привело к существенному улучшению качества модели KNN, которая, однако, уступает регрессионным. Такое же улучшение можно зафиксировать для нейросети (mlp), а в случае с rda результат как минимум не хуже при заметном ускорении.\n\nautoplot(train_res, metric = \"f_meas\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),\n            angle = 90, hjust = 1.5) +\n  coord_cartesian(ylim = c(-0.3, NA))\n\n\n\n\n\n\n\n\nОтберем наилучшие результаты.\n\nrank_results(train_res, select_best = TRUE) |&gt; \n  print()\n\n# A tibble: 64 × 9\n   wflow_id   .config       .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 base_ridge Preprocessor… accura… 0.997 0.00103     5 recipe       mult…     1\n 2 base_ridge Preprocessor… f_meas  0.996 0.00132     5 recipe       mult…     1\n 3 base_svm   Preprocessor… accura… 0.994 0.00160     5 recipe       svm_…     2\n 4 base_svm   Preprocessor… f_meas  0.991 0.00178     5 recipe       svm_…     2\n 5 base_lasso Preprocessor… accura… 0.991 0.00162     5 recipe       mult…     3\n 6 base_lasso Preprocessor… f_meas  0.986 0.00263     5 recipe       mult…     3\n 7 pca_lasso  Preprocessor… accura… 0.929 0.00293     5 recipe       mult…     4\n 8 pca_lasso  Preprocessor… f_meas  0.882 0.00806     5 recipe       mult…     4\n 9 base_rda   Preprocessor… accura… 0.872 0.00740     5 recipe       disc…     5\n10 base_rda   Preprocessor… f_meas  0.881 0.00810     5 recipe       disc…     5\n# ℹ 54 more rows\n\n\nВзглянем на параметры наилучшей модели (в данном случае это штрафные коэффициенты).\n\nautoplot(train_res, id = \"base_ridge\") +\n  theme_light()\n\n\n\n\n\n\n\n\nФинализируем воркфлоу.\n\nbest_results &lt;- \n   train_res |&gt; \n   extract_workflow_set_result(\"base_ridge\") |&gt; \n   select_best(metric = \"accuracy\")\n\nprint(best_results)\n\n# A tibble: 1 × 2\n   penalty .config             \n     &lt;dbl&gt; &lt;chr&gt;               \n1 1.07e-10 Preprocessor1_Model1\n\n\nФункция extract_workflow() используется для извлечения конкретного workflow (модели) из набора train_res. Аргумент “base_ridge” — это имя модели (или ID), которую мы использовали при создании workflow_set. Таким образом, этот шаг извлекает сам workflow для модели “base_ridge”, включая препроцессинг и модель (ещё с неуточнёнными гиперпараметрами).\nФункция finalize_workflow() подставляет наилучшие значения гиперпараметров (например, penalty) в workflow.\nНаконец, last_fit() имитирует реальный процесс разработки модели: после настройки и выбора лучшей модели, мы обучаем её на всей обучающей выборке и оцениваем на ранее отложенной тестовой выборке.\n\nridge_res &lt;- train_res |&gt; \n  extract_workflow(\"base_ridge\") |&gt; \n  finalize_workflow(best_results) |&gt; \n  last_fit(split = data_split, metrics = metric_set(f_meas, accuracy, roc_auc))\n\nНа тестовой выборке наша модель отработала идеально!\n\ncollect_metrics(ridge_res) |&gt; \n  print()\n\n# A tibble: 3 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 f_meas   macro              1 Preprocessor1_Model1\n2 accuracy multiclass         1 Preprocessor1_Model1\n3 roc_auc  hand_till          1 Preprocessor1_Model1\n\n\n\ncollect_predictions(ridge_res) |&gt; \n  conf_mat(truth = author, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\"),\n        axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\n\ncollect_predictions(ridge_res) |&gt;\n  roc_curve(truth = author, .pred_Bulgakov:.pred_Turgenev) |&gt;\n  # или autoplot()\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#интерпретация-модели",
    "href": "multiclass.html#интерпретация-модели",
    "title": "25  Многоклассовая классификация",
    "section": "25.12 Интерпретация модели",
    "text": "25.12 Интерпретация модели\n\nfinal_model &lt;- extract_fit_parsnip(ridge_res)\n\n\ntop_terms &lt;- tidy(final_model) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  group_by(class) |&gt;                           \n  slice_max(abs(estimate), n = 7)  |&gt;             \n  ungroup()  |&gt; \n  mutate(term = fct_reorder(term, abs(estimate)))\n\nprint(top_terms)\n\n# A tibble: 56 × 4\n   class       term       estimate  penalty\n   &lt;chr&gt;       &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 Bulgakov    в            0.207  1.07e-10\n 2 Bulgakov    ответил      0.169  1.07e-10\n 3 Bulgakov    совершенно   0.145  1.07e-10\n 4 Bulgakov    затем        0.119  1.07e-10\n 5 Bulgakov    и            0.108  1.07e-10\n 6 Bulgakov    нужно        0.0949 1.07e-10\n 7 Bulgakov    снова       -0.0904 1.07e-10\n 8 Dostoyevsky вдруг        0.178  1.07e-10\n 9 Dostoyevsky всё          0.143  1.07e-10\n10 Dostoyevsky даже         0.143  1.07e-10\n# ℹ 46 more rows\n\n\n\ntop_terms  |&gt; \n  ggplot(aes(x = estimate, y = term, fill = class)) +\n  geom_col(show.legend = FALSE, alpha = 0.85) +\n  facet_wrap(~ class, scales = \"free_y\", nrow = 4) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Наиболее важные признаки для каждого автора\",\n    x = \"Коэффициент\",\n    y = \"Признак\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nУ Горького “Самгин”, у Шолохова – “Григорий”, вроде все логично. Или “совершенно” логично, как сказал бы Булгаков.\nОтличная работа 🏆 🏆 🏆",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "dnn.html#пакеты-и-виртуальное-окружение",
    "href": "dnn.html#пакеты-и-виртуальное-окружение",
    "title": "26  Глубокое обучение",
    "section": "26.2 Пакеты и виртуальное окружение",
    "text": "26.2 Пакеты и виртуальное окружение\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nconflicted::conflict_prefer(\"filter\", winner = \"dplyr\")\n\nДля работы нам понадобится дополнительно установить и загрузить несколько пакетов.\n\nПакет keras для R предоставляет удобный интерфейс для Keras, высокоуровневого API для создания нейронных сетей. Keras отвечает за компоненты глубокого обучения высокого уровня: слои, функции потерь, оптимизатор, метрики, обучающий цикл.\nKeras опирается на Tensorflow (доступный в R через одноименный пакет), который отвечает за низкоуровневые манипуляции с тензорами.\nПакет {reticulate} позволяет запускать Python-код прямо из R. Это обеспечивает интеграцию с Keras и Tensorflow: многие современные нейросетевые пакеты в R (в том числе {keras} и {tensorflow}) — всего лишь “обёртки” над Python-библиотеками.\n\n\nlibrary(keras3)\nlibrary(tensorflow)\nlibrary(reticulate)\n\nТеперь попробуем узнать, какая установлена версия Python на машине. При необходимости обновите.\n\npy_config()\n\n# python:         /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/bin/python3\n# libpython:      /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\n# pythonhome:     /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf:/Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf\n# virtualenv:     /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/bin/activate_this.py\n# version:        3.11.12 (main, Apr  9 2025, 03:49:53) [Clang 20.1.0 ]\n# numpy:          /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/lib/python3.11/site-packages/numpy\n# numpy_version:  2.1.3\n# keras:          /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/lib/python3.11/site-packages/keras\n# \n# NOTE: Python version was forced by py_require()\n\nУбедимся, что Питон работает. Если все ок, вы увидите число pi.\n\npy_run_string(\"import math; result = math.pi\")\npy$result\n\n[1] 3.141593\n\n\nПроверим наличие keras и tensorflow.\n\npy_module_available(\"keras\")\n\n[1] TRUE\n\npy_module_available(\"tensorflow\")\n\n[1] TRUE\n\n\nЕсли хоть один из них отсутствует, устанавливаем keras и tensorflow в текущее Python-окружение.\n\npy_install(c(\"keras\", \"tensorflow\"))\n\nЕсли вы используете эфемерное (временное) виртуальное окружение, которое управляется {reticulate} автоматически, то py_install() выдаст предупреждение и посоветует использовать py_require(), чтобы корректно установить или подключить пакеты без нарушения целостности окружения.\n\npy_require(c(\"keras\", \"tensorflow\"))\n\nЭто установит последние совместимые версии этих пакетов с помощью pip в вашу текущую виртуальную среду.\n\n\n\n\n\n\nНа заметку\n\n\n\nПакет {reticulate} в новых версиях может создавать временные virtualenv/conda окружения, которые управляются им автоматически — они не привязаны к системному Python и исчезают при завершении сессии (если явно не сохраняются).\n\n\nЕсли вы хотите не эфемерную, а постоянную виртуальную среду, можно создать её вручную:\n\n# Только один раз!\nvirtualenv_create(\"myenv\")\n\n# Активировать для reticulate\nuse_virtualenv(\"myenv\", required = TRUE)\n\n# Установить нужные модули\npy_install(c(\"keras\", \"tensorflow\"))\n\nТогда {reticulate} будет использовать стабильное окружение, которое сохранится между сессиями.\nУбедимся, что все работает.\n\npy_run_string(\"\nimport tensorflow as tf\nimport keras\n\nprint('TensorFlow version:', tf.__version__)\nprint('Keras version:', keras.__version__)\n\")\n\nTensorFlow version: 2.19.0\nKeras version: 3.10.0\n\n\nУра, победа 🎈🎉🎊",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#данные-категории-новостей",
    "href": "dnn.html#данные-категории-новостей",
    "title": "26  Глубокое обучение",
    "section": "26.3 Данные: категории новостей",
    "text": "26.3 Данные: категории новостей\n\nlibrary(textdata)\nag_news &lt;- textdata::dataset_ag_news()\nag_news\n\n\n  \n\n\n\n\nag_news |&gt;\n  count(class) |&gt;\n  mutate(class = forcats::fct_reorder(class, n)) |&gt;\n  ggplot(aes(x = class, y = n, fill = class)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nag_news |&gt;\n  mutate(text_length = nchar(description)) |&gt; \n  ggplot(aes(text_length, color = class)) +\n  geom_density() +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#разделение-данных",
    "href": "dnn.html#разделение-данных",
    "title": "26  Глубокое обучение",
    "section": "26.4 Разделение данных",
    "text": "26.4 Разделение данных\nФункция initial_validation_split() создает случайное разделение данных на три части: обучающую (training set), валидационную (validation set) и тестовую (testing set) выборки. Функции training(), validation() и testing() позволяют извлекать соответствующие подмножества данных после разбиения.\n\nset.seed(24052025)\ndata_split &lt;- ag_news |&gt; \n  mutate(class = as.factor(class)) |&gt; \n  initial_validation_split(strata = class)\ndata_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;72000/24000/24000/120000&gt;\n\n\n\ndata_train &lt;- training(data_split)\ndata_validate &lt;- validation(data_split)\ndata_test &lt;- testing(data_split)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#спецификация-модели-bow",
    "href": "dnn.html#спецификация-модели-bow",
    "title": "26  Глубокое обучение",
    "section": "26.7 Спецификация модели BOW",
    "text": "26.7 Спецификация модели BOW\nСоздаем пустую последовательную (sequential) модель. В последовательной модели слои идут один за другим, по порядку. Добавляем к ней два полносвязных (dense) слоя. Аргументом units = 64 указываем, что в первом и втором слое будет 64 нейрона. Число нейронов подбирается экспериментально. Наличие большей размерности (многомерное пространство представления) позволяет модели изучать более сложные представления, но делает модель более дорогостоящей в вычислительном отношении и может привести к переобучению (Шолле 2023, 149).\nАргумент activation = \"relu\" означает, что скрытые слои используют функцию активации relu (rectified linear unit, блок линейной ректификации). Эта функция преобразует отрицательные значения в ноль.\n\nБез функции активации, такой как relu (также называемой фактором нелинейности) полносвязный слой layer_dense будет состоять из двух линейных операций – скалярного произведения и сложения: output &lt;- dot(input, W) + b Такой слой может обучаться только на линейных (аффинных) преобразованиях входных данных: пространство гипотез слоя было бы совокупностью всех возможных линейных преобразований входных данных в n-мерное пространство. Такое пространство гипотез слишком ограничено, и наложение нескольких уровней представлений друг на друга не приносило бы никакой выгоды, потому что сколь угодно длинная последовательность линейных преобразований все равно остается линейным преобразованием. – (Шолле 2023, 151)\n\nПосле этого добавляем выходной слой. Здесь число нейронов соответствует числу предсказываемых классов, а активация softmax (activation = \"softmax\") превращает выходы нейронов в вероятности, сумма которых равна 1.\n\nbow_model &lt;- keras3::keras_model_sequential() |&gt; \n  layer_dense(units = 32, activation = \"relu\") |&gt; \n  layer_dense(units = 32, activation = \"relu\") |&gt; \n  layer_dense(units = 4, activation = \"softmax\")\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\nМодель готова к дальнейшему обучению и применению. Осталось выбрать функцию потерь и оптимизатор.\n\nbow_model  |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\nЗдесь compile() — функция компиляции. Она “собирает” модель для обучения: определяет, как будут считаться ошибки (функция потерь), какой алгоритм оптимизации использовать, и по каким метрикам отслеживать качество.\nОптимизатор Adam (аргумент optimizer = \"adam\") - один из самых популярных оптимизаторов в глубоком обучении. Adam автоматически подбирает скорость обучения для каждого параметра. Работает быстро и надёжно на большинстве задач — особенно если нет времени или желания подбирать сложные параметры вручную.\nПерекрестная энтропия (loss = \"categorical_crossentropy\") – функция потерь для задач многоклассовой классификации (multi-class classification). Эта функция подходит, когда на выходе модели softmax и целевая переменная — one-hot вектор.\n\n\n\n\n\n\nНа заметку\n\n\n\nПерекрестная энтропия (crossentropy) – это термин из области теории информации, обозначающий меру расстояния между распределениями вероятностей или, в данном случае, между фактическими данными и предсказаниями.\n\n\nТакже прописываем метрику качества работы модели.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#обучение-bow-модели",
    "href": "dnn.html#обучение-bow-модели",
    "title": "26  Глубокое обучение",
    "section": "26.8 Обучение BOW-модели",
    "text": "26.8 Обучение BOW-модели\nТеперь проведем обучение модели в течение 10 эпох (выполним 10 итераций по всем образцам обучающих данных) пакетами по 512 образцов.\nПакет (batch) - это небольшой набор образцов, которые одновременно обрабатываются моделью. Количество часто равно степени двойки, чтобы упростить выделение памяти на процессоре. В процессе обучения пакет используется для одного обновления градиентного спуска, применяемого к весам модели.\nЭпоха (epoch) — это один полный проход (прогон) по всему тренировочному датасету при обучении модели машинного обучения, например, нейронной сети. Например, если у вас есть 1000 картинок, а batch_size = 100, то за одну эпоху модель обработает все 1000 картинок по 100 за раз — всего 10 шагов (итераций). Модель обычно обучают несколько (десятков или сотен) эпох, чтобы она постепенно улучшала свои прогнозы.\nТакже будем следить за потерями и точностью на отложенных образцах.\n\nbow_history &lt;- bow_model |&gt; \n  fit(\n    x = train_bow_rec,\n    y = class_train_onehot,\n    batch_size = 512,\n    epochs = 10,\n    validation_data = list(valid_bow_rec, class_valid_onehot), \n    verbose = FALSE\n  )\n\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.9245\n        loss: 0.215\nval_accuracy: 0.8417\n    val_loss: 0.5269 \n\n\nПосле обучения в переменной bow_history сохраняется история процесса обучения: метрики, ошибки, прогресс и т.д. Взглянем на результат.\n\nplot(bow_history) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nbow_df &lt;- as.data.frame(bow_history)\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.9245\n        loss: 0.215\nval_accuracy: 0.8417\n    val_loss: 0.5269",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#основные-понятия",
    "href": "dnn.html#основные-понятия",
    "title": "26  Глубокое обучение",
    "section": "",
    "text": "Источник.\n\n\n\n\n\n\n\nИсточник: Шолле (2023)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-one-hot-последовательное-кодирование",
    "href": "dnn.html#препроцессинг-one-hot-последовательное-кодирование",
    "title": "26  Глубокое обучение",
    "section": "26.9 Препроцессинг: One-hot последовательное кодирование",
    "text": "26.9 Препроцессинг: One-hot последовательное кодирование\nstep_sequence_onehot() превращает токены в числовой формат аналогично step_tf() и step_tfidf(), но в отличие от них учитывает порядок следования токенов.\nРассмотрим на небольшом примере отсюда:\n\nsmall_data &lt;- tibble(text = c(\n  \"adventure dice game\",\n  \"spooky dice game\",\n  \"illustrated book of monsters\",\n  \"monsters, ghosts, goblins, me, myself and i\"\n))\n\nsmall_spec &lt;- recipe(~ text, data = small_data)  |&gt; \n  step_tokenize(text)  |&gt; \n  step_sequence_onehot(text, sequence_length = 6, prefix = \"\")\n\nprep(small_spec)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Tokenization for: text | Trained\n\n\n• Sequence 1 hot encoding for: text | Trained\n\n\n\nprep(small_spec)  |&gt; \n  # 2 означает второй шаг рецепта\n  tidy(2)\n\n\n  \n\n\n\n\nprep(small_spec) |&gt; \n  bake(new_data = NULL, composition = \"matrix\")\n\n     _text_1 _text_2 _text_3 _text_4 _text_5 _text_6\n[1,]       0       0       0       1       4       5\n[2,]       0       0       0      14       4       5\n[3,]       0       0       9       3      13      11\n[4,]       6       7      10      12       2       8\n\n\nВ четвертой строке первое слово = 6, а это не “монстры”! Так произошло, потому что предложение слишком длинное и не вмещается в длину кодируемой последовательности (ее регулирует аргумент sequence_length). В таком случае текст усекается (аргумент truncating по умолчанию имеет значение \"pre\", но можно изменить на \"post\"). В коротких текстах добавляются нули, за это отвечает параметр padding. Немного изменим рецепт:\n\nrecipe(~ text, data = small_data)  |&gt; \n  step_tokenize(text)  |&gt; \n  step_sequence_onehot(text, sequence_length = 6, \n                       prefix = \"\",\n                       padding = \"post\", \n                       truncating = \"post\")  |&gt; \n  prep()  |&gt; \n  bake(new_data = NULL, composition = \"matrix\")\n\n     _text_1 _text_2 _text_3 _text_4 _text_5 _text_6\n[1,]       1       4       5       0       0       0\n[2,]      14       4       5       0       0       0\n[3,]       9       3      13      11       0       0\n[4,]      11       6       7      10      12       2\n\n\nТеперь “монстры” в начале! А все нули сдвинулись вправо.\nТеперь напишем рецепт для новостного датасета.\n\nmax_words = 2e3\nmax_length = 150\n\nonehot_rec &lt;- recipe( ~ description, data = data_train)  |&gt;  \n  step_mutate(description = stringr::str_remove_all(description, \"\\\\d+\")) |&gt; \n  step_tokenize(description) |&gt;\n  step_stopwords(description) |&gt; \n  step_tokenfilter(description, \n                   max_tokens = max_words, \n                   min_times = 10) |&gt; \n  step_sequence_onehot(description, \n                       sequence_length = max_length,\n                       # потому что в новостях все самое важное обычно в начале\n                       truncating = \"post\",\n                       prefix = \"\")\n  \n\nonehot_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(description, \"\\\\d+\")\n\n\n• Tokenization for: description\n\n\n• Stop word removal for: description\n\n\n• Text filtering for: description\n\n\n• Sequence 1 hot encoding for: description\n\n\n\nonehot_prep &lt;- prep(onehot_rec)\nonehot_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 72000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~stringr::str_remove_all(description, \"\\\\d+\") |\n  Trained\n\n\n• Tokenization for: description | Trained\n\n\n• Stop word removal for: description | Trained\n\n\n• Text filtering for: description | Trained\n\n\n• Sequence 1 hot encoding for: description | Trained\n\n\n\nset.seed(25052025)\ntidy(onehot_prep, 5) |&gt; \n  sample_n(size = 10)\n\n\n  \n\n\n\n\nonehot_train &lt;- bake(onehot_prep, \n                     new_data = NULL, \n                     composition = \"matrix\")\n\nКоличество рядов в матрице соответствует числу наблюдений в обучающей выборке, а число столбцов – выбранной длине последовательности.\n\ndim(onehot_train)\n\n[1] 72000   150",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#полносвязная-нейросеть-на-основе-onehot",
    "href": "dnn.html#полносвязная-нейросеть-на-основе-onehot",
    "title": "26  Глубокое обучение",
    "section": "26.10 Полносвязная нейросеть на основе Onehot",
    "text": "26.10 Полносвязная нейросеть на основе Onehot\nНаша вторая модель глубокого обучения преобразует тексты в эмбеддинги, затем «расплющивает» их (делает одномерными), а после этого обучает полносвязный слой (dense network), чтобы предсказать класс новости.\n\ndense_model &lt;- keras_model_sequential() |&gt; \n  layer_embedding(input_dim = max_words + 1, \n                  output_dim = 12)  |&gt; \n  layer_flatten()  |&gt; \n  layer_dense(units = 32, activation = \"relu\")  |&gt; \n  layer_dense(units = 4, activation = \"softmax\")\n\n\ndense_model\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (Embedding)             │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ flatten (Flatten)                 │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_4 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\ndense_model |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#обучение-модели-на-основе-onehot-кодирования",
    "href": "dnn.html#обучение-модели-на-основе-onehot-кодирования",
    "title": "26  Глубокое обучение",
    "section": "26.11 Обучение модели на основе Onehot-кодирования",
    "text": "26.11 Обучение модели на основе Onehot-кодирования\n\ndense_history &lt;- dense_model  |&gt; \n  fit(\n  x = onehot_train,\n  y = class_train_onehot,\n  batch_size = 512,\n  epochs = 10,\n  #validation_data = list(onehot_valid, class_valid_onehot), \n  # заметьте еще один способ использовать часть данных для валидации\n  validation_split = 0.25, \n  verbose = FALSE\n)\n\n\nplot(dense_history) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndense_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.8991\n        loss: 0.2884\nval_accuracy: 0.8539\n    val_loss: 0.3992",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-onehot-кодирование",
    "href": "dnn.html#препроцессинг-onehot-кодирование",
    "title": "26  Глубокое обучение",
    "section": "26.9 Препроцессинг: Onehot-кодирование",
    "text": "26.9 Препроцессинг: Onehot-кодирование\nstep_sequence_onehot() превращает токены в числовой формат аналогично step_tf() и step_tfidf(), но в отличие от них учитывает порядок следования токенов.\nРассмотрим на небольшом примере отсюда:\n\nsmall_data &lt;- tibble(text = c(\n  \"adventure dice game\",\n  \"spooky dice game\",\n  \"illustrated book of monsters\",\n  \"monsters, ghosts, goblins, me, myself and i\"\n))\n\nsmall_spec &lt;- recipe(~ text, data = small_data)  |&gt; \n  step_tokenize(text)  |&gt; \n  step_sequence_onehot(text, sequence_length = 6, prefix = \"\")\n\nprep(small_spec)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Tokenization for: text | Trained\n\n\n• Sequence 1 hot encoding for: text | Trained\n\n\n\nprep(small_spec)  |&gt; \n  # 2 означает второй шаг рецепта\n  tidy(2)\n\n\n  \n\n\n\n\nprep(small_spec) |&gt; \n  bake(new_data = NULL, composition = \"matrix\")\n\n     _text_1 _text_2 _text_3 _text_4 _text_5 _text_6\n[1,]       0       0       0       1       4       5\n[2,]       0       0       0      14       4       5\n[3,]       0       0       9       3      13      11\n[4,]       6       7      10      12       2       8\n\n\nВ четвертой строке первое слово = 6, а это не “монстры”! Так произошло, потому что предложение слишком длинное и не вмещается в длину кодируемой последовательности (ее регулирует аргумент sequence_length). В таком случае текст усекается (аргумент truncating по умолчанию имеет значение \"pre\", но можно изменить на \"post\"). В коротких текстах добавляются нули, за это отвечает параметр padding. Немного изменим рецепт:\n\nrecipe(~ text, data = small_data)  |&gt; \n  step_tokenize(text)  |&gt; \n  step_sequence_onehot(text, sequence_length = 6, \n                       prefix = \"\",\n                       padding = \"post\", \n                       truncating = \"post\")  |&gt; \n  prep()  |&gt; \n  bake(new_data = NULL, composition = \"matrix\")\n\n     _text_1 _text_2 _text_3 _text_4 _text_5 _text_6\n[1,]       1       4       5       0       0       0\n[2,]      14       4       5       0       0       0\n[3,]       9       3      13      11       0       0\n[4,]      11       6       7      10      12       2\n\n\nТеперь “монстры” в начале! А все нули сдвинулись вправо.\nТеперь напишем рецепт для новостного датасета.\n\nmax_words = 1500\nmax_length = 150\n\nonehot_rec &lt;- recipe( ~ description, data = data_train)  |&gt;  \n  step_mutate(description = stringr::str_remove_all(description, \"\\\\d+\")) |&gt; \n  step_tokenize(description) |&gt;\n  step_stopwords(description) |&gt; \n  step_tokenfilter(description, \n                   max_tokens = max_words, \n                   min_times = 10) |&gt; \n  step_sequence_onehot(description, \n                       sequence_length = max_length,\n                       # потому что в новостях все самое важное обычно в начале\n                       truncating = \"post\",\n                       prefix = \"\")\n  \n\nonehot_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(description, \"\\\\d+\")\n\n\n• Tokenization for: description\n\n\n• Stop word removal for: description\n\n\n• Text filtering for: description\n\n\n• Sequence 1 hot encoding for: description\n\n\n\nonehot_prep &lt;- prep(onehot_rec)\nonehot_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 72000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~stringr::str_remove_all(description, \"\\\\d+\") |\n  Trained\n\n\n• Tokenization for: description | Trained\n\n\n• Stop word removal for: description | Trained\n\n\n• Text filtering for: description | Trained\n\n\n• Sequence 1 hot encoding for: description | Trained\n\n\n\nset.seed(25052025)\ntidy(onehot_prep, 5) |&gt; \n  sample_n(size = 10)\n\n\n  \n\n\n\n\nonehot_train &lt;- bake(onehot_prep, \n                     new_data = NULL, \n                     composition = \"matrix\")\n\nКоличество рядов в матрице соответствует числу наблюдений в обучающей выборке, а число столбцов – выбранной длине последовательности.\n\ndim(onehot_train)\n\n[1] 72000   150\n\n\nТакже подготовим валидационную выборку.\n\nonehot_valid &lt;- bake(onehot_prep, \n                     new_data = data_validate, \n                     composition = \"matrix\")\n\n\ndim(onehot_valid)\n\n[1] 24000   150\n\n\n\nonehot_test &lt;- bake(onehot_prep, \n                     new_data = data_test, \n                     composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#прогнозирование-и-оценка",
    "href": "dnn.html#прогнозирование-и-оценка",
    "title": "26  Глубокое обучение",
    "section": "26.12 Прогнозирование и оценка",
    "text": "26.12 Прогнозирование и оценка\nГрафик выше показывает, что переобучение начинается после третьей эпохи, поэтому обучим модель с нуля в течение трех эпох и затем оценим ее на контрольных данных.\n\ndense_model &lt;- keras_model_sequential() |&gt; \n  layer_embedding(input_dim = max_words + 1, \n                  output_dim = 12)  |&gt; \n  layer_flatten()  |&gt; \n  layer_dense(units = 32, activation = \"relu\")  |&gt; \n  layer_dense(units = 4, activation = \"softmax\")\n\ndense_model |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n  )\n\ndense_model  |&gt; \n  fit(\n  x = onehot_train,\n  y = class_train_onehot,\n  batch_size = 512,\n  epochs = 3\n)\n\nEpoch 1/3\n141/141 - 1s - 5ms/step - accuracy: 0.6361 - loss: 0.9703\nEpoch 2/3\n141/141 - 1s - 4ms/step - accuracy: 0.8421 - loss: 0.4596\nEpoch 3/3\n141/141 - 1s - 4ms/step - accuracy: 0.8588 - loss: 0.4126\n\ndense_model  |&gt;  evaluate(\n  x = onehot_test,\n  y = class_test_onehot\n)\n\n750/750 - 0s - 486us/step - accuracy: 0.8576 - loss: 0.4132\n\n\n$accuracy\n[1] 0.8575833\n\n$loss\n[1] 0.4132389\n\n\nПростейшая модель позволила нам добиться точности 86%. Этот результат можно улучшить, но пока используем модель для генерации предсказаний.\n\ntest_pred &lt;- dense_model |&gt; \n  predict(onehot_test,\n          verbose = FALSE)\n\nhead(test_pred)\n\n          [,1]       [,2]         [,3]        [,4]\n[1,] 0.9813854 0.01422692 1.519543e-06 0.004386192\n[2,] 0.7784712 0.22033687 2.915277e-05 0.001162768\n[3,] 0.2074653 0.68597001 4.245473e-02 0.064110003\n[4,] 0.7616387 0.19196506 1.585766e-03 0.044810418\n[5,] 0.9759125 0.01183348 1.245316e-05 0.012241541\n[6,] 0.2227788 0.42378920 6.409746e-02 0.289334506\n\n\nИзвлечем индексы классов с наибольшей вероятностью.\n\npredicted_classes &lt;- apply(test_pred, 1, which.max)  \nhead(predicted_classes)\n\n[1] 1 1 2 1 1 2\n\n\nВосстановим исходные названия классов.\n\nclass_levels &lt;- levels(data_train$class)\n\n# Преобразуем индексы в фактор с исходными метками\npredicted_labels &lt;- factor(predicted_classes, \n                          levels = 1:4,\n                          labels = class_levels)\n\nhead(predicted_labels)\n\n[1] Business Business Sci/Tech Business Business Sci/Tech\nLevels: Business Sci/Tech Sports World\n\n\n\npred_tbl &lt;- tibble(truth = data_test$class,\n                   prediction = predicted_labels)\n\npred_tbl\n\n\n  \n\n\n\n\nmetrics(pred_tbl, truth = truth, estimate = prediction)\n\n\n  \n\n\n\n\npred_tbl |&gt; \n  conf_mat(truth, prediction) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(\n    low = \"#E3ECF6\",   # очень светлый голубой\n    high = \"#758CA2\"   # спокойный темно-голубой/серо-синий\n  )\n\n\n\n\n\n\n\n\nПреобразуем матрицу вероятностей в tibble с именами классов.\n\nprob_tbl &lt;- as_tibble(test_pred) |&gt; \n  set_names(paste0(\".pred_\", class_levels))  \n\nОбъединяем с истинными метками.\n\nroc_data &lt;- bind_cols(\n  truth = data_test$class,\n  prob_tbl\n)\n\nroc_data\n\n\n  \n\n\n\n\nroc_data |&gt; \n  roc_curve(truth = truth, .pred_Business:.pred_World) |&gt; \n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#итоги-урока",
    "href": "dnn.html#итоги-урока",
    "title": "26  Глубокое обучение",
    "section": "26.13 Итоги урока",
    "text": "26.13 Итоги урока\nПоздравляем! Вы успешно построили и оценили две нейросетевые архитектуры для классификации текстов. 🎉\nВ этом уроке вы…\n\nОсвоили разные подходы к представлению текста: BOW (мешок слов) и последовательное one-hot кодирование\nИзучили архитектурные принципы: научились создавать Embedding + Dense слои\nПоняли важность борьбы с переобучением\nНаучились оценивать модели, в том числе Построили ROC-кривые для многоклассовой задачи и визуализировали матрицу ошибок\nПознакомились с новыми инструментами: {keras3} и {tensorflow}\nИнтегрировали Python-библиотеки через {reticulate}\n\nЭтот урок — ваша отправная точка в мире глубокого обучения для NLP. Каждый из использованных компонентов открывает путь к более сложным и мощным моделям. Продолжайте экспериментировать, и пусть ваши нейросети становятся все умнее! 🚀\n\n\n\n\nШолле, Франсуа. 2023. Глубокое обучение с R и Keras. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "27  Приложения Shiny",
    "section": "",
    "text": "27.1 Создание директории и файла приложения\nЗапустить приложение можно кнопкой Run App.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "shiny.html#создание-директории-и-файла-приложения",
    "href": "shiny.html#создание-директории-и-файла-приложения",
    "title": "27  Приложения Shiny",
    "section": "",
    "text": "File -&gt; New Project -&gt; New Directory -&gt; Shiny Application\nФайл App.R содержит скрипт, который\n\n\nопределяет пользовательский интерфейс - страницу html, с которой будет взаимодействовать пользователь\nформирует поведение приложения путем определения функции server\nвызывает функцию shiny(ui, server) для сборки и запуска приложения\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nПри запущенном приложении оболочка R переходит в состояние занятости: командная строка не видна, а на панели инструментов в консоли показывается иконка с символом остановки.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "shiny.html#элементы-пользовательского-интерфейса",
    "href": "shiny.html#элементы-пользовательского-интерфейса",
    "title": "27  Приложения Shiny",
    "section": "27.2 Элементы пользовательского интерфейса",
    "text": "27.2 Элементы пользовательского интерфейса\n\n27.2.1 Макет и заголовки\nВ созданном автоматически файле вы видите следующее.\n\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\nЗдесь:\n\nfluidPage() - функция разметки, отвечающая за визуальную структуру приложения. Внутрь кладём всё, что хотим увидеть на экране. Обычно это какой-то Input, с которым взаимодействует пользователь, и какой-то Output.\ntitlePanel() отвечает за заголовок.\nsidebarLayout(...) делит экран на две основные части: узкая панель с элементами управления (слева) и главная панель.\nsidebarPanel(...) отвечает за боковую панель. В ней мы видимsliderInput() - это ползунок.\nmainPanel(...) - это главная, большая панель. В ней будет что-то отображаться. Например, график: plotOutput().\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nИсправьте заголовок на \"📰 Классификатор новостей\". Так будет называться наше приложение.\nДобавьте заголовок боковой панели, используя теги: tags$h4(\"Вставьте или напечатайте новость:\"). Теги Shiny соответствуют тегам html.\nДобавьте заголовок главной панели tags$h3(\"Результат классификации).\nНе забывайте про запятые между функциями! (RStudio будет напоминать).\nИзмените ширину боковой панели. Посмотрите документацию к функции sidebarLayout().\n\n\n\nЗапустите приложение еще раз и посмотрите, что получилось.\n\n\n27.2.2 Элементы ввода\nПолный список элементов ввода доступен по ссылке.\nНебольшие фрагменты текста удобно обрабатывать при помощи функции textInput(), а если вы хотите, чтобы пользователь ввел один или несколько абзацев, используйте textAreaInput(). Для нашего классификатора подойдет последняя. Добавьте ее вместо ползунка:\n\ntextAreaInput(\"user_text\", \n              NULL, \n              placeholder = \"Введите текст новости здесь...\", \n              rows = 6)\n\nДля сравнения добавьте рядом (чуть позже мы это уберем):\n\ntextInput(\"user_name\", \n          # заметьте положение вопроса\n          \"Как вас зовут?\")\n\n\n\n\n\n\n\nЗадание\n\n\n\nСнова посмотрите, что получилось. Обратите внимание на то, что происходит при этом с выводом.\n\n\n\n\n27.2.3 Кнопки\nДля подтверждения действия пользователю можно дать в распроряжение кнопку или ссылку с помощью функций actionButton() или actionLink().\nДобавьте под областью ввода текста:\n\nactionButton(\"predict_btn\", \"🔍 Предсказать категорию\", class = \"btn-primary\")\n\nОбычно кнопки и ссылки работаю в паре с функциями observeEvent() или eventReactive().\nФункция observeEvent() используется для выполнения “побочных действий”, например, печати в консоль, записи файла, запуска функций без прямого вывода в интерфейс. Это функция для действий, она не возвращает данных.\nФункция eventReactive() используется для создания реактивного значения (которое возвращает значение, и этим значением можно пользоваться в других частях Shiny, например для построения графика).\nВ последних версиях Shiny обе функции заменяет bindEvent(). Пока мы не использовали эти функции, так что наша кнопка бездействует.\nВы можете настроить внешний вид кнопок по своему желанию, передав в качестве аргумента class одно из следующих значений: \"btn-primary\", \"btn-success\",\"btn-info\",\"btn-warning\",\"btn-danger\". Вы также можете изменить размер кнопки при помощи значений \"btn-lg\", \"btn-lg\" или \"btn-lg\". Наконец, вы можете заставить кнопку занять всю свободную ширину внутри элемента, в который она встроена, используя значение \"btn-block\".\n\n\n27.2.4 Элементы вывода\nЭлементы вывода (output) представляют собой своеобразные заглушки в интерфейсе пользователя, которые при необходимости заполняются с помощью функции server().\nКак и элементы ввода, элементы вывода принимают идентификатор в качестве обязательного первого аргумента. Если в пользовательском интерфейсе есть элемент с идентификатором “plot” (или любым другим!), в серверной части приложения обращаться к нему можно будет по имени output$plot.\nКаждая функция вывода в клиентской части сопоставляется с функцией отображения в серверной. Существует три основных типа вывода: текст, таблицы, графики.\nНачнем с текста. Добавьте в серверную часть (вместо функции, которая генерирует гистограмму):\n\noutput$user_name &lt;- renderText(paste(\"Привет, \", input$user_name, \"!\"))\n\nФункция renderText() собирает результат в строку и обычно применяется в паре с функцией textOutput(). Добавьте вывод на главную панель в пользовательском интерфейсе вместо plotOutput():\n\ntextOutput(\"user_name\")\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите исправленное приложение. Если все верно, то оно должно поприветствовать вас по имени.\n\n\nОбратите внимание: кнопка все еще бездействует, вывод обновляется реактивно.\n\n\n\n\n\n\nНа заметку\n\n\n\nРеактивное программирование - это стиль программирования, при котором данные и вычисления автоматически обновляются в ответ на изменения входных данных. В таком подходе вы описываете, какие элементы приложения зависят от каких входов, а Shiny сам следит за изменениями и пересчитывает то, что нужно обновить.\n\n\nДля того, чтобы кнопка “заработала”, необходимо внести изменения в серверную часть:\n\nserver &lt;- function(input, output) {\n  output$salutation &lt;- renderText(paste(\"Привет, \", input$user_name, \"!\")) |&gt; \n  bindEvent(input$predict_btn)\n}\n\nВ интерфейсе замените, соответственно, вывод на textOutput(\"salutation\").\nТеперь разберемся, что здесь происходит. Когда вы пишете output$salutation, вы определяете реактивный выход - объект, который потом будете выводить на UI с помощью textOutput(\"salutation\"). Функция renderText() которая возвращает текст для отображения. Внутри paste(\"Привет, \", input$user_name, \"!\") вставляет имя, который ввел пользователь. Наконец, функция функция bindEvent() (с пайпом |&gt;) говорит Shiny, что обновлять (пересчитыать) выходной текст надо только тогда, когда нажата кнопка с id “predict_btn”. Теперь только нажатие кнопки вызывает обновление вывода. Это понадобится нам чуть позже.\nЕсли все получилось и все понятно, можно удалить лишние строчки кода, которые не нужны для нашего приложения. На этом этапе у вас должно получиться вот что:\n\nlibrary(shiny)\n\n# пользовательский интерфейс\nui &lt;- fluidPage(\n\n    # название приложения\n    titlePanel(\"📰 Классификатор новостей\"),\n\n    # макет\n    sidebarLayout(\n      \n        sidebarPanel(\n            width = 6,\n            tags$h4(\"Вставьте или напечатайте новость:\"),\n            textAreaInput(\"user_text\", \n                          NULL, \n                          placeholder = \"Введите текст новости здесь...\", \n                          rows = 6),\n            actionButton(\"predict_btn\", \n                         \"🔍 Предсказать категорию\", \n                         class = \"btn-primary\")\n            ),\n        \n        mainPanel(\n           width = 6,\n           tags$h3(\"Результаты классификации\"),\n           \n        )\n    )\n)\n\n# сервер (пока пустой)\nserver &lt;- function(input, output) {\n  \n  # пока пусто\n  \n}\n\n# поехали! \nshinyApp(ui = ui, server = server)",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "shiny.html#сервер",
    "href": "shiny.html#сервер",
    "title": "27  Приложения Shiny",
    "section": "27.3 Сервер",
    "text": "27.3 Сервер\nВ предыдущем уроке мы обучили нейросетевую модель, которая предсказывает категорию новости по ее тексту. Если вы не сохранили результат, то препроцессор и модель надо забрать по ссылкам и положить в директорию с приложением.\nПосле этого прочитайте в окружение данные и загрузите нужные пакеты (пока можно это сделать в отдельном скрипте, потом добавим в приложение):\n\nlibrary(keras3)\nonehot_rec &lt;- readRDS(\"onehot_prep.rds\")\nmodel &lt;- load_model(\"my_dense_model.keras\")\n\nТакже нам понадобятся имена для классов: они соответствуют уровням фактора, который мы создали в прошлый раз при помощи as.factor(class):\n\nclass_names &lt;- c(\"Business\", \"Sci/Tech\", \"Sports\", \"World\") \n\nТеперь воспользуемся этим, чтобы получить:\n\nпредсказание для пользовательского текста;\nвектор вероятностей для каждой категории;\nвизуализацию вероятностей.\n\nКак мы помним, обученный рецепт можно использовать вместе с bake() на новых данных, но для этого строку, которую введет пользователь, нужно преобразовать в тиббл с теми же названиями столбцов, которые ожидает препроцессор.\n\nlibrary(tidyverse)\nlibrary(recipes, quietly = TRUE)\nlibrary(textrecipes)\n\n\ntext &lt;- \"A British man has been arrested after he drove his car into a crowd of Liverpool FC football fans celebrating their team's Premier League\"\n\nnew_data &lt;- tibble(description = text)\nmodel_input &lt;- bake(onehot_rec, \n                new_data = new_data,\n                composition = \"matrix\")\nprobs &lt;- as.numeric(model |&gt; predict(model_input))\npred_cat &lt;- class_names[which.max(probs)]\nnames(probs) &lt;-  class_names\n\nprobs\n\nТеперь нам надо добавить все эти вычисления на сервер.\n\nlibrary(shiny)\nlibrary(keras3)\nlibrary(recipes)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(textrecipes)\n\n\n# Загрузка обученного препроцессора и модели\nonehot_rec &lt;- readRDS(\"onehot_prep.rds\")\nmodel &lt;- load_model(\"my_dense_model.keras\")\n\n# Список названий классов \nclass_names &lt;- c(\"Business\", \"Sci/Tech\", \"Sports\", \"World\") \n\n# пользовательский интерфейс\nui &lt;- fluidPage(\n\n    # название приложения\n    titlePanel(\"📰 Классификатор новостей\"),\n\n    # макет\n    sidebarLayout(\n        sidebarPanel(\n            width = 6,\n            tags$h4(\"Вставьте или напечатайте новость:\"),\n            textAreaInput(\"user_text\", NULL, placeholder = \"Введите текст новости здесь...\", rows = 6),\n            actionButton(\"predict_btn\", \"🔍 Предсказать категорию\", class = \"btn-primary\")\n            ),\n        mainPanel(\n           width = 6,\n           tags$h3(\"Результаты классификации\"),\n           textOutput(\"result_text\")\n        )\n    )\n)\n\n# сервер \nserver &lt;- function(input, output) {\n  \n  pred_result &lt;- reactive({\n    req(input$user_text)\n    new_data &lt;- tibble(description = input$user_text)\n    model_input &lt;- bake(onehot_rec, \n                        new_data = new_data,\n                        composition = \"matrix\")\n    probs &lt;- as.numeric(model |&gt; predict(model_input))\n    pred_cat &lt;- class_names[which.max(probs)]\n    list(\n      category = pred_cat,\n      probs = setNames(probs, class_names)\n    )\n  }) |&gt; bindEvent(input$predict_btn)\n  \n  \n  output$result_text &lt;- renderText({\n    req(pred_result())\n      paste0(\n        \"🌟 Предсказанная категория: \", pred_result()$category)\n  })\n}\n\n# поехали! \nshinyApp(ui = ui, server = server)\n\nФункция reactive() означает, что мы используем реактивное выражение. Реактивные выражения — это особые части кода, которые автоматически пересчитываются, когда зависящие от них переменные изменяются.\nРеактивные выражения нужны, когда вы хотите: - Выполнить вычисления, которые используете несколько раз, не повторяя один и тот же код; - Эффективно управлять зависимостями и пересчётами: Shiny будет хранить результат вычисления, и пересчитывать, только когда реально изменились входные значения.\nВ нашем случае реактивное выражение считает результат, только когда пользователь нажимает кнопку, за связь с кнопкой отвечает bindEvent(). сли бы этого не было, приложение реагировало бы на каждый введённый символ!\nВызов req(input$user_text) - это проверка. Если поле ввода пустое, дальше ничего не происходит. Иными словами, req() останавливает выполнение реактивного выражения, если в него передано NULL, FALSE и т.п. Это гарантирует, что ваш код не будет выполняться при отсутсвии необходимых данных.\nНа шаге new_data &lt;- tibble(description = input$user_text) оборачиваем введённый текст в табличку, чтобы дальше передать в препроцессор.\nВ конце вызываем list(...), который возвращает список с двумя значениями:\n- category — категория с самой высокой вероятностью. - probs — вектор вероятностей для всех четырёх классов.\nПочему список? В реактивных выражениях, как и в базовых функциях, можно вернуть только один объект. Чтобы иметь возможность обращаться к разным значениям внутри реактива, их удобно объединить в список.\nНаконец, output$result_text – это то, что будет отображено в textOutput(\"result_text\") на главной странице приложения. Все внутри renderText({...}) – это реактивно пересчитываемый текст, который появится при обновлении pred_result().\nИтак, как всё работает вместе?\n\nПользователь вводит текст и нажимает кнопку.\nТолько в этот момент (!) вычисляется реактивное выражение pred_result:\n\nТекст подготавливается, обрабатывается препроцессором, подаётся модели.\nПолучается вектор вероятностей по имеющимся категориям.\nОпределяется категория с максимальной вероятностью.\nРезультат пакуется в список.\n\nЗначения из pred_result автоматически (реактивно!) используются в части вывода:\n\nТекстовое поле показывает предсказанную категорию.\n\n\n(Пользователь вводит текст)\n       │\n       ▼\n(Жмёт кнопку)\n       │\n       ▼\npred_result (реактивное выражение):\n ├─ 1. Обработка текста\n ├─ 2. Векторизация/onehot\n ├─ 3. Предсказание нейросетью\n └─ 4. Формирование списка с вероятностями и категорией\n       │\n       ▼\noutput$result_text (реактивный вывод)\nВместо простого текста можно использовать html-код. В таком случае вместо renderText() и textOutput() используем renderUI() и uiOutput(), например:\n\n# в серверной части\noutput$result_text &lt;- renderUI({\n    req(pred_result())\n    HTML(\n      paste0(\n        \"&lt;h4&gt;🌟 Предсказанная категория: &lt;span style='color:#0072B2;'&gt;\", pred_result()$category, \"&lt;/span&gt;&lt;/h4&gt;\"\n      )\n    )\n  })\n\n\n# в пользовательском интерфейсе\nuiOutput(\"result_text\")\n\nТеперь попробуем усовершенстовать наше приложение, добавив график.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "shiny.html#оформление",
    "href": "shiny.html#оформление",
    "title": "27  Приложения Shiny",
    "section": "27.5 Оформление",
    "text": "27.5 Оформление",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "shiny.html#публикация",
    "href": "shiny.html#публикация",
    "title": "27  Приложения Shiny",
    "section": "27.6 Публикация",
    "text": "27.6 Публикация",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "shiny.html#добавление-графика",
    "href": "shiny.html#добавление-графика",
    "title": "27  Приложения Shiny",
    "section": "27.4 Добавление графика",
    "text": "27.4 Добавление графика\nВот так мы бы визуализировали вероятности вне приложения:\n\ntibble(category = class_names, probability = probs)  |&gt;  \n  ggplot(aes(y = reorder(category, probability), x = probability, fill = category)) +\n  geom_col(width = 0.6, show.legend = FALSE) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(x = \"Вероятность\", y = \"Категория\") +\n  theme_minimal(base_size = 15) +\n  theme(\n    axis.title.y = element_blank(),\n    plot.title = element_text(face=\"bold\"),\n    axis.text = element_text(size=12)\n  )\n\nНа сервере почти все то же самое, но оборачиваем в реактивное выражение и проверяем наличие вероятностей. Код ниже нужно добавить на сервер.\n\noutput$prob_plot &lt;- renderPlot({\n    req(pred_result())\n    tibble(category = class_names,\n      probability = pred_result()$probs)  |&gt; \n      ggplot(aes(y = reorder(category, probability), x = probability, fill = category)) +\n      geom_col(width = 0.6, show.legend = FALSE) +\n      scale_fill_brewer(palette = \"Set2\") +\n      scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n      labs(x = \"Вероятность\", y = \"Категория\") +\n      theme_minimal(base_size = 15) +\n      theme(\n        axis.title.y = element_blank(),\n        plot.title = element_text(face=\"bold\"),\n        axis.text = element_text(size=12)\n      )\n  })",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Приложения Shiny</span>"
    ]
  },
  {
    "objectID": "01_start.html",
    "href": "01_start.html",
    "title": "1  Начало работы",
    "section": "",
    "text": "1.1 Установка R и RStudio\nМы будем использовать R, так что для занятий понадобятся:\nВместо RStudio можно поставить VS Code или Positron. По сути, Positron – это тот же VS Code, но без необходимости устанавливать расширения.\nМы будем использовать следующую версию R:\nR version 4.5.0 (2025-04-11)\nДля работы в облаке ☁️ можно использовать RStudio Cloud, но в бесплатной версии есть ограничения.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#установка-r-и-rstudio",
    "href": "01_start.html#установка-r-и-rstudio",
    "title": "1  Начало работы",
    "section": "",
    "text": "R\n\nна Windows\nна Mac\nна Linux.\n\nRStudio — IDE для R (можно скачать здесь)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#знакомство-с-rstudio",
    "href": "01_start.html#знакомство-с-rstudio",
    "title": "1  Начало работы",
    "section": "1.2 Знакомство с RStudio",
    "text": "1.2 Знакомство с RStudio\nRStudio — основная среда разработки (IDE) для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:\n\n\n\nRStudio при первом открытии\n\n\nПосле нажатия на двойное окошко чуть левее надписи Environment откроется окно скрипта.\n\n\n\nПодокна RStudio\n\n\nВсе следующие команды можно:\n\nвводить в окне консоли, и тогда для исполнения следует нажимать клавишу Enter.\nвводить в окне скрипта, и тогда для исполнения следует нажимать клавиши Ctrl/Cmd + Enter или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять Ctrl/Cmd + S.\n\nДля начала попробуйте получить информацию о сессии, введя в консоли такую команду:\n\nsessionInfo()\n\nsessionInfo() – это функция. О функциях можно думать как о глаголах (“сделай то-то!”). За названием функции всегда следуют круглые скобки, внутри которых могут находиться аргументы функции. Аргументы – это что-то вроде дополнений и обстоятельств. Аргументы могут быть обязательные и необязательные. Чтобы узнать, каких аргументов требует функция, надо вызывать help: ?mean(). В правой нижней панели появится техническая документация. Но также можно воспользоваться функцией args(). Попробуйте набрать в консоли args(round).\n\n\n\n\n\n\nВопрос\n\n\n\nСколько аргументов функции round() имеют значения по умолчанию?\n\n\nОтвет:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#пакеты",
    "href": "01_start.html#пакеты",
    "title": "1  Начало работы",
    "section": "1.3 Пакеты",
    "text": "1.3 Пакеты\nПосле установки R вы получите доступ к уже готовым методам статистического анализа и инструментам для визуализации. Если в базовой инсталляции R нет нужного решения – надо поискать в библиотеке пакетов. Пакет – это набор функций и иногда датасетов, созданный пользователями. На 1 июля 2023 г. в репозитории CRAN доступно 19789 пакетов. И это далеко не все: многие пакеты доступны только на GitHub.\n\n\n\n\n\n\nНа заметку\n\n\n\nНекоторые функции, которые вы найдете в пакетах, частично дублируют друг друга – это нормально, как и в естественном языке, “сказать” что-то можно разными способами.\n\n\nПо технической документации и так называемым “виньеткам” можно понять, какой пакет вам нужен. Например, вот так выглядит виньетка пакета RPerseus, при помощи которого можно получить доступ к корпусу греческой и латинской литературы.\nБывают еще “пакеты пакетов”, то есть очень большие семейства функций, своего рода “диалекты” R. Таково семейство tidyverse, объединяемое идеологией “опрятных” данных. Про него мы еще будем говорить.\nПакеты для работы устанавливаются один раз, однако подключать их надо во время каждой сессии. Чтобы установить новый пакет, можно воспользоваться меню Tools &gt; Install Packages. Также можно устанавливать пакеты из консоли. Установим пакет с интерактивными уроками программирования на языке R:\n\ninstall.packages(\"swirl\")\n\nДля подключения используем функцию library(), которой передаем в качестве аргумента название пакета без кавычек:\n\nlibrary(swirl)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#рабочая-директория",
    "href": "01_start.html#рабочая-директория",
    "title": "1  Начало работы",
    "section": "1.4 Рабочая директория",
    "text": "1.4 Рабочая директория\nПеред началом работы проверьте свою рабочую директорию при помощи getwd(). Для смены можно использовать как абсолютный, так и относительный путь:\n\nsetwd(\"/Users/name/folder\")\n\n# искать в текущей директории\nsetwd(\"./folder\")\n\n# перейти на уровень вверх\nsetwd(\"../\")\n\nТакже для выбора рабочей директории можно использовать меню R Session &gt; Set Working Directory.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#r-как-калькулятор",
    "href": "01_start.html#r-как-калькулятор",
    "title": "1  Начало работы",
    "section": "1.5 R как калькулятор",
    "text": "1.5 R как калькулятор\nМожно использовать R как калькулятор. Для этого вводим данные рядом с символом приглашения &gt;, который называется prompt.\n\nsqrt(4) # квадратный корень\n\n[1] 2\n\n2^3 # степень\n\n[1] 8\n\nlog10(100) #логарифм\n\n[1] 2\n\n\nЕсли в начале консольной строки стоит +, значит предыдущий код не завершен. Например, вы забыли закрыть скобку функции. Ее можно дописать на следующей строке. Попробуйте набрать sqrt(2 в консоли.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#операторы-присваивания",
    "href": "01_start.html#операторы-присваивания",
    "title": "1  Начало работы",
    "section": "1.6 Операторы присваивания",
    "text": "1.6 Операторы присваивания\nЧтобы в окружении появился новый объект, надо присвоить результат вычислений какой-нибудь переменной при помощи оператора присваивания &lt;- (Alt + - (Windows) или Option + - (Mac)). Знак = также работает как оператор присваивания, но не во всех контекстах, поэтому им лучше не пользоваться.\n\nx &lt;- 2 + 2 # создаем переменную\ny &lt;- 0.1 # создаем еще одну переменную\nx &lt;- y # переназначаем  \nx + y\n\n[1] 0.2\n\n\nСочетание клавиш для оператора присваивания: Option/Alt + -. Имя переменной, как и имя функции, может содержать прописные и строчные буквы, точку и знак подчеркивания.\nТеперь небольшое упражнение.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс программирования на R: install_course(\"R Programming\"). После этого привяжите пакет командой library(swirl) и наберите: swirl(). Укажите ваше имя. Пройдите урок 1 Basic Building Blocks.\n\n\nЕсли все получилось, можно двигаться дальше! Но сначала зафиксируем несколько новых функций из этих первого урока.\n\n\n\n\n\n\nВопрос\n\n\n\nЧто вычисляет функция abs()?\n\n\nОтвет: среднеемодульквадратный корень\n\n\n\n\n\n\nВопрос\n\n\n\nСколько значений вернет функция, если разделить c(2, 4, 6) на 2?\n\n\nОтвет: \n\n\n\n\n\n\nВопрос\n\n\n\nБуква “c” в названии функции c() означает…\n\n\nОтвет: covercollapseconcatenate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#пайпы-конвееры",
    "href": "01_start.html#пайпы-конвееры",
    "title": "1  Начало работы",
    "section": "1.7 Пайпы (конвееры)",
    "text": "1.7 Пайпы (конвееры)\nВ нашем коде мы часто будем использовать знаки конвеера (или пайпы): |&gt; (в вашей версии он может выглядить иначе: %&gt;%; переключить оператор можно в Global Options). Они призваны показывать последовательность действий. Сочетание клавиш: Ctrl/Cmd + Shift + M.\n\nmean(sqrt(abs(sin(1:100)))) \n\n[1] 0.7654264\n\n1:100 |&gt; \n  sin() |&gt; \n  abs() |&gt; \n  sqrt() |&gt; \n  mean()\n\n[1] 0.7654264",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#векторы",
    "href": "01_start.html#векторы",
    "title": "1  Начало работы",
    "section": "1.8 Векторы",
    "text": "1.8 Векторы\nВектор – это объект, предназначенный для хранения данных. К таким же объектам относятся также матрицы, списки, датафреймы и др. Заметим, что в языке R нет скаляров (отдельных чисел). Числа считаются векторами из одного элемента.\n\nx &lt;- 2\nclass(x) # числовой вектор\n\n[1] \"numeric\"\n\nlength(x) # длина вектора\n\n[1] 1\n\n\nКак вы уже поняли, функция c() позволяет собрать несколько элементов в единый вектор:\n\nx &lt;- c(3, 5, 7)\nx_mean &lt;- mean(x) \nx_mean\n\n[1] 5\n\n\n Над векторами можно совершать арифметические операции, но будьте внимательны, применяя операции к векторам разной длины: в этом случае более короткий вектор будет переработан, то есть повторен до тех пор, пока его длина не сравняется с длиной вектора большей длины.\n\nx &lt;- 2\ny &lt;- c(10, 20, 30)\ny / x \n\n[1]  5 10 15\n\nx + y \n\n[1] 12 22 32\n\n\nВекторы можно индексировать, то есть забирать из них какие-то элементы:\n\nx &lt;- seq(1, 5, 0.5)\nx[4:5] # индексы начинаются с 1 (в отличие от Python)\n\n[1] 2.5 3.0\n\n\nВектор может хранить данные разных типов:\n\nцелое число (integer);\nчисло с плавающей точкой (numeric, также называются double, то есть число двойной точности);\nстроку (character);\nлогическую переменную (logical);\nкатегориальную переменную, или фактор (factor).\n\n\n# проверить тип данных \nx &lt;- sqrt(2)\nclass(x)\n\n[1] \"numeric\"\n\nis.integer(x)\n\n[1] FALSE\n\nis.numeric(x)\n\n[1] TRUE\n\n\nСоздавать векторы можно не только при помощи c(). Вот еще два способа.\n\nseq(1, 5, 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nrep(\"foo\", 5)\n\n[1] \"foo\" \"foo\" \"foo\" \"foo\" \"foo\"\n\n\n\nФакторы внешне похожи на строки, но в отличие от них хранят информацию об уровнях категориальных переменных. Уровень может обозначаться как числом (например, 1 и 0), так и строкой.\n\nt &lt;- factor(c(\"A\", \"B\", \"C\"), levels = c(\"A\", \"B\", \"C\"))\nt\n\n[1] A B C\nLevels: A B C\n\n\nВажно: вектор может хранить данные только одного типа. При попытке объединить в единый вектор данные разных типов они будут принудительно приведены к одному типу:\n\nx &lt;- c(TRUE, 1, 3, FALSE)\nx # логические значения приведены к числовым\n\n[1] 1 1 3 0\n\ny &lt;- c(1, \"a\", 2, \"лукоморье\") \ny # числа превратились в строки\n\n[1] \"1\"         \"a\"         \"2\"         \"лукоморье\"\n\n\nЛогические векторы можно получить в результате применения логических операторов (== “равно”, != “не равно”, &lt;= “меньше или равно”) к данным других типов:\n\nx &lt;- c(1:10) # числа от 1 до 10\ny &lt;- x &gt; 5\ny # значения TRUE соответствуют единице, поэтому их можно складывать\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum(y)\n\n[1] 5\n\n\nФункции all() и any() также возвращают логические значения:\n\nx &lt;- 10:20 \nany(x == 15)\n\n[1] TRUE\n\nall(x &gt; 9)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#списки",
    "href": "01_start.html#списки",
    "title": "1  Начало работы",
    "section": "1.10 Списки",
    "text": "1.10 Списки\nВ отличие от векторов списки могут хранить данные разных типов.\n\nlist = list(a = c(\"a\", \"b\", \"c\"), b = c(1, 2, 3), c = c(TRUE, FALSE, TRUE))\nlist\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] 1 2 3\n\n$c\n[1]  TRUE FALSE  TRUE\n\n\nМожно получить доступ как к элементам списка целиком, так и к их содержимому.\n\nlist$a # обращение к поименованным элементам \n\n[1] \"a\" \"b\" \"c\"\n\nlist[2] # одинарные квадратные скобки извлекают элемент списка целиком\n\n$b\n[1] 1 2 3\n\nclass(list[2])\n\n[1] \"list\"\n\nlist[[2]] #  элементы второго элемента \n\n[1] 1 2 3\n\nclass(list[[2]])\n\n[1] \"numeric\"\n\nlist$c[1]# первый элемент второго элемента\n\n[1] TRUE\n\n\nОбратите внимание, что list[2] и list[[2]] возвращают объекты разных классов. Нам это еще понадобится при работе с XML.\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите библиотеку rcorpora и загрузите список с названиями хлеба и сладкой выпечки.\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nУзнайте длину my_list и введите ее в поле ниже.\n\n\nОтвет: \n\n\n\n\n\n\nВопрос\n\n\n\nДостаньте из my_list элемент pastries и узнайте его длину.\n\n\nОтвет: \n\n\n\n\n\n\nВопрос\n\n\n\nА теперь извлеките пятый элемент из pastries и введите ниже его название.\n\n\nОтвет: \nСо списками покончено. Теперь можно пойти выпить кофе с my_list$pastries[13]. Дальше будет сложнее, но интереснее.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#отсутствие-данных",
    "href": "01_start.html#отсутствие-данных",
    "title": "1  Начало работы",
    "section": "1.9 Отсутствие данных",
    "text": "1.9 Отсутствие данных\nОтсутствие данных любого типа в R передается двумя способами. NULL означает, что значение не существует. Например, если мы создадим пустой вектор, то при попытке распечатать его получим NULL. А вот длина пустого вектора равна нулю!\n\ny &lt;- c() \ny \n\nNULL\n\nlength(y) \n\n[1] 0\n\n\nNA (not available) указывает на то, что значение существует, но оно неизвестно. Любые операции с NA приводят к появлению новых NA! Сравните:\n\nx &lt;- c(1, NA, 2)\nmean(x)\n\n[1] NA\n\ny &lt;- c(1, NULL, 2)\nmean(y)\n\n[1] 1.5\n\n\nКак проверить, есть ли в данных NA или NULL? Знак == здесь не подойдет.\n\nx &lt;- NA\nx == NA\n\n[1] NA\n\ny &lt;- NULL\ny == NULL\n\nlogical(0)\n\n\nДля этого есть специальные функции.\n\nis.na(x)\n\n[1] TRUE\n\nis.null(y)\n\n[1] TRUE\n\n\n\nWhen some people first get to R, they spend a lot of time trying to get rid of NAs. People probably did the same sort of thing when zero was invented. NA is a wonderful thing to have available to you. It is seldom pleasant when your data have missing values, but life if much better with NA than without.\nBurns (2012)\n\nКак избавиться от NA? В некоторых случаях достаточно аргумента функции.\n\nmean(c(1, NA, 2), na.rm=T) \n\n[1] 1.5",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#видео-к-уроку",
    "href": "01_start.html#видео-к-уроку",
    "title": "1  Начало работы",
    "section": "1.11 Видео к уроку",
    "text": "1.11 Видео к уроку\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "01_start.html#домашнее-задание",
    "href": "01_start.html#домашнее-задание",
    "title": "1  Начало работы",
    "section": "1.12 Домашнее задание ✍️",
    "text": "1.12 Домашнее задание ✍️\nК следующему разу задание будет очень простым, почти даже символическим (будем считать, что у нас адаптационный период).\nОценка 0/1.\n\nПройдите оставшиеся уроки swirl (любые два на выбор).\nСделайте скриншот на 100% экрана и отправьте скан ассистенту курса.\n\nДедлайн строго: до 21:00 мск четверга 11 сентября 2025 (это самая сложная часть дз).\n\n\n\n\nBurns, Patrick. 2012. The R inferno. Lulu.com.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#отсутствие-данных",
    "href": "start.html#отсутствие-данных",
    "title": "1  Начало работы",
    "section": "1.9 Отсутствие данных",
    "text": "1.9 Отсутствие данных\nОтсутствие данных любого типа в R передается двумя способами. NULL означает, что значение не существует. Например, если мы создадим пустой вектор, то при попытке распечатать его получим NULL. А вот длина пустого вектора равна нулю!\n\ny &lt;- c() \ny \n\nNULL\n\nlength(y) \n\n[1] 0\n\n\nNA (not available) указывает на то, что значение существует, но оно неизвестно. Любые операции с NA приводят к появлению новых NA! Сравните:\n\nx &lt;- c(1, NA, 2)\nmean(x)\n\n[1] NA\n\ny &lt;- c(1, NULL, 2)\nmean(y)\n\n[1] 1.5\n\n\nКак проверить, есть ли в данных NA или NULL? Знак == здесь не подойдет.\n\nx &lt;- NA\nx == NA\n\n[1] NA\n\ny &lt;- NULL\ny == NULL\n\nlogical(0)\n\n\nДля этого есть специальные функции.\n\nis.na(x)\n\n[1] TRUE\n\nis.null(y)\n\n[1] TRUE\n\n\n\nWhen some people first get to R, they spend a lot of time trying to get rid of NAs. People probably did the same sort of thing when zero was invented. NA is a wonderful thing to have available to you. It is seldom pleasant when your data have missing values, but life if much better with NA than without.\nBurns (2012)\n\nКак избавиться от NA? В некоторых случаях достаточно аргумента функции.\n\nmean(c(1, NA, 2), na.rm=T) \n\n[1] 1.5",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#видео-к-уроку",
    "href": "start.html#видео-к-уроку",
    "title": "1  Начало работы",
    "section": "1.14 Видео к уроку",
    "text": "1.14 Видео к уроку\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#домашнее-задание",
    "href": "start.html#домашнее-задание",
    "title": "1  Начало работы",
    "section": "1.15 Домашнее задание ✍️",
    "text": "1.15 Домашнее задание ✍️\nК следующему разу задание будет очень простым, почти даже символическим (будем считать, что у нас адаптационный период). Оценка 0/1.\n\nПройдите любые два из оставшихся уроков swirl (на выбор).\nКогда swirl сообщит о 100% выполнении, введите свое имя в консоли или в окне скрипта, сделайте скриншот и отправьте скан ассистенту курса личным сообщением в Telegram (контакты в рабочем чате).\n\nДедлайн строго: до 21:00 мск четверга 11 сентября 2025 (это самая сложная часть дз).\n\n\n\n\nBurns, Patrick. 2012. The R inferno. Lulu.com.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#матрицы",
    "href": "start.html#матрицы",
    "title": "1  Начало работы",
    "section": "1.11 Матрицы",
    "text": "1.11 Матрицы\nМатрица – это вектор, который имеет два дополнительных атрибута: количество строк и количество столбцов. Из этого следует, что матрица, как и вектор, может хранить данные одного типа. Проверим.\n\nM = matrix(c(1, 2, 3, 4), nrow = 2)\nM # все ок\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nM = matrix(c(1, 2, 3, \"a\"), nrow = 2)\nM # все превратилось в строку! \n\n     [,1] [,2]\n[1,] \"1\"  \"3\" \n[2,] \"2\"  \"a\" \n\n\nВ матрице есть ряды и столбцы. Их количество определяет размер (порядок) матрицы. Выше мы создали матрицу 2 x 2. Элементы матрицы, как и элементы вектора, можно извлекать по индексу. Сначала указывается номер ряда (строки), потом номер столбца.\n\nM = matrix(c(1, 2, 3, 4), nrow = 2)\nM\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\nM[1, ] # первая строка полностью\n\n[1] 1 3\n\nM[,2] # второй столбец полностью\n\n[1] 3 4\n\nM[1,1] # одно значение\n\n[1] 1\n\n\nОбратите внимание, как меняется размерность при индексировании.\n\nM = matrix(c(1, 2, 3, 4), nrow = 2)\ndim(M) # функция для извлечения измерений\n\n[1] 2 2\n\ndim(M[1, ]) \n\nNULL\n\n\nПопытка узнать измерения вектора возвращает NULL, потому что, с точки зрения R, векторы не являются матрицами из одного столбца или одной строки и потому не имеют измерений.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#таблицы-датафреймы",
    "href": "start.html#таблицы-датафреймы",
    "title": "1  Начало работы",
    "section": "1.12 Таблицы (датафреймы)",
    "text": "1.12 Таблицы (датафреймы)\nЕсли матрица – это двумерный аналог вектора, то таблица (кадр данных, data frame) – это двумерный аналог списка. Как и список, датафрейм может хранить данные разного типа.\n\n# создание датафрейма\ndf &lt;- data.frame(\n  names = c(\"John\", \"Mary\"), \n  age = c(18, 25), \n  sport = c(\"basketball\", \"tennis\")\n  )\n\ndf |&gt; \n  print()\n\n  names age      sport\n1  John  18 basketball\n2  Mary  25     tennis\n\n\nДля извлечения данных можем использовать индексы или имена переменных.\n\ndf$names # или df[,\"names\"] \n\n[1] \"John\" \"Mary\"\n\ndf[1, ]  |&gt; \n  print()\n\n  names age      sport\n1  John  18 basketball\n\n\nВот так мы можем узнать имена столбцов, извлечь ряды по значению переменной или узнать тип данных в столбцах.\n\ncolnames(df) \n\n[1] \"names\" \"age\"   \"sport\"\n\ndf[df$name == \"John\", ] |&gt; \n  print()\n\n  names age      sport\n1  John  18 basketball\n\n# узнать тип данных в столбцах\nstr(df) \n\n'data.frame':   2 obs. of  3 variables:\n $ names: chr  \"John\" \"Mary\"\n $ age  : num  18 25\n $ sport: chr  \"basketball\" \"tennis\"\n\n\nПреобразовать тип данных:\n\ndf$age &lt;- as.character(df$age)\n\n\n\n\n\n\n\nЗадание\n\n\n\nДля закрепления навыка в swirl пройдите урок 7 Matrices and Data Frames. После этого можете выполнить практическое задание ниже.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "tabular.html#домашнее-задание",
    "href": "tabular.html#домашнее-задание",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.10 Домашнее задание ✍️",
    "text": "2.10 Домашнее задание ✍️\n\nСоздайте учетную запись GitHub, если у вас ее нет: https://github.com/\nПримите задание по ссылке: https://classroom.github.com/a/kZtVp96N. Если вы не видите себя в списке студентов, напишите преподавателю.\nПосле этого GitHub создаст репозиторий для сдачи домашнего задания.\nСоздайте на своем компьютере файл hw1.R, отредактируйте его, используя заготовку ниже.\n\n\n#devtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\nlibrary(tidyverse)\n\nworks &lt;- gutenberg_works()\n\n# В каждом пункте используйте оператор pipe, не сохраняйте промежуточные результаты!\n\n# (1) Отберите ряды, в которых gutenberg_author_id равен 65 или 410;\n# после этого выберите два столбца: author, title\nmy_data &lt;- works |&gt; \n  # ваш код здесь\n\n# (2) Используйте функцию separate(), чтобы разделить \n# столбец с именем и фамилией на два новых: author, name. \n# Удалите столбец name\nmy_data2 &lt;- my_data |&gt;\n  # ваш код здесь\n\n# (3) Используйте group_by() и summarise(), чтобы узнать,\n# сколько произведений Шекспира и Марлоу хранится в библиотеке Gutenberg\nmy_data3 &lt;- my_data2 |&gt;\n  # ваш код здесь\n\n\nПосле этого любым способом загрузите свой файл в репозиторий. Если умеете в git, хорошо. Если нет, используйте кнопку Upload files.\n\n\n\nНе меняйте имена переменных! Проверка будет автоматической.\nДля пересчета года в век используйте следующую формулу: (deathdate - 1) %/% 100 + 1, где %/% – целочисленное деление.\nДедлайн: 25/09/2025 21:00.\nОценка: 0/1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#практическое-задание",
    "href": "tabular.html#практическое-задание",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.8 Практическое задание",
    "text": "2.8 Практическое задание\n\nВыполните практическое задание из предыдущего урока, используя функции из библиотеки tidyverse.\nСкачайте и исследуйте датасет с корпусом русских элегий 1815-1835 г.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tabular.html#слияние-таблиц",
    "href": "tabular.html#слияние-таблиц",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.7 Слияние таблиц",
    "text": "2.7 Слияние таблиц\nФайл с библиографией забираем с сайта, как было показано выше.\n\nbib_tbl &lt;- read_tsv(\"../files/bibliography.tsv\")\n\nИспользуем уже знакомые функции для объединения данных.\n\ncurricula_joined &lt;- curricula_tbl |&gt; \n  select(author, title, year, grade, curriculum) |&gt; \n  left_join(bib_tbl, join_by(curriculum == abbreviation))\n\ncurricula_joined |&gt; \n  print()\n\n# A tibble: 10,306 × 7\n   author        title                  year  grade curriculum    id description\n   &lt;chr&gt;         &lt;chr&gt;                  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      \n 1 Андреев Л.Н.  Жили-были              1919      9 19 ИРЛ 2 …     1 Примерная …\n 2 Андреев Л.Н.  Иуда                   1919      9 19 ИРЛ 2 …     1 Примерная …\n 3 Андреев Л.Н.  Рассказ о семи повеше… 1919      9 19 ИРЛ 2 …     1 Примерная …\n 4 Бальмонт К.Д. &lt;NA&gt;                   1919      9 19 ИРЛ 2 …     1 Примерная …\n 5 Брюсов В.Я.   &lt;NA&gt;                   1919      9 19 ИРЛ 2 …     1 Примерная …\n 6 Герцен А.И.   Кто виноват?           1919      8 19 ИРЛ 2 …     1 Примерная …\n 7 Гоголь Н.В.   Вечера на Хуторе близ… 1919      8 19 ИРЛ 2 …     1 Примерная …\n 8 Гоголь Н.В.   Вий                    1919      8 19 ИРЛ 2 …     1 Примерная …\n 9 Гоголь Н.В.   Женитьба               1919      8 19 ИРЛ 2 …     1 Примерная …\n10 Гоголь Н.В.   Мертвые души           1919      8 19 ИРЛ 2 …     1 Примерная …\n# ℹ 10,296 more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "start.html#практическое-задание",
    "href": "start.html#практическое-задание",
    "title": "1  Начало работы",
    "section": "1.13 Практическое задание",
    "text": "1.13 Практическое задание\n\n# устанавливаем и загружаем нужный пакет\ninstall.packages(\"languageR\")\nlibrary(languageR)\n\n# загружаем датасет\nmeta &lt;- spanishMeta\n\n# допишите ваш код ниже\n# посчитайте средний год публикации романов Камило Хосе Селы\n\n\n# вычислите суммарное число слов в романах Эдуардо Мендосы\n\n\n# извлеките ряды с текстами, опубликованными до 1980 г.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "tabular.html#видео-к-уроку",
    "href": "tabular.html#видео-к-уроку",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.9 Видео к уроку",
    "text": "2.9 Видео к уроку\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "plot.html#домашнее-задание",
    "href": "plot.html#домашнее-задание",
    "title": "3  Визуализации",
    "section": "3.14 Домашнее задание",
    "text": "3.14 Домашнее задание\nВам надо построить график к датасету “Старофрацузская литература”. Инструкции ниже; код и само изображение надо будет загрузить в GitHub Classroom (ссылка). Дедлайн: 3 октября 2025, 21-00 мск. Оценка: 0/1.\nСохраните файл скрипта под именем hw3.R.\n\nlibrary(languageR)\nlibrary(ggplot2)\n\n# загружаем датасет\nmeta &lt;- oldFrenchMeta\n\n# допишите ваш код ниже\n\ng &lt;- meta |&gt; \n # постройте в ggplot столбиковую диаграмму (_bar), \n # показывающую распределение произведений по темам; цветом-заливкой закодируйте жанр; \n # уберите названия осей;\n #  добавьте заголовок \"Old French Data\"\n # поверните координатную ось; \n # поменяйте тему оформления на черно-белую (bw)\n # !!!! сохраните график как объект в окружении под именем g\n # !!!!вызов class(g) должен возвращать \"gg\"     \"ggplot\"\n\n\n\n\n\nUnderwood, Ted. 2019. Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nМастицкий, Сергей. 2017. Визуализация данных с помощью ggplot2. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#видео-к-уроку",
    "href": "plot.html#видео-к-уроку",
    "title": "3  Визуализации",
    "section": "3.13 Видео к уроку",
    "text": "3.13 Видео к уроку\n\nВидео 2025 г.\nВидео 2024 г.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "iterate.html#map-map_int",
    "href": "iterate.html#map-map_int",
    "title": "4  Циклы, условия, функции",
    "section": "4.4 map() & map_int()",
    "text": "4.4 map() & map_int()\nДля того, чтобы прочесть все файлы одним вызовом функции, используем map(). В качестве аргументов передаем список файлов, функцию read_csv() и аргумент этой функции col_types.\n\nlibrary(tidyverse)\n# чтение файлов \nHP &lt;- map(my_files, read_csv, show_col_types = FALSE)\n\nОбъект HP – это список. В нем пять элементов, так как на входе у нас было пять файлов. Для удобства назначаем имена элементам списка.\n\nnames_short &lt;- list.files(\"../files/HP\", pattern = \".csv\") |&gt; \n  str_remove(\".csv\")\nnames_short\n\n[1] \"classification\" \"names\"          \"records\"        \"titles\"        \n[5] \"topics\"        \n\n\n\n# присваиваем имена элементам списка\nnames(HP) &lt;- names_short\n\nДля начала узнаем число рядов в каждой таблице. На выходе мы ожидаем целое число, поэтому используем map_int(). Функция вернет именованный вектор. Чтобы избавиться от имен, можно использовать unname().\n\nmap_int(HP, nrow)\n\nclassification          names        records         titles         topics \n           567            885            755            962           2198",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#pluck-map_df",
    "href": "iterate.html#pluck-map_df",
    "title": "4  Циклы, условия, функции",
    "section": "4.5 pluck() & map_df()",
    "text": "4.5 pluck() & map_df()\nКаждую таблицу можно также рассматривать как список (переменных) и применять функции (или их сочетания) к отдельным столбцам. Получить доступ к отдельным таблицам можно при помощи квадратных скобок (HP[[\"titles\"]]) или при помощи функции pluck() из того же пакета {purrr}. Оба способа хороши, но второй удобнее при работе с вложенными списками, которая нас ждет в следующем уроке.\n\ntitles &lt;- HP |&gt; \n  pluck(\"titles\")\n\nТеперь узнаем сумму отсутствующих значений в каждом столбце таблицы titles. Здесь .x — это текущий элемент из titles, который последовательно передаётся в функцию, а тильда ( ~ ) обозначает начало выражения-функции. Если вы явно передаёте именованную или анонимную функцию, то тильда не требуется.\n\nmap_int(titles, ~sum(is.na(.x)))\n\n                     Title               Other titles \n                         0                        654 \n              BL record ID           Type of resource \n                         0                          0 \n              Content type              Material type \n                         0                         54 \n                BNB number                       ISBN \n                       363                         47 \n                      ISSN                       Name \n                       956                        262 \nDates associated with name               Type of name \n                       853                        262 \n                      Role                  All names \n                       617                        138 \n              Series title       Number within series \n                       753                        907 \n    Country of publication       Place of publication \n                        28                         80 \n                 Publisher        Date of publication \n                        20                          3 \n                   Edition       Physical description \n                       779                         30 \n      Dewey classification               BL shelfmark \n                       257                        376 \n                    Topics                      Genre \n                       158                        499 \n                 Languages                      Notes \n                        55                        465 \n\n\nТакже узнаем число уникальных значений в каждом столбце и запросим результат в виде таблицы.\n\nmap_df(\n  titles,\n  ~ tibble(\n    n_unique = n_distinct(.x),\n    nas = sum(is.na(.x)),\n    prop_nas = round(nas / nrow(titles),2)\n    ),\n  .id = \"variable\"\n  ) |&gt; \n  arrange(prop_nas) |&gt; \n  print()\n\n# A tibble: 28 × 4\n   variable               n_unique   nas prop_nas\n   &lt;chr&gt;                     &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;\n 1 Title                       698     0     0   \n 2 BL record ID                755     0     0   \n 3 Type of resource              4     0     0   \n 4 Content type                  8     0     0   \n 5 Date of publication          34     3     0   \n 6 Publisher                   250    20     0.02\n 7 Country of publication       30    28     0.03\n 8 Physical description        522    30     0.03\n 9 ISBN                        719    47     0.05\n10 Material type                16    54     0.06\n# ℹ 18 more rows\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Advanced R Programming\") и пройдите из него урок 3 Functional Programming with purrr.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#разведывательный-анализ",
    "href": "iterate.html#разведывательный-анализ",
    "title": "4  Циклы, условия, функции",
    "section": "4.6 Разведывательный анализ",
    "text": "4.6 Разведывательный анализ\nВыясним, на каких языках и когда выходили книги о Гарри Поттере. Но сначала приведем данные в опрятный вид. Там, где языков два, как правило интересен второй (первый - английский); поэтому делим столбец с языками на два и “сплавляем”. Функция coalesce(x, y) возвращает первое не-NA между x иy`.\n\ntitles_tidy &lt;- titles |&gt; \n  select(Title, Name, `Date of publication`, Languages) |&gt; \n  # избавимся от неправильных имен\n  rename(\n    Year = `Date of publication`) |&gt;\n  # избавимся от NA\n  filter(!is.na(Year), !is.na(Languages)) |&gt; \n  #  разделим кода с дефисом и преобразуем год  \n  separate(Year, into = c(\"Year\", NA)) |&gt; \n  mutate(Year = as.integer(Year)) |&gt; \n  # выбираем, где можно, второй язык\n  separate(Languages, into = c(\"Language1\", \"Language2\"), sep = \";\") |&gt; \n  mutate(Language = coalesce(Language2, Language1)) |&gt; \n  select(-Language1, -Language2)\n\ntitles_tidy\n\n\n  \n\n\n\nДля графика выберем несколько языков. Кстати, ни одного перевода на русский в данных Британской библиотеки нет.\n\ntitles_tidy |&gt; \n  add_count(Language) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n) |&gt; \n  ggplot(aes(Year, fill = Language)) + \n  geom_bar(position = \"stack\") + \n  xlab(NULL) +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  scale_x_continuous(breaks = seq(1997, 2023))\n\n\n\n\n\n\n\n\nТеперь попробуем самостоятельно написать функцию и передать ее map_*().",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#синтаксис-функции",
    "href": "iterate.html#синтаксис-функции",
    "title": "4  Циклы, условия, функции",
    "section": "4.7 Синтаксис функции",
    "text": "4.7 Синтаксис функции\nФункция и код – не одно и то же. Чтобы стать функцией, кусок кода должен получить имя. Но зачем давать имя коду, который и так работает? Вот три причины, которые приводит Хадли Уикхем:\n\nу функции есть выразительное имя, которое облегчает понимание кода;\nпри изменении требований необходимо обновлять код только в одном месте, а не во многих;\nменьше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом)\n\n\nWriting good functions is a lifetime journey.\n— Hadley Wickham\n\nМашине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена!\nДалее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть).\nНаписание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию.\nНапишем функцию, которая забирает все слова из столбца, считает частотности и строит облако слов. Сначала просто поймем, какой код нам нужен.\n\nlibrary(tidytext)\ncount_data &lt;- titles_tidy |&gt; \n  filter(Language == \"English\") |&gt; \n  select(Title) |&gt; \n  unnest_tokens(output = \"word\", input = \"Title\") |&gt; \n  anti_join(stop_words) |&gt; \n  count(word, sort = TRUE) |&gt; \n  filter(!str_detect(word, \"[0-9]\"),\n         !word %in% c(\"harry\", \"potter\", \"book\"))\n\ncount_data |&gt; \n  print()\n\n# A tibble: 902 × 2\n   word              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 magical          50\n 2 magic            48\n 3 world            47\n 4 guide            43\n 5 hogwarts         38\n 6 unofficial       38\n 7 phoenix          37\n 8 secrets          37\n 9 stone            35\n10 philosopher's    33\n# ℹ 892 more rows\n\n\nТакже построим облако слов.\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(count_data$word, \n          count_data$n,\n          min.freq = 3,\n          #max.words = 50, \n          #scale = c(3, 0.8),\n          colors = pal, \n          random.color = TRUE, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nМы готовы упаковать наш код в функцию.\n\ncolumn_to_wordcloud &lt;- function(data, colname, ...) {\n  # загружаем все пакеты\n  library(tidytext)\n  library(dplyr)\n  library(stringr)\n  library(wordcloud)\n  \n  # пишем код, подставляя имена переменных \n  count_data &lt;- data |&gt; \n  filter(str_detect(Languages, \"English\")) |&gt; \n  select(any_of(colname)) |&gt;  \n  unnest_tokens(output = \"word\", input = colname) |&gt; \n  anti_join(stop_words) |&gt; \n  count(word, sort = TRUE) |&gt; \n  filter(!str_detect(word, \"[0-9]\"),\n         !word %in% c(\"harry\", \"potter\", \"book\"))\n  \n  wordcloud(count_data$word, \n          count_data$n,\n          ...\n          )\n}\n\nПопробуем запусть нашу новую функцию.\n\ncolumn_to_wordcloud(titles, \"Topics\")\n\n\n\n\n\n\n\n\nАргумент ... позволяет обращаться к аргументам функции wordcloud, которые мы специально не прописывали.\n\ncolumn_to_wordcloud(titles, \"Topics\", colors=pal, vfont=c(\"gothic english\",\"plain\"))\n\n\n\n\n\n\n\n\nВнутри нашей функции есть переменная count_data, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции.\n\n\n\n\n\n\nЗадание\n\n\n\nЗагрузите библиотеку swirl, выберите курс R Programming и пройдите из него урок 9 Functions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#map2-walk2-walk2",
    "href": "iterate.html#map2-walk2-walk2",
    "title": "4  Циклы, условия, функции",
    "section": "4.8 map2(), walk2() & walk2()",
    "text": "4.8 map2(), walk2() & walk2()\nЧтобы несколько раз вызывать одну и ту же функцию с двумя аргументами, используется функция map2_*(). Вот простой пример:\n\nvar1 &lt;- seq(10, 50, 10)\nvar2 &lt;- seq(1, 5, 1)\n\n# формула\nmap2_int(var1, var2, ~.x+.y)\n\n[1] 11 22 33 44 55\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВо всех случаеях, когда у функции больше двух аргументов, используется pmap().\n\n\nЕсли сохранять ничего не надо (как в случае с нашим облаком слов), то используются walk() и walk2(). Попробуем.\npar(mar = c(0,0,0,0), mfrow = c(1,2))\nwalk(HP[1:2], column_to_wordcloud, \"Title\", colors=pal)\n\n\n\n\n\n\n\nВ этом случае второй аргумент был одинаков для двух вызовов функции. Если второй аргумент отличается, вызываем walk2().\npar(mar = c(0,0,0,0))\nwalk2(HP[1:2], c(\"Topics\", \"Publisher\"), column_to_wordcloud, colors = c(\"navyblue\", \"magenta\", \"grey\"), random.color = TRUE, scale = c(4,1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#сообщения-и-остановка",
    "href": "iterate.html#сообщения-и-остановка",
    "title": "4  Циклы, условия, функции",
    "section": "4.10 Сообщения и остановка",
    "text": "4.10 Сообщения и остановка\nЧасто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении.\n\ncolumn_counts &lt;- function(data, colname, remove_stopwords = TRUE) {\n  library(dplyr)\n  library(tidytext)\n  library(stringr)\n  \n  # Условие остановки функции, если такого столбца нет\n  if (!(colname %in% names(data))) {\n    stop(paste(\"Столбец\", colname, \"не найден в данных!\"))\n  }\n  \n  # Сообщение о выбранном режиме стоп-слов\n  if (remove_stopwords) {\n    message(\"Стоп-слова будут удалены.\")\n  } else {\n    message(\"Стоп-слова НЕ будут удалены.\")\n  }\n  \n  # базовая обработка\n  count_data &lt;- data |&gt; \n    select(any_of(colname)) |&gt;  \n    unnest_tokens(output = \"word\", input = colname)\n  \n  # удаляем стоп-слова, если требуется\n  if (remove_stopwords) {\n    count_data &lt;- count_data |&gt; \n      anti_join(stop_words)\n  }\n  \n  # частотности и сортировка\n  count_data &lt;- count_data |&gt;\n    count(word, sort = TRUE)\n  \n  return(count_data)\n}\n\n\ncolumn_counts(titles, \"Genre\")\n\n\n  \n\n\ncolumn_counts(titles, \"Date\")\n\nError in column_counts(titles, \"Date\"): Столбец Date не найден в данных!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#бонус-интерактивное-облако",
    "href": "iterate.html#бонус-интерактивное-облако",
    "title": "4  Циклы, условия, функции",
    "section": "4.11 Бонус: Интерактивное облако",
    "text": "4.11 Бонус: Интерактивное облако\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\ntitle_counts &lt;- titles |&gt; \n  column_counts(\"Title\") |&gt; \n  rename(freq = n)\n\ntitle_counts |&gt; \n  print()\n\n# A tibble: 1,439 × 2\n   word           freq\n   &lt;chr&gt;         &lt;int&gt;\n 1 potter          790\n 2 harry           787\n 3 book             78\n 4 stone            62\n 5 philosopher's    56\n 6 magical          50\n 7 magic            48\n 8 secrets          48\n 9 world            48\n10 phoenix          45\n# ℹ 1,429 more rows\n\n\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\nwordcloud2(title_counts, \n           figPath = \"./images/hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\"\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#домашнее-задание",
    "href": "iterate.html#домашнее-задание",
    "title": "4  Циклы, условия, функции",
    "section": "4.13 Домашнее задание",
    "text": "4.13 Домашнее задание\nНапишите функцию count_words, которая будет:\n\nпринимать на входе токенизированный (разбитый на слова) текст (символьный вектор!),\nпереводить все слова в нижний регистр,\nсчитать частотность для каждого слова,\nупорядочивать по убыванию,\nвозвращать n наиболее частотных слов (без частотностей, только слова!) в виде вектора.\n\nN.B. Эту задачу можно решить разными способами; подойдет любой, если на выходе будут нужные слова. Как вы назовете аргументы функции, неважно. У аргумента для числа возвращаемых слов значение по умолчанию поставьте 10. Имя функции должно быть строго count_words.\nТренироваться можете на векторе languageR::alice, но в сдаваемом файле должна быть только функция.\nВам могут пригодиться функции table, sort, tolower, names – посмотрите документацию, чтобы понять, как они работают.\nФайл под названием hw4.R загрузите в GitHub Classroom по ссылке до 11 октября 11:00 мск.\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#видео-к-уроку",
    "href": "iterate.html#видео-к-уроку",
    "title": "4  Циклы, условия, функции",
    "section": "4.12 Видео к уроку",
    "text": "4.12 Видео к уроку\n\nВидео 2024\nВидео 2025",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "xml.html",
    "href": "xml.html",
    "title": "6  Разметка TEI XML",
    "section": "",
    "text": "6.1 Основы XML\nXML (от англ. eXtensible Markup Language) — расширяемый язык разметки. Слово “расширяемый” означает, что список тегов не зафиксирован раз и навсегда: пользователи могут вводить свои собственные теги и создавать так называемые настраиваемые языки разметки (Холзнер 2004, 29). Один из таких настраиваемых языков – это TEI (Text Encoding Initiative), о котором будет сказано дальше.\nДля работы нам понадобятся следующие библиотеки:\nНазначение языков разметки заключается в описании структурированных документов. Структура документа представляется в виде набора вложенных в друг друга элементов (дерева XML). У элементов есть открывающие и закрывающие теги.\nВсе составляющие части документа обобщаются в пролог и корневой элемент. Корневой элемент — обязательная часть документа, в которую вложены все остальные элементы. Пролог может включать объявления, инструкции обработки, комментарии.\nВ правильно сформированном XML открывающий и закрывающий тег вложенного элемента всегда находятся внутри одного родительского элемента.\nСоздадим простой XML из строки. Сначала идет инструкция по обработке XML (со знаком вопроса), за ней следует объявление типа документа (с восклицательным знаком) и открывающий тег корневого элемента. В этот корневой элемент вложены все остальные элементы.\nstring_xml &lt;- '&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!DOCTYPE recipe&gt;\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n   &lt;title&gt;\n      Простой хлеб\n   &lt;/title&gt;\n   &lt;composition&gt;\n      &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n      &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n      &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n   &lt;/composition&gt;\n   &lt;instructions&gt;\n     &lt;step&gt;\n        Смешать все ингредиенты и тщательно замесить. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Закрыть тканью и оставить на один час в тёплом помещении. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Замесить ещё раз, положить на противень и поставить в духовку.\n     &lt;/step&gt;\n   &lt;/instructions&gt;\n&lt;/recipe&gt;'",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "xml.html#основы-xml",
    "href": "xml.html#основы-xml",
    "title": "6  Разметка TEI XML",
    "section": "",
    "text": "6.1.1 Библиотека xml2\nДля работы с xml понадобится установить библиотеку xml2. Функция read_xml() создаст объект, представляющий дерево XML.\n\ndoc &lt;- read_xml(string_xml)\nclass(doc)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nФункция xml_root() позволяет извлечь корневой элемент вместе со всеми детьми.\n\nrootnode &lt;- xml_root(doc)\nrootnode\n\n{xml_document}\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n[1] &lt;title&gt;\\n      Простой хлеб\\n   &lt;/title&gt;\n[2] &lt;composition&gt;\\n  &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\\n ...\n[3] &lt;instructions&gt;\\n  &lt;step&gt;\\n        Смешать все ингредиенты и тщательно зам ...\n\n\nУ корневого элемента есть “дети”. Это набор узлов.\n\nxml_children(rootnode)\n\n{xml_nodeset (3)}\n[1] &lt;title&gt;\\n      Простой хлеб\\n   &lt;/title&gt;\n[2] &lt;composition&gt;\\n  &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\\n ...\n[3] &lt;instructions&gt;\\n  &lt;step&gt;\\n        Смешать все ингредиенты и тщательно зам ...\n\n\nУ детей есть имена, которые можно извлечь специальной функцией.\n\nxml_name(xml_children(rootnode))\n\n[1] \"title\"        \"composition\"  \"instructions\"\n\n\n\n\n6.1.2 Выбор элементов\nФункция xml_find_first() вернет первый подходящий узел.\n\ncomposition_node &lt;- xml_find_first(rootnode, \"composition\")\ncomposition_node\n\n{xml_node}\n&lt;composition&gt;\n[1] &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n[2] &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n[3] &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\nВыбирать узлы можно и по индексу:\n\ninstructions_node &lt;- xml_children(rootnode)[[3]]\ninstructions_node\n\n{xml_node}\n&lt;instructions&gt;\n[1] &lt;step&gt;\\n        Смешать все ингредиенты и тщательно замесить. \\n     &lt;/step&gt;\n[2] &lt;step&gt;\\n        Закрыть тканью и оставить на один час в тёплом помещении. ...\n[3] &lt;step&gt;\\n        Замесить ещё раз, положить на противень и поставить в дух ...\n\n\n\n\n6.1.3 Значения узлов и атрибутов\nНо обычно нам нужен не элемент как таковой, а его содержание (значение). Чтобы добраться до него, используем функцию xml_text():\n\ncomposition_node |&gt; \n  xml_children() |&gt; \n  xml_text()\n\n[1] \"Мука\"        \"Дрожжи\"      \"Тёплая вода\"\n\n\nМожно уточнить атрибуты узла при помощи xml_attrs():\n\ncomposition_node |&gt; \n  xml_children() |&gt; \n  xml_attrs()\n\n[[1]]\n  amount     unit \n     \"3\" \"стакан\" \n\n[[2]]\n amount    unit \n \"0.25\" \"грамм\" \n\n[[3]]\n  amount     unit \n   \"1.5\" \"стакан\" \n\n\nЧтобы извлечь значение атрибута, используем функцию xml_attr(). Первым аргументом функции передаем xml-узел, вторым – имя атрибута.\n\ncomposition_node |&gt; \n  xml_children() |&gt; \n  xml_attr(\"unit\")\n\n[1] \"стакан\" \"грамм\"  \"стакан\"\n\n\n\n\n6.1.4 Синтаксис XPath\nДобраться до узлов определенного уровня можно также при помощи синтаксиса XPath. XPath – это язык запросов к элементам XML-документа. С его помощью можно описать “путь” до нужного узла: абсолютный (начиная с корневого элемента) или относительный. В пакете xml синтаксис XPath поддерживают функции xml_find_first() и xml_find_all().\n\n# абсолютный путь\nxml_find_all(rootnode, \"/recipe//composition//ingredient\")\n\n{xml_nodeset (3)}\n[1] &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n[2] &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n[3] &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n# относительный путь\nxml_find_all(rootnode, \"//composition//ingredient\")\n\n{xml_nodeset (3)}\n[1] &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n[2] &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n[3] &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n# атрибут unit == \"стакан\"\nxml_find_all(rootnode, \"//composition//ingredient[@unit='стакан']\")\n\n{xml_nodeset (2)}\n[1] &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n[2] &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВ большинстве случаев функция требует задать пространство имен (namespace), но в нашем случае оно не определено, поэтому пока передаем только дерево и путь до узла. С пространством имен встретимся чуть позже!\n\n\n\n\n6.1.5 От дерева к таблице\nПри работе с xml в большинстве случаев наша задача – извлечь значения определеннных узлов или их атрибутов и сохранить их в прямоугольном формате.\n\ntitle &lt;- rootnode |&gt; \n  xml_find_all(\"title\") |&gt;  \n  xml_text() |&gt; \n  trimws() \n  \ntitle\n\n[1] \"Простой хлеб\"\n\n\nДостаем три узла с ингредиентами.\n\ningredient_ns &lt;- rootnode |&gt; \n  xml_find_all(\"//composition//ingredient\")\n\ningredient_ns \n\n{xml_nodeset (3)}\n[1] &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n[2] &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n[3] &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\ntibble(\n  title  = title |&gt; \n    trimws(),\n  ingredients = xml_text(ingredient_ns) |&gt; \n    trimws(),\n  unit = xml_attr(ingredient_ns, \"unit\"),\n  amount = xml_attr(ingredient_ns, \"amount\")\n) |&gt; \n  print()\n\n# A tibble: 3 × 4\n  title        ingredients unit   amount\n  &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; \n1 Простой хлеб Мука        стакан 3     \n2 Простой хлеб Дрожжи      грамм  0.25  \n3 Простой хлеб Тёплая вода стакан 1.5   \n\n\nТеперь рассмотрим более сложные примеры.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "xml.html#разметка-tei",
    "href": "xml.html#разметка-tei",
    "title": "6  Разметка TEI XML",
    "section": "6.2 Разметка TEI",
    "text": "6.2 Разметка TEI\nTEI (Text Encoding Initiative) — специализированный язык разметки на основе XML, разработанный как средство формального кодирования наиболее значимых текстологических свойств документа: физических параметров рукописи, критического аппарата, лингвистической информации, выходных данных, сведений об авторе, обстоятельствах публикации и первоисточнике (Скоринкин 2016). TEI появился в 1987 г. и в наши дни стал де-факто стандартом для создания цифровых гуманитарных ресурсов.\nОсновная задача структурированной разметки — формальное эксплицитное представление некоторых свойств документа, заложенных в нем имплицитно. Например, для человека очевидно, где в тексте романа кончается одна часть и начинается другая, какие герои упоминаются в той или иной главе, какие реплики кем произнесены. Однако для машины ничего из этого не является «очевидным» — электронный текст без разметки остается не более чем цепочкой символов.\nБольшая часть размеченных литературных корпусов хранится именно в формате XML. Это очень удобно, и вот почему: с помощью тегов XML мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п.\nИспользование TEI обеспечивает:\n\nХранение богатой метаинформации о тексте и его носителях;\nКодирование структуры текста и лингвистической разметки;\nНезависимость от конкретного ПО;\nОткрытость для доработки и расширения;\nОптимизацию для автоматической обработки.\n\nДобавлять и удалять разметку может любой пользователь в редакторе XML кода или даже в простом текстовом редакторе. Стандарт TEI предоставляет исследователям универсальный метаязык для обмена текстологической информацией и встраивает документы в мировую коллекцию машиночитаемых текстов.\n\n6.2.1 Структура документа TEI\nКорневой элемент в документах TEI называется TEI, внутри него располагается элемент teiHeader с метаинформацией о документе и элемент text. Последний содержит текст документа с элементами, определяющими его структурное членение.\n&lt;TEI&gt;\n  &lt;teiHeader&gt;&lt;/teiHeader&gt;\n  &lt;text&gt;&lt;/text&gt;\n&lt;/TEI&gt;\nПример оформления документа можно посмотреть по ссылке.\n\n\n6.2.2 teiHeader\nУ teiHeader есть четыре главных дочерних элемента:\n\nfileDesc (описание документа c библиографической информацией)\nencodingDesc (описание способа кодирование первоисточника)\nprofileDesc (“досье” на текст, например отправитель и получатель для писем, жанр, используемые языки, обстоятельства создания, место написания и т.п.)\nrevisionDesc (история изменений документа).\n\nЭлемент fileDesc должен содержать полную библиографическую информацию о первоисточнике. Пример для повести Л.Н. Толстого «Детство»:\n&lt;fileDesc&gt;\n  &lt;titleStmt&gt;\n    &lt;title&gt;Повесть «Детство». Электронное издание.&lt;/title&gt;\n    &lt;author&gt;Толстой Л.Н.&lt;/author&gt;\n    &lt;editor&gt;Иванов И.И.&lt;/editor&gt;\n    &lt;respStmt&gt;\n      &lt;resp&gt;Подготовка и разметка метаинформации для электронного издания&lt;/resp&gt;\n      &lt;name&gt;Иванов И.И.&lt;/name&gt;\n    &lt;/respStmt&gt;\n  &lt;/titleStmt&gt;\n  &lt;publicationStmt&gt;\n    &lt;publisher&gt;Школа лингвистики &lt;orgName&gt;НИУ ВШЭ&lt;/orgName&gt;&lt;/publisher&gt;\n    &lt;availability&gt;\n      &lt;p&gt;Распространяется свободно&lt;/p&gt;\n    &lt;/availability&gt;\n  &lt;/publicationStmt&gt;\n  &lt;sourceDesc&gt;\n    &lt;biblStruct&gt;\n      &lt;author&gt;Толстой Л.Н.&lt;/author&gt;\n      &lt;title level=\"a\"&gt;Детство&lt;/title&gt;\n      &lt;monogr&gt;\n        &lt;title level=\"m\"&gt;Полное собрание сочинений. Том 1&lt;/title&gt;\n        &lt;imprint&gt;\n          &lt;pubPlace&gt;Москва&lt;/pubPlace&gt;\n          &lt;publisher&gt;Государственное издательство \"Художественная литература\"&lt;/publisher&gt;\n          &lt;date when=\"1935\"/&gt;\n        &lt;/imprint&gt;\n      &lt;/monogr&gt;\n    &lt;/biblStruct&gt;\n  &lt;/sourceDesc&gt;\n&lt;/fileDesc&gt;\nЭлемент &lt;profileDesc&gt; содержит метаданные, относящиеся непосредственно к тексту:\n&lt;profileDesc&gt;\n  &lt;creation&gt;\n    &lt;date when=\"1852\"&gt;1852&lt;/date&gt;\n    &lt;placeName&gt;Москва&lt;/placeName&gt;\n    &lt;placeName&gt;станица Старогладковская&lt;/placeName&gt;\n    &lt;placeName&gt;Тифлис&lt;/placeName&gt;\n  &lt;/creation&gt;\n  &lt;langUsage&gt;\n    &lt;language ident=\"rus\" usage=\"99\"&gt;Русский&lt;/language&gt;\n    &lt;language ident=\"fra\" usage=\"0,5\"&gt;Французский&lt;/language&gt;\n    &lt;language ident=\"deu\" usage=\"0,5\"&gt;Немецкий&lt;/language&gt;\n  &lt;/langUsage&gt;\n  &lt;textClass&gt;\n    &lt;catRef type=\"type\" target=\"#short_novel\"/&gt;\n  &lt;/textClass&gt;\n&lt;/profileDesc&gt;\n\n\n6.2.3 Варианты и исправления\nВ самом тексте язык TEI дает возможность представлять разные варианты (авторские, редакторские, корректорские и др.) Основным средством параллельного представления является элемент choice. Например, в тексте Лукреция вы можете увидеть такое:\nsic calor atque &lt;choice&gt;&lt;reg&gt;aer&lt;/reg&gt;&lt;orig&gt;aër&lt;/orig&gt;&lt;/choice&gt; et venti caeca potestas\nЗдесь reg указывает на нормализованное написание, а orig – на оригинальное.\nДля исправления ошибок используются элементы &lt;sic&gt; (“так у автора”) и &lt;corr&gt; (“исправленное написание”):\n&lt;choice&gt;\n  &lt;sic&gt;вихремъ&lt;/sic&gt;\n  &lt;corr resp=\"#editor1\"&gt;верхомъ&lt;/corr&gt;\n&lt;/choice&gt;\nАтрибут resp содержит ссылку на идентификатор редактора.\n\n\n6.2.4 Структурная разметка\nTEI предоставляет богатый набор элементов для разметки структуры текста:\n\n&lt;text&gt; — текст целиком\n&lt;body&gt; — основное содержание текста\n&lt;div&gt; — структурное деление (глава, часть, раздел)\n&lt;p&gt; — параграф\n&lt;l&gt; — стихотворная строка\n&lt;lg&gt; — группа стихотворных строк (строфа)\n&lt;sp&gt; — речь персонажа в драме\n&lt;stage&gt; — ремарка\n\nПример разметки поэзии:\n&lt;lg type=\"quatrain\"&gt;\n  &lt;l met=\"+-|+-|+-|+-\"&gt;Дар напрасный, дар случайный,&lt;/l&gt;\n  &lt;l met=\"+-|+-|+-|+\"&gt;Жизнь, зачем ты мне дана?&lt;/l&gt;\n  &lt;l met=\"+-|+-|+-|+-\"&gt;Иль зачем судьбою тайной&lt;/l&gt;\n  &lt;l met=\"+-|+-|--|+\"&gt;Ты на казнь осуждена?&lt;/l&gt;\n&lt;/lg&gt;",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "xml.html#кейс-горе-от-ума",
    "href": "xml.html#кейс-горе-от-ума",
    "title": "6  Разметка TEI XML",
    "section": "6.3 Кейс: “Горе от ума”",
    "text": "6.3 Кейс: “Горе от ума”\nСкачаем по из репозитория проекта Dracor “Горе от ума” Грибоедова и преобразуем xml в прямоугольный формат таким образом, чтобы для каждой реплики был указан акт, сцена и действующее лицо.\n\n\n\n\n\n\nНа заметку\n\n\n\nДля работы с корпусом Dracor в среде R существует пакет rdracor. Он позволяет доставать тексты пьес сразу в виде таблицы.\n\n\n\nurl &lt;- \"https://raw.githubusercontent.com/dracor-org/rusdracor/main/tei/griboyedov-gore-ot-uma.xml\"\ndownload_xml(url, file = \"griboedov.xml\")\n\n\ndoc &lt;- read_xml(\"../files/griboedov.xml\")\n\n# определить пространство имён\nns &lt;- xml_ns(doc)\nns\n\nd1 &lt;-&gt; http://www.tei-c.org/ns/1.0\n\nns &lt;- xml_ns_rename(xml_ns(doc), d1 = \"tei\")\nns\n\ntei &lt;-&gt; http://www.tei-c.org/ns/1.0\n\n\nПространство имён (namespace) в XML — это механизм, который позволяет однозначно различать элементы и атрибуты с одинаковыми именами, но из разных словарей или стандартов. Оно действует как “фамилия” для элемента. Чтобы задать “фамилию”, её связывают с уникальным идентификатором (обычно это URI, в нашем случае http://www.tei-c.org/ns/1.0). Для удобства этому идентификатору присваивают короткий префикс.\n\n# Найти все строки (tei:l)\nline_nodes &lt;- xml_find_all(doc, \"//tei:l\", ns)\n\n# Извлечь текст каждой строки\nline_text &lt;- line_nodes |&gt; \n  xml_text()\n\nline_text |&gt; \n  head()\n\n[1] \"Светает!.. Ах! как скоро ночь минула!\"  \n[2] \"Вчера просилась спать — отказ.\"         \n[3] \"«Ждем друга». — Нужен глаз да глаз,\"    \n[4] \"Не спи, покудова не скатишься со стула.\"\n[5] \"Теперь вот только что вздремнула,\"      \n[6] \"Уж день!.. сказать им...\"               \n\n\nТеперь нам надо для каждой реплики найти информацию о том, кто говорит: она хранится в теге &lt;speaker&gt;. То есть нам надо подняться на два этажа вверх (на уровень &lt;sp&gt;), а потом спуститься к его другому “ребенку”, &lt;speaker&gt;.\nДля этого используем синтаксис XPath: сначала при помощи ancestor::tei:sp поднимаемся вверх по дереву и выбираем всех предков узла, которые являются элементами sp, а затем спускаемся к ребенку speaker этого найденного sp. Так список спикеров будет равно числу стихов.\n\n# line_nodes — вектор узлов &lt;l&gt;\nspeakers &lt;- line_nodes |&gt; \n  xml_find_first(\"ancestor::tei:sp/tei:speaker\", ns = ns) |&gt; \n  xml_text()\n\nspeakers |&gt; \n  head()\n\n[1] \"Лизанька\" \"Лизанька\" \"Лизанька\" \"Лизанька\" \"Лизанька\" \"Лизанька\"\n\n\nАналогичным образом находим явление и акт.\n\nscenes &lt;- line_nodes |&gt; \n  xml_find_first(\"ancestor::tei:div[@type='scene']/tei:head\", ns = ns) |&gt; \n  xml_text()\n\nscenes |&gt; \n  tail()\n\n[1] \"Явление 15\" \"Явление 15\" \"Явление 15\" \"Явление 15\" \"Явление 15\"\n[6] \"Явление 15\"\n\n\n\nacts &lt;- line_nodes |&gt; \n  xml_find_first(\"ancestor::tei:div[@type='act']/tei:head\", ns = ns) |&gt; \n  xml_text()\n\nacts |&gt; \n  sample(6)\n\n[1] \"Действие первое\"    \"Действие четвертое\" \"Действие четвертое\"\n[4] \"Действие второе\"    \"Действие третье\"    \"Действие четвертое\"\n\n\nНам осталось объединить все векторы в одну таблицу.\n\nwoe_from_wit &lt;- tibble(\n  act = acts,\n  scene = scenes,\n  speaker = speakers,\n  text = line_text\n)\n\nwoe_from_wit |&gt; \n  head(6) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nact\nscene\nspeaker\ntext\n\n\n\n\nДействие первое\nЯвление 1\nЛизанька\nСветает!.. Ах! как скоро ночь минула!\n\n\nДействие первое\nЯвление 1\nЛизанька\nВчера просилась спать — отказ.\n\n\nДействие первое\nЯвление 1\nЛизанька\n«Ждем друга». — Нужен глаз да глаз,\n\n\nДействие первое\nЯвление 1\nЛизанька\nНе спи, покудова не скатишься со стула.\n\n\nДействие первое\nЯвление 1\nЛизанька\nТеперь вот только что вздремнула,\n\n\nДействие первое\nЯвление 1\nЛизанька\nУж день!.. сказать им...",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "xml.html#кейс-война-и-мир",
    "href": "xml.html#кейс-война-и-мир",
    "title": "6  Разметка TEI XML",
    "section": "6.4 Кейс: “Война и мир”",
    "text": "6.4 Кейс: “Война и мир”\nВ качестве примера загрузим датасет “Пушкинского дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”.\n\nfilename = \"../files/War_and_Peace.xml\"\ndoc &lt;- read_xml(filename)\nns &lt;- xml_ns_rename(xml_ns(doc), d1 = \"tei\")\nns\n\ntei &lt;-&gt; http://www.tei-c.org/ns/1.0\n\n\n\nrootnode &lt;- xml_root(doc)\n\nКорневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена:\n\nxml_children(rootnode) |&gt;  \n  xml_name()\n\n[1] \"teiHeader\" \"text\"     \n\n\nОчевидно, что что-то для нас интересное будет спрятано в ветке text, смотрим на нее и ее детей:\n\ndivs &lt;- xml_find_all(rootnode, \"//tei:text//tei:div\", ns) \ndivs\n\n{xml_nodeset (380)}\n [1] &lt;div n=\"1\" type=\"volume\" xml:id=\"Volume1\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml ...\n [2] &lt;div n=\"1\" type=\"part\" xml:id=\"part1Volume1\"&gt;\\n  &lt;div n=\"1\" type=\"chapte ...\n [3] &lt;div n=\"1\" type=\"chapter\" xml:id=\"chapter1part1Volume1\"&gt;\\n  &lt;p/&gt;\\n  &lt;p&gt;\\ ...\n [4] &lt;div n=\"2\" type=\"chapter\" xml:id=\"chapter2part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n [5] &lt;div n=\"3\" type=\"chapter\" xml:id=\"chapter3part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n [6] &lt;div n=\"4\" type=\"chapter\" xml:id=\"chapter4part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n [7] &lt;div n=\"5\" type=\"chapter\" xml:id=\"chapter5part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n [8] &lt;div n=\"6\" type=\"chapter\" xml:id=\"chapter6part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n [9] &lt;div n=\"7\" type=\"chapter\" xml:id=\"chapter7part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n[10] &lt;div n=\"8\" type=\"chapter\" xml:id=\"chapter8part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n[11] &lt;div n=\"9\" type=\"chapter\" xml:id=\"chapter9part1Volume1\"&gt;\\n  &lt;p&gt;\\n     &lt;/ ...\n[12] &lt;div n=\"10\" type=\"chapter\" xml:id=\"chapter10part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[13] &lt;div n=\"11\" type=\"chapter\" xml:id=\"chapter11part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[14] &lt;div n=\"12\" type=\"chapter\" xml:id=\"chapter12part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[15] &lt;div n=\"13\" type=\"chapter\" xml:id=\"chapter13part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[16] &lt;div n=\"14\" type=\"chapter\" xml:id=\"chapter14part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[17] &lt;div n=\"15\" type=\"chapter\" xml:id=\"chapter15part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[18] &lt;div n=\"16\" type=\"chapter\" xml:id=\"chapter16part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[19] &lt;div n=\"17\" type=\"chapter\" xml:id=\"chapter17part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n[20] &lt;div n=\"18\" type=\"chapter\" xml:id=\"chapter18part1Volume1\"&gt;\\n  &lt;p&gt;\\n      ...\n...\n\n\nОбратите вниманию на разницу: при помощи одного слеша ищем только прямых потомков.\n\nxml_find_all(rootnode, \"//tei:text/tei:div\", ns) \n\n{xml_nodeset (5)}\n[1] &lt;div n=\"1\" type=\"volume\" xml:id=\"Volume1\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[2] &lt;div n=\"2\" type=\"volume\" xml:id=\"Volume2\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[3] &lt;div n=\"3\" type=\"volume\" xml:id=\"Volume3\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[4] &lt;div n=\"4\" type=\"volume\" xml:id=\"Volume4\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[5] &lt;div type=\"epilogue\" xml:id=\"novel_epilogue\"&gt;\\n  &lt;div n=\"1\" type=\"part\" x ...\n\n\nТак отбираем узлы по конкретному атрибуту.\n\nxml_find_all(rootnode, \"//tei:text//tei:div[@type='volume']\", ns)\n\n{xml_nodeset (4)}\n[1] &lt;div n=\"1\" type=\"volume\" xml:id=\"Volume1\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[2] &lt;div n=\"2\" type=\"volume\" xml:id=\"Volume2\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[3] &lt;div n=\"3\" type=\"volume\" xml:id=\"Volume3\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[4] &lt;div n=\"4\" type=\"volume\" xml:id=\"Volume4\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n\n\nТак ищем книгу или эпилог:\n\nxml_find_all(rootnode, \"//tei:text//tei:div[@type='volume' or @type='epilogue']\", ns)\n\n{xml_nodeset (5)}\n[1] &lt;div n=\"1\" type=\"volume\" xml:id=\"Volume1\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[2] &lt;div n=\"2\" type=\"volume\" xml:id=\"Volume2\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[3] &lt;div n=\"3\" type=\"volume\" xml:id=\"Volume3\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[4] &lt;div n=\"4\" type=\"volume\" xml:id=\"Volume4\"&gt;\\n  &lt;div n=\"1\" type=\"part\" xml: ...\n[5] &lt;div type=\"epilogue\" xml:id=\"novel_epilogue\"&gt;\\n  &lt;div n=\"1\" type=\"part\" x ...\n\n\nЗабрать конкретную главу можно по значению соответствующего атрибута. Извлечем только прямую речь Анны Павловны. Точка перед слешами означает поиск в одном (родительском) узле.\n\nrootnode |&gt; \n  xml_find_first(\"//tei:text//tei:div[@xml:id='chapter1part1Volume1']\", ns) |&gt; \n  xml_find_all(\".//tei:said[@who='Anna_Pavlovna_Scherer']\", ns) |&gt; \n  xml_attr(\"speech_text\") \n\n [1] \"- Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j'y crois) — je ne vous connais plus, vous n'êtes plus mon ami, vous n'êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\"\n [2] \"Вы весь вечер у меня, надеюсь?. Как можно быть здоровой... когда нравственно страдаешь?. Разве можно, имея чувство, оставаться спокойною в наше время?\"                                                                                                                                                                                                                                                                                                                                                                 \n [3] \"- Я думала, что нынешний праздник отменен. Je vous avoue que toutes ces t'êtes et tous ces feux d'artifice commencent à devenir insipides.\"                                                                                                                                                                                                                                                                                                                                                                             \n [4] \"- Ne me tourmentez pas. Eh bien, qu'a-t-on décidé par rapport à la dépêche de Novosilzoff? Vous savez tout.\"                                                                                                                                                                                                                                                                                                                                                                                                            \n [5] \"- Ах, не говорите мне про Австрию!\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n [6] \"A propos,. Сейчас. нынче у меня два очень интересные человека, le vicomte de Mortemart, il est allié aux Montmorency par les Rohans, одна из лучших фамилий Франции.\"                                                                                                                                                                                                                                                                                                                                                   \n [7] \"- А! Я очень рад буду,- сказал князь. — Скажите,- прибавил он, как будто только что вспомнив что-то и особенно-небрежно, тогда как то, о чем он спрашивал, было главной целью его посещения,- правда, что l'impératrice-mère желает назначения барона Функе первым секретарем в Вену?\"                                                                                                                                                                                                                                  \n [8] \"Monsieur le baron de Funke a été recommandé à l'impératrice-mère par sa sœur,\"                                                                                                                                                                                                                                                                                                                                                                                                                                          \n [9] \"Mais à propos de votre famille,\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n[10] \"- Я часто думаю, я часто думаю, как иногда несправедливо распределяется счастие жизни.\"                                                                                                                                                                                                                                                                                                                                                                                                                                 \n[11] \"За что вам дала судьба таких двух славных детей (исключая Анатоля, вашего меньшого, я его не люблю. таких прелестных детей? А вы, право, менее всех цените их и потому их не сто́ите.\"                                                                                                                                                                                                                                                                                                                                   \n[12] \"- Перестаньте шутить.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n[13] \"Перестаньте шутить. Я хотела серьезно поговорить с вами. Знаете, я недовольна вашим меньшим сыном. Между нами будь сказано (лицо ее приняло грустное выражение), о нем говорили у ее величества и жалеют вас...\"                                                                                                                                                                                                                                                                                                        \n[14] \"- И зачем родятся дети у таких людей, как вы? Ежели бы вы не были отец, я бы ни в чем не могла упрекнуть вас,- сказала Анна Павловна, задумчиво поднимая глаза.\"                                                                                                                                                                                                                                                                                                                                                        \n[15] \"- И зачем родятся дети у таких людей, как вы? Ежели бы вы не были отец, я бы ни в чем не могла упрекнуть вас\"                                                                                                                                                                                                                                                                                                                                                                                                           \n[16] \"Вы никогда не думали о том, чтобы женить вашего блудного сына Анатоля.. Говорят,\"                                                                                                                                                                                                                                                                                                                                                                                                                                       \n[17] \"- Отец очень богат и скуп.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n[18] \"Attendez. Я нынче же поговорю Lise (la femme du jeune Болконский).\"                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n\n\nПодбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan и Lang 2014) и (Холзнер 2004).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "json.html",
    "href": "json.html",
    "title": "5  Импорт: JSON",
    "section": "",
    "text": "5.1 Формат JSON\nФормат JSON (JavaScript Object Notation) предназначен для представления структурированных данных. JSON имеет шесть основных типов данных. Четыре из них - скаляры:\nСтроки, числа и булевы значения в JSON очень похожи на символьные, числовые и логические векторы в R. Основное отличие заключается в том, что скаляры JSON могут представлять только одно значение. Для представления нескольких значений необходимо использовать один из двух оставшихся типов: массивы и объекты.\nИ массивы, и объекты похожи на списки в R, разница заключается в том, именованы они или нет. Массив подобен безымянному списку и записывается через []. Например, [1, 2, 3] - это массив, содержащий 3 числа, а [null, 1, \"string\", false] - массив, содержащий ноль, число, строку и булево значение.\nОбъект подобен именованному списку и записывается через {}. Имена (ключи в терминологии JSON) являются строками, поэтому должны быть заключены в кавычки. Например, {\"x\": 1, \"y\": 2} - это объект, который сопоставляет x с 1, а y – с 2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#json",
    "href": "json.html#json",
    "title": "5  Импорт: JSON",
    "section": "",
    "text": "cамый простой тип - null, который играет ту же роль, что и NA в R. Он представляет собой отсутствие данных;\ncтрока (string) похожа на строку в R, но в ней всегда должны использоваться двойные кавычки;\nчисло аналогично числам в R, при этом поддерживается целочисленная (например, 123), десятичная (например, 123.45) или научная (например, 1,23e3) нотация. JSON не поддерживает Inf, -Inf или NaN;\nлогическое значение аналогично TRUE и FALSE в R, но использует строчные буквы true и false.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#пакет-jsonlite",
    "href": "json.html#пакет-jsonlite",
    "title": "5  Импорт: JSON",
    "section": "5.2 Пакет jsonlite",
    "text": "5.2 Пакет jsonlite\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\n\npath &lt;- \"../files/TBBT.json\"\ntbbt &lt;- read_json(path)\n\nФункция read_json() вернула нам список со следующими элементами:\n\nsummary(tbbt)\n\n                          Length Class  Mode     \nname                        1    -none- character\nseason_count                1    -none- character\nepisodes_count_total        1    -none- character\nepisodes_count_per_season  12    -none- list     \ncasting                    11    -none- list     \nepisode_list              280    -none- list     \nreferences                  1    -none- list",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#от-списка-к-таблице",
    "href": "json.html#от-списка-к-таблице",
    "title": "5  Импорт: JSON",
    "section": "5.3 От списка к таблице",
    "text": "5.3 От списка к таблице\nВыборочно преобразуем список в тиббл, используя функцию pluck() из пакета {purrr}.\n\nepisodes_count &lt;- tibble(\n  season = tbbt |&gt;  \n    pluck(\"episodes_count_per_season\") |&gt; \n    names(),\n  n = tbbt |&gt; \n    pluck(\"episodes_count_per_season\") |&gt; \n    as.integer()\n)\n\nepisodes_count |&gt; \n  print()\n\n# A tibble: 12 × 2\n   season     n\n   &lt;chr&gt;  &lt;int&gt;\n 1 S01       17\n 2 S02       23\n 3 S03       23\n 4 S04       24\n 5 S05       24\n 6 S06       24\n 7 S07       24\n 8 S08       24\n 9 S09       24\n10 S10       24\n11 S11       24\n12 S12       25\n\n\nФункция transpose() берет список списков и выворачивает его наизнанку: вместо списка, в котором для каждого из персонажей указан актер и первое появление, мы получаем три списка: с персонажами, актерами и эпизодами.\n\ntbbt |&gt; \n  pluck(\"casting\") |&gt; \n  map_dfr(as_tibble) |&gt; \n  print()\n\n# A tibble: 11 × 3\n   character_name          actor_name      first_appearance\n   &lt;chr&gt;                   &lt;chr&gt;           &lt;chr&gt;           \n 1 Sheldon Cooper          Jim Parsons     S01E01          \n 2 Leonard Hofstadter      Johnny Galecki  S01E01          \n 3 Penny                   Kaley Cuoco     S01E01          \n 4 Raj Kooththirapll       Kunal Nayyar    S01E01          \n 5 Howard Wolowitz         Simon Helberg   S01E01          \n 6 Bernadette Rostenkowski Melissa Rauch   S03E05          \n 7 Amy Farrah Fowler       Mayim Bialik    S03E23          \n 8 Stuart Bloom            Kevin Sussman   &lt;NA&gt;            \n 9 Wil Wheaton             Wil Wheaton     &lt;NA&gt;            \n10 Barry Kripke            John Ross Bowie &lt;NA&gt;            \n11 Zack                    &lt;NA&gt;            &lt;NA&gt;            \n\n\nЕще один способ.\n\ntibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\")) |&gt; \n  print()\n\n# A tibble: 280 × 2\n   episode_id title                             \n   &lt;chr&gt;      &lt;chr&gt;                             \n 1 S01E01     Pilot                             \n 2 S01E02     The Big Bran Hypothesis           \n 3 S01E03     The Fuzzy Boots Corollary         \n 4 S01E04     The Luminous Fish Effect          \n 5 S01E05     The Hamburger Postulate           \n 6 S01E06     The Middle-Earth Paradigm         \n 7 S01E07     The Dumpling Paradox              \n 8 S01E08     The Grasshopper Experiment        \n 9 S01E09     The Cooper-Hofstadter Polarization\n10 S01E10     The Loobenfeld Decay              \n# ℹ 270 more rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#кейс-шедевры-пушкинского-музея",
    "href": "json.html#кейс-шедевры-пушкинского-музея",
    "title": "5  Импорт: JSON",
    "section": "5.3 Кейс: Шедевры Пушкинского музея",
    "text": "5.3 Кейс: Шедевры Пушкинского музея\nJSON – популярный формат для публикации открытых данных. В таком виде часто публикуют данные органы государственной власти, культурные и некоммерческие организации и др. Например, Пушкинский музей.\nВзглянем на датасет “Шедевры из коллекции музея”. JSON можно прочитать напрямую из Сети.\n\ndoc &lt;- read_json(\"https://pushkinmuseum.art/json/masterpieces.json\")\n\nДатасет содержит информацию о 97 единицах хранения.\n\nnames(doc)\n\n [1] \"3687\"  \"3675\"  \"3706\"  \"3708\"  \"3713\"  \"3716\"  \"4005\"  \"4011\"  \"4014\" \n[10] \"4023\"  \"4030\"  \"4131\"  \"4147\"  \"4149\"  \"4161\"  \"4163\"  \"4178\"  \"4180\" \n[19] \"4191\"  \"4193\"  \"4198\"  \"4209\"  \"4244\"  \"4255\"  \"4260\"  \"4262\"  \"4266\" \n[28] \"4291\"  \"4325\"  \"4338\"  \"4350\"  \"4421\"  \"4450\"  \"4518\"  \"4543\"  \"4641\" \n[37] \"4711\"  \"4724\"  \"4767\"  \"7563\"  \"4782\"  \"4783\"  \"4788\"  \"4844\"  \"4906\" \n[46] \"4932\"  \"4936\"  \"4941\"  \"4949\"  \"4950\"  \"5238\"  \"5239\"  \"5297\"  \"5347\" \n[55] \"5591\"  \"5798\"  \"5910\"  \"5913\"  \"5992\"  \"6187\"  \"6226\"  \"6564\"  \"6584\" \n[64] \"6586\"  \"6629\"  \"6632\"  \"6886\"  \"7034\"  \"7151\"  \"7457\"  \"7468\"  \"7564\" \n[73] \"7565\"  \"7566\"  \"7567\"  \"7568\"  \"7569\"  \"7570\"  \"9464\"  \"9415\"  \"9046\" \n[82] \"10253\" \"10284\" \"10266\" \"10277\" \"10282\" \"10278\" \"10279\" \"10280\" \"10281\"\n[91] \"10285\" \"10286\" \"10287\" \"10288\" \"10289\" \"10290\" \"10291\"\n\n\nДля каждого предмета дано подробное описание.\n\nsummary(doc[[1]])\n\n                   Length Class  Mode     \npath               1      -none- character\nm_parent_id        1      -none- character\nyear               1      -none- numeric  \nget_year           1      -none- character\ninv_num            1      -none- character\ntype               2      -none- list     \ncountry            2      -none- list     \nperiod             2      -none- list     \npaint_school       1      -none- character\ngraphics_type      1      -none- character\ndepartment         1      -none- character\nmasterpiece        1      -none- character\nshow_in_hall       1      -none- character\nshow_in_collection 1      -none- numeric  \nname               2      -none- list     \nnamecom            2      -none- list     \nsize               2      -none- list     \ntext               2      -none- list     \nannotation         2      -none- list     \nlitra              2      -none- list     \nrestor             2      -none- list     \naudioguide         2      -none- list     \nvideoguide         2      -none- list     \nlink               2      -none- list     \nlinktext           2      -none- list     \nproducein          2      -none- list     \nmaterial           2      -none- list     \nfrom               2      -none- list     \nmatvos             2      -none- list     \nsizevos            2      -none- list     \nprodcast           2      -none- list     \nsearcha            2      -none- list     \nseakeys            2      -none- list     \nhall               1      -none- character\nbuilding           1      -none- character\ngallery            1      -none- list     \nauthors            1      -none- character\ncollectors         1      -none- list     \ncast               1      -none- character\nshop               1      -none- character\n\n\nЗаберем только то, что нам интересно.\n\nmasterpieces &lt;- tibble(\n  name = map_chr(doc, pluck, \"name\", \"ru\"),\n  #get_year = map_chr(doc, pluck, \"get_year\"),\n  year = map_int(doc, pluck, \"year\"),\n  period = map_chr(doc, pluck, \"period\", \"name\", \"ru\"),\n  country = map_chr(doc, pluck, \"country\", \"ru\"),\n  gallery = paste0(\"https://pushkinmuseum.art\", map_chr(doc, pluck, \"gallery\", 1, 1)))\n\nБиблиотека imager позволяет работать с изображениями из датасета. Вот так мы могли бы забрать одно из них.\n\nload.image(masterpieces$gallery[1]) |&gt; \n  plot()\n\n\nВ пакете imager есть функция map_il(), которая похожа на свою родню из {purrr}, но возвращает список изображений.\n\nimg_gallery &lt;- map_il(masterpieces$gallery, load.image)\n\nФункция walk() из пакета purrr – это аналог map() для тех случаев, когда нас интересует только вывод, т.е.не надо ничего сохранять в окружение.\n\npar(mfrow = c(10, 10), mar = rep(0,4))\nwalk(img_gallery, plot, axes = FALSE)\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПопробуйте самостоятельно узнать, когда приобретена большая часть шедевров и из каких регионов они происходят.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#api-запрос",
    "href": "json.html#api-запрос",
    "title": "5  Импорт: JSON",
    "section": "5.5 API запрос",
    "text": "5.5 API запрос\nМы заберем данные о нобелевских лауреатах по литературе. В данном случае API не требует ключа авторизации.\n\n# Базовый URL API Нобелевской премии\nbase_url &lt;- \"https://api.nobelprize.org/2.1/laureates\"\n\n\n# Параметры запроса (фильтрация и ограничение результатов)\nquery_params &lt;- list(\n  nobelPrizeCategory = \"lit\",  # Фильтр по категории\n  limit = 100                  # Ограничение количества результатов\n)\n\nДругие категории:\n\nphy (физика),\nche (химия),\nmed (медицина),\nlit (литература),\npea (мир),\neco (экономика).\n\n\n# Выполнение GET-запроса\nresponse &lt;- GET(url = base_url, query = query_params)\n\nФункция content() берет сырой ответ от API и возвращает готовые к анализу данные.\n\nnobel_data &lt;- content(response, \"text\")\n\n\nlaureates_data &lt;- fromJSON(nobel_data) \n\nНам осталось преобразовать данные в таблицу.\n\nlaureates_tbl &lt;- laureates_data |&gt; \n  pluck(\"laureates\") \n\n\nlaureates_tbl |&gt; \n  unnest_wider(knownName, names_sep  = \"_\") |&gt; \n  select(-fullName, -givenName, -knownName_se, -familyName, -fileName) |&gt; \n  unnest_wider(birth) |&gt; \n  unnest_wider(place) \n\n\n  \n\n\n\nИ так далее.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#формат-json",
    "href": "json.html#формат-json",
    "title": "5  Импорт: JSON",
    "section": "",
    "text": "cамый простой тип - null, который играет ту же роль, что и NA в R. Он представляет собой отсутствие данных;\ncтрока (string) похожа на строку в R, но в ней всегда должны использоваться двойные кавычки;\nчисло аналогично числам в R, при этом поддерживается целочисленная (например, 123), десятичная (например, 123.45) или научная (например, 1,23e3) нотация. JSON не поддерживает Inf, -Inf или NaN;\nлогическое значение аналогично TRUE и FALSE в R, но использует строчные буквы true и false.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#от-json-к-таблице",
    "href": "json.html#от-json-к-таблице",
    "title": "5  Импорт: JSON",
    "section": "5.2 От JSON к таблице",
    "text": "5.2 От JSON к таблице\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\n\npath &lt;- \"https://raw.githubusercontent.com/locusclassicus/text_analysis_2024/refs/heads/main/files/TBBT.json\"\ntbbt &lt;- read_json(path)\n\nФункция read_json() вернула нам список со следующими элементами:\n\nsummary(tbbt)\n\n                          Length Class  Mode     \nname                        1    -none- character\nseason_count                1    -none- character\nepisodes_count_total        1    -none- character\nepisodes_count_per_season  12    -none- list     \ncasting                    11    -none- list     \nepisode_list              280    -none- list     \nreferences                  1    -none- list     \n\n\nВыборочно преобразуем список в тиббл, используя функцию pluck() из пакета {purrr}.\n\nepisodes_count &lt;- tibble(\n  season = tbbt |&gt;  \n    pluck(\"episodes_count_per_season\") |&gt; \n    names(),\n  n = tbbt |&gt; \n    pluck(\"episodes_count_per_season\") |&gt; \n    as.integer()\n)\n\nepisodes_count |&gt; \n  print()\n\n# A tibble: 12 × 2\n   season     n\n   &lt;chr&gt;  &lt;int&gt;\n 1 S01       17\n 2 S02       23\n 3 S03       23\n 4 S04       24\n 5 S05       24\n 6 S06       24\n 7 S07       24\n 8 S08       24\n 9 S09       24\n10 S10       24\n11 S11       24\n12 S12       25\n\n\n\ntbbt |&gt; \n  pluck(\"casting\") |&gt; \n  map_dfr(as_tibble) |&gt; \n  print()\n\n# A tibble: 11 × 3\n   character_name          actor_name      first_appearance\n   &lt;chr&gt;                   &lt;chr&gt;           &lt;chr&gt;           \n 1 Sheldon Cooper          Jim Parsons     S01E01          \n 2 Leonard Hofstadter      Johnny Galecki  S01E01          \n 3 Penny                   Kaley Cuoco     S01E01          \n 4 Raj Kooththirapll       Kunal Nayyar    S01E01          \n 5 Howard Wolowitz         Simon Helberg   S01E01          \n 6 Bernadette Rostenkowski Melissa Rauch   S03E05          \n 7 Amy Farrah Fowler       Mayim Bialik    S03E23          \n 8 Stuart Bloom            Kevin Sussman   &lt;NA&gt;            \n 9 Wil Wheaton             Wil Wheaton     &lt;NA&gt;            \n10 Barry Kripke            John Ross Bowie &lt;NA&gt;            \n11 Zack                    &lt;NA&gt;            &lt;NA&gt;            \n\n\nЕще один способ.\n\ntibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\")\n  ) |&gt; \n  print()\n\n# A tibble: 280 × 2\n   episode_id title                             \n   &lt;chr&gt;      &lt;chr&gt;                             \n 1 S01E01     Pilot                             \n 2 S01E02     The Big Bran Hypothesis           \n 3 S01E03     The Fuzzy Boots Corollary         \n 4 S01E04     The Luminous Fish Effect          \n 5 S01E05     The Hamburger Postulate           \n 6 S01E06     The Middle-Earth Paradigm         \n 7 S01E07     The Dumpling Paradox              \n 8 S01E08     The Grasshopper Experiment        \n 9 S01E09     The Cooper-Hofstadter Polarization\n10 S01E10     The Loobenfeld Decay              \n# ℹ 270 more rows",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#кейс-нобелевские-лауреаты",
    "href": "json.html#кейс-нобелевские-лауреаты",
    "title": "5  Импорт: JSON",
    "section": "5.4 Кейс: Нобелевские лауреаты",
    "text": "5.4 Кейс: Нобелевские лауреаты\nМы заберем данные о нобелевских лауреатах по литературе. В данном случае API не требует ключа авторизации.\n\n# Базовый URL API Нобелевской премии\nbase_url &lt;- \"https://api.nobelprize.org/2.1/laureates\"\n\n\n# Параметры запроса (фильтрация и ограничение результатов)\nquery_params &lt;- list(\n  #nobelPrizeCategory = \"lit\",  # Фильтр по категории\n  limit = 1100                 # Ограничение количества результатов\n)\n\nДругие категории:\n\nphy (физика),\nche (химия),\nmed (медицина),\nlit (литература),\npea (мир),\neco (экономика).\n\n\n# Выполнение GET-запроса\nresponse &lt;- GET(url = base_url, query = query_params)\n\nФункция content() берет сырой ответ от API и возвращает готовые к анализу данные.\n\nnobel_data &lt;- content(response, \"text\")\n\n\nlaureates_data &lt;- fromJSON(nobel_data) \n\nНам осталось преобразовать данные в таблицу.\n\nlaureates_tbl &lt;- laureates_data |&gt; \n  pluck(\"laureates\") \n\nНе все столбцы в этой таблице представляют собой вектор, это можно проверить.\n\ntibble(name = map_chr(laureates_tbl, class) |&gt; \n         names(),\n       type = map_chr(laureates_tbl, class) |&gt; \n         as.character()\n)\n\n\n  \n\n\n\n\nlist_cols &lt;- laureates_tbl |&gt; \n  select_if(is.list) |&gt; \n  names()\n\nlist_cols\n\n [1] \"knownName\"         \"givenName\"         \"familyName\"       \n [4] \"fullName\"          \"birth\"             \"wikipedia\"        \n [7] \"wikidata\"          \"sameAs\"            \"links\"            \n[10] \"nobelPrizes\"       \"death\"             \"orgName\"          \n[13] \"founded\"           \"penNameOf\"         \"foundedCountry\"   \n[16] \"foundedCountryNow\" \"foundedContinent\" \n\n\nТеперь все распакуем. Заметьте, как увеличилось количество столбцов.\n\nlaureates_unnested &lt;- laureates_tbl |&gt; \n  unnest_wider(all_of(list_cols), names_sep = \"_\")\n\nlist_cols &lt;- laureates_unnested |&gt; \n  select_if(is.list) |&gt; \n  names()\n\nУберем все столбцы, которые содержат информацию на шведском и норвежском.\n\nlaureates_en &lt;- laureates_unnested |&gt; \n  select(-ends_with(\"_se\"), -ends_with(\"_no\"))\n\nlaureates_en\n\n\n  \n\n\n\nСнова проделаем фокус с “распаковкой” целой серии столбцов.\n\nlist_cols &lt;- laureates_en |&gt; \n  select_if(is.list) |&gt; \n  names()\n\nlist_cols\n\n [1] \"birth_place\"                     \"links_rel\"                      \n [3] \"links_href\"                      \"links_action\"                   \n [5] \"links_types\"                     \"links_title\"                    \n [7] \"links_class\"                     \"nobelPrizes_awardYear\"          \n [9] \"nobelPrizes_category\"            \"nobelPrizes_categoryFullName\"   \n[11] \"nobelPrizes_sortOrder\"           \"nobelPrizes_portion\"            \n[13] \"nobelPrizes_dateAwarded\"         \"nobelPrizes_prizeStatus\"        \n[15] \"nobelPrizes_motivation\"          \"nobelPrizes_prizeAmount\"        \n[17] \"nobelPrizes_prizeAmountAdjusted\" \"nobelPrizes_affiliations\"       \n[19] \"nobelPrizes_links\"               \"nobelPrizes_residences\"         \n[21] \"nobelPrizes_topMotivation\"       \"death_place\"                    \n[23] \"founded_place\"                  \n\n\n\nlaureates_en_unnested &lt;- laureates_en |&gt; \n  unnest_wider(all_of(list_cols), names_sep = \"_\") |&gt; \n  select(-contains(\"links\"))\n\nУберем лишние столбцы.\n\nlaureates_en_unnested |&gt; \n  colnames()\n\n [1] \"id\"                                \"knownName_en\"                     \n [3] \"givenName_en\"                      \"familyName_en\"                    \n [5] \"fullName_en\"                       \"fileName\"                         \n [7] \"gender\"                            \"birth_date\"                       \n [9] \"birth_place_city\"                  \"birth_place_country\"              \n[11] \"birth_place_cityNow\"               \"birth_place_countryNow\"           \n[13] \"birth_place_continent\"             \"birth_place_locationString\"       \n[15] \"wikipedia_slug\"                    \"wikipedia_english\"                \n[17] \"wikidata_id\"                       \"wikidata_url\"                     \n[19] \"sameAs_1\"                          \"sameAs_2\"                         \n[21] \"nobelPrizes_awardYear_1\"           \"nobelPrizes_awardYear_2\"          \n[23] \"nobelPrizes_awardYear_3\"           \"nobelPrizes_category_en\"          \n[25] \"nobelPrizes_category_no\"           \"nobelPrizes_category_se\"          \n[27] \"nobelPrizes_categoryFullName_en\"   \"nobelPrizes_categoryFullName_no\"  \n[29] \"nobelPrizes_categoryFullName_se\"   \"nobelPrizes_sortOrder_1\"          \n[31] \"nobelPrizes_sortOrder_2\"           \"nobelPrizes_sortOrder_3\"          \n[33] \"nobelPrizes_portion_1\"             \"nobelPrizes_portion_2\"            \n[35] \"nobelPrizes_portion_3\"             \"nobelPrizes_dateAwarded_1\"        \n[37] \"nobelPrizes_dateAwarded_2\"         \"nobelPrizes_dateAwarded_3\"        \n[39] \"nobelPrizes_prizeStatus_1\"         \"nobelPrizes_prizeStatus_2\"        \n[41] \"nobelPrizes_prizeStatus_3\"         \"nobelPrizes_motivation_en\"        \n[43] \"nobelPrizes_motivation_se\"         \"nobelPrizes_motivation_no\"        \n[45] \"nobelPrizes_prizeAmount_1\"         \"nobelPrizes_prizeAmount_2\"        \n[47] \"nobelPrizes_prizeAmount_3\"         \"nobelPrizes_prizeAmountAdjusted_1\"\n[49] \"nobelPrizes_prizeAmountAdjusted_2\" \"nobelPrizes_prizeAmountAdjusted_3\"\n[51] \"nobelPrizes_affiliations_1\"        \"nobelPrizes_affiliations_2\"       \n[53] \"nobelPrizes_residences_1\"          \"nobelPrizes_topMotivation_en\"     \n[55] \"nobelPrizes_topMotivation_se\"      \"death_date\"                       \n[57] \"death_place_city\"                  \"death_place_country\"              \n[59] \"death_place_cityNow\"               \"death_place_countryNow\"           \n[61] \"death_place_continent\"             \"death_place_locationString\"       \n[63] \"orgName_en\"                        \"acronym\"                          \n[65] \"founded_date\"                      \"founded_place_city\"               \n[67] \"founded_place_country\"             \"founded_place_cityNow\"            \n[69] \"founded_place_countryNow\"          \"founded_place_continent\"          \n[71] \"founded_place_locationString\"      \"nativeName\"                       \n[73] \"penName\"                           \"penNameOf_fullName\"               \n[75] \"foundedCountry_en\"                 \"foundedCountryNow_en\"             \n[77] \"foundedContinent_en\"              \n\n\n\nlist_cols &lt;- laureates_en_unnested |&gt; \n  select_if(is.list) |&gt; \n  names()\n\nlist_cols\n\n [1] \"birth_place_city\"                \"birth_place_country\"            \n [3] \"birth_place_cityNow\"             \"birth_place_countryNow\"         \n [5] \"birth_place_continent\"           \"birth_place_locationString\"     \n [7] \"nobelPrizes_category_en\"         \"nobelPrizes_category_no\"        \n [9] \"nobelPrizes_category_se\"         \"nobelPrizes_categoryFullName_en\"\n[11] \"nobelPrizes_categoryFullName_no\" \"nobelPrizes_categoryFullName_se\"\n[13] \"nobelPrizes_motivation_en\"       \"nobelPrizes_motivation_se\"      \n[15] \"nobelPrizes_motivation_no\"       \"nobelPrizes_affiliations_1\"     \n[17] \"nobelPrizes_affiliations_2\"      \"nobelPrizes_residences_1\"       \n[19] \"death_place_city\"                \"death_place_country\"            \n[21] \"death_place_cityNow\"             \"death_place_countryNow\"         \n[23] \"death_place_continent\"           \"death_place_locationString\"     \n[25] \"founded_place_city\"              \"founded_place_country\"          \n[27] \"founded_place_cityNow\"           \"founded_place_countryNow\"       \n[29] \"founded_place_continent\"         \"founded_place_locationString\"   \n\n\nНекоторые из этих столбцов сразу уберем, другие распакуем.\n\nlaureates_en_tidy &lt;- laureates_en_unnested |&gt; \n  select(-contains(\"_se\"), -contains(\"_no\"), -contains(\"locationString\")) |&gt; \n  unnest_wider(where(is.list), names_sep = \"_\") |&gt; \n  # удаляю столбцы на свое усмотрение\n  select(-contains(\"wikipedia\"), -contains(\"_se\"), -contains(\"_no\"),\n         -contains(\"longitude\"), -contains(\"latitude\"), -contains(\"sameAs\"),\n         -contains(\"wikidata\"), -contains(\"portion\"), -contains(\"Amount\"), \n         -contains(\"locationString\"), -givenName_en, -fullName_en, \n         -familyName_en, -contains(\"city\"), -contains(\"sortOrder\"),\n         -contains(\"continent\"), -contains(\"nativeName\"), -contains(\"penName\")\n         )\n\n\nlaureates_en_tidy |&gt; \n  colnames()\n\n [1] \"id\"                                   \n [2] \"knownName_en\"                         \n [3] \"fileName\"                             \n [4] \"gender\"                               \n [5] \"birth_date\"                           \n [6] \"birth_place_country_en\"               \n [7] \"birth_place_countryNow_en\"            \n [8] \"nobelPrizes_awardYear_1\"              \n [9] \"nobelPrizes_awardYear_2\"              \n[10] \"nobelPrizes_awardYear_3\"              \n[11] \"nobelPrizes_category_en_1\"            \n[12] \"nobelPrizes_category_en_2\"            \n[13] \"nobelPrizes_category_en_3\"            \n[14] \"nobelPrizes_categoryFullName_en_1\"    \n[15] \"nobelPrizes_categoryFullName_en_2\"    \n[16] \"nobelPrizes_categoryFullName_en_3\"    \n[17] \"nobelPrizes_dateAwarded_1\"            \n[18] \"nobelPrizes_dateAwarded_2\"            \n[19] \"nobelPrizes_dateAwarded_3\"            \n[20] \"nobelPrizes_prizeStatus_1\"            \n[21] \"nobelPrizes_prizeStatus_2\"            \n[22] \"nobelPrizes_prizeStatus_3\"            \n[23] \"nobelPrizes_motivation_en_1\"          \n[24] \"nobelPrizes_motivation_en_2\"          \n[25] \"nobelPrizes_motivation_en_3\"          \n[26] \"nobelPrizes_affiliations_1_name\"      \n[27] \"nobelPrizes_affiliations_1_nameNow\"   \n[28] \"nobelPrizes_affiliations_1_country\"   \n[29] \"nobelPrizes_affiliations_1_countryNow\"\n[30] \"nobelPrizes_affiliations_2_name\"      \n[31] \"nobelPrizes_affiliations_2_nameNow\"   \n[32] \"nobelPrizes_affiliations_2_country\"   \n[33] \"nobelPrizes_affiliations_2_countryNow\"\n[34] \"nobelPrizes_residences_1_country\"     \n[35] \"nobelPrizes_residences_1_countryNow\"  \n[36] \"nobelPrizes_topMotivation_en\"         \n[37] \"death_date\"                           \n[38] \"death_place_country_en\"               \n[39] \"death_place_countryNow_en\"            \n[40] \"orgName_en\"                           \n[41] \"acronym\"                              \n[42] \"founded_date\"                         \n[43] \"founded_place_country_en\"             \n[44] \"founded_place_countryNow_en\"          \n[45] \"foundedCountry_en\"                    \n[46] \"foundedCountryNow_en\"                 \n\n\nВ оставшихся данных нарушен принцип опрятного хранения: если на одного человека приходится несколько премий, они хранятся как отдельные столбцы, а не наблюдения. Это можно попробовать исправить (или же просто запросить данные, организованные по премиям, а не по людям – см. документацию).\n\nlaur_prize2 &lt;- laureates_en_tidy |&gt; \n  filter(!is.na(nobelPrizes_awardYear_2) & is.na(nobelPrizes_awardYear_3))\n\nlaur_prize3 &lt;- laureates_en_tidy |&gt; \n  filter(!is.na(nobelPrizes_awardYear_2) & !is.na(nobelPrizes_awardYear_3))\n\nС тремя премиями (1917, 1944, 1963) – только Красный Крест. Среди обладателей двух премий – некто refugees (1954 и 1981), за этим стоит Служба Верховного комиссара ООН по делам беженцев. Для остальных пока для простоты возьмем только первую премию.\n\nlaureates_final &lt;- laureates_en_tidy |&gt; \n  filter(is.na(nobelPrizes_awardYear_3)) |&gt; \n  select(-contains(\"_2\"), -contains(\"_3\"), -fileName, -acronym) |&gt; \n  select(-contains(\"countryNow\"), -contains(\"cityNow\"), -contains(\"nameNow\"),\n         -contains(\"residences\"), -contains(\"topMotivation\"), -contains(\"penName\"))\n\nДальше там надо еще много чистить, но чтобы немного ускорить:\n\ncol_names_old &lt;- colnames(laureates_final)\ncol_names_old\n\n [1] \"id\"                                 \"knownName_en\"                      \n [3] \"gender\"                             \"birth_date\"                        \n [5] \"birth_place_country_en\"             \"nobelPrizes_awardYear_1\"           \n [7] \"nobelPrizes_category_en_1\"          \"nobelPrizes_categoryFullName_en_1\" \n [9] \"nobelPrizes_dateAwarded_1\"          \"nobelPrizes_prizeStatus_1\"         \n[11] \"nobelPrizes_motivation_en_1\"        \"nobelPrizes_affiliations_1_name\"   \n[13] \"nobelPrizes_affiliations_1_country\" \"death_date\"                        \n[15] \"death_place_country_en\"             \"orgName_en\"                        \n[17] \"founded_date\"                       \"founded_place_country_en\"          \n[19] \"foundedCountry_en\"                 \n\n\n\ncol_names_new &lt;- str_remove_all(col_names_old, \"_en\") |&gt; \n  str_remove_all(\"_1\") |&gt; \n  str_remove_all(\"nobelPrizes_\")\n\ncolnames(laureates_final) &lt;- col_names_new\n\n\nlaureates_2025 &lt;- laureates_final |&gt; \n  mutate(awardYear = as.numeric(awardYear)) |&gt; \n  filter(awardYear == 2025)\n\nlaureates_2025 |&gt; \n  print()\n\n# A tibble: 11 × 19\n   id    knownName      gender birth_date birth_place_country awardYear category\n   &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;   \n 1 1048  Fred Ramsdell  male   1960-12-04 USA                      2025 Physiol…\n 2 1050  John Clarke    male   1942-00-00 United Kingdom           2025 Physics \n 3 1052  John M. Marti… male   1958-00-00 &lt;NA&gt;                     2025 Physics \n 4 1056  László Kraszn… male   1954-01-05 Hungary                  2025 Literat…\n 5 1057  Maria Corina … female 1967-00-00 Venezuela                2025 Peace   \n 6 1047  Mary E. Brunk… female 1961-00-00 &lt;NA&gt;                     2025 Physiol…\n 7 1051  Michel H. Dev… male   1953-00-00 France                   2025 Physics \n 8 1055  Omar M. Yaghi  male   1965-02-09 Jordan                   2025 Chemist…\n 9 1054  Richard Robson male   1937-06-04 United Kingdom           2025 Chemist…\n10 1049  Shimon Sakagu… male   1951-01-19 Japan                    2025 Physiol…\n11 1053  Susumu Kitaga… male   1951-07-04 Japan                    2025 Chemist…\n# ℹ 12 more variables: categoryFullName &lt;chr&gt;, dateAwarded &lt;chr&gt;,\n#   prizeStatus &lt;chr&gt;, motivation &lt;chr&gt;, affiliations_name &lt;list&lt;df[,3]&gt;&gt;,\n#   affiliations_country &lt;list&lt;df[,3]&gt;&gt;, death_date &lt;chr&gt;,\n#   death_place_country &lt;chr&gt;, orgName &lt;chr&gt;, founded_date &lt;chr&gt;,\n#   founded_place_country &lt;chr&gt;, foundedCountry &lt;chr&gt;\n\n\nВ качестве упражнения посчитайте статистику слов в различных номинациях за все годы и визуализируйте результат.\nПример исследования, выполненного по итогам этого курса, можно посмотреть по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#кейс-google-books",
    "href": "json.html#кейс-google-books",
    "title": "5  Импорт: JSON",
    "section": "5.5 Кейс: Google Books",
    "text": "5.5 Кейс: Google Books\nПодробные инструкции для разработчиков. Значение maxResults не может превышать 40 за один запрос, в день не более 1000.\n\nsearch_google_books &lt;- function(query, max_results = 15, start_index = 0) {\n  \n  base_url &lt;- \"https://www.googleapis.com/books/v1/volumes\"\n  \n  # Ограничиваем max_results 40\n  if (max_results &gt; 40) {\n    warning(\"max_results не может быть больше 40. Установлено 40.\")\n    max_results &lt;- 40\n  }\n  \n  full_url &lt;- paste0(\n    base_url, \n    \"?q=\", URLencode(query), \n    \"&maxResults=\", max_results,\n    \"&startIndex=\", start_index,\n    \"&printType=books\"\n  )\n  \n  response &lt;- GET(full_url)\n  \n  if (status_code(response) != 200) {\n    stop(\"Ошибка при запросе к API Google Books\")\n  }\n\n  content &lt;- content(response, \"text\", encoding = \"UTF-8\")\n  data &lt;- fromJSON(content)\n  \n  return(data)\n}\n\nПопробуем в действии.\n\nres &lt;- search_google_books('plato')\n\n\n# Извлекаем метаданные\nplato_data &lt;- res$items$volumeInfo |&gt; \n  mutate(authors = map_chr(authors, ~paste(.x, collapse = \", \"))) |&gt; \n  select(-industryIdentifiers, -readingModes, -printType, -maturityRating, -allowAnonLogging, -contentVersion, -panelizationSummary) |&gt; \n  unnest_wider(imageLinks, names_sep = \"_\") \n\nplato_data |&gt; \n  print()\n\n# A tibble: 15 × 16\n   title                    authors publisher publishedDate pageCount categories\n   &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;             &lt;int&gt; &lt;list&gt;    \n 1 PLATO                    Harold… Educatio… 1980                120 &lt;chr [1]&gt; \n 2 Plato                    Julia … Sterling… 2009                192 &lt;chr [1]&gt; \n 3 Plato                    George… &lt;NA&gt;      1867                656 &lt;chr [1]&gt; \n 4 Plato                    N. Jay… Atlantic… 1999                116 &lt;NULL&gt;    \n 5 Plato on Punishment      Mary M… Univ of … 1985-01-01          292 &lt;chr [1]&gt; \n 6 Plato's Theory of Art    Rupert… Psycholo… 1953                336 &lt;chr [1]&gt; \n 7 The Theaetetus of Plato  Myles … Hackett … 1990-01-01          376 &lt;chr [1]&gt; \n 8 Plato and the Foundatio… Hans J… SUNY Pre… 1990-01-01          354 &lt;chr [1]&gt; \n 9 Plato's Republic, Books… Plato   Agora Pu… 2001                414 &lt;chr [1]&gt; \n10 Plato's Defence of Poet… Julius… SUNY Pre… 1984-01-01          280 &lt;chr [1]&gt; \n11 A History of Greek Phil… W. K. … Cambridg… 1986-04-24          626 &lt;chr [1]&gt; \n12 Plato's Republic         Platón  &lt;NA&gt;      1894                532 &lt;NULL&gt;    \n13 Plato on Knowledge and … Nichol… Hackett … 1976-01-01          276 &lt;chr [1]&gt; \n14 Plato's Craft of Justice Richar… SUNY Pre… 1996-01-01          288 &lt;chr [1]&gt; \n15 Self-Knowledge in Plato… Charle… Penn Sta… 2010-11-01          337 &lt;chr [1]&gt; \n# ℹ 10 more variables: imageLinks_smallThumbnail &lt;chr&gt;,\n#   imageLinks_thumbnail &lt;chr&gt;, language &lt;chr&gt;, previewLink &lt;chr&gt;,\n#   infoLink &lt;chr&gt;, canonicalVolumeLink &lt;chr&gt;, description &lt;chr&gt;,\n#   subtitle &lt;chr&gt;, averageRating &lt;int&gt;, ratingsCount &lt;int&gt;\n\n\nСоздадим галерею обложек.\n\nlibrary(magick)\n\ncatch_cover &lt;- function(url) {\n   \n   img &lt;- image_read(url) |&gt; \n        image_border(\"white\", \"10x10\")  # Добавляем рамку\n   return(img)\n}\n\nplato_gallery &lt;- map(plato_data$imageLinks_smallThumbnail, catch_cover)\n\n\n\nФункция для создания сетки обложек.\n\n\n\n# Функция для создания сетки обложек\ncreate_cover_grid &lt;- function(images, cols = 4, target_width = 200) {\n  if (length(images) == 0) return(NULL)\n  \n  # Ресайзим все изображения к одинаковой ширине\n  images_resized &lt;- map(images, ~ image_scale(.x, paste0(target_width, \"x\")))\n  \n  # Вычисляем количество строк\n  rows &lt;- ceiling(length(images_resized) / cols)\n  \n  # Создаем строки\n  gallery_rows &lt;- map(1:rows, function(row) {\n    start_idx &lt;- (row - 1) * cols + 1\n    end_idx &lt;- min(row * cols, length(images_resized))\n    \n    row_images &lt;- images_resized[start_idx:end_idx]\n    \n    # Если в последней строке меньше изображений, добавляем пустые места\n    if (length(row_images) &lt; cols) {\n      empty_count &lt;- cols - length(row_images)\n      empty_images &lt;- map(1:empty_count, ~ image_blank(target_width, 300, \"white\"))\n      row_images &lt;- c(row_images, empty_images)\n    }\n    \n    # Объединяем изображения в строку\n    image_append(do.call(c, row_images), stack = FALSE)\n  })\n  \n  # Объединяем все строки\n  image_append(do.call(c, gallery_rows), stack = TRUE)\n}\n\n\n\n\n# Создаем и отображаем галерею\ngallery &lt;- create_cover_grid(plato_gallery, cols = 5)\nprint(gallery)\n\n\nПри работе важно учитывать лимиты API. Множественные запросы отправляем, например, так (но можно придумать и другие решения).\n\nstart_idx &lt;- seq(0, 200, 10)\ngoogle_data &lt;- map(start_idx, ~search_google_books(\"plato\", start_index = .x))\n\nИзвлекаем данные из списка.\n\ngoogle_tbl &lt;-  map_dfr(1:length(google_data), ~pluck(google_data, .x, 3, \"volumeInfo\"))\n\nИ дальше работаем с ними как обычно: приводим к опрятному виду, обобщаем, строим разведывательные графики.\n\ngoogle_tbl |&gt; \n  as_tibble() |&gt; \n  print()\n\n# A tibble: 210 × 22\n   title   authors publisher publishedDate industryIdentifiers readingModes$text\n   &lt;chr&gt;   &lt;list&gt;  &lt;chr&gt;     &lt;chr&gt;         &lt;list&gt;              &lt;lgl&gt;            \n 1 PLATO   &lt;chr&gt;   Educatio… 1980          &lt;df [2 × 2]&gt;        TRUE             \n 2 Plato   &lt;chr&gt;   Sterling… 2009          &lt;df [2 × 2]&gt;        FALSE            \n 3 Plato   &lt;chr&gt;   &lt;NA&gt;      1867          &lt;df [1 × 2]&gt;        FALSE            \n 4 Plato   &lt;chr&gt;   Atlantic… 1999          &lt;df [2 × 2]&gt;        FALSE            \n 5 Plato … &lt;chr&gt;   Univ of … 1985-01-01    &lt;df [2 × 2]&gt;        FALSE            \n 6 Plato'… &lt;chr&gt;   Psycholo… 1953          &lt;df [2 × 2]&gt;        TRUE             \n 7 The Th… &lt;chr&gt;   Hackett … 1990-01-01    &lt;df [2 × 2]&gt;        FALSE            \n 8 Plato … &lt;chr&gt;   SUNY Pre… 1990-01-01    &lt;df [2 × 2]&gt;        FALSE            \n 9 Plato'… &lt;chr&gt;   Agora Pu… 2001          &lt;df [2 × 2]&gt;        TRUE             \n10 Plato'… &lt;chr&gt;   SUNY Pre… 1984-01-01    &lt;df [2 × 2]&gt;        TRUE             \n# ℹ 200 more rows\n# ℹ 17 more variables: readingModes$image &lt;lgl&gt;, pageCount &lt;int&gt;,\n#   printType &lt;chr&gt;, categories &lt;list&gt;, maturityRating &lt;chr&gt;,\n#   allowAnonLogging &lt;lgl&gt;, contentVersion &lt;chr&gt;, panelizationSummary &lt;df[,2]&gt;,\n#   imageLinks &lt;df[,2]&gt;, language &lt;chr&gt;, previewLink &lt;chr&gt;, infoLink &lt;chr&gt;,\n#   canonicalVolumeLink &lt;chr&gt;, description &lt;chr&gt;, subtitle &lt;chr&gt;,\n#   averageRating &lt;int&gt;, ratingsCount &lt;int&gt;\n\n\nИ дальше работаем с ними как обычно, приводим к опрятному виду т.д.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#видео",
    "href": "json.html#видео",
    "title": "5  Импорт: JSON",
    "section": "5.6 Видео",
    "text": "5.6 Видео\n\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "json.html#домашнее-задание",
    "href": "json.html#домашнее-задание",
    "title": "5  Импорт: JSON",
    "section": "5.7 Домашнее задание",
    "text": "5.7 Домашнее задание\nДля этого задания необходимо исследовать датасет Министерства Культуры о репертуарах российских театров (источник).\nЭто ПОЛОВИНА задания на оценку 0-10, т.е. максимальная оценка за него = 5 баллов. Вторая половина будет следующий раз (результаты суммируются). Все подробности в GitHub Classroom по ссылке. Дедлайн 19 октября 21-00 мск.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON</span>"
    ]
  },
  {
    "objectID": "xml.html#видео",
    "href": "xml.html#видео",
    "title": "6  Разметка TEI XML",
    "section": "6.5 Видео",
    "text": "6.5 Видео\n\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "xml.html#домашнее-задание",
    "href": "xml.html#домашнее-задание",
    "title": "6  Разметка TEI XML",
    "section": "6.6 Домашнее задание",
    "text": "6.6 Домашнее задание\nИспользуя библиотеки {tidyverse} и {xml2}, вам необходимо запарсить датасет писем Л.Н. Толстого (всего 9087). Источник.\nВы найдете письма в репозитории, приняв задание через GitHub Classroom https://classroom.github.com/a/8LHrY3gq.\nПожалуйста, не переименовывайте папки и не меняйте структуру рабочей директории. Заготовки для кода в этот раз в репозитории нет: файл с решением (любое имя, расширение .R) загрузите самостоятельно — задание проверяется вручную.\nВаша задача – написать функцию на R, которая для каждого письма извлекает том, дату и адресата, и собрать эти данные в одну таблицу. XML имеет очень ветвистую структуру, для начала испытайте свой код на любом одном письме. Потом оберните свое решение в функцию read_letter() и прочтите все письма в один тиббл при помощи map_dfr().\nЧто вам надо извлечь:\n\nдата письма: header -&gt; correspAction, тип sending -&gt; тег date, атрибут when;\nв header –&gt; тег correspAction, тип receiving —&gt; имя получателя (текст)\nв header — biblScope, юнит vol = номер тома\n\nКритерии оценивания (1 п. = 1 б.):\n\nкорректно извлечены все сведения для одного письма (без функции);\nкорректно извлечены все сведения для всех писем (с функцией), задействован map_dfr();\nаккуратное оформление кода, воспроизводимость;\nиз столбца с датой извлечен (любым способом, можно с {lubridate}) год, хранится как тип numeric, каждое письмо снабжено уникальным id (можно просто номер ряда), пустые строки заменены на NA;\nесть разведывательный анализ данных и график, например: письма за какие года хранятся в каком томе? сколько NA по томам? в каком году кому чаще всего писал Толстой, сколько писем в год и т.д. Достаточно чего-то одного. Снабдите код к этому пункту своими комментариями.\n\nТекст писем извлекать (пока) не надо. Не забывайте использовать trimws()!\nНа выполнение задания дается две недели (до 3 ноября 23:59).\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Разметка TEI XML</span>"
    ]
  },
  {
    "objectID": "share.html#выделенные-блоки",
    "href": "share.html#выделенные-блоки",
    "title": "7  Публикационная система Quarto",
    "section": "7.6 Выделенные блоки",
    "text": "7.6 Выделенные блоки\nCallouts (выделенные блоки) используются для привлечения дополнительного внимания читателя к определённому содержанию или чтобы указать, что информация может быть вспомогательной или относиться только к определённым случаям.\nТипы callout-блоков:\n\nNote (заметка)\nTip (совет)\nWarning (предупреждение)\nCaution (осторожно)\nImportant (важно)\n\nПодробнее.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#видео-к-этому-уроку",
    "href": "share.html#видео-к-этому-уроку",
    "title": "7  Публикационная система Quarto",
    "section": "7.9 Видео к этому уроку",
    "text": "7.9 Видео к этому уроку\n\nВидео 2025 г.\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using R. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "regex.html#str_sub-str_count-str_length",
    "href": "regex.html#str_sub-str_count-str_length",
    "title": "8  Регулярные выражения",
    "section": "8.2 str_sub(), str_count(), str_length()",
    "text": "8.2 str_sub(), str_count(), str_length()\nФункция str_sub() извлекает подстроку из строки.\n\nopening &lt;- str_sub(pp, 46, 162)\nopening\n\n[1] \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n\n\nЕе также можно использовать для замены. Удалим название и автора.\n\nstr_sub(pp, 1, 35) &lt;- \"\"\nstr_trunc(pp, 35)\n\n[1] \"Chapter 1 It is a truth universa...\"\n\n\nУзнаем длину первого предложения в символах.\n\nstr_length(opening) # или: nchar(opening)\n\n[1] 117\n\n\nУзнать длину в словах можно, посчитав число пробелов и добавив 1 (лишние пробелы мы уже удалили).\n\nstr_count(opening, \" \") + 1\n\n[1] 23",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#классы-символов",
    "href": "regex.html#классы-символов",
    "title": "8  Регулярные выражения",
    "section": "8.3 Классы символов",
    "text": "8.3 Классы символов\nВ таблице показаны наиболее часто используемые специальные обозначения, которые применяются в регулярных выражениях для удобного поиска разных типов символов. (Пишем две косые черты, т.к. вторая косая черта “экранирует” первую, см. далее).\n\n\n\n\n\n\n\n\nПредставление\nЭквивалент\nЗначение\n\n\n\n\n\\\\d\n[0-9]\nЦифра\n\n\n\\\\D\n[^\\\\d]\nЛюбой символ, кроме цифры\n\n\n\\\\w\n[A-Za-zА-Яа-я0-9_]\nСимволы, образующие «слово» (буквы, цифры и символ подчёркивания)\n\n\n\\\\W\n[^\\\\w]\nСимволы, не образующие «слово»\n\n\n\\\\s\n[ \\\\t\\\\v\\\\r\\\\n\\\\f]\nПробельный символ\n\n\n\\\\S\n[^\\\\s]\nНепробельный символ\n\n\n\nТак называемые символьные классы (или по-другому — POSIX-классы) также используются в регулярных выражениях для обозначения различных групп символов.\n\n\n\n\n\n\n\n\nКласс\nЭквивалент\nЗначение\n\n\n\n\n[:upper:]\n[A-Z]\nСимволы верхнего регистра\n\n\n[:lower:]\n[a-z]\nСимволы нижнего регистра\n\n\n[:alpha:]\n[[:upper:][:lower:]]\nБуквы\n\n\n[:digit:]\n[0-9], т. е. \\\\d\nЦифры\n\n\n[:alnum:]\n[[:alpha:][:digit:]]\nБуквы и цифры\n\n\n[:word:]\n[[:alnum:]_], т. е. \\\\w\nСимволы, образующие “слово”\n\n\n[:punct:]\n[!\"#$%&'()*+,\\-./:;&lt;=&gt;?@[\\]^_{|}~]\nЗнаки пунктуации\n\n\n[:blank:]\n[\\\\s\\\\t]\nПробел и табуляция\n\n\n[:space:]\n[[:blank:]\\\\v\\\\r\\\\n\\\\f], т. е. \\\\s\nПробельные символы\n\n\n[:cntrl:]\n\nУправляющие символы (перевод строки, табуляция и т.п.)\n\n\n[:graph:]\n\nПечатные символы\n\n\n[:print:]\n\nПечатные символы с пробелом",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_split-boundary-классы",
    "href": "regex.html#str_split-boundary-классы",
    "title": "8  Регулярные выражения",
    "section": "8.4 str_split(), boundary(), классы",
    "text": "8.4 str_split(), boundary(), классы\nФункция str_split() разбивает одну строку на несколько и возвращает список. Чтобы получить вектор, используйте ее с unlist() или замените на str_split_1():\n\nstr_split_1(pp, pattern = \"Chapter \\\\d+\") |&gt; \n  length()\n\n[1] 62\n\n\nВсего в романе 61 глава, но у нас получилось 62 элемента вместе с названием. В качестве шаблона мы использовали сочетание \"Chapter \\\\d+\", где \\\\d+ - это класс цифр (\\\\d) и квантификатор (+). Все вместе означает “одна или больше цифр”. Это можно записать иначе:\n\nstr_split_1(pp, pattern = \"Chapter [0-9]+\") |&gt; \n  length()\n\n[1] 62\n\n\nРазобьем первое предложение на слова, используя предопределенный класс пробельных символов.\n\nstr_split_1(opening, \"[[:space:]]\") \n\n [1] \"It\"            \"is\"            \"a\"             \"truth\"        \n [5] \"universally\"   \"acknowledged,\" \"that\"          \"a\"            \n [9] \"single\"        \"man\"           \"in\"            \"possession\"   \n[13] \"of\"            \"a\"             \"good\"          \"fortune,\"     \n[17] \"must\"          \"be\"            \"in\"            \"want\"         \n[21] \"of\"            \"a\"             \"wife.\"        \n\n\nКлассы можно комбинировать и сочетать с квантификаторами.\n\nstr_split_1(opening, \"[[:space:][:punct:]]+\") \n\n [1] \"It\"           \"is\"           \"a\"            \"truth\"        \"universally\" \n [6] \"acknowledged\" \"that\"         \"a\"            \"single\"       \"man\"         \n[11] \"in\"           \"possession\"   \"of\"           \"a\"            \"good\"        \n[16] \"fortune\"      \"must\"         \"be\"           \"in\"           \"want\"        \n[21] \"of\"           \"a\"            \"wife\"         \"\"            \n\n\nДелить на слова можно и без регулярных выражений.\n\nstr_split_1(opening, boundary(\"word\"))\n\n [1] \"It\"           \"is\"           \"a\"            \"truth\"        \"universally\" \n [6] \"acknowledged\" \"that\"         \"a\"            \"single\"       \"man\"         \n[11] \"in\"           \"possession\"   \"of\"           \"a\"            \"good\"        \n[16] \"fortune\"      \"must\"         \"be\"           \"in\"           \"want\"        \n[21] \"of\"           \"a\"            \"wife\"        \n\n\nС предложениями так просто разобраться не получиться из-за сокращений Mr. и Mrs.\n\nstr_split_1(pp, boundary(\"sentence\")) |&gt; \n  head()\n\n[1] \"Chapter 1 It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife. \"                                                                                                                                  \n[2] \"However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered the rightful property of some one or other of their daughters. \"\n[3] \"\\\"My dear Mr. \"                                                                                                                                                                                                                                                    \n[4] \"Bennet,\\\" said his lady to him one day, \\\"have you heard that Netherfield Park is let at last?\\\" \"                                                                                                                                                                 \n[5] \"Mr. \"                                                                                                                                                                                                                                                              \n[6] \"Bennet replied that he had not. \"                                                                                                                                                                                                                                  \n\n\nПопробуем решить эту проблему при помощи замены. Но сначала узнаем кое-что о точке, которую нам предстоит заменить.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#метасимволы-и-экранирование",
    "href": "regex.html#метасимволы-и-экранирование",
    "title": "8  Регулярные выражения",
    "section": "8.5 Метасимволы и экранирование",
    "text": "8.5 Метасимволы и экранирование\nВсе метасимволы представлены в таблице ниже.\n\n\n\nОписание\nСимвол\n\n\n\n\nоткрывающая квадратная скобка\n[\n\n\nзакрывающая квадратная скобка\n]\n\n\nобратная косая черта\n\\\n\n\nкарет\n^\n\n\nзнак доллара\n$\n\n\nточка\n.\n\n\nвертикальная черта\n|\n\n\nзнак вопроса\n?\n\n\nастериск\n*\n\n\nплюс\n+\n\n\nоткрывающая фигурная скобка\n{\n\n\nзакрывающая фигурная скобка\n}\n\n\nоткрывающая круглая скобка\n(\n\n\nзакрывающая круглая скобка\n)\n\n\n\nКвадратные скобки используются для создания классов, карет и знак доллара – это якоря (см. далее), но карет внутри квадратных скобок может также быть отрицанием. Точка – это любой знак, “джокер”.\nЕсли необходимо найти буквальную точку, буквальный знак вопроса и т.п., то используется экранирование: перед знаком ставится косая черта. Но так как сама косая черта – это метасимвол, но нужно две косые черты, первая из которых экранирует вторую.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_replace-str_replace_all",
    "href": "regex.html#str_replace-str_replace_all",
    "title": "8  Регулярные выражения",
    "section": "8.6 str_replace(), str_replace_all()",
    "text": "8.6 str_replace(), str_replace_all()\nИспытаем функции str_replace() и str_replace_all() на одном предложении. Обратите внимание на кавычки: двойные можно использовать внутри одинарных (и наоборот).\n\ndialogue &lt;- c('\"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\" Mr. Bennet replied that he had not. \"But it is,\" returned she; \"for Mrs. Long has just been here, and she told me all about it.\"')\n\nstr_replace_all(dialogue, \"Mr\\\\.\", \"Mr\") |&gt; \n  str_split_1(boundary(\"sentence\"))\n\n[1] \"\\\"My dear Mr Bennet,\\\" said his lady to him one day, \\\"have you heard that Netherfield Park is let at last?\\\" \"\n[2] \"Mr Bennet replied that he had not. \"                                                                           \n[3] \"\\\"But it is,\\\" returned she; \\\"for Mrs. \"                                                                      \n[4] \"Long has just been here, and she told me all about it.\\\"\"                                                      \n\n\nМы не убрали точку после сокращения Mrs, поэтому последнее предложение разделилось на два. Используем круглые скобки для группировки и оператор “или”. Третий аргумент означает, что мы оставляем первую группу, остальное отбрасываем.\n\nstr_replace_all(dialogue, \"(Mr|Mrs)\\\\.\", \"\\\\1\") \n\n[1] \"\\\"My dear Mr Bennet,\\\" said his lady to him one day, \\\"have you heard that Netherfield Park is let at last?\\\" Mr Bennet replied that he had not. \\\"But it is,\\\" returned she; \\\"for Mrs Long has just been here, and she told me all about it.\\\"\"\n\n\nПрежде чем проделать такую замену для всего текста, проверим, нет ли там других подобных сокращений. Нам надо найти все последовательности символов, образующих слово, до точки, и посчитать частотность. Для этого понадобятся квантификаторы.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#квантификаторы",
    "href": "regex.html#квантификаторы",
    "title": "8  Регулярные выражения",
    "section": "8.7 Квантификаторы",
    "text": "8.7 Квантификаторы\nКвантификатор после символа, символьного класса или группы определяет, сколько раз предшествующее выражение может встречаться. Квантификатор может относиться более чем к одному символу в регулярном выражении, только если это символьный класс или группа.\n\n\n\nПредставление\nЧисло повторений\nЭквивалент\n\n\n\n\n?\nноль или одно\n{0,1}\n\n\n*\nноль или более\n{0,}\n\n\n+\nодно или более\n{1,}\n\n\n\nТочное число повторений (интервал) можно задать в фигурных скобках:\n\n\n\nПредставление\nЧисло повторений\n\n\n\n\n{n}\nровно n раз\n\n\n{m,n}\nот m до n включительно\n\n\n{m,}\nне менее m\n\n\n{,n}\nне более n\n\n\n\nЧасто используется последовательность .* для обозначения любого количества любых символов между двумя частями регулярного выражения. Попробуем найти символы между прописной буквой и знаком препинания.\n\nstr_view(dialogue, \"[A-Z].+[[:punct:]]\")\n\n[1] │ \"&lt;My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\" Mr. Bennet replied that he had not. \"But it is,\" returned she; \"for Mrs. Long has just been here, and she told me all about it.\"&gt;\n\n\nПо умолчанию квантификаторам соответствует максимально длинная строка из возможных: квантификаторы являются жадными (greedy). Чтобы этого избежать, надо поставить после квантификатора знак вопроса. Это сделает его ленивым.\n\n\n\nregex\nзначение\n\n\n\n\n??\n0 или 1, лучше 0\n\n\n*?\n0 или больше, как можно меньше\n\n\n+?\n1 или больше, как можно меньше\n\n\n{n,m}?\nот n до m, как можно меньше\n\n\n\n\nstr_view(dialogue, \"[A-Z].+?[[:punct:]]\")\n\n[1] │ \"&lt;My dear Mr.&gt; &lt;Bennet,&gt;\" said his lady to him one day, \"have you heard that &lt;Netherfield Park is let at last?&gt;\" &lt;Mr.&gt; &lt;Bennet replied that he had not.&gt; \"&lt;But it is,&gt;\" returned she; \"for &lt;Mrs.&gt; &lt;Long has just been here,&gt; and she told me all about it.\"\n\n\nСокращения больше трех букв вряд ли встретятся, поэтому можем уточнить код.\n\npattern &lt;- \"[A-Z].{1,2}\\\\.\"\nstr_view(dialogue, pattern)\n\n[1] │ \"My dear &lt;Mr.&gt; Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\" &lt;Mr.&gt; Bennet replied that he had not. \"But it is,\" returned she; \"for &lt;Mrs.&gt; Long has just been here, and she told me all about it.\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#наблюдающие-конструкции",
    "href": "regex.html#наблюдающие-конструкции",
    "title": "8  Регулярные выражения",
    "section": "8.8 Наблюдающие конструкции",
    "text": "8.8 Наблюдающие конструкции\nВ нашем тексте иногда встречается написание имен прописными буквами, в первую очередь в письмах.\n\nletter &lt;- \"I shall write again as soon as anything more is determined on. Yours, etc., EDW. GARDINER.\"\n\nstr_view(letter, pattern)\n\n[1] │ I shall write again as soon as anything more is determined on. Yours, etc., &lt;EDW.&gt; GARDI&lt;NER.&gt;\n\n\nИсключим те случаи, когда перед паттерном нет пробела.\n\n\n\n\n\n\n\nregex\nзначение\n\n\n\n\n(?=...)\nПоложительный просмотр вперёд: далее должно идти …\n\n\n(?!...)\nОтрицательный просмотр вперёд: далее не должно идти …\n\n\n(?&lt;=...)\nПоложительный просмотр назад: перед этим должно быть …\n\n\n(?&lt;!...)\nОтрицательный просмотр назад: перед этим не должно быть …\n\n\n\nВнутри скобок вместо троеточий (...) пишется нужное регулярное выражение.\n\npattern &lt;- \"(?&lt;![A-Z])[A-Z].{1,2}\\\\.\"\nstr_view(letter, pattern)\n\n[1] │ I shall write again as soon as anything more is determined on. Yours, etc., &lt;EDW.&gt; GARDINER.\n\n\nСнова поправим: вместо любого символа после прописной ищем только строчные.\n\npattern &lt;- \"(?&lt;![A-Z])[A-Z][a-z]{1,2}\\\\.\"\nstr_view(letter, pattern)\n\n\nstr_view(dialogue, pattern)\n\n[1] │ \"My dear &lt;Mr.&gt; Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\" &lt;Mr.&gt; Bennet replied that he had not. \"But it is,\" returned she; \"for &lt;Mrs.&gt; Long has just been here, and she told me all about it.\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_extract-str_match",
    "href": "regex.html#str_extract-str_match",
    "title": "8  Регулярные выражения",
    "section": "8.9 str_extract(), str_match()",
    "text": "8.9 str_extract(), str_match()\nФункция str_extract() ищет совпадения и возвращает только первое вхождение, str_extract_all() возвращает все. Функция str_match() работает похожим образом, но возвращает матрицу.\n\nabbreviations &lt;- str_extract_all(pp, pattern) |&gt; \n  unlist() |&gt; \n  table() |&gt; \n  sort(decreasing = TRUE)\n\nabbreviations\n\n\n Mr. Mrs.  St. Esq. Yes. \n 786  343    7    1    1 \n\n\nТеперь можно удалить точки из подобных сокращений.\n\npattern &lt;- abbreviations[-5] |&gt; \n  names() |&gt; \n  str_remove(\"\\\\.\") |&gt; \n  str_c(collapse = \"|\")\n\npattern &lt;- str_c(pattern, \"|Sept|EDW\", collapse = \"\")\n\npattern &lt;- str_c(\"(\", pattern, \")\\\\.\")\npattern\n\n[1] \"(Mr|Mrs|St|Esq|Sept|EDW)\\\\.\"\n\n\n\npp_new &lt;- str_replace_all(pp, pattern, \"\\\\1\")\n\nТеперь разделим текст на предложения и сохраним их в тиббл.\n\npp_tbl &lt;- tibble(text = str_split_1(pp_new, boundary(\"sentence\")))\npp_tbl\n\n\n  \n\n\n\nНемного приберемся в таблице: удалим подчеркивания (они соответствуют курсиву) и кавычки.\n\npp_tbl &lt;- pp_tbl |&gt; \n  mutate(text = str_remove_all(text, \"_\"),\n         text = str_remove_all(text, '\\\\\"'))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_detect-str_which-str_subset",
    "href": "regex.html#str_detect-str_which-str_subset",
    "title": "8  Регулярные выражения",
    "section": "8.10 str_detect(), str_which(), str_subset()",
    "text": "8.10 str_detect(), str_which(), str_subset()\nДля поиска совпадений используются три функции: str_detect(), str_which() и str_subset(). Первая возвращает логический вектор (то есть вектор значений TRUE / FALSE); вторая - индексы элементов, а третья - сами эти элементы.\nДобавим столбец с номером главы и удалим “Chapter …” из текста.\n\npp_tbl &lt;- pp_tbl |&gt; \n  mutate(chapter_nr = cumsum(str_detect(text, \"Chapter \\\\d+\"))) |&gt; \n  mutate(text = str_remove(text, \"Chapter \\\\d+\"))\n\npp_tbl",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#раз-дарси-два-дарси",
    "href": "regex.html#раз-дарси-два-дарси",
    "title": "8  Регулярные выражения",
    "section": "8.11 Раз Дарси, два Дарси…",
    "text": "8.11 Раз Дарси, два Дарси…\nСколько раз в каждой главе упоминается мистер Дарси?\n\ndarcy_tbl &lt;- pp_tbl |&gt; \n  mutate(n = str_count(text, \"(Mr )?Darcy\")) |&gt; \n  group_by(chapter_nr) |&gt; \n  summarise(Darcy = sum(n))\n\ndarcy_tbl\n\n\n  \n\n\n\nПосчитаем то же для Элизабет (“мисс Беннет” не считаем, их слишком много).\n\nliz_tbl &lt;- pp_tbl |&gt; \n  mutate(n = str_count(text, \"Elizabeth\")) |&gt; \n  group_by(chapter_nr) |&gt; \n  summarise(Elizabeth = sum(n))\n\nliz_tbl\n\n\n  \n\n\n\nПостроим импровизированный таймлайн.\n\ndarcy_tbl |&gt; \n  left_join(liz_tbl) |&gt; \n  pivot_longer(-chapter_nr, values_to = \"n\") |&gt; \n  ggplot(aes(chapter_nr, n, color = name)) +\n  geom_step(linewidth = 1.2, alpha = 0.7) +\n  theme_light() + \n  scale_color_manual(values = c(\"royalblue\", \"pink\"))\n\n\n\n\n\n\n\n\nСимметрия - признак мастерства!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_to_upper-и-str_to_lower",
    "href": "regex.html#str_to_upper-и-str_to_lower",
    "title": "8  Регулярные выражения",
    "section": "8.12 str_to_upper() и str_to_lower()",
    "text": "8.12 str_to_upper() и str_to_lower()\nФункции str_to_upper() и str_to_lower() меняют начертание с прописного на строчное или наоборот.\n\npp_tbl |&gt; \n  mutate(text = str_to_lower(text)) |&gt; \n  mutate(n = str_count(text, \"married\")) |&gt; \n  group_by(chapter_nr) |&gt; \n  summarise(married = sum(n)) |&gt; \n  arrange(-married)\n\n\n  \n\n\n\nБольше всего шума из-за замужества Лидии в главе 49!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#якоря-и-отрицания",
    "href": "regex.html#якоря-и-отрицания",
    "title": "8  Регулярные выражения",
    "section": "8.13 Якоря и отрицания",
    "text": "8.13 Якоря и отрицания\nЯкоря позволяют искать последовательности символов в начале или в конце строки. Знак ^ (вне квадратных скобок!) означает начало строки, а знак $ – конец. Мнемоническое правило: First you get the power (^) and then you get the money ($).\nКакие предложения начинаются с Dear…?\n\npp_tbl |&gt; \n  filter(str_detect(text, \"^Dear\"))\n\n\n  \n\n\n\nКакие предложения не заканчиваются точкой?\n\npp_tbl |&gt; \n  mutate(text = str_squish(text)) |&gt; \n  filter(!str_detect(text, \"\\\\.$\"))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#упражнения",
    "href": "regex.html#упражнения",
    "title": "8  Регулярные выражения",
    "section": "8.14 Упражнения",
    "text": "8.14 Упражнения\n\nВ пакете stringr есть небольшой датасет words. Найдите все слова с последовательностью символов wh.\nСколько слов содержат два гласных после w?\nНайдите все слова в words, которые заканчиваются на x.\nНайдите все слова, которые начинаются на b или на g. Какое из них самое длинное?\nНайдите все слова в words, в которых за w следует согласный.\nЗамените всю пунктуацию в строке “tomorrow?and-tomorrow_and!tomorrow” на пробелы.\nНайдите все слова в words, в которых есть любые два символа между b и k.\nДана строка “tomorrow (and) tomorrow (and) tomorrow”. Необходимо удалить первые скобки с их содержанием.\nНайдите в “Гордости и предубеждении” все предложения, где есть to, и выберите следующее за этим слово. Узнайте, сколько всего уникальных сочетаний.\nСоздайте тиббл с двумя столбцами: letters и numbers (1-26). Преобразуйте, чтобы в третьем столбце появился результат соединения первых двух через подчеркивание, например a_1. Отфильтруйте, чтобы остались только ряды, где есть цифра 3 или буква x.\nДана библиографическая запись: Ast, Friedrich. 1816. Platon’s Leben und Schriften. Leipzig, Weidmann. Используя регулярные выражения, замените полное имя на инициал. Запятую перед инициалом удалите. Уберите название издательства. Год поставьте в круглые скобки.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#видео",
    "href": "regex.html#видео",
    "title": "8  Регулярные выражения",
    "section": "8.15 Видео",
    "text": "8.15 Видео\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "scrape.html#сетевая-вежливость",
    "href": "scrape.html#сетевая-вежливость",
    "title": "9  Веб-скрапинг",
    "section": "9.9 Сетевая вежливость",
    "text": "9.9 Сетевая вежливость\nКогда вы работаете с веб-сайтами и автоматизированно загружаете множество страниц, важно:\n\nне перегружать сервер (быстрые, частые запросы могут быть расценены как атака или приведут к бану).\nсоблюдать правила robots.txt — например, некоторые страницы могут быть закрыты для парсинга ботами.\nуважать сайт: добавлять задержки между запросами.\n\npolite помогает автоматизировать соблюдение этих правил:\n\nbow() проверяет robots.txt и заводит “виртуальный браузер”, который автоматически подставляет правильные технические детали при каждом запросе,\nnod() — готовит переход к дополнительной ссылке с сохранением контекста.\nscrape() — скачивает html-страницу, после проверки доступа.\n\n\n# раскланиваемся\nurl &lt;- \"https://la.wikisource.org/wiki/Commentarii_de_bello_Gallico\"\nsession &lt;- bow(url)\n\n# загружаем HTML главной страницы раздела с помощью polite\nhtml &lt;- scrape(session)\n\nПосле этого с помощью Selector Gadget выбираем нужные элементы оглавления, как раньше.\n\ntoc &lt;- html |&gt; \n   html_elements(\"#mw-content-text li\")\n\ntoc\n\n{xml_nodeset (8)}\n[1] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_I\" title=\"Commentar ...\n[2] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_II\" title=\"Commenta ...\n[3] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_III\" title=\"Comment ...\n[4] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_IV\" title=\"Commenta ...\n[5] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_V\" title=\"Commentar ...\n[6] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VI\" title=\"Commenta ...\n[7] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VII\" title=\"Comment ...\n[8] &lt;li&gt;&lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VIII\" title=\"Commen ...\n\n\nПолучив список ссылок, используем функцию для аккуратного скачивания и парсинга текста с задержками.\n\n# раскланиваемся перед каждой страницей\nget_text_polite &lt;- function(url) {\n  bow_obj &lt;- bow(url)\n  html_page &lt;- scrape(bow_obj)\n  res = tibble(\n    liber = str_extract(url, \"Liber_.+$\"),\n    text = html_page |&gt;\n      html_elements(\".mw-heading3+ p\") |&gt;\n      html_text2())\n  return(res)\n}\n\nПолучаем тексты ко всем главам с помощью map().\n\n# polite автоматически добавляет рекомендованную паузу между запросами\nlibri_text &lt;- map_df(urls, get_text_polite)\n\nИтого:\n\nbow(url) — создаёт аккуратную сессию с соблюдением robots.txt.\nscrape() — загружает страницу, только если это позволено.\nВ функции get_text_polite каждому URL соответствует своя bow()-сессия. Такой подход снижает риск блокировки.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#видео",
    "href": "scrape.html#видео",
    "title": "9  Веб-скрапинг",
    "section": "9.10 Видео",
    "text": "9.10 Видео\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#домашнее-задание",
    "href": "scrape.html#домашнее-задание",
    "title": "9  Веб-скрапинг",
    "section": "9.11 Домашнее задание",
    "text": "9.11 Домашнее задание\n🔗 Задание в Classroom: https://classroom.github.com/a/svdMrfyd\n⌛ Дедлайн: 23/11, время 23:59\n✅ Оценка 0/1\nПо ссылке вы найдете пустой репозиторий. Вам надо самостоятельно запарсить любой сайт и положить туда файл с кодом. Если вы парсите локальную html-страницу (например, архив какого-то канала), ее следует загрузить в репозиторий вместе с кодом.\nЗадание считается выполненным, если:\n\nиспользован пакет {rvest} и задействованы селекторы css;\nна выходе таблица с несколькими столбцами (например, title/id, stable_url, text, rating);\nссылки собраны не вручную, а автоматически (кроме первой);\nзадействованы итерации (предпочтительно map_());\nв репозиторий загружен скриншот с кусочком таблицы, которая у вас получилась;\nкод запускается без ошибки; дополнительный плюс – внятные аннотации, поясняющие, что он делает;\nиспользовать пакет {polite} рекомендуется, но не обязательно.\n\nНе обязательно собирать сотни ссылок, их может быть 10-15, но я должна видеть, что вы освоили материал, то есть не применяете копипасту.\nКакие это могут быть страницы:\n\nколлекции текстов: Викитека, lib.ru, др. Кроме русского, можете взять любой знакомый вам европейский язык, который использует латиницу (английский, французский, немецкий, испанский, итальянский);\nонлайн-издания, особенно образовательные или просветительские;\nинтернет-энциклопедии, в т.ч. Википедия, Новая философская энциклопедия и др. С НЭФ мы немного поработали, но статьи оттуда не извлекали, так что ее тоже можно;\nсайты организаций: университетов, библиотек, музеев и т.д.;\nнаучные журналы, и т.д.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "llm.html#что-такое-llm",
    "href": "llm.html#что-такое-llm",
    "title": "29  Работа с LLM",
    "section": "",
    "text": "автоматическая разметка текстов (TEI);\nизвлечение структурированных данных;\nраспознавание изображений;\nавтоматическая классификация;\nи многое другое.\n\n\n\nбольшая часть моделей требует денег за доступ по API;\nк некоторым моделям не получится подключиться без VPN;\nполностью бесплатные локальные модели тяжелые и не всегда “умные”;\nкопипаста через телеграм-чат - не наш метод.\n\n\n\nOpenRouter https://openrouter.ai/ — это агрегатор LLM‑моделей (OpenAI, Anthropic, Meta, Mistral и др.) с единым API. Можно выбрать бесплатные модели (с лимитами) и вызывать их из R/RStudio. Обратите внимание: число запросов в день на бесплатном плане ограничено 10 кредитами! Перед началом работы проверьте, на что вы даете разрешение моделям: https://openrouter.ai/settings/privacy.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#получение-ключа-api",
    "href": "llm.html#получение-ключа-api",
    "title": "29  Работа с LLM",
    "section": "29.2 Получение ключа API",
    "text": "29.2 Получение ключа API\nAPI (Application programming interface) это набор правил, по которым приложения или части программы общаются друг с другом.\nИдем на сайт https://openrouter.ai/, регистрируемся, получаем ключ (дайте ему осмысленное название), копируем и сразу сохраняем.\n\nSys.setenv(OPENROUTER_API_KEY = \"ваш_ключ_api\")\n\nИли отредактируйте файл .Renviron в домашней директории:\n\n# usethis::edit_r_environ()\n\nПосле чего добавьте строку в файл OPENROUTER_API_KEY=ваш_ключ_api и перезапустите сессию.\nПроверить:\n\nSys.getenv(\"OPENROUTER_API_KEY\")",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#начало-работы-c-openrouter",
    "href": "llm.html#начало-работы-c-openrouter",
    "title": "29  Работа с LLM",
    "section": "29.3 Начало работы c OpenRouter",
    "text": "29.3 Начало работы c OpenRouter\nСоздаем объект chat, который представляет собой интерфейс для общения с языковой моделью через определённый сервис (в данном случае — сервис OpenRouter). Это специальная переменная, которая хранит в себе всю нужную информацию для того, чтобы отправлять запросы и получать ответы от нейросети (например, GPT).\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Отвечай по-русски, будь краток.\",\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"openai/gpt-oss-20b:free\"\n)\n\n\nchat$chat(\"Что такое метафора?\")\n\nПопробуйте разные бесплатные модели.\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Отвечай по-русски, будь краток.\",\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"meta-llama/llama-3.3-70b-instruct:free\"\n)\n\nchat$chat(\"Что такое метафора?\")\n\nДля интерактивного взаимодействия с моделью из консоли используйте команду:\n\nlive_console(chat)\n\n# ╔═══════════════════════════════╗\n# ║ Entering chat console.        ║\n# ║ Use \"\"\" for multi-line input. ║\n# ║ Type 'Q' to quit.             ║\n# ╚═══════════════════════════════╝",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#начало-работы-с-ollama",
    "href": "llm.html#начало-работы-с-ollama",
    "title": "29  Работа с LLM",
    "section": "29.4 Начало работы с Ollama",
    "text": "29.4 Начало работы с Ollama\nOllama — это инструмент для запуска и использования больших языковых моделей (LLM, Large Language Models) на вашем компьютере. В отличие от облачных сервисов (OpenAI, Google, Mistral и др.), Ollama работает локально: ваши данные никуда не уходят, модели скачиваются прямо на ваш компьютер.\nСкачайте и установите Ollama. Перейдите на сайт: https://ollama.com/ Выберите вашу операционную систему (Windows, Mac, Linux) и следуйте инструкциям по установке.\nПосле этого установите пакет {ollamar} и скачайте нужные модели.\n\nlibrary(ollamar)\ntest_connection() \n# &lt;httr2_response&gt;\n# GET http://localhost:11434/\n# Status: 200 OK\n# Content-Type: text/plain\n# Body: In memory (17 bytes)\n\n\nollamar::list_models()\n# ollamar::pull(\"gemma2:2b\")\n\nПосле установки эти модели доступны также через {ellmer}. Локальные модели могут медленнее работать (зависит от вычислительных ресурсов компьютера и параметров модели).\n\nchat &lt;- chat_ollama(\n  model = \"gemma2:2b\"\n)\n\nchat$chat(\"Что такое метафора?\")\n\nПоследние версии Ollama позволяют работать в облаке; для этого требуется регистрация. После регистрации идете на сайте Settings -&gt; Keys и создаете новый ключ API. Список облачных моделей доступен здесь https://ollama.com/blog/cloud-models.\n\nchat &lt;- chat_ollama(\n  system_prompt = \"Отвечай по-русски, будь краток.\",\n  api_key = Sys.getenv(\"OLLAMA_API_KEY\"),\n  # важно указать именно облачную модель\n  model = \"gpt-oss:120b-cloud\"\n)\n\nchat$chat(\"Что такое метафора?\")\n# Метафора — образный приём, при котором слово или выражение \n# переносит значение с одного предмета на другой за счёт их \n# сходства, но без использования сравнительных союзов (как, будто). \n# Это способ сравнения, когда один объект «становится» другим в \n# переносном смысле.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#файлы-приложения",
    "href": "llm.html#файлы-приложения",
    "title": "29  Работа с LLM",
    "section": "29.6 Файлы-приложения",
    "text": "29.6 Файлы-приложения\nК запросу можно прикрепить файл pdf, например, для реферирования или перевода. Результат может вас разочаровать.\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Ты профессор философии с 30-летним опытом, специалист по теории познания.\",\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"openai/gpt-oss-20b:free\"\n)\n\nchat$chat(\n  content_pdf_url(\"https://fitelson.org/proseminar/gettier.pdf\"),\n  \"Резюмируй в одном предложении статью Э. Геттиера.\"\n)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#извлечение-структурированных-данных",
    "href": "llm.html#извлечение-структурированных-данных",
    "title": "29  Работа с LLM",
    "section": "29.7 Извлечение структурированных данных",
    "text": "29.7 Извлечение структурированных данных\nСтруктурированные данные можно извлекать из текста, pdf или изображений.\n\nchat &lt;- chat_openrouter(\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"openai/gpt-oss-20b:free\"\n)\n\nchat$chat_structured(\n  \"Extract metadata from the attached pdf file.\",\n content_pdf_url(\"https://fitelson.org/proseminar/gettier.pdf\"),\n type = type_object(\n   author_name = type_string(\"Surname, Name\"), \n   title = type_string(\"Title of the publication\"),\n   year = type_number(\"year of publication\"),\n   publication_name = type_string(\"Journal title\")\n )\n)\n\n# $author_name\n# [1] \"Edmund L. Gettier\"\n# \n# $title\n# [1] \"Is Justified True Belief Knowledge?\"\n# \n# $year\n# [1] 1963\n# \n# $publication_name\n# [1] \"Philosophical Review\"",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#конвейер-обработки",
    "href": "llm.html#конвейер-обработки",
    "title": "29  Работа с LLM",
    "section": "29.8 Конвейер обработки",
    "text": "29.8 Конвейер обработки\nПишем функцию-помощника и проверяем на одной ссылке.\n\nget_summary_pdf &lt;- function(path) {\n  message(paste0(\"Writing summary for \", path))\n  \n  prompt &lt;- \"Summarize the following article concisely in a single paragraph, using only information contained within the article. Do not use markdown formatting or bullet points.\"\n  \n  response &lt;- chat$chat(prompt,\n                        content_pdf_url(path),\n                        echo = FALSE\n                        )\n  return(response)\n}\n\nПоехали!\n\nget_summary(\"https://fitelson.org/proseminar/gettier.pdf\")\n\n# Gettier shows that the classical tripartite definition of knowledge, (a) “S knows \n# that P iff P is true, S believes P, and S is justified in believing P” (p. 1), fails \n# to give a sufficient condition for knowing, arguing that justification can be \n# satisfied by false premises or by logical entailments that do not guard against \n# mistaken sources (“First, in that sense of ‘justified’ … it is possible for a person \n# to be justified in believing a proposition that is in fact false” – p. 1). He notes \n# that the same reasoning applies to Chisholm’s and Ayer’s formulations, writing “The \n# same argument will show that (b) and (c) fail if ‘has adequate evidence for’ or ‘has \n# the right to be sure that’ is substituted for ‘is justified in believing that’ \n# throughout” (p. 1). Case I presents a situation in which Smith, justified by evidence\n# about Jones, accepts a true claim about the future office‑holder but does not know \n# it; the text states “Smith does not know that (e) is true” (p. 2). Case II constructs\n# disjunctive claims from justified belief in Jones owning a Ford, yet one claim is \n# true for an unrelated reason and the author notes “Smith does not know that (h) is \n# true” (p. 3). These counterexamples demonstrate that conditions (b) and (c) are \n# likewise insufficient, and that justified‑true belief is not a sufficient account of \n# knowledge (p. 3).\n\nПохожая функция для чтения html-страницы.\n\nget_summary_html&lt;- function(url) {\n  message(paste0(\"Writing summary for \", url))\n  \n  prompt &lt;- \"Summarize the following article concisely in a single paragraph, using only information contained within the article. Do not use markdown formatting or bullet points.\"\n  \n  response &lt;- chat$chat(prompt,\n                        url,\n                        #echo = FALSE\n                        )\n  return(response)\n}\n\n\nget_summary_html(\"https://plato.stanford.edu/archives/sum2022/entries/plato-parmenides/\")\n\n# The Stanford Encyclopedia entry on Plato’s dialogue *Parmenides* explains that the \n# work is one of the earliest surviving Platonic dialogues in which the young Socrates \n# encounters the older Parmenides, a pre‑Socratic philosopher famous for his paradoxes \n# and his doctrine of the One, and a young Zeno whose paradoxes he must defend. In the \n# dialogue Parmenides first showcases the difficulty of reconciling the multiplicity of\n# the world with the Unity of Being, using a series of thought experiments and \n# arguments that reveal the contradictions in Zeno’s reasoning. Plato uses this \n# encounter to probe the limits of dialectical method, illustrating how Socrates, so \n# far defended as a model of philosophical inquiry, is brought into conflict with the \n# very challenge of logical certainty; the discussion ends with Parmenides urging a \n# more rigorous formulation of ideas before they can be related to the sensible world. \n# The entry further demonstrates how the dialogue serves as a vehicle for questioning \n# the method of Platonic theory‑of‑forms by showing that even the most careful \n# dialectical examination can expose hidden contradictions, thereby encouraging the \n# reader to consider whether the Forms can be established on the basis of mere logical \n# manipulation alone. (See sections “Context” and “Solution”)\n\nТеперь сохраним вектор ссылок и применим к каждой из них нашу функцию.\n\nurls &lt;- c(\"https://plato.stanford.edu/archives/sum2022/entries/plato-timaeus/\",\n          \"https://plato.stanford.edu/archives/sum2022/entries/plato-parmenides/\"\n          )\n\nФормируем таблицу.\n\nsummaries &lt;- map(urls, get_summary_html)\n\nresults &lt;- tibble(\n  url = urls,\n  summary = summaries\n)\n\n\nresults$summary[1]\n\n# Timaeus, a Platonic dialogue written around 360 BCE, centers on the speaker Timaeus \n# who reports the conversation with Socrates and aged Parmenides during which a \n# cosmological system is presented: the world is described as a rational, living \n# organism created by an infinitely good Craftsman (the Demiurge) who imposes \n# mathematical order on chaos through the arrangement of the celestial spheres, the \n# Earth, and the elements; the dialogue also addresses the nature of the soul, the \n# faculties of perception, and the relationship between the sensible world and the \n# intelligible Forms, raising questions about the existence of a perfect, unchanging \n# realm of Ideas and the way this realm informs the physical cosmos; the article \n# examines the text’s historical context, its methodological significance for Platonic \n# epistemology and metaphysics, and the interpretive debates over how thoroughly the \n# dialogue can be read as a cosmological treatise rather than mere philosophical \n# rhetoric.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#разметка-текстов",
    "href": "llm.html#разметка-текстов",
    "title": "29  Работа с LLM",
    "section": "29.9 Разметка текстов",
    "text": "29.9 Разметка текстов\nСистемный промпт для TEI разметки. Подробнее о стратегиях написания промптов см. в материале “Системного блока”.\n\nsystem_prompt &lt;- \"You are an expert TEI encoder specializing in Russian literary texts.\n\nCRITICAL RULES:\n1. Preserve the original text EXACTLY - no spelling corrections, no modernization\n2. Tag only clear geographic references\n3. Maintain original punctuation and capitalization\n\nTAGGING GUIDELINES:\n- Cities: &lt;place type='city'&gt;Москва&lt;/place&gt;\n- Rivers: &lt;place type='river'&gt;Волга&lt;/place&gt;  \n- Buildings: &lt;place type='building'&gt;Кремль&lt;/place&gt;\n- Regions: &lt;place type='region'&gt;Сибирь&lt;/place&gt;\n- Use 'fictional' for imaginary places\n\nRETURN FORMAT:\nReturn only the text with added TEI tags, no additional commentary.\"\n\nТекст, который будем аннотировать.\n\nuser_prompt &lt;- \"Может быть, никто из живущих в Москве не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели — куда глаза глядят — по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты.\nНо всего приятнее для меня то место, на котором возвышаются мрачные, готические башни Си...нова монастыря. Стоя на сей горе, видишь на правой стороне почти всю Москву, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную Москву хлебом. На другой стороне реки видна дубовая роща, подле которой пасутся многочисленные стада; там молодые пастухи, сидя под тению дерев, поют простые, унылые песни и сокращают тем летние дни, столь для них единообразные. Подалее, в густой зелени древних вязов, блистает златоглавый Данилов монастырь; еще далее, почти на краю горизонта, синеются Воробьевы горы. На левой же стороне видны обширные, хлебом покрытые поля, лесочки, три или четыре деревеньки и вдали село Коломенское с высоким дворцом своим.\"\n\n\nchat &lt;- chat_openrouter(\n  system_prompt = system_prompt,\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"openai/gpt-oss-20b:free\", \n  echo = FALSE\n)\n\nresult &lt;- chat$chat(user_prompt)\n\n\nwrite_lines(result, file = \"test.xml\")\n\n\nmarked_text &lt;- read_lines(\"test.xml\")\nmarked_text\n\n[1] \"Может быть, никто из живущих в &lt;place type='city'&gt;Москву&lt;/place&gt; не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели — куда глаза глядят — по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты. Но всего приятнее для меня то место, на котором возвышаются мрачные, готические башни &lt;place type='building'&gt;Си...нова монастыря&lt;/place&gt;. Стоя на сей горе, видишь на правой стороне почти всю &lt;place type='city'&gt;Москва&lt;/place&gt;, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную &lt;place type='city'&gt;Москву&lt;/place&gt; хлебом. На другой стороне реки видна дубовая роща, подле которой пасутся многочисленные стада; там молодые пастухи, сидя под тенью дерев, поют простые, унылые песни и сокращают тем летние дни, столь для них единообразные. Подалее, в густой зелени древних вязов, блистает златоглавый &lt;place type='building'&gt;Данилов монастырь&lt;/place&gt;; еще далее, почти на краю горизонта, синеются &lt;place type='region'&gt;Воробьевы горы&lt;/place&gt;. На левой же стороне видны обширные, хлебом покрытые поля, лесочки, три или четыре деревеньки и вдали село &lt;place type='city'&gt;Коломенское&lt;/place&gt; с высоким дворцом своим.\"\n\n\nФайл можно отредактировать вручную, но прежде, чем это делать, попробуйте\n\nсравнить работу разных моделей;\nусовершенствовать системный промпт, добавить больше примеров.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#создание-полных-tei-документов",
    "href": "llm.html#создание-полных-tei-документов",
    "title": "29  Работа с LLM",
    "section": "29.10 Создание полных TEI документов",
    "text": "29.10 Создание полных TEI документов\nНиже показано минимальное решение. Отредактируйте код с учетом тех метаданных, которые необходимо сохранить. Ориентируйтесь на стандарты TEI.\n\ncreate_complete_tei &lt;- function(marked_text, metadata = list()) {\n  default_metadata &lt;- list(\n    title = \"Неизвестное произведение\",\n    author = \"Неизвестный автор\", \n    date = \"Не датировано\",\n    language = \"ru\"\n  )\n  \n  metadata &lt;- modifyList(default_metadata, metadata)\n  \n  tei_template &lt;- '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\"\n  &lt;teiHeader&gt;\n    &lt;fileDesc&gt;\n      &lt;titleStmt&gt;\n        &lt;title&gt;%s&lt;/title&gt;\n        &lt;author&gt;%s&lt;/author&gt;\n      &lt;/titleStmt&gt;\n      &lt;publicationStmt&gt;\n        &lt;p&gt;Автоматически размеченный текст для исследовательских целей&lt;/p&gt;\n      &lt;/publicationStmt&gt;\n      &lt;sourceDesc&gt;\n        &lt;p&gt;Оригинальный текст: %s&lt;/p&gt;\n      &lt;/sourceDesc&gt;\n    &lt;/fileDesc&gt;\n    &lt;profileDesc&gt;\n      &lt;langUsage&gt;\n        &lt;language ident=\"%s\"&gt;Русский&lt;/language&gt;\n      &lt;/langUsage&gt;\n    &lt;/profileDesc&gt;\n  &lt;/teiHeader&gt;\n  &lt;text&gt;\n    &lt;body&gt;\n      &lt;div&gt;\n        &lt;p&gt;%s&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/body&gt;\n  &lt;/text&gt;\n&lt;/TEI&gt;'\n  \n  sprintf(tei_template, \n          metadata$title, \n          metadata$author, \n          metadata$date, \n          metadata$language,\n          marked_text)\n}\n\n\n# Пример использования\nmetadata &lt;- list(\n  title = \"Отрывок из 'Бедной Лизы'\",\n  author = \"Карамзин, Николай Михайлович\",\n  date = \"1792\",\n  language = \"ru\"\n)\n\ncomplete_tei &lt;- create_complete_tei(marked_text, metadata)\n\ncat(complete_tei)",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#сравнение-с-оригиналом",
    "href": "llm.html#сравнение-с-оригиналом",
    "title": "29  Работа с LLM",
    "section": "29.11 Сравнение с оригиналом",
    "text": "29.11 Сравнение с оригиналом\nC валидным xml можно работать так обычно (см. урок 5)\n\ndoc &lt;- read_xml(\"complete_tei.xml\") \nns &lt;- xml_ns(doc)\n\ntext_content &lt;- doc |&gt; \n    xml_find_all(\"//d1:body//d1:p\") |&gt; \n    xml_text() \n\ntext_content\n\n[1] \"Может быть, никто из живущих в Москву не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели — куда глаза глядят — по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты. Но всего приятнее для меня то место, на котором возвышаются мрачные, готические башни Си...нова монастыря. Стоя на сей горе, видишь на правой стороне почти всю Москва, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную Москву хлебом. На другой стороне реки видна дубовая роща, подле которой пасутся многочисленные стада; там молодые пастухи, сидя под тенью дерев, поют простые, унылые песни и сокращают тем летние дни, столь для них единообразные. Подалее, в густой зелени древних вязов, блистает златоглавый Данилов монастырь; еще далее, почти на краю горизонта, синеются Воробьевы горы. На левой же стороне видны обширные, хлебом покрытые поля, лесочки, три или четыре деревеньки и вдали село Коломенское с высоким дворцом своим.\"\n\n\n\nlibrary(diffobj)\ndiffobj::diffChr(user_prompt, text_content, mode = \"sidebyside\")",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#параметры-модели",
    "href": "llm.html#параметры-модели",
    "title": "29  Работа с LLM",
    "section": "29.12 Параметры модели",
    "text": "29.12 Параметры модели\nДля настройки параметров модели в {ellmer} есть специальная функция. Мы отрегулируем температуру – гиперпараметр, который контролирует уровень случайности и “креативности” при генерации текста. Манипулируя температурой, можно управлять балансом между предсказуемостью и разнообразием генерируемого текста.\n\nparams_cold &lt;- params(temperature = 0)\nparams_hot &lt;- params(temperature = 2)\n\nСравним поведение одной модели с разной температурой. Максимальная креативность (для моделей OpenAi = 2) граничит с бессвязностью:\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Ты всегда отвечаешь одним предложением\",\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"openai/gpt-oss-20b:free\", \n  params = params_hot\n  )\n\nchat$chat(\"Что такое метафора?\")\n# Метафора – это перенесённое, сравнениеобразное \n# соображение, когда признак либо качества предмета \n# описывается термином, относящимся именно быту, но \n# обозначая другой предмет.\n\nЧем “холоднее” модель, тем более предсказуема выдача. Результаты повторных запросов будут отличаться минимально.\n\nchat &lt;- chat_openrouter(\n  system_prompt = \"Ты всегда отвечаешь одним предложением\",\n  api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n  model = \"openai/gpt-oss-20b:free\",\n  params = params_cold\n)\n\nchat$chat(\"Что такое метафора?\")\n# Метафора — это фигура речи, при которой слово или \n# выражение переносит значение из одного предмета в \n# другой, создавая образное сравнение без использования \n# союзов «как», «словно» и т.п.",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "tokenize.html#токенизация-и-анализ-частотностей",
    "href": "tokenize.html#токенизация-и-анализ-частотностей",
    "title": "10  Токенизация и лемматизация",
    "section": "",
    "text": "10.1.1 Виды токенов\nТокенизация — процесс разделения текста на составляющие (их называют «токенами»). Токенами могут быть слова, символьные или словесные энграмы (n-grams), то есть сочетания символов или слов, даже предложения или параграфы.\nТокенизировать можно в базовом R с использованием регулярных выражений, и Jockers (2014) прекрасно показывает, как это можно делать. Но мы воспользуемся двумя пакетами, которые предназначены специально для работы с текстовыми данными и разделяют идеологию tidyverse: tidytext (Silge и Robinson 2017) и tokenizers (Hvitfeldt и Silge 2022).\nДля анализа воспользуемся датасетом c латинским текстом “Записок о Галльской войне”, который мы подготовили в предыдущем уроке. Его можно забрать отсюда.\n\nload(\"../data/caesar.RData\")\n\nВ нашем тексте есть макроны, которые можно убрать при помощи регулярных выражений.\n\ncaesar &lt;- caesar |&gt; \n  group_by(liber) |&gt; \n  mutate(liber = cur_group_id()) |&gt; \n  mutate(text = str_replace_all(text, c(\"ā\"=\"a\", \"ī\"=\"i\", \"ē\"=\"e\", \"ō\"=\"o\", \"ū\"=\"u\"))) |&gt; \n  ungroup()\n\ncaesar \n\n\n  \n\n\n\nФункция unnest_tokens() из пакета tidytext принимает на входе тиббл, название столбца, в котором хранится текст для токенизации, а также название нового столбца, куда будут “сложены” отдельные токены (зачастую это слова, но не обязательно).\nunnest_tokens(\n  tbl,\n  output,\n  input,\n  token = \"words\",\n  format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"),\n  to_lower = TRUE,\n  drop = TRUE,\n  collapse = NULL,\n  ...\n)\nАргумент token принимает следующие значения:\n\n“words” (default),\n“characters”,\n“character_shingles”,\n“ngrams”,\n“skip_ngrams”,\n“sentences”,\n“lines”,\n“paragraphs”,\n“regex”,\n“ptb” (Penn Treebank).\n\nИспользуя уже знакомую функцию map, можно запустить unnest_tokens() с разными аргументами:\n\ntest &lt;- tibble(text = \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt.\")\n\n\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token,\n  test, \"word\", \"text\", \"words\", \n  test, \"sentence\", \"text\", \"sentences\",\n  test, \"char\", \"text\", \"characters\", \n)\n\nparams\n\n\n  \n\n\n\n\nparams |&gt; \n  pmap(unnest_tokens) \n\n[[1]]\n# A tibble: 29 × 1\n   word    \n   &lt;chr&gt;   \n 1 gallia  \n 2 est     \n 3 omnis   \n 4 divisa  \n 5 in      \n 6 partes  \n 7 tres    \n 8 quarum  \n 9 unam    \n10 incolunt\n# ℹ 19 more rows\n\n[[2]]\n# A tibble: 2 × 1\n  sentence                                                                      \n  &lt;chr&gt;                                                                         \n1 gallia est omnis divisa in partes tres, quarum unam incolunt belgae, aliam aq…\n2 hi omnes lingua, institutis, legibus inter se differunt.                      \n\n[[3]]\n# A tibble: 166 × 1\n   char \n   &lt;chr&gt;\n 1 g    \n 2 a    \n 3 l    \n 4 l    \n 5 i    \n 6 a    \n 7 e    \n 8 s    \n 9 t    \n10 o    \n# ℹ 156 more rows\n\n\nСледующие значения аргумента token требуют также аргумента n:\n\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token, ~n,\n  test, \"ngram\", \"text\", \"ngrams\", 3,\n  test, \"shingles\", \"text\", \"character_shingles\", 3\n)\n\nparams  |&gt; \n  pmap(unnest_tokens)  |&gt; \n  head()\n\n[[1]]\n# A tibble: 27 × 1\n   ngram                \n   &lt;chr&gt;                \n 1 gallia est omnis     \n 2 est omnis divisa     \n 3 omnis divisa in      \n 4 divisa in partes     \n 5 in partes tres       \n 6 partes tres quarum   \n 7 tres quarum unam     \n 8 quarum unam incolunt \n 9 unam incolunt belgae \n10 incolunt belgae aliam\n# ℹ 17 more rows\n\n[[2]]\n# A tibble: 164 × 1\n   shingles\n   &lt;chr&gt;   \n 1 gal     \n 2 all     \n 3 lli     \n 4 lia     \n 5 iae     \n 6 aes     \n 7 est     \n 8 sto     \n 9 tom     \n10 omn     \n# ℹ 154 more rows\n\n\nДальше мы будем работать со словами, поэтому сохраним токенизированный текст “Записок” в виде “опрятного” датасета (одно наблюдение - один ряд).\n\ncaesar_tokens &lt;- caesar |&gt; \n  unnest_tokens(\"word\", \"text\")\n\ncaesar_tokens\n\n\n  \n\n\n\nПри работе с данными в текстовом формате unnest_tokens() опирается на пакет tokenizers, из которого в нашем случае подтягивает функцию tokenize_words. У этой функции есть несколько полезных аргументов: strip_non_alphanum (удаляет пробельные символы и пунктуацию), strip_punct (удаляет пунктуацию), strip_numeric (удаляет числа).\nЭти аргументы мы тоже можем задать через unnest_tokens(), поскольку у функции есть аргумент ... (загляните в документацию, чтобы убедиться).\n\ncaesar |&gt; \n  unnest_tokens(\"word\", \"text\", strip_punct = FALSE)\n\n\n  \n\n\n\n\n\n10.1.2 Cтоп-слова\nБольшая часть слов, которые мы сейчас видим в корпусе – это стоп-слова, не несущие смысловой нагрузки. Для английского языка список стоп-слов уже доступен в пакете tidytext; в других случаях их следует загружать отдельно. Для многих языков стоп-слова доступны в пакете {stopwords}.\n\nstopwords_getlanguages(\"stopwords-iso\")\n\n [1] \"af\" \"ar\" \"hy\" \"eu\" \"bn\" \"br\" \"bg\" \"ca\" \"zh\" \"hr\" \"cs\" \"da\" \"nl\" \"en\" \"eo\"\n[16] \"et\" \"fi\" \"fr\" \"gl\" \"de\" \"el\" \"ha\" \"he\" \"hi\" \"hu\" \"id\" \"ga\" \"it\" \"ja\" \"ko\"\n[31] \"ku\" \"la\" \"lt\" \"lv\" \"ms\" \"mr\" \"no\" \"fa\" \"pl\" \"pt\" \"ro\" \"ru\" \"sk\" \"sl\" \"so\"\n[46] \"st\" \"es\" \"sw\" \"sv\" \"th\" \"tl\" \"tr\" \"uk\" \"ur\" \"vi\" \"yo\" \"zu\"\n\n\n\nsw &lt;- stopwords(language = \"la\", source = \"ancient\")\nhead(sw)\n\n[1] \"a\"     \"ab\"    \"abiud\" \"abs\"   \"ac\"    \"ad\"   \n\n\nРасширенный список стоп-слов для латыни и греческого можно найти здесь.\n\ncaesar_words_tidy &lt;- caesar_tokens  |&gt;  \n  filter(!word %in% sw)\n\ncaesar_words_tidy\n\n\n  \n\n\n\nУборка закончена, мы готовы к подсчетам.\n\n\n10.1.3 Абсолютная частотность\nДля начала посмотрим на самые частотные слова по книгам.\n\ncaesar_counts &lt;- caesar_words_tidy |&gt; \n  count(word, liber, sort = TRUE)  \n\ncaesar_counts |&gt; \n  group_by(liber) |&gt; \n  slice_head(n = 10) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(reorder_within(word, n, liber), n, fill = as.factor(liber))) +\n  geom_col(show.legend = F) + \n  scale_x_reordered() +\n  facet_wrap(~liber, nrow = 2, scales = \"free\") +\n  coord_flip()  +\n  theme_light() +\n  xlab(NULL)\n\n\n\n\n\n\n\n\n\n\n10.1.4 TF и закон Ципфа\nАбсолютная частотность – плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте. Этот показатель принято обозначать как tf (term frequency). Анализируя относительные частотности, важно иметь в виду, что подавляющее большинство слов встречается очень редко, а слов с высокой частотностью - мало. Считаем по токенам до удаления стоп-слов:\n\ntokens_tf &lt;- caesar_tokens |&gt; \n  mutate(total = nrow(caesar_tokens)) |&gt; \n  add_count(word, sort = TRUE) |&gt; \n  distinct(word, total, n) |&gt; \n  mutate(tf = n / total) |&gt; \n  arrange(-tf) \n\ntokens_tf\n\n\n  \n\n\n\n\ntokens_tf |&gt; \n  ggplot(aes(tf)) +\n  geom_histogram(show.legend = FALSE, \n                 fill = \"aliceblue\", \n                 color = \"grey\",\n                 bins = 50) +\n  theme_light() #+\n\n\n\n\n\n\n\n  #coord_cartesian(ylim = c(NA, 3000), xlim = c(NA, 0.016))\n\nПодобная картина характерна для естественных языков. Распределения слов в них подчиняются закону Ципфа. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) и утверждает следующее: если все слова языка или длинного текста упорядочить по убыванию частоты использования, частотность (tf) n-го слова в списке окажется обратно пропорциональной его рангу (r) в степени α. Это значит (в самом общем случае), что если ранг увеличится в n раз, то частотность во столько же раз должна упасть: второе слово в корпусе встречается примерно в два раза реже, чем первое (Savoy 2020, 24).\n\\[tf_{r_i} = \\frac{c}{r^α_i}\\]\nЗдесь c - это константа, которая оценивается для каждого случая отдельно, как и параметр α. Иначе говоря:\n\\[ tf_{r_i} \\times r^α_i = c \\] Посмотрим на ранги и частотность первых 50 слов.\n\ntokens_tf_rank &lt;- tokens_tf |&gt; \n  mutate(rank = row_number()) \n\ntokens_tf_rank |&gt; \n  ggplot(aes(rank, tf)) +\n  geom_line(linewidth = 1.1, alpha = 0.7, color = \"navyblue\") +\n  theme_light()\n\n\n\n\n\n\n\n\nВспомнив, что логарифм дроби равен разности логарифмов числителя и знаменателя, запишем:\n\\[log(tf_{r_i}) = c - α \\times log(r_i) \\]\nТаким образом, мы получаем близкую к линейность зависимость, где константа c определяет точку пересечения оси y, a коэффициентα - угол наклона прямой. Графически это выглядит так:\n\ntokens_tf_rank |&gt; \n  ggplot(aes(rank, tf)) +\n  geom_line(size = 1.1, alpha = 0.7, color = \"navyblue\") +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЧтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом поговорим подробнее в следующих уроках):\n\ntokens_tf_rank |&gt; \n  ggplot(aes(rank, tf)) +\n  geom_line(size = 1.1, alpha = 0.7, color = \"navyblue\") +\n  geom_smooth(method = \"lm\",\n              linetype = 2, \n              color = \"tomato\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  theme_light()\n\n\n\n\n\n\n\n\nОтклонения от линии регрессии наиболее заметны в “хвостах” графика. Это характерно для многих корпусов: как очень редких, так и самых частотных слов не так много, как предсказывает закон Ципфа. Кроме того, внизу кривая почти всегда приобретает ступенчатый вид, потому что слова встречаются в корпусе дискретное число раз: ранг у них разный, а частотности одинаковые.\n\n\n10.1.5 TF-IDF\nНаиболее частотные слова (с низким рангом) наименее подвержены влиянию тематики, поэтому их используют для стилометрического анализа. Если отобрать наиболее частотные после удаления стоп-слов, то мы получим достаточно адекватное отражение тематики документов. Если же мы необходимо найти наиболее характерные для документов токены, то применяется другая мера, которая называется tf-idf (term frequency - inverse document frequency).\n\nЛогарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для документа. При этом относительная частотность тоже учитывается.\nФункция bind_tf_idf() принимает на входе тиббл с абсолютной частотностью для каждого слова.\n\ncaesar_tfidf &lt;- caesar_counts |&gt; \n  bind_tf_idf(word, liber, n)\n\ncaesar_tfidf\n\n\n  \n\n\n\nСнова визуализируем.\n\ncaesar_tfidf |&gt; \n  arrange(-tf_idf) |&gt; \n  group_by(liber) |&gt; \n  slice_head(n = 10) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(reorder_within(word, tf_idf, liber), tf_idf, fill = as.factor(liber))) +\n  geom_col(show.legend = F) +\n  labs(x = NULL, y = \"tf-idf\") +\n  facet_wrap(~liber, scales = \"free\", nrow = 2) +\n  scale_x_reordered() +\n  coord_flip() +\n  theme_light()\n\n\n\n\n\n\n\n\nЗаметим появление друидов в шестой книге, когда Цезарь добирается до Британии, и Квинта Цицерона – в пятой.\n\n\n10.1.6 Сравнение при помощи диаграммы рассеяния\nСтолбиковая диаграмма – не единственный способ сравнить частотности слов. Еще один наглядный метод – это диаграмма рассеяния с относительными частотностями. Сначала “расширим” наш тиббл.\n\nspread_freq &lt;- caesar_tfidf  |&gt; \n  select(-n, -idf, -tf) |&gt; \n  pivot_wider(names_from = liber, values_from = tf_idf, \n              values_fill = 0, names_prefix = \"liber_\") \n\nspread_freq\n\n\n  \n\n\n\nМожно визуализировать.\n\nlibrary(scales)\n\nspread_freq |&gt; \n  select(word, liber_1, liber_2) |&gt; \n  filter(liber_1 &gt; 0.0001 & liber_2 &gt; 0.0001) |&gt; \n  ggplot(aes(x = liber_1, y = liber_2)) +\n  geom_abline(color = \"grey40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, \n              height = 0.3, color = \"darkblue\") +\n  geom_text(aes(label = word), check_overlap = TRUE, \n            color = \"grey30\"\n            ) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  theme(legend.position = \"none\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nКак читать:\n\nось X соответствует первой книге (liber_1), ось Y — второй;\nвсё, что на линии, одинаково характерно для обеих книг, далеко от линии — характерно для одной;\nна самих осях ничего нет, т.к. мы отфильтровали нули.\n\nЧто можно заметить:\n\nпервая книга: справа по оси X — haedui, provincia, rhenum, oratione, voluntate, gratia, iure. Это хорошо соответствует сюжету первой книги (дипломатия, переговорные формулы).\nвторая книга: вверху по оси Y — belgarum, oppidum, murus, cohortatus, conantes, latitudinem. Это типично для бельгийской кампании и осадно‑полевых описаний второй книги.\nхарактерный для двух книг военный словарь находится около диагонали: legiones, acies, milia, pila, signum, castra/iter и т.п.\n\nЛемматизация латинского текста существенно улучшит картину.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#лемматизация-по-api",
    "href": "tokenize.html#лемматизация-по-api",
    "title": "10  Токенизация и лемматизация",
    "section": "10.3 Лемматизация по API",
    "text": "10.3 Лемматизация по API\nПоследние модели для лемматизации доступны здесь https://lindat.mff.cuni.cz/services/udpipe/. Можно загружать файлы вручную или через API.\n\n\nПодробнее о лемматизации по API.\n\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# функция для лемматизации латинского текста\nlemmatize_latin_text &lt;- function(text, model = \"latin-perseus-ud-2.15-241121\") {\n  url &lt;- \"https://lindat.mff.cuni.cz/services/udpipe/api/process\"\n  \n  # Параметры запроса\n  params &lt;- list(\n    model = model,  \n    data = text,\n    tokenizer = \"\",\n    tagger = \"\",\n    parser = \"\"\n  )\n  \n  # Отправка POST запроса\n  response &lt;- POST(\n    url = url,\n    body = params,\n    encode = \"form\"\n  )\n  \n  # Проверка статуса ответа\n  if (status_code(response) == 200) {\n    # Парсинг JSON ответа\n    result &lt;- content(response, \"parsed\")\n    \n    # Извлечение данных\n    if (!is.null(result$result)) {\n      return(result$result)\n    } else {\n      stop(\"Ошибка: результат не содержит данных\")\n    }\n  } else {\n    stop(paste(\"Ошибка API:\", status_code(response)))\n  }\n}\n\n\n# пример использования\nlatin_text &lt;- \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.\"\n\n# лемматизация текста\nresult &lt;- lemmatize_latin_text(latin_text)\n\ncat(result)\n\n# # generator = UDPipe 2, https://lindat.mff.cuni.cz/services/udpipe\n# # udpipe_model = latin-perseus-ud-2.15-241121\n# # udpipe_model_licence = CC BY-NC-SA\n# # newdoc\n# # newpar\n# # sent_id = 1\n# # text = Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.\n# 1 Gallia  Gallia  PROPN   n-s---fn-   Case=Nom|Gender=Fem|Number=Sing 4   nsubj:pass  _   _\n# 2 est sum AUX v3spia---   Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin    4   aux:pass    _   _\n# 3 omnis   omnis   DET a-s---fn-   Case=Acc|Gender=Fem|Number=Plur|PronType=Tot    1   det _   _\n# 4 divisa  divido  VERB    v-srppfn-   Aspect=Perf|Case=Nom|Gender=Fem|Number=Sing|VerbForm=Part|Voice=Pass    0   root    _   _\n# 5 in  in  ADP r--------   _   6   case    _   _\n# 6 partes  pars    NOUN    n-p---fa-   Case=Acc|Gender=Fem|Number=Plur 4   obl _   _\n# 7 tres    tres    NUM C1|grn1|casM|gen2|vgr1  Case=Acc|Gender=Fem|Number=Plur|NumForm=Word|NumType=Card   6   nummod  _   SpaceAfter=No\n# 8 ,   ,   PUNCT   u--------   _   11  punct   _   _\n# 9 quarum  qui PRON    p-p---fg-   Case=Gen|Gender=Fem|Number=Plur|PronType=Rel    10  nmod    _   _\n# 10    unam    unus    DET p-s---fa-   Case=Acc|Gender=Fem|InflClass=LatPron|Number=Sing|NumType=Card|NumValue=1|PronType=Ind  11  obj _   _\n# 11    incolunt    incolo  VERB    v3ppia---   Aspect=Imp|Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act  6   acl:relcl   _   _\n# 12    Belgae  Belgae  NOUN    n-p---mn-   Case=Nom|Gender=Masc|Number=Plur    11  nsubj   _   SpaceAfter=No\n# 13    ,   ,   PUNCT   u--------   _   15  punct   _   _\n# 14    aliam   alius   DET a-s---fa-   Case=Acc|Gender=Fem|Number=Sing|PronType=Con    15  det _   _\n# 15    Aquitani    Aquitani    NOUN    n-p---mn-   Case=Nom|Gender=Masc|Number=Plur    11  nsubj   _   SpaceAfter=No\n# 16    ,   ,   PUNCT   u--------   _   17  punct   _   _\n# 17    tertiam tertius ADJ a-s---fa-   Case=Acc|Gender=Fem|Number=Sing|NumType=Ord 15  amod    _   _\n# 18    qui qui PRON    p-p---mn-   Case=Nom|Gender=Masc|Number=Plur|PronType=Rel   25  nsubj:pass  _   _\n# 19    ipsorum ipse    DET p-p---mg-   Case=Gen|Gender=Masc|Number=Plur|Person=3|PronType=Prs  20  nmod    _   _\n# 20    lingua  lingua  NOUN    n-s---fb-   Case=Abl|Gender=Fem|Number=Sing 21  obl _   _\n# 21    Celtae  Celta   ADJ a-s---fg-   Case=Gen|Gender=Fem|Number=Sing 25  xcomp   _   SpaceAfter=No\n# 22    ,   ,   PUNCT   u--------   _   21  punct   _   _\n# 23    nostra  noster  DET p-p---nn-   Case=Abl|Gender=Fem|Number=Sing|Number[psor]=Plur|Person[psor]=1|Poss=Yes|PronType=Prs  24  det _   _\n# 24    Galli   Galli   NOUN    n-p---mn-   Case=Nom|Gender=Masc|Number=Plur    25  nsubj:pass  _   _\n# 25    appellantur appello VERB    v3ppip---   Aspect=Imp|Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Pass 15  acl:relcl   _   SpaceAfter=No\n# 26    .   .   PUNCT   u--------   _   4   punct   _   SpaceAfter=No\n\nСохраняем и парсим CoNLL-U как было показано выше.\n\nwrite_lines(result, \"test_conllu_caesar.txt\")\nudpipe_read_conllu(\"test_conllu_caesar.txt\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "pmi.html#векторы-в-лингвистике",
    "href": "pmi.html#векторы-в-лингвистике",
    "title": "12  Векторные представления слов",
    "section": "",
    "text": "на основе совместной встречаемости слов с использованием PMI;\nс использованием поверхностной нейросети Word2Vec;\nc использованием трансформера BERT.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "pmi.html#подготовка-данных",
    "href": "pmi.html#подготовка-данных",
    "title": "12  Векторные представления слов",
    "section": "12.2 Подготовка данных",
    "text": "12.2 Подготовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для ускорения вычислений возьмем из него лишь один год). Файл в формате .Rdata в формате .Rdata доступен по ссылке.\n\n\nКод для приведения датасета к опрятному виду.\n\n\n\nload(\"../data/news.Rdata\")\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\nnews_tokens_pruned\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\n\n\nПодготовленные данные можно забрать по ссылке.\n\nload(\"../data/news_tokens_pruned.Rdata\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "pmi.html#pmi-svd-эмбеддинги",
    "href": "pmi.html#pmi-svd-эмбеддинги",
    "title": "12  Векторные представления слов",
    "section": "12.3 PMI-SVD эмбеддинги",
    "text": "12.3 PMI-SVD эмбеддинги\n\n12.3.1 Скользящее окно\nПрежде всего разделим новости на контекстные окна фиксированной величины. Чем меньше окно, тем больше синтаксической информации оно хранит.\n\nnested_news &lt;- news_tokens_pruned |&gt; \n  dplyr::select(-topic) |&gt; \n  nest(tokens = c(token))\n\nnested_news\n\n\n  \n\n\n\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out  |&gt; \n    transpose()  |&gt; \n    pluck(\"result\")  |&gt; \n    compact()  |&gt; \n    bind_rows()\n}\n\nДеление на окна может потребовать нескольких минут. Чем больше окно, тем больше потребуется времени и тем больше будет размер таблицы.\n\nnews_windows &lt;- nested_news |&gt; \n  mutate(tokens = map(tokens, slide_windows, 8L))  |&gt;  \n  unnest(tokens) |&gt; \n  unite(window_id, id, window_id)\n\nnews_windows\n\n\nload(\"../data/news_windows.Rdata\")\n\n\n\n12.3.2 Что такое PMI\nОбычная мера ассоциации между словами, которой пользуются лингвисты, — точечная взаимная информация, или PMI (pointwise mutual information). Она рассчитывается по формуле:\n\\[PMI\\left(x;y\\right)=\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}}\\]\nВ числителе — вероятность встретить два слова вместе (например, в пределах одного документа или одного «окна» длинной n слов). В знаменателе — произведение вероятностей встретить каждое из слов отдельно. Если слова чаще встречаются вместе, логарифм будет положительным; если по отдельности — отрицательным.\nПосчитаем PMI на наших данных, воспользовавшись подходящей функцией из пакета {widyr}.\n\nnews_pmi  &lt;- news_windows  |&gt; \n  pairwise_pmi(token, window_id)\n\nnews_pmi |&gt; \n  arrange(-abs(pmi))\n\n\n  \n\n\n\n\n\n12.3.3 Почему PPMI\nВ отличие от коэффициента корреляции, PMI может варьироваться от \\(-\\infty\\) до \\(+\\infty\\), но негативные значения проблематичны. Они означают, что вероятность встретить эти два слова вместе меньше, чем мы бы ожидали в результате случайного совпадения. Проверить это без огромного корпуса невозможно: если у нас есть \\(w_1\\) и \\(w_2\\), каждое из которых встречается с вероятностью \\(10^{-6}\\), то трудно удостовериться в том, что \\(p(w_1, w_2)\\) значимо отличается от \\(10^{-12}\\). Поэтому негативные значения PMI принято заменять нулями. В таком случае формула выглядит так:\n\\[ PMI\\left(x;y\\right)=max(\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}},0) \\] Для подобной замены подойдет векторизованное условие.\n\nnews_ppmi &lt;- news_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi)) \n\nnews_ppmi |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\n\n\n12.3.4 SVD\nДля любых текстовых данных и матрица термин-термин будет очень разряженной (то есть большая часть значений будет равна нулю). Необходимо “переупорядочить” ее так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD). При сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы (К. Маннинг, П. Рагхаван, Х. Шютце 2020, 407).\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. О сингулярных векторах можно думать как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.\nПередадим подготовленные данные фунции widely_svd() для вычисления сингулярного разложения. Число измерений для усеченного SVD задается вручную. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов:\n\nword_emb &lt;- news_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n\n\nword_emb\n\n\n  \n\n\n\n\n\n12.3.5 Визуализация топиков\nВизуализируем главные компоненты нашего векторного пространства.\n\nword_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n12.3.6 Ближайшие соседи\nИсследуем наши эмбеддинги, используя функцию, которая считает косинусное сходство между словами.\n\nnearest_neighbors &lt;- function(df, feat, doc=F) {\n  inner_f &lt;- function() {\n    widely(\n        ~ {\n          y &lt;- .[rep(feat, nrow(.)), ]\n          res &lt;- rowSums(. * y) / \n            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))\n          \n          matrix(res, ncol = 1, dimnames = list(x = names(res)))\n        },\n        sort = TRUE\n    )}\n  if (doc) {\n    df |&gt; inner_f()(doc, dimension, value) }\n  else {\n    df |&gt; inner_f()(word, dimension, value)\n  } |&gt; \n    select(-item2)\n}\n\n\nword_emb |&gt; \n  nearest_neighbors(\"сборная\")\n\n\n  \n\n\nword_emb |&gt; \n  nearest_neighbors(\"завод\")\n\n\n  \n\n\n\n\n\n12.3.7 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы используем алгоритм UMAP. Это алгоритм нелинейного снижения размерности.\n\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 5, ymax = 8, xmin = -1, xmax = 2, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; -1 & V1 &lt; 2) |&gt; \n  filter(V2 &gt; 5 & V2 &lt; 8) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 5, alpha = 0.5, \n            position = position_jitter(width = 0.5, height = 0.5)\n            ) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🥊 Теперь попробуем построить векторное пространство с использованием поверхностных нейросетей.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "pmi.html#bert",
    "href": "pmi.html#bert",
    "title": "12  Векторные представления слов",
    "section": "12.5 BERT",
    "text": "12.5 BERT\nBERT (Bidirectional Encoder Representations from Transformers) — это глубокая нейронная модель для обработки естественного языка, представленная Google в 2018 году. Главная особенность BERT — двунаправленное (bidirectional) чтение текста, позволяющее учитывать контекст слова как слева, так и справа. BERT обучается на задаче восстановления пропущенных слов в предложении и на предсказании, идут ли два предложения подряд.\nВарианты BERT:\n\nBERT-base (12 слоев), BERT-large (24 слоя) — оригинальные модели разного размера.\nDistilBERT (6 слоев) — облегчённая и быстрая версия.\nRoBERTa, ALBERT, TinyBERT и др. — различные модификации BERT, оптимизированные для конкретных задач.\nMultilingual BERT (mBERT) — для многих языков сразу.\n\nBERT генерирует контекстные эмбеддинги — числовые векторы, которые учитывают значение слова в конкретном контексте предложения. Благодаря этому BERT-эмбеддинги позволяют машинам лучше понимать смысл текста и эффективно применять их для поиска, классификации, вопросов-ответов и других NLP-задач.\nДля большинства задач на русском языке (эмбеддинги, классификация, поиск) лучше использовать одну из RuBERT (или RuRoBERT) моделей, они дают лучшие результаты, чем англоязычные или многоязычные варианты.\n\n12.5.1 Reticulate\nДля работы с трансформерами понадобится Python. Создадим виртуальное окружение, которое нужно для корректной работы с пакетом {text}.\n\nlibrary(reticulate)\nuse_python(\"/usr/bin/python3\")\n\n\npy_config()\n# python:         /usr/bin/python3\n# libpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/config-3.9-darwin/libpython3.9.dylib\n# pythonhome:     /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9:/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9\n# version:        3.9.6 (default, Feb  3 2024, 15:58:27)  [Clang 15.0.0 (clang-1500.3.9.4)]\n# numpy:          /Users/olga/Library/Python/3.9/lib/python/site-packages/numpy\n# numpy_version:  1.23.0\n# \n# NOTE: Python version was forced by use_python() function\n\n\npy_eval(\"1+1\")\n\nДля работы понадобятся модули nltk и transformers. Проверьте в терминале, установлены ли они, и, если надо, установите.\n\n# проверить \n/usr/bin/python3 -c \"import nltk; print('nltk version:', nltk.__version__)\"\n/usr/bin/python3 -c \"import transformers; print('transformers version:', transformers.__version__)\"\n# установить\n/usr/bin/python3 -m pip install nltk transformers\n\nСкачайте необходимые данные для NLTK.\n\npy_run_string(\"\nimport nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\")\n\n\n\n12.5.2 Эмбеддинги\nПолучаем эмбеддинги с помощью textEmbed(). Это долго (для нашего датасета около двух часов). Для пробы берем лишь несколько новостей.\n\nset.seed(18112025)\nnews_sample &lt;- news_2019 |&gt; \n  filter(topic %in% c(\"Экономика\", \"Культура\", \"Спорт\")) |&gt; \n  sample_n(size = 25)\n\nВо время обучения вы увидите предупреждение о non-ASCII символах. Если ваша модель обучена на русском или многоязычная, это предупреждение можно игнорировать.\n\nemb &lt;- textEmbed(\n  texts = news_sample$text,\n  # или другая модель\n  model = \"cointegrated/rubert-tiny2\",  \n  # по умолчанию \n  layers = -2,     \n  remove_non_ascii = FALSE\n)\n\nПолучаем эмбеддинги для токенов. 313 - размерность эмбеддинга для каждого токена. BERT-токенизатор использует WordPiece токенизацию, которая разбивает слова на субтокены. Это позволяет обрабатывать редкие и неизвестные слова.\n\nemb$tokens$texts[[2]] \n\n\n  \n\n\n\nТокен [CLS] (=classification) — это специальный служебный токен BERT (и его производных моделей), который автоматически добавляется в самое начало каждого входного текста.\n\nemb$texts$texts\n\n\n  \n\n\n\n\n\n12.5.3 2D-визуализации\n\nemb_texts &lt;- emb$texts$texts  |&gt; \n  mutate(text_id = news_sample$id) |&gt; \n  mutate(topic = news_sample$topic)\n\numap_res &lt;- emb_texts |&gt; \n  dplyr::select(-text_id, -topic) |&gt; \n  as.matrix() |&gt; \n  umap(n_neighbors = 15, min_dist = 0.1, metric = \"cosine\")\n\nplot_df &lt;- as.data.frame(umap_res) |&gt; \n  setNames(c(\"UMAP1\", \"UMAP2\")) |&gt; \n  mutate(text_id = emb_texts$text_id, \n         topic = emb_texts$topic)\n\nplot_df |&gt; \n  ggplot(aes(UMAP1, UMAP2, color = topic)) +\n  geom_text(aes(label = text_id), alpha = 0.8, size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDoc2672 содержит новость о дочери актера, а doc519 – о рейтинге самых высокооплачиваемых иллюзионистов.\n\n\n12.5.4 Ближайшие соседи\n\nnearest_neighbors_matrix &lt;- function(df, feat_row) {\n  # df: датафрейм или матрица, в каждой строке эмбеддинг\n  # feat_row: индекс строки/имя строки, для которой ищем соседа\n  m &lt;- as.matrix(df)\n  \n  # вектор-эмбеддинг выбранного текста\n  v &lt;- m[feat_row, ]\n  \n  # косинусное сходство\n  similarities &lt;- (m %*% v) / (sqrt(rowSums(m^2)) * sqrt(sum(v^2)))\n  \n  # самого себя не берём\n  similarities[feat_row] &lt;- -Inf\n  \n  # ближайший – максимальное сходство\n  nn_idx &lt;- which.max(similarities)\n  \n  # выводим результат\n  list(\n    index = nn_idx,\n    similarity = similarities[nn_idx]\n  )\n}\n\n\nnn &lt;- nearest_neighbors_matrix(df = emb_texts %&gt;% dplyr::select(-text_id, -topic), feat_row = 5)\n\nnn\n\n$index\n[1] 21\n\n$similarity\n[1] 0.949336\n\n\n\nnews_sample |&gt; \n  filter(row_number() == 5) |&gt; \n  pull(text)\n\n[1] \"Российская рок-группа «Браво» выступит в Москве в новогоднюю ночь. Об этом сообщается в пресс-релизе, поступившем в редакцию «Ленты.ру». Концерт начнется после полуночи в клубе «16 Тонн». Отмечается, что на мероприятии прозвучат все хиты коллектива, включая такие песни, как «Московский бит», «Старый отель», «Любите девушки», «Верю я», «Дорога в облака», «Этот город», «Ветер знает» и «Любовь не горит». Кроме того, на двух этажах клуба будут работать танцполы с диджеями. Билеты на концерт можно приобрести на сайте площадке. В их стоимость будет включено не только посещения праздника, но и все блюда из новогоднего меню. Группа «Браво» была основана Евгением Хавтаном в 1983 году. Солистами коллектива в разные годы были Жанна Агузарова, Валерий Сюткин и Роберт Ленц. Во время новогодних праздников в клубе «16 тонн» выступят и такие артисты, как «Рекорд Оркестр» (1 января), «НОМ» (2 января), Андрей Князев (3 января), On-the-Go (4 января), Найк Борзов (6 января), Zero People (7 января) и «Буерак» (8 января).\"\n\n\n\nnews_sample |&gt; \n  filter(row_number() == nn$index) |&gt; \n  pull(text)\n\n[1] \"Группа «Алиса» даст традиционный концерт в день рождения фронтмена Константина Кинчева. Об этом сообщается в пресс-релизе, поступившем в редакцию «Ленты.ру». Отмечается, что выступление состоится 28 декабря в московском клубе «Известия Hall». На таких мероприятиях коллектив, как правило, исполняет «внепрограмнные» песни, которые редко звучат со сцены. Билеты можно приобрести на сайте. Группа« Алиса» была образована в 1983 году в Ленинграде. С тех пор она записала более 20 альбомов. Автором многих песен коллектива является Константин Кинчев, который стал вокалистом «Алисы» в 1984 году. За время своего существования в группе сменилось около десятка разных музыкантов. Кинчев вместе с остальными участниками «Алисы» создал такие альбомы, как «Энергия», «Блок ада», «Шестой лесничий», «Черная метка», «Солнцеворот» и другие.\"\n\n\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html",
    "href": "embed.html",
    "title": "11  Векторные представления слов",
    "section": "",
    "text": "11.1 Векторы в лингвистике\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Есть несколько возможных решений.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравнивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход – зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы, или эмбеддинги. Для этого к исходной матрице применяются различные методы снижения размерности.\nПодробнее о векторных моделях можено почитать статью В. Селеверстова на “Системном блоке”, а также посмотреть видео с лекцией Д. Рыжовой.\nВ этом уроке мы изучим несколько способов пострения эмбеддингов:\nТакже мы поговорим об использовании готовых (предобученных) эмбеддингов для разных языков.\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(stopwords)\nlibrary(widyr)\nlibrary(uwot)\nlibrary(word2vec)\nlibrary(text)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#векторы-в-лингвистике",
    "href": "embed.html#векторы-в-лингвистике",
    "title": "11  Векторные представления слов",
    "section": "",
    "text": "на основе совместной встречаемости слов с использованием PMI;\nс использованием поверхностной нейросети Word2Vec;\nc использованием трансформера BERT.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#подготовка-данных",
    "href": "embed.html#подготовка-данных",
    "title": "11  Векторные представления слов",
    "section": "11.2 Подготовка данных",
    "text": "11.2 Подготовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для ускорения вычислений возьмем из него лишь один год). Файл в формате .Rdata в формате .Rdata доступен по ссылке.\n\n\nКод для приведения датасета к опрятному виду.\n\n\n\nload(\"../data/news.Rdata\")\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\nnews_tokens_pruned\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\n\n\nПодготовленные данные можно забрать по ссылке.\n\nload(\"../data/news_tokens_pruned.Rdata\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#pmi-svd-эмбеддинги",
    "href": "embed.html#pmi-svd-эмбеддинги",
    "title": "11  Векторные представления слов",
    "section": "11.3 PMI-SVD эмбеддинги",
    "text": "11.3 PMI-SVD эмбеддинги\n\n11.3.1 Скользящее окно\nПрежде всего разделим новости на контекстные окна фиксированной величины. Чем меньше окно, тем больше синтаксической информации оно хранит.\n\nnested_news &lt;- news_tokens_pruned |&gt; \n  dplyr::select(-topic) |&gt; \n  nest(tokens = c(token))\n\nnested_news\n\n\n  \n\n\n\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out  |&gt; \n    transpose()  |&gt; \n    pluck(\"result\")  |&gt; \n    compact()  |&gt; \n    bind_rows()\n}\n\nДеление на окна может потребовать нескольких минут. Чем больше окно, тем больше потребуется времени и тем больше будет размер таблицы.\n\nnews_windows &lt;- nested_news |&gt; \n  mutate(tokens = map(tokens, slide_windows, 8L))  |&gt;  \n  unnest(tokens) |&gt; \n  unite(window_id, id, window_id)\n\nnews_windows\n\n\n\n11.3.2 Что такое PMI\nОбычная мера ассоциации между словами, которой пользуются лингвисты, — точечная взаимная информация, или PMI (pointwise mutual information). Она рассчитывается по формуле:\n\\[PMI\\left(x;y\\right)=\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}}\\]\nВ числителе — вероятность встретить два слова вместе (например, в пределах одного документа или одного «окна» длинной n слов). В знаменателе — произведение вероятностей встретить каждое из слов отдельно. Если слова чаще встречаются вместе, логарифм будет положительным; если по отдельности — отрицательным.\nПосчитаем PMI на наших данных, воспользовавшись подходящей функцией из пакета {widyr}.\n\nnews_pmi  &lt;- news_windows  |&gt; \n  pairwise_pmi(token, window_id)\n\nnews_pmi |&gt; \n  arrange(-abs(pmi))\n\n\n  \n\n\n\n\n\n11.3.3 Почему PPMI\nВ отличие от коэффициента корреляции, PMI может варьироваться от \\(-\\infty\\) до \\(+\\infty\\), но негативные значения проблематичны. Они означают, что вероятность встретить эти два слова вместе меньше, чем мы бы ожидали в результате случайного совпадения. Проверить это без огромного корпуса невозможно: если у нас есть \\(w_1\\) и \\(w_2\\), каждое из которых встречается с вероятностью \\(10^{-6}\\), то трудно удостовериться в том, что \\(p(w_1, w_2)\\) значимо отличается от \\(10^{-12}\\). Поэтому негативные значения PMI принято заменять нулями. В таком случае формула выглядит так:\n\\[ PPMI\\left(x;y\\right)=max(\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}},0) \\] Для подобной замены подойдет векторизованное условие.\n\nnews_ppmi &lt;- news_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi)) \n\nnews_ppmi |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\n\n\n11.3.4 SVD\nДля любых текстовых данных и матрица термин-термин будет очень разряженной (то есть большая часть значений будет равна нулю). Необходимо “переупорядочить” ее так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD). При сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы (К. Маннинг, П. Рагхаван, Х. Шютце 2020, 407).\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. О сингулярных векторах можно думать как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.\nПередадим подготовленные данные фунции widely_svd() для вычисления сингулярного разложения. Число измерений для усеченного SVD задается вручную. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов:\n\nset.seed(123)\nword_emb &lt;- news_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) \n\n\nword_emb\n\n\n  \n\n\n\n\n\n11.3.5 Визуализация топиков\nВизуализируем главные компоненты нашего векторного пространства.\n\nword_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(reorder_within(item1, value, dimension), value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n11.3.6 Ближайшие соседи\nИсследуем наши эмбеддинги, используя функцию, которая считает косинусное сходство между словами.\n\nnearest_neighbors &lt;- function(df, token) {\n  df %&gt;%\n    widely(\n      ~ {\n        y &lt;- .[rep(token, nrow(.)), ]\n        res &lt;- rowSums(. * y) / \n          (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2)))\n        \n        matrix(res, ncol = 1, dimnames = list(x = names(res)))\n      },\n      sort = TRUE\n    )(item1, dimension, value) %&gt;%\n    select(-item2)\n}\n\n\nword_emb |&gt; \n  nearest_neighbors(\"сборная\")\n\n\n  \n\n\nword_emb |&gt; \n  nearest_neighbors(\"завод\")\n\n\n  \n\n\n\n\n\n11.3.7 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(item1, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы используем алгоритм UMAP. Это алгоритм нелинейного снижения размерности.\n\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = -2, ymax = 1, xmin = 8, xmax = 11, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 8 & V1 &lt; 11) |&gt; \n  filter(V2 &gt; -2 & V2 &lt; 1) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 5, alpha = 0.5, \n            position = position_jitter(width = 0.5, height = 0.5)\n            ) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🥊 Теперь попробуем построить векторное пространство с использованием поверхностных нейросетей.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#word2vec",
    "href": "embed.html#word2vec",
    "title": "11  Векторные представления слов",
    "section": "11.4 Word2Vec",
    "text": "11.4 Word2Vec\nWord2vec – это полносвязаная нейросеть с одним скрытым слоем. Такое обучение называется не глубоким, а поверхностным (shallow).\n\ncorpus_w2v &lt;- news_tokens_pruned |&gt; \n  group_by(id) |&gt; \n  mutate(text = str_c(token, collapse = \" \")) |&gt; \n  distinct(id, text)\n\n\n# устанавливаем зерно, т.к. начальные веса устанавливаются произвольно\nset.seed(02062024) \nmodel &lt;- word2vec(x = corpus_w2v$text, \n                  type = \"skip-gram\",\n                  dim = 100,\n                  window = 10,\n                  iter = 20,\n                  hs = TRUE,\n                  min_count = 5,\n                  threads = 6)\n\nНаша модель содержит эмбеддинги для слов; посмотрим на матрицу.\n\nemb &lt;- as.matrix(model)\ndim(emb)\n\n[1] 6305  100\n\n\n\npredict(model, c(\"погода\", \"спорт\"), type = \"nearest\", top_n = 10) |&gt; \n  bind_rows()\n\n\n  \n\n\n\nПолучившуюся модель можно визуализировать, как мы это делали выше.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#bert",
    "href": "embed.html#bert",
    "title": "11  Векторные представления слов",
    "section": "11.5 BERT",
    "text": "11.5 BERT\nBERT (Bidirectional Encoder Representations from Transformers) — это глубокая нейронная модель для обработки естественного языка, представленная Google в 2018 году. Главная особенность BERT — двунаправленное (bidirectional) чтение текста, позволяющее учитывать контекст слова как слева, так и справа. BERT обучается на задаче восстановления пропущенных слов в предложении и на предсказании, идут ли два предложения подряд.\nВарианты BERT:\n\nBERT-base (12 слоев), BERT-large (24 слоя) — оригинальные модели разного размера.\nDistilBERT (6 слоев) — облегчённая и быстрая версия.\nRoBERTa, ALBERT, TinyBERT и др. — различные модификации BERT, оптимизированные для конкретных задач.\nMultilingual BERT (mBERT) — для многих языков сразу.\n\nBERT генерирует контекстные эмбеддинги — числовые векторы, которые учитывают значение слова в конкретном контексте предложения. Благодаря этому BERT-эмбеддинги позволяют машинам лучше понимать смысл текста и эффективно применять их для поиска, классификации, вопросов-ответов и других NLP-задач.\nДля большинства задач на русском языке (эмбеддинги, классификация, поиск) лучше использовать одну из RuBERT (или RuRoBERT) моделей, они дают лучшие результаты, чем англоязычные или многоязычные варианты.\n\n11.5.1 Reticulate\nДля работы с трансформерами понадобится Python. Создадим виртуальное окружение, которое нужно для корректной работы с пакетом {text}.\n\nlibrary(reticulate)\nuse_python(\"/usr/bin/python3\")\n\n\npy_config()\n# python:         /usr/bin/python3\n# libpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/config-3.9-darwin/libpython3.9.dylib\n# pythonhome:     /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9:/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9\n# version:        3.9.6 (default, Feb  3 2024, 15:58:27)  [Clang 15.0.0 (clang-1500.3.9.4)]\n# numpy:          /Users/olga/Library/Python/3.9/lib/python/site-packages/numpy\n# numpy_version:  1.23.0\n# \n# NOTE: Python version was forced by use_python() function\n\n\npy_eval(\"1+1\")\n\nДля работы понадобятся модули nltk и transformers. Проверьте в терминале, установлены ли они, и, если надо, установите.\n\n# проверить \n/usr/bin/python3 -c \"import nltk; print('nltk version:', nltk.__version__)\"\n/usr/bin/python3 -c \"import transformers; print('transformers version:', transformers.__version__)\"\n# установить\n/usr/bin/python3 -m pip install nltk transformers\n\nСкачайте необходимые данные для NLTK.\n\npy_run_string(\"\nimport nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\")\n\n\n\n11.5.2 Эмбеддинги\nПолучаем эмбеддинги с помощью textEmbed(). Это долго (для нашего датасета около двух часов). Для пробы берем лишь несколько новостей.\n\nset.seed(18112025)\nnews_sample &lt;- news_2019 |&gt; \n  filter(topic %in% c(\"Экономика\", \"Культура\", \"Спорт\")) |&gt; \n  sample_n(size = 25)\n\nВо время обучения вы увидите предупреждение о non-ASCII символах. Если ваша модель обучена на русском или многоязычная, это предупреждение можно игнорировать.\n\nemb &lt;- textEmbed(\n  texts = news_sample$text,\n  # или другая модель\n  model = \"cointegrated/rubert-tiny2\",  \n  # по умолчанию \n  layers = -2,     \n  remove_non_ascii = FALSE\n)\n\nПолучаем эмбеддинги для токенов. 313 - размерность эмбеддинга для каждого токена. BERT-токенизатор использует WordPiece токенизацию, которая разбивает слова на субтокены. Это позволяет обрабатывать редкие и неизвестные слова.\n\nemb$tokens$texts[[2]] \n\n\n  \n\n\n\nТокен [CLS] (=classification) — это специальный служебный токен BERT (и его производных моделей), который автоматически добавляется в самое начало каждого входного текста.\n\nemb$texts$texts\n\n\n  \n\n\n\n\n\n11.5.3 2D-визуализации\n\nemb_texts &lt;- emb$texts$texts  |&gt; \n  mutate(text_id = news_sample$id) |&gt; \n  mutate(topic = news_sample$topic)\n\numap_res &lt;- emb_texts |&gt; \n  dplyr::select(-text_id, -topic) |&gt; \n  as.matrix() |&gt; \n  umap(n_neighbors = 15, min_dist = 0.1, metric = \"cosine\")\n\nplot_df &lt;- as.data.frame(umap_res) |&gt; \n  setNames(c(\"UMAP1\", \"UMAP2\")) |&gt; \n  mutate(text_id = emb_texts$text_id, \n         topic = emb_texts$topic)\n\nplot_df |&gt; \n  ggplot(aes(UMAP1, UMAP2, color = topic)) +\n  geom_text(aes(label = text_id), alpha = 0.8, size = 3) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nDoc2672 содержит новость о дочери актера, а doc519 – о рейтинге самых высокооплачиваемых иллюзионистов.\n\n\n11.5.4 Ближайшие соседи\n\nnearest_neighbors_matrix &lt;- function(df, feat_row) {\n  # df: датафрейм или матрица, в каждой строке эмбеддинг\n  # feat_row: индекс строки/имя строки, для которой ищем соседа\n  m &lt;- as.matrix(df)\n  \n  # вектор-эмбеддинг выбранного текста\n  v &lt;- m[feat_row, ]\n  \n  # косинусное сходство\n  similarities &lt;- (m %*% v) / (sqrt(rowSums(m^2)) * sqrt(sum(v^2)))\n  \n  # самого себя не берём\n  similarities[feat_row] &lt;- -Inf\n  \n  # ближайший – максимальное сходство\n  nn_idx &lt;- which.max(similarities)\n  \n  # выводим результат\n  list(\n    index = nn_idx,\n    similarity = similarities[nn_idx]\n  )\n}\n\n\nnn &lt;- nearest_neighbors_matrix(df = emb_texts %&gt;% dplyr::select(-text_id, -topic), feat_row = 5)\n\nnn\n\n$index\n[1] 21\n\n$similarity\n[1] 0.949336\n\n\n\nnews_sample |&gt; \n  filter(row_number() == 5) |&gt; \n  pull(text)\n\n[1] \"Российская рок-группа «Браво» выступит в Москве в новогоднюю ночь. Об этом сообщается в пресс-релизе, поступившем в редакцию «Ленты.ру». Концерт начнется после полуночи в клубе «16 Тонн». Отмечается, что на мероприятии прозвучат все хиты коллектива, включая такие песни, как «Московский бит», «Старый отель», «Любите девушки», «Верю я», «Дорога в облака», «Этот город», «Ветер знает» и «Любовь не горит». Кроме того, на двух этажах клуба будут работать танцполы с диджеями. Билеты на концерт можно приобрести на сайте площадке. В их стоимость будет включено не только посещения праздника, но и все блюда из новогоднего меню. Группа «Браво» была основана Евгением Хавтаном в 1983 году. Солистами коллектива в разные годы были Жанна Агузарова, Валерий Сюткин и Роберт Ленц. Во время новогодних праздников в клубе «16 тонн» выступят и такие артисты, как «Рекорд Оркестр» (1 января), «НОМ» (2 января), Андрей Князев (3 января), On-the-Go (4 января), Найк Борзов (6 января), Zero People (7 января) и «Буерак» (8 января).\"\n\n\n\nnews_sample |&gt; \n  filter(row_number() == nn$index) |&gt; \n  pull(text)\n\n[1] \"Группа «Алиса» даст традиционный концерт в день рождения фронтмена Константина Кинчева. Об этом сообщается в пресс-релизе, поступившем в редакцию «Ленты.ру». Отмечается, что выступление состоится 28 декабря в московском клубе «Известия Hall». На таких мероприятиях коллектив, как правило, исполняет «внепрограмнные» песни, которые редко звучат со сцены. Билеты можно приобрести на сайте. Группа« Алиса» была образована в 1983 году в Ленинграде. С тех пор она записала более 20 альбомов. Автором многих песен коллектива является Константин Кинчев, который стал вокалистом «Алисы» в 1984 году. За время своего существования в группе сменилось около десятка разных музыкантов. Кинчев вместе с остальными участниками «Алисы» создал такие альбомы, как «Энергия», «Блок ада», «Шестой лесничий», «Черная метка», «Солнцеворот» и другие.\"",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#предобученные-эмбеддинги",
    "href": "embed.html#предобученные-эмбеддинги",
    "title": "11  Векторные представления слов",
    "section": "11.6 Предобученные эмбеддинги",
    "text": "11.6 Предобученные эмбеддинги\nПреобученные эмбеддинги можно скачать отсюда или (только для русского) отсюда. Компактные модели для русского также можно найти здесь.\n\n11.6.1 Что учитывать при выборе модели\n\nКОНТЕКСТУАЛИЗИРОВАННЫЕ vs. СТАТИЧЕСКИЕ МОДЕЛИ.\n\n\nВ статических моделях типа Word2Vec или GloVe каждое слово имеет только один фиксированный вектор (например, слово “замок” — один и тот же вектор, независимо от значения: “castle” или “lock”). Они легче и быстрее в использовании, требуют меньше ресурсов, чаще всего доступны в виде простых таблиц: слово — вектор.\nВ контекстуализированных моделях типа BERT одно и то же слово имеет РАЗНЫЕ вектора в зависимости от окружения (“контекста”). Они применимы для анализа значения в конкретном предложении, больше по размеру, нужны скрипты/программы для генерации эмбеддингов, иногда требуют GPU. Обычно их используют для представления предложений или текстов целиком, а не отдельных слов.\n\n\nКОРПУС — из какого текстового источника училась модель.\n\n\nНКРЯ: Национальный корпус русского языка — самый сбалансированный и научный.\nВикипедия: Энциклопедическая, больше терминов, меньше разговорных выражений.\nНовости: Хороша для современных новостных тем, немного односторонний стиль.\nAraneum, Тайга, GeoWAC, Веб: Огромные веб-корпусы — больше сленга, просторечий, шире охват, но меньше структурированности (могут быть ошибки, дубли, шум).\n\n\nПРЕДОБРАБОТКА — как были подготовлены тексты.\n\n\nЛемматизация (все слова приведены к начальной форме) или токенизация.\nРазметка (PoS): Теги частей речи (“делать_VERB”).\n\n\nРЕЖИМ ОБУЧЕНИЯ: CBOW vs. SKIPGRAM\n\n\nCBOW: Предсказывает слово по его контексту (соседним словам). Пример: контекст — “Я ___ в кино”.\nSkip-gram: Делает наоборот — по слову предсказывает окружающие его слова. Пример: “иду” → [“Я”, “в”, “кино”]\n\n\nДля большинства задач рекомендуется выбирать предобученные модели, обученные с помощью Skip-gram, поскольку они лучше работают с редкими словами и устойчивыми выражениями, что особенно важно для русского языка с его богатой морфологией; CBOW можно использовать, если нужен более быстрый расчет и основное внимание уделяется массовым (частотным) словам, однако качество представлений на редких и сложных словах у Skip-gram почти всегда выше, поэтому если вы не ограничены во времени и ресурсах — отдавайте предпочтение именно ей.\n\n\n11.6.2 Операции с эмбеддингами\nМодель ruwikiruscorpora_upos_cbow_300_10_2021 весит 638 Мб при скачивании.\n\nvec_path &lt;- \"../files/220/model.bin\"\n\nembeddings &lt;- word2vec::read.wordvectors(vec_path, type = \"bin\")\n\n\n\ndim(embeddings)\n\n\n# Создаем свою функцию косинусного сходства\ncosine_similarity &lt;- function(x, y) {\n  sum(x * y) / (sqrt(sum(x^2)) * sqrt(sum(y^2)))\n}\n\n# Матричная версия для многих векторов\ncosine_similarity_matrix &lt;- function(x, y) {\n  x_norm &lt;- x / sqrt(rowSums(x^2))\n  y_norm &lt;- y / sqrt(rowSums(y^2))\n  tcrossprod(x_norm, y_norm)\n}\n\nfind_similar &lt;- function(word, embeddings, top_n = 10) {\n  if (!word %in% rownames(embeddings)) {\n    return(paste(\"Слово\", word, \"не найдено в словаре\"))\n  }\n  \n  word_vec &lt;- matrix(embeddings[word, ], nrow = 1)\n  \n  # Считаем косинусное сходство со всеми словами\n  similarities &lt;- apply(embeddings, 1, function(x) {\n    cosine_similarity(word_vec[1, ], x)\n  })\n  \n  # Сортируем по убыванию сходства\n  sorted_indices &lt;- order(similarities, decreasing = TRUE)\n  similar_words &lt;- names(similarities)[sorted_indices[2:(top_n + 1)]]  # пропускаем само слово\n  similarity_scores &lt;- similarities[sorted_indices[2:(top_n + 1)]]\n  \n  data.frame(word = similar_words, similarity = round(similarity_scores, 4))\n}\n\n\n# Использование\nfind_similar(\"время_NOUN\", embeddings)\nfind_similar(\"человек_NOUN\", embeddings)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "share.html#видео",
    "href": "share.html#видео",
    "title": "7  Публикационная система Quarto",
    "section": "7.9 Видео",
    "text": "7.9 Видео\n\nВидео 2025 г.\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using R. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "iterate.html#видео",
    "href": "iterate.html#видео",
    "title": "4  Циклы, условия, функции",
    "section": "4.12 Видео",
    "text": "4.12 Видео\n\nВидео 2024\nВидео 2025",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "plot.html#видео",
    "href": "plot.html#видео",
    "title": "3  Визуализации",
    "section": "3.13 Видео",
    "text": "3.13 Видео\n\nВидео 2025 г.\nВидео 2024 г.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "tabular.html#видео",
    "href": "tabular.html#видео",
    "title": "2  Таблицы. Опрятные данные",
    "section": "2.9 Видео",
    "text": "2.9 Видео\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы. Опрятные данные</span>"
    ]
  },
  {
    "objectID": "tokenize.html#видео",
    "href": "tokenize.html#видео",
    "title": "10  Токенизация и лемматизация",
    "section": "10.4 Видео",
    "text": "10.4 Видео\n\nВидео 2024 г. (раз и два)\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "tokenize.html#домашнее-задание",
    "href": "tokenize.html#домашнее-задание",
    "title": "10  Токенизация и лемматизация",
    "section": "10.5 Домашнее задание",
    "text": "10.5 Домашнее задание\nПо ссылке вы найдете датасет со сказками Салтыкова-Щедрина (в формате .Rdata). Вам необходимо аннотировать сказки, используя модель SynTagRus через пакет {udpipe} (версия модели 2.5).\nПосле этого ответьте на вопросы по ссылке. Задание считается выполненным, если верные ответы даны на 3 из 5 вопросов.\nОшибки лемматизации и морфологического анализа игнорируйте.\nДедлайн: 28 ноября, 21-00.\n\n\n\n\nHvitfeldt, Emil, и Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. Taylor; Francis.\n\n\nJockers, Matthew L. 2014. Text Analysis with R for Students of Literature. Springer.\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Токенизация и лемматизация</span>"
    ]
  },
  {
    "objectID": "embed.html#видео",
    "href": "embed.html#видео",
    "title": "11  Векторные представления слов",
    "section": "11.7 Видео",
    "text": "11.7 Видео\n\nВидео 2024 г.\nВидео 2025 г.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embed.html#домашнее-задание",
    "href": "embed.html#домашнее-задание",
    "title": "11  Векторные представления слов",
    "section": "11.8 Домашнее задание",
    "text": "11.8 Домашнее задание\n\nПримите ссылку на домашнее задание через Classroom https://classroom.github.com/a/LZo22NPk\nВ репозитории вы найдете код и инструкцию по скачиванию данных.\nВам надо осмыслить (!) код и ответить на практические и теоретические вопросы в Forms (ссылка)\nРабота оценивается по шкале 0-10.\nДедлайн для сдачи: 5 декабря 2025 г., 21-00.\n\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "sentiment.html#метод-словарей",
    "href": "sentiment.html#метод-словарей",
    "title": "12  Эмоциональная тональность",
    "section": "12.2 Метод словарей",
    "text": "12.2 Метод словарей\n\n12.2.1 Лексиконы для русского языка\nПакет с лексиконами устанавливается напрямую из GitHub.\n\nremotes::install_github(\"dmafanasyev/rulexicon\")\n\nНачало работы.\n\nlibrary(rulexicon)\nlibrary(tidyverse)\nlibrary(tidytext)\n\nРусский язык входит в языков, для которых Й. Чен и С. Скиена собрали оценочную лексику (Chen и Skiena 2014). Их лексикон построен на основе графа знаний, связывающего слова на разных языках (на основе Wiktionary, Google Translate, транслитерационных ссылок и WordNet). Слова оцениваются по бинарной шкале ( -1 / 1).\n\nset.seed(0211)\nchen_skiena &lt;- hash_sentiment_chen_skiena\nsample_n(chen_skiena, 10)\n\n\n  \n\n\n\nСловарь AFINN содержит 7268 оценочных слов. Их тональность оценивается по шкале от -5 (крайне негативная) до 5 (в высшей степени положительная). Например, слово “адский” имеет оценку -5, а слово “ангельский” – +5.\n\nset.seed(0211)\nafinn &lt;- hash_sentiment_afinn_ru\n\nsample_n(afinn, 10) |&gt; \n  print()\n\n            token score\n1   экстатический   1.7\n2        знаковый   1.7\n3     счастливчик   5.0\n4      суматошный  -3.3\n5             гад  -5.0\n6   выразительный   5.0\n7      жутковатый  -5.0\n8    креативность   5.0\n9    обнадёживать   2.5\n10 привлекательно   5.0\n\n\nNRC** для русского языка – это переведенная версия списка положительных и отрицательных слов Mohammad & Turney (2010). Таблица содержит 5179 слов с не нейтральными оценками. Бинарная шкала: -1 / 1.\n\nset.seed(1102)\nnrc &lt;- hash_sentiment_nrc_emolex_ru\nsample_n(nrc, 10)\n\n\n  \n\n\n\n\n\n12.2.2 Опрятный подход\nСогласно Silge и Robinson (2017), анализе эмоциональной тональности в духе tidy data предполагает следующий алгоритм работы:\n\nПрежде всего текст делится на токены (или лемматизируется), затем каждому токену присваивается некое значение тональности, после чего эти значения суммируются и визуализируются.\nПрежде всего текст необходимо токенизировать, лемматизировать и привести в опрятный формат. Можно загрузить уже подготовленные данные по ссылке.\n\nload(\"../data/liza_tbl.Rdata\")\n\nРазделим весь текст “Лизы” на отрывки по 100 слов: это позволит понять, как меняется эмоциональная тональность произведения по мере развития сюжета.\n\nliza_tbl &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  select(lemma) |&gt;  \n  rename(token = lemma)  |&gt;  \n  mutate(chunk = round(((row_number() + 50) / 100), 0))\n\nliza_tbl\n\n\n  \n\n\n\nВ тексте чуть более 5000 слов, у нас получился 51 отрывок.\n\n\n12.2.3 Модификация лексикона\nСовременные лексиконы могут не очень подходят для анализа классической литературы. Например, в лексиконе AFINN, доступном в пакете rulexicon, слово “старый” имеет отрицательную оценку, как и слово “чувствительный”.\nКод ниже показывает, как можно удалить слово или поменять его знак в R. Разумеется, все то же самое можно сделать вручную, сохранив лексикон локально в виде файла.\n\nlex &lt;- hash_sentiment_afinn_ru |&gt; \n  filter(token != \"старый\")\n\nlex &lt;- lex  |&gt; \n  mutate_at(vars(score), ~ case_when(token == \"чувствительный\" ~  1.7,\n                 TRUE ~ .))\n\nlex |&gt; \n  filter(str_detect(token, \"чувств\"))\n\n\n  \n\n\n\n\n\n12.2.4 Соединение лексикона с документом\nСтоп-слова, то есть слова, не несущие никакой смысловой нагрузки, нам не нужны, но удалять их отдельно нет смысла: мы соединим, при помощи функции inner_join(), документ с одним из лексиконов, в котором не будет стоп-слов. Функция inner_join() работает так:\n\n\nliza_sent &lt;- liza_tbl |&gt; \n  inner_join(lex)\n\nliza_sent\n\n\n  \n\n\n\nЗдесь “горе” – ошибка лемматизации (“стоя на сей горе…”).\nСложив положительно и отрицательно окрашенную лексику для каждого отрывка, получаем значение, позволяющее судить о доминирующей тональности:\n\nliza_chunk_sent &lt;- liza_sent |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  arrange(sum)\n\nliza_chunk_sent\n\n\n  \n\n\n\nДовольно неожиданно, что самый негативный отрывок находится не в конце повести, ближе к трагической развязке, а почти в начале (отрывок 5, ср. отрывки 3 и 4 рядом).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#визуализации",
    "href": "sentiment.html#визуализации",
    "title": "12  Эмоциональная тональность",
    "section": "12.3 Визуализации",
    "text": "12.3 Визуализации\n\n12.3.1 Сравнительное облако слов\nПредставим эмоционально окрашенную лексику отрывков 3-5 в виде сравнительного облака слов. Палитру берем отсюда.\n\nlibrary(reshape2)\nlibrary(wordcloud)\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\n# добавляем новый столбец для удобства визуализации\nliza_sent_class &lt;- liza_sent |&gt; \n  mutate(tone = case_when( score &gt;= 0 ~ \"pos\",\n                           score &lt; 0 ~ \"neg\"))\n\nset.seed(0211)\nliza_sent_class |&gt; \n  filter(chunk  %in%  c(3, 4, 5)) |&gt; \n  count(token, tone, sort = TRUE) |&gt; \n  acast(token ~ tone, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = c(pal[1], pal[5]),\n                   max.words = 99)\n\n\n\n\n\n\n\n\nЗдесь видно, что негативная тональность в этой части не связана с судьбой героев: об этом говорят такие слова, как “лютый”, “враг”, “свирепый”. Рассказчик, глядя на заброшенный Симонов монастырь, вспоминает о “печальной истории” Москвы. Если верить нашей модели, самый мрачный фрагмент повести посвящен не судьбе бедной девушки, а “глухому стону времен”:\n\nИногда на вратах храма рассматриваю изображение чудес, в сем монастыре случившихся, там рыбы падают с неба для насыщения жителей монастыря, осажденного многочисленными врагами; тут образ богоматери обращает неприятелей в бегство. Все сие обновляет в моей памяти историю нашего отечества — печальную историю тех времен, когда свирепые татары и литовцы огнем и мечом опустошали окрестности российской столицы и когда несчастная Москва, как беззащитная вдовица, от одного бога ожидала помощи в лютых своих бедствиях.\n\n\n\n12.3.2 Ось времени\nТаблица, которую мы подготовили, позволяет наглядно показать, как меняется тональность во времени – разумеется, речь идет о повествовательном времени, которое измеряется не в минутах, а в словах.\nОбозначим как положительный или отрицательный каждый из отрывков, как мы это делали для слов.\n\nliza_chunk_sent &lt;- liza_chunk_sent |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_chunk_sent\n\n\n  \n\n\n\nПалитра у нас уже сохранена.\n\nlibrary(showtext)\nfont_add(family = \"vibes\", \"GreatVibes-Regular.ttf\")\nshowtext_auto()\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\np1 &lt;- liza_chunk_sent |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (без учета отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\np1\n\n\n\n\n\n\n\n\nВ целом график получился осмысленным. Мы уже сказали выше про отрывки 3-4. Дальше немного скорби в отрывке 8 посвящено покойному отцу Лизы. В 11-м отрывке отразилась тревога матери за судьбу дочери: “коварно”, “обидеть”, “дурной” вносят вклад в настроение этого фрагмента. Это достаточно характерно для сентиментальной прозы с ее противопоставлением пороков городской жизни и пасторальных добродетелей.\n\nУ меня всегда сердце бывает не на своем месте, когда ты ходишь в город; я всегда ставлю свечу перед образ и молю господа бога, чтобы он сохранил тебя от всякой беды и напасти.\n\nЕще два минимума: отрывки 31 и 34. В первом из них Лиза встревожена вестью о возможном замужестве с сыном крестьянина. Отрывок 34 – это падение Лизы:\n\nГрозно шумела буря, дождь лился из черных облаков — казалось, что натура сетовала о потерянной Лизиной невинности.\n\nНа графике видно, что это место гораздо более эмоционально, чем эпизод самоубийства Лизы: именно после знаменитых карамзинских многоточий и тире события устремляются к трагическому финалу. О самой смерти девушки Карамзин говорит, конечно, с грустью, но без надрыва: “Тут она бросилась в воду”.\nОтрывки 38, 39, 42 – Эраст отправляется на войну. Все, как положено, плачут, что зафиксировал и наш график.\nНаконец, в отрывках 49-51 доминирует тема смерти, причем часть этих слов относится не к самой девушке, а к ее матери.\n\nliza_sent_class |&gt; \n  filter(chunk %in% c(49:51)) |&gt; \n  filter(tone == \"neg\") |&gt; \n  count(token, sort = T) |&gt; \n  with(wordcloud(token, n, max.words = 100, colors = pal[2]))\n\n\n\n\n\n\n\n\nВ отрывке 15 несколько негативных слов имеют перед собой отрицания (“не подозревая”, “никакого худого намерения” и т.п.), поэтому к числу отрицательно окрашенных он отнесен ошибочно. К сожалению, это недостаток подхода, основанного на словарях, не принимающего в учет синтаксические связи в предложении.\nОдно из самых простых решений заключается в том, что бы соединить отрицание и следующее за ним слово (или добавить отрицание ко всем словам до следующего знака препинания).\n\nneg_sent &lt;- \"Старушка с охотою приняла сие предложение, не подозревая в нем никакого худого намерения.\"\n\nstr_replace_all(neg_sent, \"( не | никакого )(\\\\w+)\", \" NEG_\\\\2\")\n\n[1] \"Старушка с охотою приняла сие предложение, NEG_подозревая в нем NEG_худого намерения.\"\n\n\nЧтобы систематически применить этот подход ко всему документу (или коллекции документов), необходим список отрицаний для выбранного языка. Список ниже не претендует на полноту, но иллюстрирует общий принцип.\n\nnegations &lt;- c(\"никто\", \"никого\", \"никем\", \"ничто\", \"ничем\", \"ничего\", \"ни\", \"никакой\", \"никакого\", \"никаких\", \"никаким\", \"никак\", \"ничей\", \"ничьих\", \"нисколько\", \"никогда\", \"нигде\", \"никуда\", \"некого\", \"нельзя\", \"нечего\", \"незачем\", \"нет\", \"едва\", \"не\", \"ничуть\")\n\nregex &lt;- str_c(negations, collapse = \" | \")\nregex &lt;- paste0(\"( \", regex, \"  )(\\\\w+)\")\nregex\n\n[1] \"( никто | никого | никем | ничто | ничем | ничего | ни | никакой | никакого | никаких | никаким | никак | ничей | ничьих | нисколько | никогда | нигде | никуда | некого | нельзя | нечего | незачем | нет | едва | не | ничуть  )(\\\\w+)\"\n\n\n\nload(\"../data/liza_tbl.Rdata\") \ntext &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  pull(lemma) |&gt; \n  str_c(collapse = \" \")\n\nЗаменяем отрицания и считаем статистику по отрывкам.\n\ntext &lt;-  str_replace_all(text, regex, \" NEG_\\\\2\")\n\n\nliza_NEG &lt;- tibble(text = text) |&gt; \n  unnest_tokens(token, text) |&gt; \n  mutate(chunk = round(((row_number() + 50) / 100), 0)) |&gt; \n  inner_join(lex) \n\nJoining with `by = join_by(token)`\n\nliza_NEG_chunk &lt;- liza_NEG |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_NEG_chunk \n\n\n  \n\n\n\nОсталось заново построить график. Для сравнения оставим рядом старую версию.\n\nlibrary(gridExtra)\n\np2 &lt;- liza_NEG_chunk |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (с учетом отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nИз-за изменения числа токенов отрывки сдвинулись, но незначительно. Бывший отрывок 15, как мы и ожидали, перешел в число положительно окрашенных (несмотря на ошибочную оценку слова “левый”).\n\n\nБыло:\n\n\n# A tibble: 8 × 3\n  token       chunk score\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 принудить      15  -2.5\n2 радость        15   5  \n3 тщетно         15  -2.5\n4 ясный          15   3.3\n5 левый          15  -3.3\n6 подозревать    15  -2.5\n7 никакой        15  -1.7\n8 худой          15  -3.3\n\n\n\nСтало:\n\n\n# A tibble: 7 × 3\n  token    chunk score\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 радость     15   5  \n2 тщетно      15  -2.5\n3 ясный       15   3.3\n4 левый       15  -3.3\n5 хороший     15   3.3\n6 добрый      15   5  \n7 ласковый    15   5  \n\n\n\n\nПомимо этого, повысилось абсолютное значение негативной тональности в последних отрывках, хотя на это повлияли не столько отрицания, сколько изменение числа слов и перераспределение их по отрывкам.\n\n\n12.3.3 Тепловая карта\n\nlibrary(ggpage)\n\npage_data &lt;- liza_tbl |&gt; \n  select(lemma) |&gt; \n  rename(text = lemma)  # required by ggpage_build()\n\npage_data |&gt; \n  ggpage_build(lpp = 22, character_height = 3) |&gt; \n  rename(token = word) |&gt; # required by join\n  left_join(lex) |&gt; \n  rename(text = token) |&gt; \n  mutate(neg = case_when(score &lt; 0 ~ TRUE,\n                         .default = FALSE)) |&gt; \n  ggpage_plot(aes(fill = neg), page.number = \"top-left\") #+\n  labs(title = \"Негативная лексика в «Бедной Лизе»\", x = NULL, y = NULL) +\n  scale_fill_manual(values = c(pal[5], pal[1]),\n                    labels = c(\"другая\", \"негативная\"),\n                    name = NULL) +\n  theme(axis.title = element_blank(),\n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_blank(),\n        text = element_text(family = \"vibes\", size = 12, color = \"grey30\"),\n        )\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nДля языков, которые используют латиницу, в R есть пакет под названием syuzhet, разработанный Мэтью Джокерсом. Название пакета, как говорит его разработчик, подсмотрено у русских формалистов Виктора Шкловского и Владимира Проппа. Возможности и ограничения этого пакета обсуждались в специальной литературе.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#трансформеры",
    "href": "sentiment.html#трансформеры",
    "title": "12  Эмоциональная тональность",
    "section": "12.4 Трансформеры",
    "text": "12.4 Трансформеры\n\n12.4.1 Настройка окружения и конвейера\nДля работы с трансформерами понадобится вирутальное окружение Python.\n\n#install.packages(\"reticulate\")\nlibrary(reticulate)\nuse_python(\"/usr/bin/python3\")\npy_config()\n\nПроверьте наличие модулей.\n\npy_module_available(\"transformers\")\npy_module_available(\"torch\")\n\nЕсли надо, установите.\n\npy_install(\"transformers\")\npy_install(\"torch\") \n\nИмпортируем модули.\n\ntransformers &lt;- import(\"transformers\")\ntorch &lt;- import(\"torch\")\n\nФункция pipeline() — одна из ключевых в библиотеке transformers. Она позволяет создать удобный “конвейер” для решения конкретной задачи обработки естественного языка (например, анализа тональности, извлечения именованных сущностей и др.).\nПод капотом она автоматически загружает необходимую модель, токенизатор и обеспечивает последовательный анализ текста: предварительную обработку (preprocessing), применение модели, а также постобработку (postprocessing) результатов. Всё это собирается в единый простой интерфейс, что позволяет использовать сложные технологии машинного обучения одним вызовом.\n\nsentiment_pipeline &lt;- transformers$pipeline(\n  \"sentiment-analysis\", \n  model=\"seara/rubert-tiny2-russian-sentiment\"\n)\n\n\ntext_data &lt;- c(\n  \"Какая ужасная погода, гремит гром и сверкают жуткие молнии.\",\n  \"Мы очень довольны этой великолепной покупкой.\",\n  \"Лиза отдала цветы и взяла деньги.\"\n)\n\nresults &lt;- sentiment_pipeline(text_data)\nresults\n\n\n\n12.4.2 Анализ тональности\n\nliza_tbl &lt;- tibble(text = read_lines(\"../files/karamzin_liza.txt\"))\nliza_tbl\n\n\nresults &lt;- sentiment_pipeline(liza_tbl$text)\n\n\nresults_tbl &lt;- tibble(\n  label = map_chr(results, function(x) x$label),\n  score = map_dbl(results, function(x) x$score)\n)\n\n\nresults_tbl |&gt; \n  mutate(index = row_number()) |&gt; \n  filter(label != \"neutral\") |&gt; \n  mutate(score = case_when(label == \"negative\" ~ score * -1,\n                           .default = score)) |&gt; \n  ggplot(aes(index, score, fill = label)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 46, 2)) + \n  labs(title = \"Эмоциональная тональность (BERT)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\n\n\n\n\n\nChen, Y., и S. Skiena. 2014. «Building Sentiment Lexicons for All Major Languages». Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, 383–89.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#бонус-трансформеры",
    "href": "sentiment.html#бонус-трансформеры",
    "title": "12  Эмоциональная тональность",
    "section": "12.4 Бонус: трансформеры",
    "text": "12.4 Бонус: трансформеры\n\n12.4.1 Настройка окружения и конвейера\nДля работы с трансформерами понадобится вирутальное окружение Python.\n\n#install.packages(\"reticulate\")\nlibrary(reticulate)\nuse_python(\"/usr/bin/python3\")\npy_config()\n\nПроверьте наличие модулей.\n\npy_module_available(\"transformers\")\npy_module_available(\"torch\")\n\nЕсли надо, установите.\n\npy_install(\"transformers\")\npy_install(\"torch\") \n\nИмпортируем модули.\n\ntransformers &lt;- import(\"transformers\")\n\nФункция pipeline() — одна из ключевых в библиотеке transformers. Она позволяет создать удобный “конвейер” для решения конкретной задачи обработки естественного языка (например, анализа тональности, извлечения именованных сущностей и др.).\nПод капотом она автоматически загружает необходимую модель, токенизатор и обеспечивает последовательный анализ текста: предварительную обработку (preprocessing), применение модели, а также постобработку (postprocessing) результатов. Всё это собирается в единый простой интерфейс, что позволяет использовать сложные технологии машинного обучения одним вызовом.\n\nsentiment_pipeline &lt;- transformers$pipeline(\n  \"sentiment-analysis\", \n  model=\"seara/rubert-tiny2-russian-sentiment\"\n)\n\n\ntext_data &lt;- c(\n  \"Какая ужасная погода, гремит гром и сверкают жуткие молнии.\",\n  \"Мы очень довольны этой великолепной покупкой.\",\n  \"Лиза отдала цветы и взяла деньги.\"\n)\n\nresults &lt;- sentiment_pipeline(text_data)\nresults\n\n\n\n12.4.2 Классификация\n\nliza_tbl &lt;- tibble(text = read_lines(\"../files/karamzin_liza.txt\"))\nliza_tbl\n\n\nresults &lt;- sentiment_pipeline(liza_tbl$text)\n\n\nresults_tbl &lt;- tibble(\n  label = map_chr(results, function(x) x$label),\n  score = map_dbl(results, function(x) x$score)\n)\n\n\nresults_tbl |&gt; \n  mutate(index = row_number()) |&gt; \n  filter(label != \"neutral\") |&gt; \n  mutate(score = case_when(label == \"negative\" ~ score * -1,\n                           .default = score)) |&gt; \n  ggplot(aes(index, score, fill = label)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 46, 2)) + \n  labs(title = \"Эмоциональная тональность (BERT)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#видео",
    "href": "sentiment.html#видео",
    "title": "12  Эмоциональная тональность",
    "section": "12.5 Видео",
    "text": "12.5 Видео\n\nВидео 2024 г.\nВидео 2025 г.\n\n\n\n\n\nChen, Y., и S. Skiena. 2014. «Building Sentiment Lexicons for All Major Languages». Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, 383–89.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  }
]