[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Компьютерный анализ текста",
    "section": "",
    "text": "Введение",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#об-этом-курсе",
    "href": "index.html#об-этом-курсе",
    "title": "Компьютерный анализ текста",
    "section": "Об этом курсе",
    "text": "Об этом курсе\nЭтот сайт содержит материалы к курсу “Компьютерный анализ текста в R” для магистерской программы НИУ ВШЭ “Цифровые методы в гуманитарных науках”. Предыдущую версию курса можно найти здесь.\nИ тексты, и инструменты для работы с ними подобраны таким образом, чтобы помочь студентам гуманитарных специальностей (филологам, философам, историкам и др.) как можно быстрее, но с полным пониманием дела перейти к применению количественных методов в собственной работе.\nЧтобы лучше понимать, какие из этих методов более всего востребованы в научной работе, преподаватели магистратуры “Цифровые методы в гуманитарных науках” – Б.В. Орехов, А.А. Осмоловская и О.В. Алиева – организовали в 2024 г. серию встреч с ведущими представителями отрасли. Видео этих встреч и литературу к семинарам можно найти на сайте http://criticaldh.ru/.\nТам мы собрали именно теоретические обсуждения и литературу к ним, а в этом курсе предлагаем приступить к практике DH (на языке R). Оба этих аспекта, в нашем представлении и в программе магистратуры тесно связаны: одного программирования не хватит, чтобы стать “цифровым гуманистом”, а теории недостаточно, чтобы судить об успешности тех или иных цифровых проектов. Поэтому этот курс старается стоять на двух ногах и соединять кодинг с теоретической рефлексией. Это почти невыполнимая задача но когда нам это мешало.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#ресурсы",
    "href": "index.html#ресурсы",
    "title": "Компьютерный анализ текста",
    "section": "Ресурсы",
    "text": "Ресурсы\nИ в теоретическом, и в практическом плане курс опирается на огромную работу, уже проделанную преподавателями магистратуры ЦМГН. Важнейшие наши достижения собрал Б.В. Орехов: https://github.com/nevmenandr/awesome-dh-hse. Здесь вы найдете ссылки на видео, научно-популярные и научные публикации и датасеты.\nЕсли вдруг вам не хватит практических заданий по R, то в качестве дополнения к оффлайн-курсу можно рекомендовать онлайн-курс Георгия Мороза “Введение в анализ данных на R для гуманитарных и социальных наук”. К этому курсу прилагается онлайн-ноутбук (https://agricolamz.github.io/daR4hs/) с комментариями и всем кодом, и он полностью открыт. Надо иметь в виду, однако, что онлайн-курс рассчитан всего на 9 недель, в то время как наш – на два семестра, так что его можно использовать лишь как вспомогательный ресурс, но не замену.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#программа",
    "href": "index.html#программа",
    "title": "Компьютерный анализ текста",
    "section": "Программа",
    "text": "Программа\nКурс 2024/2025 г. включает в себя три основных блока и 24 темы. Программа носит предварительный характер и может быть чуть изменена.\nМодуль 1. Основы работы в R\n\nНачало работы.\nТаблицы. Опрятные данные.\nВизуализации.\nЦиклы, условия, функции.\nИмпорт: JSON & XML.\nПубликационная система Quarto.\nРегулярные выражения.\nКонсолидация.\n\nМодуль 2. Текст-майнинг\n\nВеб-скрапинг.\nТокенизация, лемматизация, POS-тэггинг и синтаксический анализ.\nРаспределения слов и анализ частотностей.\nЭмоциональная тональность (метод словарей).\nЛатентно-семантический анализ.\nВекторные представления слов на основе PMI. Word2Vec.\nТематическое моделирование c LDA.\nКонсолидация.\n\nМодуль 3. Деревья, сети, карты\n\nКластеризация и метод главных компонент.\nСтилометрический анализ с пакетом stylo.\nКонсенсусные деревья и сети.\nСетевые данные в igraph.\nГрафический дизайн сетей в ggraph.\nАнализ сетей и обнаружение сообществ.\nПространственные данные в R.\nКонсолидация.\n\nМодуль 4. Машинное обучение\n\nРегрессионный анализ.\nРегрессионные модели с tidymodels.\nБинарная классификация.\nМногоклассовая классификация.\nГлубокое обучение.\nLLM в анализе текста.\nВоркфлоу с использованием LLM.\nКонсолидация.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#оценивание",
    "href": "index.html#оценивание",
    "title": "Компьютерный анализ текста",
    "section": "Оценивание",
    "text": "Оценивание\nДомашние задания выполняются в GitHub Classroom. Еженедельно выполняются небольшие задания, которые оцениваются по бинарной шкале (1/0), раз в месяц – консолидирующие задания на весь пройденный материал (оценка 0-10). Все необходимые ссылки вы найдете в чате курса в Telegram.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#благодарности",
    "href": "index.html#благодарности",
    "title": "Компьютерный анализ текста",
    "section": "Благодарности",
    "text": "Благодарности\nЗа помощь в разработке курса и подготовке датасетов к нему автор благодарит Георгия Мороза, Бориса Орехова и Софью Порфирьеву. Даниилу Скоринкину я признательная за помощь в работе над главами, посвященными сетевому анализу. Идеей количественного сравнения британских эмпириков в десятой главе я обязана своей коллеге по Школе философии и культурологии НИУ ВШЭ Дарье Дроздовой. Вдохновением для некоторых технических и визуальных решений послужили телеграм-блоги “Наука и данные” (@naukaidannye) и “People Analytics” (@People_Analytics). За отдельные дельные замечания, которые помогли улучшить курс, спасибо Софии Федотовой, Дмитрию Бергу и Дарье Галкиной.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#обратная-связь",
    "href": "index.html#обратная-связь",
    "title": "Компьютерный анализ текста",
    "section": "Обратная связь",
    "text": "Обратная связь\nЕсли вы заметили ошибку или опечатку, можно сообщить по почте oalieva@hse.ru или оставить issue в репозитории курса на GitHub.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "1  Начало работы",
    "section": "",
    "text": "1.1 Установка R и RStudio\nМы будем использовать R, так что для занятий понадобятся:\nМы будем использовать следующую версию R:\nR version 4.3.3 (2024-02-29)\nНекоторые люди не любят устанавливать лишние программы себе на компьютер, несколько вариантов есть и для них:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#установка-r-и-rstudio",
    "href": "start.html#установка-r-и-rstudio",
    "title": "1  Начало работы",
    "section": "",
    "text": "R\n\nна Windows\nна Mac\nна Linux.\n\nRStudio — IDE для R (можно скачать здесь)\n\n\n\n\n\nRStudio cloud — полная функциональность RStudio с некоторыми ограничениями;\nwebR REPL — ограниченная версия компилятора R, которая работает в вашем браузере и не требует никаких установок на компьютер\nJupyter ноутбуки;\nGoogle Colab (нужно в настройках переключить ядро);\nVS Code — другое IDE, которое также позволяет работать с R;\nв принципе, в IDE нет нужды, можно работать из терминала, после установки, нужно всего лишь набрать R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#знакомство-с-rstudio",
    "href": "start.html#знакомство-с-rstudio",
    "title": "1  Начало работы",
    "section": "1.2 Знакомство с RStudio",
    "text": "1.2 Знакомство с RStudio\nRStudio — основная среда разработки (IDE) для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:\n\n\n\nRStudio при первом открытии\n\n\nПосле нажатия на двойное окошко чуть левее надписи Environment откроется окно скрипта.\n\n\n\nПодокна RStudio\n\n\nВсе следующие команды можно:\n\nвводить в окне консоли, и тогда для исполнения следует нажимать клавишу Enter.\nвводить в окне скрипта, и тогда для исполнения следует нажимать клавиши Ctrl/Cmd + Enter или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять Ctrl/Cmd + S.\n\nДля начала попробуйте получить информацию о сессии, введя в консоли такую команду:\n\nsessionInfo()\n\nsessionInfo() – это функция. О функциях можно думать как о глаголах (“сделай то-то!”). За названием функции всегда следуют круглые скобки, внутри которых могут находиться аргументы функции. Аргументы – это что-то вроде дополнений и обстоятельств. Аргументы могут быть обязательные и необязательные. Чтобы узнать, каких аргументов требует функция, надо вызывать help: ?mean(). В правой нижней панели появится техническая документация. Но также можно воспользоваться функцией args(). Попробуйте набрать в консоли args(round).\n\n\n\n\n\n\nВопрос\n\n\n\nСколько аргументов функции round() имеют значения по умолчанию?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#пакеты",
    "href": "start.html#пакеты",
    "title": "1  Начало работы",
    "section": "1.3 Пакеты",
    "text": "1.3 Пакеты\nПосле установки R вы получите доступ к уже готовым методам статистического анализа и инструментам для визуализации. Если в базовой инсталляции R нет нужного решения – надо поискать в библиотеке пакетов. Пакет – это набор функций и иногда датасетов, созданный пользователями. На 1 июля 2023 г. в репозитории CRAN доступно 19789 пакетов. И это далеко не все: многие пакеты доступны только на GitHub.\n\n\n\n\n\n\nНа заметку\n\n\n\nНекоторые функции, которые вы найдете в пакетах, частично дублируют друг друга – это нормально, как и в естественном языке, “сказать” что-то можно разными способами.\n\n\nПо технической документации и так называемым “виньеткам” можно понять, какой пакет вам нужен. Например, вот так выглядит виньетка пакета RPerseus, при помощи которого можно получить доступ к корпусу греческой и латинской литературы.\nБывают еще “пакеты пакетов”, то есть очень большие семейства функций, своего рода “диалекты” R. Таково семейство tidyverse, объединяемое идеологией “опрятных” данных. Про него мы еще будем говорить.\nПакеты для работы устанавливаются один раз, однако подключать их надо во время каждой сессии. Чтобы установить новый пакет, можно воспользоваться меню Tools &gt; Install Packages. Также можно устанавливать пакеты из консоли. Установим пакет с интерактивными уроками программирования на языке R:\n\ninstall.packages(\"swirl\")\n\nДля подключения используем функцию library(), которой передаем в качестве аргумента название пакета без кавычек:\n\nlibrary(swirl)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#рабочая-директория",
    "href": "start.html#рабочая-директория",
    "title": "1  Начало работы",
    "section": "1.4 Рабочая директория",
    "text": "1.4 Рабочая директория\nПеред началом работы проверьте свою рабочую директорию при помощи getwd(). Для смены можно использовать как абсолютный, так и относительный путь:\n\nsetwd(\"/Users/name/folder\")\n\n# искать в текущей директории\nsetwd(\"./folder\")\n\n# перейти на уровень вверх\nsetwd(\"../\")\n\nТакже для выбора рабочей директории можно использовать меню R Session &gt; Set Working Directory. А теперь – первое задание.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс программирования на R: install_course(\"R Programming\"). После этого привяжите пакет командой library(swirl) и наберите: swirl(). Укажите ваше имя. Пройдите урок 2 Workspace and Files.\n\n\n После выполнения ответьте на несколько вопросов на закрепление материала.\n\n\n\n\n\n\nВопрос\n\n\n\nКакие действия в рабочей директории можно совершать из консоли?\n\n\n\n\nсоздать директорию\n\n\nудалить директорию\n\n\nсоздать файл\n\n\nпереименовать файл\n\n\nкопировать файл\n\n\nудалить файл\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nЧтобы создать вложенную директорию при помощи функции dir.create(), аргументу recursive следует задать значение…\n\n\n\n\nTRUE\nFALSE\n\n\n\n\n\nЕсли все получилось, двигаемся дальше.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#r-как-калькулятор",
    "href": "start.html#r-как-калькулятор",
    "title": "1  Начало работы",
    "section": "1.5 R как калькулятор",
    "text": "1.5 R как калькулятор\nМожно использовать R как калькулятор. Для этого вводим данные рядом с символом приглашения &gt;, который называется prompt.\n\nsqrt(4) # квадратный корень\n\n[1] 2\n\n2^3 # степень\n\n[1] 8\n\nlog10(100) #логарифм\n\n[1] 2\n\n\nЕсли в начале консольной строки стоит +, значит предыдущий код не завершен. Например, вы забыли закрыть скобку функции. Ее можно дописать на следующей строке. Попробуйте набрать sqrt(2 в консоли.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#операторы-присваивания",
    "href": "start.html#операторы-присваивания",
    "title": "1  Начало работы",
    "section": "1.6 Операторы присваивания",
    "text": "1.6 Операторы присваивания\nЧтобы в окружении появился новый объект, надо присвоить результат вычислений какой-нибудь переменной при помощи оператора присваивания &lt;- (Alt + - (Windows) или Option + - (Mac)). Знак = также работает как оператор присваивания, но не во всех контекстах, поэтому им лучше не пользоваться.\n\nx &lt;- 2 + 2 # создаем переменную\ny &lt;- 0.1 # создаем еще одну переменную\nx &lt;- y # переназначаем  \nx + y\n\n[1] 0.2\n\n\nСочетание клавиш для оператора присваивания: Option/Alt + -. Имя переменной, как и имя функции, может содержать прописные и строчные буквы, точку и знак подчеркивания.\nТеперь небольшое упражнение.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(). Укажите ваше имя. Пройдите урок 1 Basic Building Blocks.\n\n\nЕсли все получилось, можно двигаться дальше! Но сначала зафиксируем несколько новых функций из этих первого урока.\n\n\n\n\n\n\nВопрос\n\n\n\nЧто вычисляет функция abs()?\n\n\n\n\nсреднее\n\n\nмодуль\n\n\nквадратный корень\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько значений вернет функция, если разделить c(2, 4, 6) на 2?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nБуква “c” в названии функции c() означает…\n\n\n\n\ncover\n\n\ncollapse\n\n\nconcatenate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#пайпы-конвееры",
    "href": "start.html#пайпы-конвееры",
    "title": "1  Начало работы",
    "section": "1.7 Пайпы (конвееры)",
    "text": "1.7 Пайпы (конвееры)\nВ нашем коде мы часто будем использовать знаки конвеера (или пайпы): |&gt; (в вашей версии он может выглядить иначе: %&gt;%; переключить оператор можно в Global Options). Они призваны показывать последовательность действий. Сочетание клавиш: Ctrl/Cmd + Shift + M.\n\nmean(sqrt(abs(sin(1:100)))) \n\n[1] 0.7654264\n\n1:100 |&gt; \n  sin() |&gt; \n  abs() |&gt; \n  sqrt() |&gt; \n  mean()\n\n[1] 0.7654264",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#векторы",
    "href": "start.html#векторы",
    "title": "1  Начало работы",
    "section": "1.8 Векторы",
    "text": "1.8 Векторы\nВектор – это объект, предназначенный для хранения данных. К таким же объектам относятся также матрицы, списки, таблицы данных и др. Заметим, что в языке R нет скаляров (отдельных чисел). Числа считаются векторами из одного элемента.\n\nx &lt;- 2\nclass(x) # числовой вектор\n\n[1] \"numeric\"\n\nlength(x) # длина вектора\n\n[1] 1\n\n\nКак вы уже поняли, функция c() позволяет собрать несколько элементов в единый вектор:\n\nx &lt;- c(3, 5, 7)\nx_mean &lt;- mean(x) \nx_mean\n\n[1] 5\n\n\n Над векторами можно совершать арифметические операции, но будьте внимательны, применяя операции к векторам разной длины: в этом случае более короткий вектор будет переработан, то есть повторен до тех пор, пока его длина не сравняется с длиной вектора большей длины.\n\nx &lt;- 2\ny &lt;- c(10, 20, 30)\ny / x \n\n[1]  5 10 15\n\nx + y \n\n[1] 12 22 32\n\n\nВекторы можно индексировать, то есть забирать из них какие-то элементы:\n\nx &lt;- seq(1, 5, 0.5)\nx[4:5] # индексы начинаются с 1 (в отличие от Python)\n\n[1] 2.5 3.0\n\n\nВектор может хранить данные разных типов:\n\nцелое число (integer);\nчисло с плавающей точкой (numeric, также называются double, то есть число двойной точности);\nстроку (character);\nлогическую переменную (logical);\nкатегориальную переменную, или фактор (factor).\n\n\n# проверить тип данных \nx &lt;- sqrt(2)\nclass(x)\n\n[1] \"numeric\"\n\nis.integer(x)\n\n[1] FALSE\n\nis.numeric(x)\n\n[1] TRUE\n\n\nСоздавать векторы можно не только при помощи c(). Вот еще два способа.\n\nseq(1, 5, 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nrep(\"foo\", 5)\n\n[1] \"foo\" \"foo\" \"foo\" \"foo\" \"foo\"\n\n\nНаучиться генерировать векторы поможет небольшое упражнение.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 3 Sequences of Numbers.\n\n\nПроверьте свои знания, прежде чем двигаться дальше.\n\n\n\n\n\n\nВопрос\n\n\n\nКакие числа вернет команда pi:10?\n\n\n\n\nнатуральные\n\n\nцелые\n\n\nрациональные\n\n\nвещественные\n\n\nкомплексные\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие функции могут использоваться для создания символьных векторов?\n\n\n\n\nseq()\n\n\nrep()\n\n\nc()\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько значений вернет команда rep(c(0, 1, 2), times = 10)? Посчитайте в уме, не выполняя код.\n\n\n\n\n\n\n\n\n Факторы внешне похожи на строки, но в отличие от них хранят информацию об уровнях категориальных переменных. Уровень может обозначаться как числом (например, 1 и 0), так и строкой.\n\nt &lt;- factor(c(\"A\", \"B\", \"C\"), levels = c(\"A\", \"B\", \"C\"))\nt\n\n[1] A B C\nLevels: A B C\n\n\nВажно: вектор может хранить данные только одного типа. При попытке объединить в единый вектор данные разных типов они будут принудительно приведены к одному типу:\n\nx &lt;- c(TRUE, 1, 3, FALSE)\nx # логические значения приведены к числовым\n\n[1] 1 1 3 0\n\ny &lt;- c(1, \"a\", 2, \"лукоморье\") \ny # числа превратились в строки\n\n[1] \"1\"         \"a\"         \"2\"         \"лукоморье\"\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 4 Vectors. Это позволит больше узнать про логические и символьные векторы.\n\n\nНесколько вопросов для самопроверки.\n\n\n\n\n\n\nВопрос\n\n\n\nКакие значение вернет команда (3 &gt; 5) & (4 == 4)?\n\n\n\n\nTRUE\nFALSE\nNA\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие значения вернет команда (TRUE == TRUE) | (TRUE == FALSE)?\n\n\n\n\nTRUE\nFALSE\nNA\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКоманда paste(LETTERS, 1:4, sep = \"-\") вернет…\n\n\n\n\nчисловой вектор длиной 26\n\n\nсимвольный вектор длиной 26\n\n\nчисловой вектор длиной 4\n\n\nсимвольный вектор длиной 4\n\n\nошибку\n\n\n\n\n\n Логические векторы можно получить в результате применения логических операторов (== “равно”, != “не равно”, &lt;= “меньше или равно”) к данным других типов:\n\nx &lt;- c(1:10) # числа от 1 до 10\ny &lt;- x &gt; 5\ny # значения TRUE соответствуют единице, поэтому их можно складывать\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum(y)\n\n[1] 5\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗдесь можно запустить swirl() и пройти урок 8 Logic. Это не обязательно, но очень полезно, если хотите разобраться в операторах!\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nПопробуйте посчитать в уме: какое из выражений ниже вернет значение TRUE?\n\n\n\n\n7 == 9\n\n\n!(57 != 8)\n\n\n9 &gt;= 10\n\n\n-6 &gt; -7\n\n\n\n\n\nФункции all() и any() также возвращают логические значения:\n\nx &lt;- 10:20 \nany(x == 15)\n\n[1] TRUE\n\nall(x &gt; 9)\n\n[1] TRUE\n\n\nОтсутствие данных любого типа в R передается двумя способами. NULL означает, что значение не существует. Например, если мы создадим пустой вектор, то при попытке распечатать его получим NULL. А вот длина пустого вектора равна нулю!\n\ny &lt;- c() \ny \n\nNULL\n\nlength(y) \n\n[1] 0\n\n\nNA (not available) указывает на то, что значение существует, но оно неизвестно. Любые операции с NA приводят к появлению новых NA! Сравните:\n\nx &lt;- c(1, NA, 2)\nmean(x)\n\n[1] NA\n\ny &lt;- c(1, NULL, 2)\nmean(y)\n\n[1] 1.5\n\n\nКак проверить, есть ли в данных NA или NULL? Знак == здесь не подойдет.\n\nx &lt;- NA\nx == NA\n\n[1] NA\n\ny &lt;- NULL\ny == NULL\n\nlogical(0)\n\n\nДля этого есть специальные функции.\n\nis.na(x)\n\n[1] TRUE\n\nis.null(y)\n\n[1] TRUE\n\n\n\nWhen some people first get to R, they spend a lot of time trying to get rid of NAs. People probably did the same sort of thing when zero was invented. NA is a wonderful thing to have available to you. It is seldom pleasant when your data have missing values, but life if much better with NA than without.\nBurns (2012)\n\nКак избавиться от NA? В некоторых случаях достаточно аргумента функции.\n\nmean(c(1, NA, 2), na.rm=T) \n\n[1] 1.5\n\n\nЧуть более сложные способы вы узнаете из урока swirl ниже.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 5 Missing Values.\n\n\nГотово? Тогда попробуйте ответить на вопрос ниже, не выполняя вычислений в R.\n\n\n\n\n\n\nВопрос\n\n\n\nДан вектор x &lt;- c(44, NA, 5, NA). Сколько NA вернет команда x == NA?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 6 Subsetting Vectors.\n\n\nПроверьте, все ли вы поняли из этого урока.\n\n\n\n\n\n\nВопрос\n\n\n\nЕсли вектор x содержит числовые значения и некоторое количество NA, то что вернет команда x[is.na(x)]?\n\n\n\n\nвектор длиной 0\n\n\nвектор всех NA\n\n\nлогический вектор\n\n\nвектор без NA\n\n\nошибку\n\n\n\n\n\nЧто надо изменить в этом коде, чтобы получить все, кроме NA?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#списки",
    "href": "start.html#списки",
    "title": "1  Начало работы",
    "section": "1.9 Списки",
    "text": "1.9 Списки\nВ отличие от векторов списки могут хранить данные разных типов.\n\nlist = list(a = c(\"a\", \"b\", \"c\"), b = c(1, 2, 3), c = c(TRUE, FALSE, TRUE))\nlist\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] 1 2 3\n\n$c\n[1]  TRUE FALSE  TRUE\n\n\nМожно получить доступ как к элементам списка целиком, так и к их содержимому.\n\nlist$a # обращение к поименованным элементам \n\n[1] \"a\" \"b\" \"c\"\n\nlist[2] # одинарные квадратные скобки извлекают элемент списка целиком\n\n$b\n[1] 1 2 3\n\nclass(list[2])\n\n[1] \"list\"\n\nlist[[2]] #  элементы второго элемента \n\n[1] 1 2 3\n\nclass(list[[2]])\n\n[1] \"numeric\"\n\nlist$c[1]# первый элемент второго элемента\n\n[1] TRUE\n\n\nОбратите внимание, что list[2] и list[[2]] возвращают объекты разных классов. Нам это еще понадобится при работе с XML.\n\n\n\nИндексирование списка в R. Источник 🧂\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите библиотеку rcorpora и загрузите список с названиями хлеба и сладкой выпечки.\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nУзнайте длину my_list и введите ее в поле ниже.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДостаньте из my_list элемент pastries и узнайте его длину.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nА теперь извлеките пятый элемент из pastries и введите ниже его название.\n\n\n\n\n\n\n\n\nСо списками покончено. Теперь можно пойти выпить кофе с my_list$pastries[13]. Дальше будет сложнее, но интереснее.\n\n\n\n\nBurns, Patrick. 2012. The R inferno. Lulu.com.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "tabular.html",
    "href": "tabular.html",
    "title": "2  Таблицы",
    "section": "",
    "text": "2.1 Матрицы\nМатрица – это вектор, который имеет два дополнительных атрибута: количество строк и количество столбцов. Из этого следует, что матрица, как и вектор, может хранить данные одного типа. Проверим.\nM = matrix(c(1, 2, 3, 4), nrow = 2)\nM # все ок\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nM = matrix(c(1, 2, 3, \"a\"), nrow = 2)\nM # все превратилось в строку! \n\n     [,1] [,2]\n[1,] \"1\"  \"3\" \n[2,] \"2\"  \"a\"\nВ матрице есть ряды и столбцы. Их количество определяет размер (порядок) матрицы. Выше мы создали матрицу 2 x 2. Элементы матрицы, как и элементы вектора, можно извлекать по индексу. Сначала указывается номер ряда (строки), потом номер столбца.\nM = matrix(c(1, 2, 3, 4), nrow = 2)\nM\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\nM[1, ] # первая строка полностью\n\n[1] 1 3\n\nM[,2] # второй столбец полностью\n\n[1] 3 4\n\nM[1,1] # одно значение\n\n[1] 1\nОбратите внимание, как меняется размерность при индексировании.\nM = matrix(c(1, 2, 3, 4), nrow = 2)\ndim(M) # функция для извлечения измерений\n\n[1] 2 2\n\ndim(M[1, ]) \n\nNULL\nПопытка узнать измерения вектора возвращает NULL, потому что, с точки зрения R, векторы не являются матрицами из одного столбца или одной строки и потому не имеют измерений.\nВ этом уроке мы не будем много работать с матрицами, но полезно помнить, что они существуют: матрицы и алгебраические операции с ними задействованы при латентно-семантическом анализе и построении эмбеддингов (см. ниже).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#таблицы-датафреймы",
    "href": "tabular.html#таблицы-датафреймы",
    "title": "2  Таблицы",
    "section": "2.2 Таблицы (датафреймы)",
    "text": "2.2 Таблицы (датафреймы)\nЕсли матрица – это двумерный аналог вектора, то таблица (кадр данных, data frame) – это двумерный аналог списка. Как и список, датафрейм может хранить данные разного типа.\n\n# создание датафрейма\ndf &lt;- data.frame(names = c(\"John\", \"Mary\"), age = c(18, 25), sport = c(\"basketball\", \"tennis\"))\ndf\n\n\n  \n\n\n\nИзвлечение данных тоже напоминает работу со списком.\n\ndf$names # забирает весь столбец\n\n[1] \"John\" \"Mary\"\n\ndf[,\"names\"] # то же самое, другой способ\n\n[1] \"John\" \"Mary\"\n\ndf[1, ] # забирает ряд",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#импорт-табличных-данных",
    "href": "tabular.html#импорт-табличных-данных",
    "title": "2  Таблицы",
    "section": "2.3 Импорт табличных данных",
    "text": "2.3 Импорт табличных данных\nВ этом уроке мы будем работать с датасетом из Репозитория открытых данных по русской литературе и фольклору под названием “Программы по литературе для средней школы с 1919 по 1991 гг.” Этот датасет был использован при подготовке интерактивной карты российского школьного литературного канона (1852-2023). Карта была представлена в 2023 г. Лабораторией проектирования содержания образования ВШЭ. Подробнее о проекте можно посмотреть материал “Системного блока”.\nОсновная функция для скачивания файлов из Сети – download.file(), которой необходимо задать в качестве аргументов url, название сохраняемого файла, иногда также метод.\n\nurl &lt;- \"https://dataverse.pushdom.ru/api/access/datafile/4229\"\n\n# скачивание в папку files в родительской директории\ndownload.file(url, destfile = \"../files/curricula.tsv\") \n\nОсновные функции для чтения табличных данных в базовом R - это read.table() и read.csv(). Файл, который мы скачали, имеет расширение .tsv (tab separated values). Чтобы его прочитать, используем read.table(), указав тип разделителя:\n\ncurricula_df &lt;- read.table(\"../files/curricula.tsv\", sep = \"\\t\", header = TRUE)\n\ncurricula_df\n\n\n  \n\n\n\nФункция read.csv() отличается лишь тем, что автоматически выставляет значения аргументов sep = \",\", header = TRUE.\nФункция class() позволяет убедиться, что перед нами датафрейм.\n\nclass(curricula_df)\n\n[1] \"data.frame\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#работа-с-датафреймами",
    "href": "tabular.html#работа-с-датафреймами",
    "title": "2  Таблицы",
    "section": "2.4 Работа с датафреймами",
    "text": "2.4 Работа с датафреймами\n\n# узнать имена столбцов\ncolnames(curricula_df) \n\n[1] \"author\"     \"title\"      \"comment\"    \"curriculum\" \"id\"        \n[6] \"year\"       \"grade\"      \"priority\"  \n\n\n\n# извлечь ряд(ы) по значению\ncurricula_df[curricula_df$year == \"1919\", ]\n\n\n  \n\n\n\n\n# извлечь столбец \ncurricula_df$year |&gt; head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\ncurricula_df[ , \"year\"] |&gt; head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\ncurricula_df[ , 6] |&gt;  head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\n\n\n# узнать тип данных в столбцах\nstr(curricula_df) \n\n'data.frame':   10306 obs. of  8 variables:\n $ author    : chr  \"Андреев Л.Н.\" \"Андреев Л.Н.\" \"Андреев Л.Н.\" \"Бальмонт К.Д.\" ...\n $ title     : chr  \"Жили-были\" \"Иуда\" \"Рассказ о семи повешенных\" \"\" ...\n $ comment   : chr  \"\" \"\" \"\" \"\" ...\n $ curriculum: chr  \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" ...\n $ id        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : chr  \"1919\" \"1919\" \"1919\" \"1919\" ...\n $ grade     : int  9 9 9 9 9 8 8 8 8 8 ...\n $ priority  : chr  \"\" \"\" \"*\" \"*\" ...\n\n\n\n# преобразовать тип данных в столбцах\ncurricula_df$year &lt;- as.numeric(curricula_df$year)\n\n\n# вывести сводку\nsummary(curricula_df)\n\n    author             title             comment           curriculum       \n Length:10306       Length:10306       Length:10306       Length:10306      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n       id             year          grade          priority        \n Min.   : 1.00   Min.   :1919   Min.   : 5.000   Length:10306      \n 1st Qu.:13.00   1st Qu.:1946   1st Qu.: 8.000   Class :character  \n Median :31.00   Median :1966   Median :10.000   Mode  :character  \n Mean   :28.01   Mean   :1963   Mean   : 9.195                     \n 3rd Qu.:42.00   3rd Qu.:1981   3rd Qu.:10.000                     \n Max.   :50.00   Max.   :1991   Max.   :11.000                     \n                 NA's   :12                                        \n\n\nНебольшое упражнение на кодинг позволит закрепить навыки работы с матрицами и датафреймами.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 7 Matrices and Data Frames.\n\n\nВсе ли вы запомнили?\n\n\n\n\n\n\nВопрос\n\n\n\nДля чего нужна функция cbind()?\n\n\n\n\nдля добавления рядов\n\n\nдля добавления столбцов\n\n\nдля извлечения имен столбцов\n\n\nдля извлечения имен рядов\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nФункция colnames() позволяет как назначать новые имена таблице, так и извлекать существующие.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Испанские писатели”.\n\n\n\n# устанавливаем и загружаем нужный пакет\ninstall.packages(\"languageR\")\nlibrary(languageR)\n\n# загружаем датасет\nmeta &lt;- spanishMeta\n\n# допишите ваш код ниже\n# посчитайте средний год публикации романов Камило Хосе Селы\n\n\n# вычислите суммарное число слов в романах Эдуардо Мендосы\n\n\n# извлеките ряды с текстами, опубликованными до 1980 г.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#tibble",
    "href": "tabular.html#tibble",
    "title": "2  Таблицы",
    "section": "2.5 Tibble",
    "text": "2.5 Tibble\nСуществуют два основных “диалекта” R, один из которых опирается главным образом на функции и структуры данных базового R, а другой пользуется синтаксисом tidyverse. Tidyverse – это семейство пакетов (метапакет), разработанных Хадли Уикхемом и др., которое включает в себя в том числе пакеты dplyr, ggplot2 и многие другие.\n\n# загрузить все семейство\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nОсновная структура данных в tidyverse – это tibble, современный вариант датафрейма. Тиббл, как говорят его разработчики, это ленивые и недовольные датафреймы: они делают меньше и жалуются больше. Это позволяет решать проблемы на более ранних этапах, что, как правило, приводит к созданию более чистого и выразительного кода.\nОсновные отличия от обычного датафрейма:\n\nусовершенствованный метод print(), не нужно постоянно вызывать head();\nнет имен рядов;\nдопускает синтаксически “неправильные” имена столбцов;\nпри индексировании не превращается в вектор.\n\nПреобразуем наш датафрейм в тиббл для удобства работы с ним.\n\ncurricula_tbl &lt;- as_tibble(curricula_df)\n\nСравним поведение датафрейма и тиббла.\n\n# индексирование \ncurricula_df[, 1] |&gt; class()\n\n[1] \"character\"\n\ncurricula_tbl[,1]  |&gt; class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nПора тренироваться.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Getting and Cleaning Data\"). Загрузите библиотеку library(swirl), запустите swirl(), выберите этот курс и пройдите из него урок 1 Manipulating Data with dplyr. При попытке загрузить урок 1 вы можете получить сообщение об ошибке. В таком случае установите версию курса из github, как указано здесь, или загрузите файл вручную, как указано здесь.\n\n\nВремя вопросов! Обычный датафрейм или тиббл?\n\n\n\n\n\n\nВопрос\n\n\n\nПо умолчанию распечатывает только первые 10 рядов в консоль.\n\n\n\n\nдатафрейм\n\n\nтиббл\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nМолчаливо исправляет некорректные названия столбцов.\n\n\n\n\nдатафрейм\n\n\nтиббл\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nНе имеет названий рядов.\n\n\n\n\nдатафрейм\n\n\nтиббл\n\n\n\n\n\nКстати, обратили внимание, как работает оператор &lt;= с символьным вектором?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#dplyr",
    "href": "tabular.html#dplyr",
    "title": "2  Таблицы",
    "section": "2.6 Dplyr",
    "text": "2.6 Dplyr\nВ уроке swirl выше вы уже немного познакомились с “грамматикой манипуляции данных”, лежащей в основе dplyr. Здесь об этом будет сказано подробнее. Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными:\n\nmutate() добавляет новые переменные, которые являются функциями существующих переменных;\nselect() выбирает переменные (столбцы) на основе их имен;\nfilter() выбирает наблюдения (ряды) на основе их значений;\nsummarise() обобщает значения;\narrange() изменяет порядок следования строк.\n\nВсе эти глаголы естественным образом сочетаются с функцией group_by(), которая позволяет выполнять любые операции “по группам”, и с оператором pipe |&gt; из пакета magrittr.\nВ итоге получается более лаконичный и читаемый код. Узнаем, за какие года у нас есть программы по литературе.\n\ncurricula_tbl |&gt; \n  count(curriculum, year) \n\n\n  \n\n\n\nОтберем две программы для 8 класса и выясним, какие авторы в них представлены лучше всего.\n\ncurricula_tbl |&gt; \n  filter(year %in% c(1919, 1922), grade == 8) |&gt; \n  count(author, year) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nТеперь упражнения в swirl. Вам придется редактировать код, который предложит программа, так что сгруппируйтесь.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(), выберите курс Getting and Cleaning Data и пройдите из него урок 2 Grouping and Chaining with dplyr.\n\n\nПравда или ложь?\n\n\n\n\n\n\nВопрос\n\n\n\nФункция n_distinct() возвращает все уникальные значения.\n\n\n\n\nПравда\n\n\nЛожь",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#опрятные-данные",
    "href": "tabular.html#опрятные-данные",
    "title": "2  Таблицы",
    "section": "2.7 Опрятные данные",
    "text": "2.7 Опрятные данные\n\nTidy datasets are all alike, but every messy dataset is messy in its own way.\n— Hadley Wickham\n\nTidyverse – это не только особый синтаксис, но и отдельная идеология “опрятных данных”. “Сырые” данные, с которыми мы работаем, редко бывают опрятны, и перед анализом их следует “почистить” и преобразовать.\nОсновные принципы опрятных данных:\n\nотдельный столбец для каждой переменной;\nотдельный ряд для каждого наблюдения;\nу каждого значения отдельная ячейка;\nодин датасет – одна таблица.\n\n\n\n\nПринципы опрятных данных. Источник.\n\n\n\nПосмотрите на учебные тибблы из пакета tidyr и подумайте, какое из этих правил нарушено в каждом случае.\n\ndata(\"table2\")\ntable2\n\n\n  \n\n\ndata(\"table3\")\ntable3\n\n\n  \n\n\ndata(\"table4a\")\ntable4a\n\n\n  \n\n\ndata(\"table4b\")\ntable4b\n\n\n  \n\n\n\nВажные функции для преобразования данных из пакета tidyr:\n\nseparate() делит один столбец на новые;\nunite() объединяет столбцы;\npivot_longer() удлиняет таблицу;\npivot_wider() расширяет таблицу;\ndrop_na() и replace_na() указывают, что делать с NA и др.\n\nКроме того, в dplyr есть полезное семейство функций _join, позволяющих объединять данные в различных таблицах.\n\n\n\nИсточник.\n\n\nДальше мы потренируемся с ними работать, но сначала пройдем урок swirl. Это достаточно сложный урок (снова понадобится редактировать скрипт), но он нам дальше здорово поможет.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(), выберите курс Getting and Cleaning Data и пройдите из него урок 3 Tidying Data with tidyr.\n\n\nПравда или ложь?\n\n\n\n\n\n\nВопрос\n\n\n\nФункция separate() обязательно требует указать разделитель.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nПринципы опрятных данных требуют, чтобы одному наблюдению соответствовал один столбец.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nФункция contains() используется вместе с filter() для выбора рядов.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n Отличная работа! Прежде чем двигаться дальше, приведите в порядок table2, 3, 4a-4b, используя dplyr и tidyr.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Библиотека Gutenberg”\n\n\n\ndevtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\nlibrary(dplyr)\nlibrary(tidyr)\n\nworks &lt;- gutenberg_works()\n\n# Отберите ряды, в которых gutenberg_author_id равен 65;\n# после этого выберите два столбца: author, title\nmy_data &lt;- works |&gt; \n  # ваш код здесь\n  \n# Загрузите данные об авторах и выберите столбцы: author, deathdate\nauthors &lt;- gutenberg_authors |&gt; \n  # ваш код здесь\n\n# Соедините my_data с данными о смерти автора из authors, \n# так чтобы к my_data добавился новый столбец. \n# После этого используйте функцию separate, \n# чтобы разделить столбец с именем и фамилией на два новых: author, name. \n# Удалите столбец с именем автора, оставив только фамилию.\n# Добавьте новый столбец century, \n# используя функцию mutate и данные из столбца deathdate. \n# Используйте оператор pipe, не сохраняйте промежуточные результаты!\n\nmy_data |&gt; \n  # ваш код здесь",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#обобщение-данных",
    "href": "tabular.html#обобщение-данных",
    "title": "2  Таблицы",
    "section": "2.8 Обобщение данных",
    "text": "2.8 Обобщение данных\nТеперь вернемся к датасету curricula и попробуем частично воспроизвести результаты, полученные авторами проекта “Список чтения”, упомянутого выше.\nУ каких авторов больше всего произведений (во всех программах)?\n\ncurricula_tbl |&gt; \n  group_by(author, title) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nКакие произведения упоминаются в программах чаще всего?\n\ncurricula_tbl |&gt; \n  count(author, title) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nНа принятые в каких годах программы приходится больше всего произведений? (Объяснение здесь.)\n\ncurricula_tbl |&gt; \n  group_by(year) |&gt; \n  distinct(author, title) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nВ заключение попробуйте сформулировать новые вопросы и ответить на них при помощи этого датасета.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "plot.html",
    "href": "plot.html",
    "title": "3  Визуализации",
    "section": "",
    "text": "3.1 Графические системы\nВ R есть несколько графических систем: базовый R, lattice и ggplot2. В этом курсе мы будем работать лишь с ggplot2 как с наиболее современной. Если вам интересны первые две, то вы можете обратиться к версии курса 2023/2024 г. и к интерактивным урокам swirl.\nНастоящая графическая сила R – это пакет ggplot2. В его основе лежит идея “грамматики графических элементов” Лиланда Уилкинсона (Мастицкий 2017) (отсюда “gg” в названии). С одной стороны, вы можете постепенно достраивать график, добавляя элемент за элементом (как в базовом R); с другой – множество параметров подбираются автоматически, как в Lattice.\nО различных видах графиков можно почитать по ссылке. В этом уроке мы научимся строить диаграмму рассеяния (scatter plot) и столбиковую диаграмму (bar chart). Вот к чему мы стремимся.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#графические-системы",
    "href": "plot.html#графические-системы",
    "title": "3  Визуализации",
    "section": "",
    "text": "Задание\n\n\n\nЗапустите swirl(); курс R Programming у вас уже установлен. Из него сделайте урок 15 Base Graphics. Также установите курс swirl::install_course(\"Exploratory Data Analysis\"). Из него можно пройти любые уроки: это необязательно, но поможет разобраться в теме.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#датасет-метаданные-романов-xix-xx-вв.",
    "href": "plot.html#датасет-метаданные-романов-xix-xx-вв.",
    "title": "3  Визуализации",
    "section": "3.2 Датасет: метаданные романов XIX-XX вв.",
    "text": "3.2 Датасет: метаданные романов XIX-XX вв.\nЗнакомиться с ggplot2 мы будем на примере датасета из коллекции “NovelTM Datasets for English-Language Fiction, 1700-2009”, подготовленного Тедом Андервудом, Патриком Кимутисом и Джессикой Уайт. Они собрали метаданные о 210,266 томах художественной прозы в HathiTrust Digital Library и составили из них несколько датасетов.\nМы возьмем небольшой датасет, который содержит провернные вручную метаданные, а также сведения о категории художественной прозы для 2,730 произведений, созданных в период 1799-2009 г. (равные выборки для каждого года). Об особенностях сбора и подготовки данных можно прочитать по ссылке.\nМы попробуем проверить наблюдение, сделанное Франко Моретти в статье “Корпорация стиля: размышления о 7 тысячах заглавий (британские романы 1740-1850)” (2009 г., рус. перевод в книге “Дальнее чтение”, 2016 г.). Моретти заметил, что на протяжении XVIII-XIX вв. названия становятся короче, причем уменьшается не только среднее, но и стандартное отклонение (т.е. разброс значений). В публикации он предлагает несколько возможных объяснений для этого тренда. В датасете NovelTM есть не только романы (и не только британские), но тем более интересно будет сравнить результат.\nДля этого урока данные были немного трансформированы: в частности, мы добавили столбец n_words, в котором хранятся сведения о числе слов в названии. Файл в формате .Rdata надо забрать из репозитория курса и прочитать в окружение.\n\nlibrary(tidyverse)\n\n\nload(\"../data/noveltm.Rdata\")\nnoveltm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#диаграмма-рассеяния",
    "href": "plot.html#диаграмма-рассеяния",
    "title": "3  Визуализации",
    "section": "3.3 Диаграмма рассеяния",
    "text": "3.3 Диаграмма рассеяния\nФункция ggplot() имеет два основных аргумента: data и mapping. Аргумент mapping задает эстетические атрибуты геометрических объектов. Обычно используется в виде mapping = aes(x, y), где aes() означает aesthetics.\nПод “эстетикой” подразумеваются графические атрибуты, такие как размер, форма или цвет. Вы не увидите их на графике, пока не добавите какие-нибудь “геомы” – геометрические объекты (точки, линии, столбики и т.п.). Эти объекты могут слоями накладываться друг на друга (Wickham и Grolemund 2016).\nДиаграмма рассеяния, которая подходит для отражения связи между двумя переменными, делается при помощи geom_point(). Попробуем настройки по умолчанию.\n\nnoveltm |&gt; \n  ggplot(aes(inferreddate, n_words)) + \n  geom_point()\n\n\n\n\n\n\n\n\nУпс. Точек очень много, и они накладываются друг на друга, так как число слов – дискретная величина. Поступим так же, как Моретти, который отразил на графике среднее для каждого года. Для этого нам надо снова поколдовать над данными.\n\nnoveltm_summary &lt;- noveltm |&gt;\n  group_by(inferreddate) |&gt;\n  summarise(n = n(),\n            mean_w = mean(n_words, na.rm = TRUE)) |&gt; \n  filter(n &gt; 1)\n\nnoveltm_summary\n\n\n  \n\n\n\nСнова построим диаграмму рассеяния. Добавим линию тренда, изменим внешний вид точек и тему оформления, а также уберем подпись оси X.\n\nnoveltm_summary |&gt; \n  ggplot(aes(inferreddate, mean_w)) +\n  geom_point(color = \"steelblue\", alpha = 0.7, size = 2) +\n  geom_smooth(color = \"tomato\") + \n  theme_bw() +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nНисходящая тенденция, о которой писал Моретти, хорошо прослеживается. Но, возможно, она характерна не для всех стран?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#сравнение-двух-групп",
    "href": "plot.html#сравнение-двух-групп",
    "title": "3  Визуализации",
    "section": "3.5 Сравнение двух групп",
    "text": "3.5 Сравнение двух групп\nВ столбце nationality хранятся данные о происхождении писателя.\n\nnoveltm |&gt; \n  group_by(nationality) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nОтберем только английских и американских авторов и сравним тенденции в этих двух группах. Категориальную переменную (национальность) в нашем случае проще всего закодировать цветом. Также добавим заголовок и подзаголовок и поменяем тему.\n\nnoveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  # убираем 1799, для которого только одно наблюдение\n  filter(n &gt; 1) |&gt; \n  # код как выше, но убираем цвет для геомов\n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth() +\n  # новая тема\n  theme_bw() +\n  # заголовки\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  )\n\n\n\n\n\n\n\n\nДля разведывательного анализа данных вполне достаточно настроек по умолчанию, но для публикации вы, вероятно, захотите вручную поправить шрифтовое и цветовое оформление.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#цветовые-шкалы",
    "href": "plot.html#цветовые-шкалы",
    "title": "3  Визуализации",
    "section": "3.6 Цветовые шкалы",
    "text": "3.6 Цветовые шкалы\nGgplot2 дает возможность легко поменять цветовую палитру и шрифтовое оформление, а также добавить фон.\nФункции scale_color_brewer() и scale_fill_brewer() позволяют использовать специально подобранные палитры хорошо сочетаемых цветов.\nОбщее правило для выбора таково.\n\nЕсли дана качественная переменная с упорядоченными уровнями (например, “холодный”, “теплый”, “горячий”) или количественная переменная, и необходимо подчеркнуть разницу между высокими и низкими значениями, то для визуализации подойдет последовательная шкала.\nЕсли дана количественная переменная с осмысленным средним значением, например нулем, 50%, медианой, целевым показателем и т.п., то выбираем расходящуюся шкалу.\nЕсли дана качественная переменная, уровни которой невозможно упорядочить (названия городов, имена авторов и т.п.), ищем качественную шкалу.\n\n\n\n\nИсточник.\n\n\nВот основные (но не единственные!) цветовые шкалы в R. Также цвета можно задавать и вручную – по названию или коду.\n\n\n# тут все по-старому\nnoveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  filter(n &gt; 1) |&gt; \n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth() +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  ) +\n  # тут новое\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#шрифты",
    "href": "plot.html#шрифты",
    "title": "3  Визуализации",
    "section": "3.7 Шрифты",
    "text": "3.7 Шрифты\nПакет ggplot2 и расширения для него дают возможность использовать пользовательские шрифты.\n\n# тут новое\nlibrary(showtext)\nfont_add_google(\"Special Elite\", family = \"special\")\nshowtext_auto()\n\n# тут почти все по-старому...\nnoveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  filter(n &gt; 1) |&gt; \n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth() +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  ) +\n  scale_color_brewer(palette = \"Dark2\") + \n  # кроме этих строк, тут новое\n  theme(\n    axis.title = element_text(family = \"special\"),\n    title = element_text(family = \"special\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#изображения",
    "href": "plot.html#изображения",
    "title": "3  Визуализации",
    "section": "3.8 Изображения",
    "text": "3.8 Изображения\nИзображения можно добавлять и в качестве фона, и вместо отдельных геомов, например точек. Поправим цвета, чтобы они лучше сочетались с цветом изображения.\n\nlibrary(ggimage)\nurl &lt;- \"./images/book.jpg\"\n\n# почти все по-старому...\nfont_add_google(\"Special Elite\", family = \"special\")\nshowtext_auto()\n\n# ...но график сохраняем в окружение под именем g\ng &lt;- noveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  add_count(nationality, inferreddate) |&gt; \n  filter(n &gt; 1) |&gt; \n  ggplot(aes(inferreddate, n_words, color = nationality)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\") +\n  geom_smooth() +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  ) +\n  # подбираем новые цвета, в т.ч. для текста\n  scale_color_manual(\"country\", values = c(\"#A03B37\", \"#50684E\")) + \n  theme(\n    axis.title = element_text(family = \"special\", color = \"#8B807C\"),\n    title = element_text(family = \"special\", color = \"#52211E\"),\n    axis.text = element_text(color = \"#52211E\"),\n    axis.ticks = element_blank(),\n    # расширяем правое поле, чтобы все влезло\n    plot.margin = unit(c(0.4, 3, 0.4, 0.4), \"inches\"), # t, r, b, l\n    # рамочка\n    panel.border = element_rect(color = \"#8B807C\"),\n    # перемещаем легенду\n    legend.position = c(0.8, 0.8)\n  )\n\n# let the magic start!\nggbackground(g, url)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#столбиковая-диаграмма",
    "href": "plot.html#столбиковая-диаграмма",
    "title": "3  Визуализации",
    "section": "3.9 Столбиковая диаграмма",
    "text": "3.9 Столбиковая диаграмма\nДля визуализации распределений качественных переменных подходит стобиковая диаграмма, которая наглядно показывает число наблюдений в каждой группе. В датасете NovelTM представлены следующие категории литературы.\n\nnoveltm |&gt; \n  ggplot(aes(category, fill = category)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nНас будет интересовать категория longfiction, т.к. именно сюда попадает популярный в XIX в. жанр романа. Известно, что примерно до 1840 г. почти половина романистов были женщинами, но к началу XX в. их доля снизилась (Underwood 2019, 133). Отчасти это объясняется тем, что после середины XIX в. профессия писателя становится более престижной, а его социальный статус повышается, что приводит к “джентрификации” романа. Посмотрим, что на этот счет могут сказать данные NovelTM. Переменная gender хранит данные о гендере автора.\n\nnoveltm |&gt; \n  ggplot(aes(gender, fill = gender)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nОтберем лишь одну категорию и два гендера.\n\nnoveltm_new &lt;- noveltm |&gt; \n  select(inferreddate, gender, category) |&gt; \n  filter(gender != \"u\", category == \"longfiction\") |&gt; \n  select(-category)\n\nnoveltm_new\n\n\n  \n\n\n\nМожно предположить, что соотношение мужчин и женщин в разные десятилетия менялось. Чтобы это выяснить, нам надо преобразовать данные, указав для каждого года соответствующую декаду, и посчитать число мужчин и женщин в каждой декаде.\n\nnoveltm_new &lt;- noveltm_new |&gt; \n  mutate(decade = (inferreddate %/% 10) * 10) \n\nnoveltm_new\n\n\n  \n\n\n\nЭтого уже достаточно для визуализации, но она будет не очень наглядная.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nНайдем долю мужчин и женщин по декадам.\n\nnoveltm_new_prop &lt;- noveltm_new |&gt; \n  add_count(decade, name = \"total\") |&gt; \n  select(-inferreddate) |&gt; \n  add_count(gender, decade, name = \"counts\") |&gt; \n  distinct(gender, decade, counts, total) |&gt; \n  mutate(share = counts / total) \n\nnoveltm_new_prop |&gt; \n  # тот же график, но...\n  ggplot(aes(decade, share, fill = gender)) +\n  # тут просим ничего не считать, а брать что дают\n  geom_bar(stat = \"identity\") + \n  coord_flip() \n\n\n\n\n\n\n\n\nКод выше хорошо читается (и ничего плохого в нем нет), но то же самое можно сделать и более лаконично:\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  # вся магия здесь\n  geom_bar(position = \"fill\") +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#линейная-диаграмма",
    "href": "plot.html#линейная-диаграмма",
    "title": "3  Визуализации",
    "section": "3.9 Линейная диаграмма",
    "text": "3.9 Линейная диаграмма\nДанные о доли женщин-писателей можно представить и в виде линии: в нашем случае это не лишено смысла, поскольку ось x – это временная шкала.\n\nnoveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = gender)) +\n  geom_line(show.legend = FALSE)\n\n\n\n\n\n\n\n\nПо умолчанию ось y усекается, и создается впечатление, что доля женщин ок. 1900 падает чуть ли не до нуля. Поправим вручную границы оси.\n\nnoveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = gender)) +\n  geom_line(show.legend = FALSE) +\n  expand_limits(y = 0)\n\n\n\n\n\n\n\n\nГрафик, кажется, подтверждает, что доля женщин в литературе снижалась примерно до середины XX в. Однако при разделении данных на группы можно заметить другую тенденцию.\n\nnoveltm_nation &lt;- noveltm |&gt; \n  filter(category == \"longfiction\") |&gt; \n  select(inferreddate, gender, nationality) |&gt; \n  mutate(nationality = case_when(!nationality %in% c(\"uk\", \"us\") ~ \"other\",\n                                 .default = nationality)) |&gt; \n  filter(gender != \"u\") |&gt; \n  mutate(decade = (inferreddate %/% 10) * 10)\n\nnoveltm_nation\n\n\n  \n\n\ntotal_nation &lt;- noveltm_nation |&gt; \n  group_by(decade) |&gt; \n  summarise(total = n()) |&gt; \n  filter(total &gt; 1)\n\nsummary_nation &lt;- noveltm_nation |&gt; \n  group_by(decade, nationality, gender) |&gt; \n  summarise(counts = n()) |&gt; \n  filter(counts &gt; 1)\n\nsummary_nation\n\n\n  \n\n\nnoveltm_nation_share &lt;- summary_nation |&gt; \n  left_join(total) |&gt; \n  mutate(share = counts / total) |&gt; \n  select(-counts, -total)\n\nnoveltm_nation_share\n\n\n  \n\n\n\n\nnoveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line() \n\n\n\n\n\n\n\n\nДобавим название и немного поменяем оформление.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"rcartocolor::Pastel\")[1:3]\n\nnoveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_minimal() + \n  labs(\n    title = \"Female Writers' Share\",\n    subtitle = \"NovelTM Data 1800-2009 \\n \",\n    x = NULL,\n    y = NULL) +\n  theme(text=element_text(size=14, family=\"serif\")) + \n  scale_color_manual(values = cols)\n\n\n\n\n\n\n\n\nМожно добавить рамку и переместить легенду.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"rcartocolor::Pastel\")[c(1,3,5)]\n\n\ng &lt;- noveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_light() + \n  labs(\n    title = \"Female Writers' Share\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL,\n    y = NULL) +\n  theme(text=element_text(size=14, family=\"serif\"),\n        axis.text = element_text(color = \"#F0F0F0\"),\n        axis.title = element_text(color = \"#F0F0F0\"),\n        legend.position = c(0.5, 0.87), \n        legend.direction = \"horizontal\",\n        legend.title = element_blank(),\n        legend.text = element_text(color = \"#D2B48C\"),\n        legend.background = element_blank(),\n        plot.title = element_text(hjust=0.5, color = \"#F0F0F0\", face=\"bold\"),\n        plot.subtitle = element_text(hjust=0.5, color = \"#F0F0F0\"),\n        plot.background = element_rect(fill = \"#D2B48C\"),\n        panel.background = element_rect(fill = \"#F0F0F0\"),\n        panel.grid.major = element_line(color = \"#87CEEB\"),\n        panel.grid.minor = element_line(color = \"#87CEEB\"),\n        panel.grid.major.y = element_line(linewidth = 0.5)) + \n  scale_color_manual(values = cols)\n\ng",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#экспорт-графиков-из-среды-r",
    "href": "plot.html#экспорт-графиков-из-среды-r",
    "title": "3  Визуализации",
    "section": "3.12 Экспорт графиков из среды R",
    "text": "3.12 Экспорт графиков из среды R\nСпособы:\n\nреализованные в R драйверы стандартных графических устройств;\nфункция ggsave()\nменю программы RStudio.\n\n\n# код сохранит pdf в рабочую директорию \npdf(file = \"plot.pdf\")\n \ng \n\ndev.off()\n\nЕще один способ сохранить последний график из пакета ggplot2.\n\nggsave(\n  filename = \"plot.png\",\n  plot = last_plot(),\n  device = \"png\",\n  scale = 1,\n  width = NA,\n  height = 500,\n  units = \"px\",\n  dpi = 300\n)\n\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Старофрацузская литература”\n\n\n\n# загружаем нужные пакеты\nlibrary(languageR)\nlibrary(ggplot2)\n\n# загружаем датасет\nmeta &lt;- oldFrenchMeta\n\n# допишите ваш код ниже\n# постройте столбиковую диаграмму, \n# показывающую распределение произведений по темам; цветом закодируйте жанр; \n# уберите названия осей; \n# поверните координатную ось; \n# поменяйте тему оформления на черно-белую, \n# а шрифт -- на Palatino; \n# добавьте заголовок \"Plot by [Your Name]\"\n\n\n\n#  экспортируйте график в формате jpg \n# с раширением 300 dpi; \n# в названии файла должна быть \n# ваша фамилия и номер группы\n\n\n\n\n\nUnderwood, Ted. 2019. Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nМастицкий, Сергей. 2017. Визуализация данных с помощью ggplot2. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "iterate.html",
    "href": "iterate.html",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "4.1 Векторизованные вычисления\nХорошая новость: многие функции в R уже векторизованы, и если необходимо применить функцию к каждому элементу вектора, в большинстве случаев достаточно просто вызвать функцию. Это называется векторизация.\nНапример, у нас есть символьный вектор, и мы хотим узнать количество символов в каждом слове.\nhomer &lt;- c(\"в\", \"мысли\", \"ему\", \"то\", \"вложила\", \"богиня\", \"державная\", \"гера\")\nДля каждого компонента вектора необходимо выполнить одну итерацию цикла, в нашем случае – применить функцию nchar(). В некоторых языках программирования это делается как-то так:\nfor(i in homer) print(nchar(i))\n\n[1] 1\n[1] 5\n[1] 3\n[1] 2\n[1] 7\n[1] 6\n[1] 9\n[1] 4\nМы написали цикл for, который считает количество букв для каждого слова в векторе. Как видно, все сработало. Но в R это избыточно, потому что nchar() уже векторизована:\nnchar(homer)\n\n[1] 1 5 3 2 7 6 9 4\nЭто относится не только ко многим встроенным функциям R, но и к даже к операторам. x + 4 в действительности представляет собой +(x, 4):\nx &lt;- c(1.2, 2.51, 3.8)\n\n`+`(x, 4) \n\n[1] 5.20 6.51 7.80\nКлючевую роль здесь играет переработка данных, о которой мы говорили в первом уроке: короткий вектор повторяется до тех пор, пока его длина не сравняется с длиной более длинного вектора. Как-то так:\n\\[ \\left(\n    \\begin{array}{c}\n      1.2 \\\\\n      2.51 \\\\\n      3.8\n    \\end{array}\n  \\right) + \\left(\n    \\begin{array}{c}\n      4 \\\\\n      4 \\\\\n      4\n    \\end{array}\n  \\right) \\]\nЛишний цикл может замедлить вычисления. Проверим. Дан вектор x &lt;- c(3, 5, 7, 13). Необходимо возвести в квадрат каждое число, а из результата вычесть 100. Выполним двумя способами.\nlibrary(tictoc)\nx &lt;- c(2, 3, 5, 7, 11, 13)\n\n# способ первый\ntic()\nfor (i in x) print(i^2 - 100)\n\n[1] -96\n[1] -91\n[1] -75\n[1] -51\n[1] 21\n[1] 69\n\ntoc()\n\n0.002 sec elapsed\n\n# способ второй \ntic()\nx^2 - 100\n\n[1] -96 -91 -75 -51  21  69\n\ntoc()\n\n0.001 sec elapsed",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#циклы-и-их-аналоги",
    "href": "iterate.html#циклы-и-их-аналоги",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "На заметку\n\n\n\nВ циклах часто используется буква i. Но никакой особой магии в ней нет, имя переменной можно изменить.\n\n\n\n\n\n\n\n\n\n\n\nОдин из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми.\n— Нормат Мэтлофф\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПройдите урок 10 lapply and sapply и урок 11 vapply and tapply из курса R Programming в swirl.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nПосчитайте среднее для всех столбцов в mtcars.\nОпределите тип данных во всех столбцах nycflights13::flights.\nПосчитайте число уникальных значений в каждом столбце iris.\nСгенерируйте 10 случайных чисел из нормального распределения - это делает функция rnorm() - со средним -10, 0, 10.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПопробуйте избавиться от цикла 😜.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#условия",
    "href": "iterate.html#условия",
    "title": "4  Циклы, условия, функции",
    "section": "4.5 Условия",
    "text": "4.5 Условия\nИногда необходимо ограничить выполнение функции неким условием. Короткие условия можно писать в одну строку без фигурных скобок.\n\nword &lt;-  \"Эйяфьятлайокудль\"\n\nif(nchar(word) &gt; 10) print(\"много букв\")\n\n[1] \"много букв\"\n\n\nБолее сложные и множественные условия требуют фигурных скобок. Можно сравнить это с условным периодом: протасис (всегда либо TRUE, либо FALSE) в круглых скобках, аподосис в фигурных.\n\nif (nchar(word) &gt; 10) {\n  print(\"много букв\")\n} else if (nchar(word) &lt; 5) {\n  print(\"мало букв\")\n} else {\n  print(\"норм букв\")\n}\n\n[1] \"много букв\"\n\n\nТакже в R можно использовать специальную функцию:\n\nifelse(nchar(word) &gt; 10, \"много букв\", \"мало букв\")\n\n[1] \"много букв\"\n\n\nПрописывая условие, не забывайте, что применение булева оператора к вектору возвращает логический вектор:\n\nx &lt;- 1:10\nx &gt;= 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nТакое условие вернет ошибку.\n\nif (x &gt;= 5) print(\"все сломалось\")\n\nError in if (x &gt;= 5) print(\"все сломалось\"): the condition has length &gt; 1\n\n\nМожно скорректировать код так:\n\nif (any(x &gt;= 5)) print(\"все сработало\")\n\n[1] \"все сработало\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#функции",
    "href": "iterate.html#функции",
    "title": "4  Циклы, условия, функции",
    "section": "4.3 Функции",
    "text": "4.3 Функции\nФункция и код – не одно и то же. Чтобы стать функцией, кусок кода должен получить имя. Но зачем давать имя коду, который и так работает?\nВот три причины, которые приводит Хадли Уикхем:\n\nу функции есть выразительное имя, которое облегчает понимание кода;\nпри изменении требований необходимо обновлять код только в одном месте, а не во многих;\nменьше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом)\n\n\nWriting good functions is a lifetime journey.\n— Hadley Wickham\n\nМашине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена!\nДалее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть).\nНаписание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию.\nНапример, нам нужна функция, которая ищет совпадения в двух векторах и возвращает совпавшие элементы. Сначала решим задачу для двух векторов.\n\nline1 &lt;- c(\"гнев\", \"богиня\", \"воспой\")\nline2 &lt;- c(\"в\", \"мысли\", \"ему\", \"то\", \"вложила\", \"богиня\", \"державная\", \"гера\")\nidx &lt;- which(line2 %in% line1) # 2\nline2[idx]\n\n[1] \"богиня\"\n\n\nТеперь заменяем фактические переменные на формальные.\n\ncommon_words &lt;- function(x, y){\n  idx &lt;- which(x %in% y)\n  x[idx]\n}\n\nИ применяем к новым данным.\n\nline3 &lt;- c(\"лишь\", \"явилась\", \"заря\", \"розоперстая\", \"вестница\", \"утра\")\nline4 &lt;- c(\"вестница\", \"утра\", \"заря\", \"на\", \"великий\", \"олимп\", \"восходила\")\ncommon_words(line4, line3)\n\n[1] \"вестница\" \"утра\"     \"заря\"    \n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗагрузите библиотеку swirl, выберите курс R Programming и пройдите из него урок 9 Functions.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДля просмотра исходного кода любой функции необходимо…\n\n\n\n\n\n\nнабрать имя функции без аргументов и без скобок\n\n\nиспользовать специальную функцию для просмотра кода\n\n\nвызвать help к функции\n\n\nединственный способ — найти код функции в репозитории на GitHub\n\n\n\n\n\n\n\n\nНапишем функцию, которая будет центрировать данные, то есть вычитать среднее из каждого значения (забудем на время, что это уже делает базовая scale()):\n\ncenter &lt;- function(x){ \n  n = x - mean(x)\n  return(n) \n}\n\nx &lt;- c(5, 10, 15)\ncenter(x) \n\n[1] -5  0  5\n\n\nВнутри нашей функции есть переменная n, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции.\nФункция может принимать произвольное число аргументов. Доработаем наш код:\n\ncenter &lt;- function(x, na.rm = F){\n  if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие\n  x - mean(x) # на этот раз без return()\n}\n\nx &lt;- c(5, 10, NA)\ncenter(x)\n\n[1] NA NA NA\n\n\nЧто произошло? Почему следующий код выдает другой результат?\n\ncenter(x, na.rm = T)\n\n[1] -2.5  2.5\n\n\nВычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет.\n\ncenter &lt;- function(x, na.rm = F, what_is_your_name){\n  if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие\n  x - mean(x) # на этот раз без return()\n}\n\ncenter(x, na.rm = T)\n\n[1] -2.5  2.5\n\ncenter(x, na.rm = T, what_is_your_name = \"Locusclassicus\")\n\n[1] -2.5  2.5\n\n\nЧасто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении.\n\ncenter &lt;- function(x){\n  if (length(x) == 1) {stop(\"Отстань, старушка, я в печали.\")}\n  x - mean(x) # на этот раз без return()\n}\n\nx &lt;- 10\ncenter(x) # вернет ошибку\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию awesome_plot, которая будет принимать в качестве аргументов два вектора, трансформировать их в тиббл и строить диаграмму рассеяния при помощи ggplot(). Задайте цвет и прозрачность точек, а в подзаголовке выведите коэффициент корреляции.\n\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nНапишите код, который распечатает стихи детской песни “Alice the Camel”.\nПревратите детскую потешку “Ted in the Bed” в функцию. Обобщите до любого числа спящих.\nЗапишите в виде функции текст песни “99 Bottles of Beer on the Wall”. Обобщите до любого числа любых напитков на любой поверхности.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию, которая будет говорить “доброе утро”, “добрый день” или “добрый вечер” в зависимости от времени суток. Используйте lubridate::now() в качестве значения аргумента по умолчанию.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#purrr",
    "href": "iterate.html#purrr",
    "title": "4  Циклы, условия, функции",
    "section": "4.7 Purrr",
    "text": "4.7 Purrr\nПо-настоящему мощный инструмент для итераций – это пакет purrr из семейства tidyverse. Разработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham и Grolemund 2016).\n\nYou should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work.\n— Hadley Wickham & Garrett Grolemund\n\nВ семействе функций map_ из этого пакета всего 23 вариации. Вот основные из них:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nВсе они принимают на входе данные и функцию (или формулу), которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_int() – целочисленный вектор, и т.д.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#гарри-поттер-цикл-vs.-map_",
    "href": "iterate.html#гарри-поттер-цикл-vs.-map_",
    "title": "4  Циклы, условия, функции",
    "section": "4.5 Гарри Поттер: цикл vs. map_()",
    "text": "4.5 Гарри Поттер: цикл vs. map_()\nКак вы уже поняли, одни и те же задачи можно решать при помощи циклов и при помощи map_. Мы потренируемся на датасете, который в 2023 г. был доступен на сайте Британской библиотеки (https://www.bl.uk/), но потом оттуда исчез (но у нас сохранилась копия).\nДатасет представляет собой набор файлов .csv, содержащих метаданные о ресурсах, связанных с Гарри Поттером, из коллекций Британской библиотеки. Первоначально он был выпущен к 20-летию публикации книги «Гарри Поттер и философский камень» 26 июня 2017 года и с тех пор ежегодно обновлялся. Всего в датасете пять файлов, каждый из которых содержит разное представление данных.\nСкачаем архив.\n\nmy_url &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/HP.zip\"\ndownload.file(url = my_url, destfile = \"../files/HP.zip\")\n\nПосле этого переходим в директорию с архивом и распаковываем его.\n\nunzip(\"../files/HP.zip\")\n\nСохраним список всех файлов с расширением .csv, используя подходящую функцию из base R.\n\nmy_files &lt;- list.files(\"../files/HP\", pattern = \".csv\", full.names = TRUE)\nmy_files\n\n[1] \"../files/HP/classification.csv\" \"../files/HP/names.csv\"         \n[3] \"../files/HP/records.csv\"        \"../files/HP/titles.csv\"        \n[5] \"../files/HP/topics.csv\"        \n\n\n\n4.5.1 Цикл\nТеперь напишем цикл, который\n\nпрочитает все файлы из my_files, используя для этого функцию read_csv() из пакета readr;\nдля каждого датасета выяснит количество рядов без NA в столбце BNB Number;\nразделит число таких рядов на общее число рядов;\nвернет таблицу c четырьми столбцами:\n\nназвание файла (id),\nчисло рядов (total),\nчисло рядов без NA (complete),\nдоля полных рядов (ratio).\n\n\nСначала создаем таблицу, в которую будем складывать результат.\n\nmy_files_short &lt;- list.files(\"../files/HP\", pattern = \".csv\")\n\nmy_df &lt;- data.frame(id = my_files_short, \n                    total = 0,\n                    complete = 0,\n                    ratio = 0)\n\nmy_df\n\n\n  \n\n\n\nТеперь тело цикла:\n\nfor (i in 1:length(my_files)) {\n\n  # читаем очередной файл из my_files\n  current_file &lt;- my_files[i]\n  current_df &lt;- readr::read_csv(current_file, show_col_types = FALSE) \n\n  # выявляем общее число рядов и число рядов без NA в BNB number\n  # из-за пробела в названии столбца BNB number нужно использовать\n  # с бэктиками ``, а не с \"такими\" или 'такими' кавычками \n  current_total &lt;- nrow(current_df)\n  current_complete &lt;- sum(!is.na(current_df$`BNB number`))\n    \n\n  # помещаем значения в нужное место в заранее созданном my_df вместо нулей\n  my_df$total[i] &lt;- current_total  \n  my_df$complete[i] &lt;- current_complete\n  my_df$ratio[i] &lt;- current_complete / current_total\n}\n\nСмотрим на результат.\n\nmy_df\n\n\n  \n\n\n\n\n\n4.5.2 map_()\nТеперь исследуем датасет при помощи функционалов. Прочитаем все файлы одним вызовом функции.\n\n# чтение файлов \nHP &lt;- map(my_files, read_csv, col_types = cols())\n\nОбъект HP – это список. В нем пять элементов, так как на входе у нас было пять файлов. Для удобства назначим имена элементам списка.\n\nnames(HP) &lt;- my_files_short\n\n\nНачнем с простого: при помощи map можно извлечь столбцы (по имени) или ряды (по условию) из всех пяти таблиц. Прежде чем выполнить код ниже, подумайте, как будет выглядеть результат.\n\n# извлечь столбцы\nmap(HP, select, `BNB number`)\n\n# извлечь ряды\nmap(HP, filter, !(is.na(`BNB number`)))\n\n\n\n\n\n\n\nЗадание\n\n\n\nИзвлеките все уникальные названия (столбец Title) из всех пяти таблиц в HP. Используйте функцию distinct.\n\n\nЧто, если мы не знаем заранее, какие столбцы есть во всех пяти таблицах, и хотим это выяснить? Для этого подойдет функция reduce() из того же purrr. Она принимает на входе вектор (или список) и функцию и применяет функцию последовательно к каждой паре значений.\n\n\n\nИсточник.\n\n\n\nВоспользуемся этим, чтобы найти общие для всех таблиц имена столбцов.\n\nmap(HP, colnames) |&gt; \n  reduce(intersect)\n\n [1] \"Dewey classification\"       \"BL record ID\"              \n [3] \"Type of resource\"           \"Content type\"              \n [5] \"Material type\"              \"BNB number\"                \n [7] \"ISBN\"                       \"ISSN\"                      \n [9] \"Name\"                       \"Dates associated with name\"\n[11] \"Type of name\"               \"Role\"                      \n[13] \"Title\"                      \"Series title\"              \n[15] \"Number within series\"       \"Country of publication\"    \n[17] \"Place of publication\"       \"Publisher\"                 \n[19] \"Date of publication\"        \"Edition\"                   \n[21] \"Physical description\"       \"BL shelfmark\"              \n[23] \"Genre\"                      \"Languages\"                 \n[25] \"Notes\"                     \n\n\nЕще одна неочевидная возможность функции reduce - объединение нескольких таблиц в одну одним вызовом. Например, так:\n\nHP_joined &lt;- HP |&gt; \n  reduce(left_join)\n\nHP_joined\n\n\n  \n\n\n\n\n\n4.5.3 EDA\nТеперь можно почистить данные и построить несколько разведывательных графиков.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(`Country of publication`, into = c(\"country\", NA), sep = \";\") |&gt;\n  mutate(country = str_squish(country)) |&gt; \n  mutate(country = \n           case_when(country == \"England\" ~ \"United Kingdom\",\n                     country == \"Scotland\" ~ \"United Kingdom\",\n                     TRUE ~ country)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(country)) |&gt; \n  group_by(year, country) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \n\ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = country)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nВ качестве небольшого бонуса к этому уроку построим облако слов. Вектор слов возьмем из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./book/images/Wizard-Hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "5  Импорт: JSON & XML",
    "section": "",
    "text": "5.1 JSON\nФормат JSON (JavaScript Object Notation) предназначен для представления структурированных данных. JSON имеет шесть основных типов данных. Четыре из них - скаляры:\nСтроки, числа и булевы значения в JSON очень похожи на символьные, числовые и логические векторы в R. Основное отличие заключается в том, что скаляры JSON могут представлять только одно значение. Для представления нескольких значений необходимо использовать один из двух оставшихся типов: массивы и объекты.\nИ массивы, и объекты похожи на списки в R, разница заключается в том, именованы они или нет. Массив подобен безымянному списку и записывается через []. Например, [1, 2, 3] - это массив, содержащий 3 числа, а [null, 1, \"string\", false] - массив, содержащий ноль, число, строку и булево значение.\nОбъект подобен именованному списку и записывается через {}. Имена (ключи в терминологии JSON) являются строками, поэтому должны быть заключены в кавычки. Например, {“x”: 1, “y”: 2} - это объект, который сопоставляет x с 1, а y – с 2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON & XML</span>"
    ]
  },
  {
    "objectID": "import.html#json",
    "href": "import.html#json",
    "title": "5  Импорт: JSON & XML",
    "section": "",
    "text": "cамый простой тип - null, который играет ту же роль, что и NA в R. Он представляет собой отсутствие данных;\ncтрока (string) похожа на строку в R, но в ней всегда должны использоваться двойные кавычки;\nчисло аналогично числам в R, при этом поддерживается целочисленная (например, 123), десятичная (например, 123.45) или научная (например, 1,23e3) нотация. JSON не поддерживает Inf, -Inf или NaN;\nлогическое значение аналогично TRUE и FALSE в R, но использует строчные буквы true и false.\n\n\n\n\n\n5.1.1 Пакет jsonlite\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\n\nlibrary(jsonlite)\n\npath &lt;- \"../files/TBBT.json\"\ntbbt &lt;- read_json(path)\n\nФункция read_json() вернула нам список со следующими элементами:\n\nsummary(tbbt)\n\n                          Length Class  Mode     \nname                        1    -none- character\nseason_count                1    -none- character\nepisodes_count_total        1    -none- character\nepisodes_count_per_season  12    -none- list     \ncasting                    11    -none- list     \nepisode_list              280    -none- list     \nreferences                  1    -none- list     \n\n\n\n\n5.1.2 От списка к таблице\nВыборочно преобразуем список в тиббл. Функция transpose() берет список списков и выворачивает его наизнанку: вместо списка, в котором для каждого из персонажей указан актер и первое появление, мы получаем три списка: с персонажами, актерами и эпизодами. На месте отсутствующих значений ставится NULL.\n\nlibrary(tidyverse)\n\ncast_tbl &lt;- tbbt$casting |&gt; \n  transpose() |&gt; \n  map(as.character) |&gt; \n  as_tibble()\n\ncast_tbl\n\n\n  \n\n\n\nПроделаем то же самое для списка эпизодов, но другим способом. Функция pluck() представляет собой аналог [[, который можно использовать в пайпе. Она позволяет эффективно индексировать многоуровневые списки. Поскольку списков много, мы используем ее в сочетании с map_chr().\n\nepisodes_tbl &lt;- tibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\"))\n\nepisodes_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСамостоятельно создайте тиббл, в котором будет храниться количество серий для каждого сезона.\n\n\nЕще один способ описан здесь.\n\n\n5.1.3 Датасет: Шедевры Пушкинского музея\nJSON – популярный формат для публикации открытых данных. В таком виде часто публикуют данные органы государственной власти, культурные и некоммерческие организации и др. Например, Пушкинский музей.\nВзглянем на датасет “Шедевры из коллекции музея”. JSON можно прочитать напрямую из Сети.\n\ndoc &lt;- read_json(\"https://pushkinmuseum.art/json/masterpieces.json\")\n\nДатасет содержит информацию о 97 единицах хранения.\n\nnames(doc)\n\n [1] \"3687\"  \"3675\"  \"3706\"  \"3708\"  \"3713\"  \"3716\"  \"4005\"  \"4011\"  \"4014\" \n[10] \"4023\"  \"4030\"  \"4131\"  \"4147\"  \"4149\"  \"4161\"  \"4163\"  \"4178\"  \"4180\" \n[19] \"4191\"  \"4193\"  \"4198\"  \"4209\"  \"4244\"  \"4255\"  \"4260\"  \"4262\"  \"4266\" \n[28] \"4291\"  \"4325\"  \"4338\"  \"4350\"  \"4421\"  \"4450\"  \"4518\"  \"4543\"  \"4641\" \n[37] \"4711\"  \"4724\"  \"4767\"  \"7563\"  \"4782\"  \"4783\"  \"4788\"  \"4844\"  \"4906\" \n[46] \"4932\"  \"4936\"  \"4941\"  \"4949\"  \"4950\"  \"5238\"  \"5239\"  \"5297\"  \"5347\" \n[55] \"5591\"  \"5798\"  \"5910\"  \"5913\"  \"5992\"  \"6187\"  \"6226\"  \"6564\"  \"6584\" \n[64] \"6586\"  \"6629\"  \"6632\"  \"6886\"  \"7034\"  \"7151\"  \"7457\"  \"7468\"  \"7564\" \n[73] \"7565\"  \"7566\"  \"7567\"  \"7568\"  \"7569\"  \"7570\"  \"9464\"  \"9415\"  \"9046\" \n[82] \"10253\" \"10284\" \"10266\" \"10277\" \"10282\" \"10278\" \"10279\" \"10280\" \"10281\"\n[91] \"10285\" \"10286\" \"10287\" \"10288\" \"10289\" \"10290\" \"10291\"\n\n\nДля каждого предмета дано подробное описание.\n\nsummary(doc[[1]])\n\n                   Length Class  Mode     \npath               1      -none- character\nm_parent_id        1      -none- character\nyear               1      -none- numeric  \nget_year           1      -none- character\ninv_num            1      -none- character\ntype               2      -none- list     \ncountry            2      -none- list     \nperiod             2      -none- list     \npaint_school       1      -none- character\ngraphics_type      1      -none- character\ndepartment         1      -none- character\nmasterpiece        1      -none- character\nshow_in_hall       1      -none- character\nshow_in_collection 1      -none- numeric  \nname               2      -none- list     \nnamecom            2      -none- list     \nsize               2      -none- list     \ntext               2      -none- list     \nannotation         2      -none- list     \nlitra              2      -none- list     \nrestor             2      -none- list     \naudioguide         2      -none- list     \nvideoguide         2      -none- list     \nlink               2      -none- list     \nlinktext           2      -none- list     \nproducein          2      -none- list     \nmaterial           2      -none- list     \nfrom               2      -none- list     \nmatvos             2      -none- list     \nsizevos            2      -none- list     \nprodcast           2      -none- list     \nsearcha            2      -none- list     \nseakeys            2      -none- list     \nhall               1      -none- character\nbuilding           1      -none- character\ngallery            1      -none- list     \nauthors            1      -none- character\ncollectors         1      -none- list     \ncast               1      -none- character\nshop               1      -none- character\n\n\nЗаберем только то, что нам интересно.\n\nmasterpieces &lt;- tibble(\n  name = map_chr(doc, pluck, \"name\", \"ru\"),\n  get_year = map_chr(doc, pluck, \"get_year\"),\n  year = map_int(doc, pluck, \"year\"),\n  period = map_chr(doc, pluck, \"period\", \"name\", \"ru\"),\n  country = map_chr(doc, pluck, \"country\", \"ru\"),\n  gallery = paste0(\"https://pushkinmuseum.art\", map_chr(doc, pluck, \"gallery\", 1, 1)))\n\nБиблиотека imager позволяет работать с изображениями из датасета. Вот так мы могли бы забрать одно из них.\n\nlibrary(imager)\nimg &lt;- load.image(masterpieces$gallery[1]) |&gt; \n  plot()\nimg\n\n\nВ пакете imager есть функция map_il(), которая похожа на свою родню из purrr, но возвращает список изображений.\n\nimg_gallery &lt;- map_il(masterpieces$gallery, ~load.image(.x))\n\nФункция walk() из пакета purrr – это аналог map() для тех случаев, когда нас интересует только вывод, т.е.не надо ничего сохранять в окружение.\n\npar(mfrow = c(10, 10), mar = rep(0,4))\nwalk(img_gallery, plot, axes = FALSE)\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПопробуйте самостоятельно узнать, когда приобретена большая часть шедевров и из каких регионов они происходят.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON & XML</span>"
    ]
  },
  {
    "objectID": "import.html#xml",
    "href": "import.html#xml",
    "title": "5  Импорт: JSON & XML",
    "section": "5.2 XML",
    "text": "5.2 XML\nXML (от англ. eXtensible Markup Language) — расширяемый язык разметки. Слово “расширяемый” означает, что список тегов не зафиксирован раз и навсегда: пользователи могут вводить свои собственные теги и создавать так называемые настраиваемые языки разметки (Холзнер 2004, 29). Один из таких настраиваемых языков – это TEI (Text Encoding Initiative), о котором будет сказано дальше.\nНазначение языков разметки заключается в описании структурированных документов. Структура документа представляется в виде набора вложенных в друг друга элементов (дерева XML). У элементов есть открывающие и закрывающие теги.\nВсе составляющие части документа обобщаются в пролог и корневой элемент. Корневой элемент — обязательная часть документа, в которую вложены все остальные элементы. Пролог может включать объявления, инструкции обработки, комментарии.\nВ правильно сформированном XML открывающий и закрывающий тег вложенного элемента всегда находятся внутри одного родительского элемента.\nСоздадим простой XML из строки. Сначала идет инструкция по обработке XML (со знаком вопроса), за ней следует объявление типа документа (с восклицательным знаком) и открывающий тег корневого элемента. В этот корневой элемент вложены все остальные элементы.\n\nstring_xml &lt;- '&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!DOCTYPE recipe&gt;\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n   &lt;title&gt;\n      Простой хлеб\n   &lt;/title&gt;\n   &lt;composition&gt;\n      &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n      &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n      &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n   &lt;/composition&gt;\n   &lt;instructions&gt;\n     &lt;step&gt;\n        Смешать все ингредиенты и тщательно замесить. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Закрыть тканью и оставить на один час в тёплом помещении. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Замесить ещё раз, положить на противень и поставить в духовку.\n     &lt;/step&gt;\n   &lt;/instructions&gt;\n&lt;/recipe&gt;'\n\n\n5.2.1 Библиотека XML\nДля работы с xml понадобится установить одноименную библиотеку. Функция xmlTreeParse() создаст R-структуру, представляющую дерево XML.\n\nlibrary(XML)\ndoc &lt;- xmlTreeParse(string_xml)\nclass(doc)\n\n[1] \"XMLDocument\"         \"XMLAbstractDocument\"\n\n\nФункция xmlRoot() позволяет извлечь корневой элемент вместе со всеми детьми.\n\nrootnode &lt;- xmlRoot(doc)\nrootnode\n\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n &lt;title&gt;Простой хлеб&lt;/title&gt;\n &lt;composition&gt;\n  &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n  &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n  &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n &lt;/composition&gt;\n &lt;instructions&gt;\n  &lt;step&gt;Смешать все ингредиенты и тщательно замесить.&lt;/step&gt;\n  &lt;step&gt;Закрыть тканью и оставить на один час в тёплом помещении.&lt;/step&gt;\n  &lt;step&gt;Замесить ещё раз, положить на противень и поставить в духовку.&lt;/step&gt;\n &lt;/instructions&gt;\n&lt;/recipe&gt;\n\n\nЕсли документ большой, бывает удобнее не распечатывать все дерево, а вывести имена дочерних элементов.\n\nnames(xmlChildren(rootnode))\n\n[1] \"title\"        \"composition\"  \"instructions\"\n\n\nРазмер узла – это число вложенных в него “детей”. Его можно узнать, применив к узлу функцию xmlSize() – или посчитав число “детей”.\n\nxmlSize(rootnode) == length(xmlChildren(rootnode))\n\n[1] TRUE\n\n\n\n\n5.2.2 Выбор элементов\nРаботать с xml можно как с обычным списком, то есть индексировать узлы по имени или по номеру элемента при помощи квадратных скобок. Так мы достаем узел по имени:\n\nrootnode[[\"composition\"]]\n\n&lt;composition&gt;\n &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n&lt;/composition&gt;\n\n\nА так – по индексу:\n\nrootnode[[2]]\n\n&lt;composition&gt;\n &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n&lt;/composition&gt;\n\n\nКак и с обычными списками, мы можем использовать последовательности квадратных скобок:\n\ningr_node &lt;- rootnode[[2]][[\"ingredient\"]]\ningr_node\n\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n\n\n\n5.2.3 Значения узлов и атрибутов\nНо обычно нам нужен не элемент как таковой, а его содержание (значение). Чтобы добраться до него, используем функцию xmlValue():\n\nxmlValue(ingr_node)\n\n[1] \"Мука\"\n\n\nМожно уточнить атрибуты узла при помощи xmlAttrs():\n\nxmlAttrs(ingr_node)\n\n  amount     unit \n     \"3\" \"стакан\" \n\n\nЧтобы извлечь значение атрибута, используем функцию xmlGetAttr(). Первым аргументом функции передаем xml-узел, вторым – имя атрибута.\n\nxmlGetAttr(ingr_node, \"unit\")\n\n[1] \"стакан\"\n\n\n\n\n5.2.4 Обход дерева узлов\nКак насчет того, чтобы применить функцию к набору узлов – например, ко всем инредиентам? Вспоминаем функции для работы со списками – sapply() из базового R или map() из пакета purrr:\n\ningr_nodes &lt;- xmlChildren(rootnode[[2]])\n\nsapply(ingr_nodes, xmlValue)\n\n   ingredient    ingredient    ingredient \n       \"Мука\"      \"Дрожжи\" \"Тёплая вода\" \n\n\n\nsapply(ingr_nodes, xmlGetAttr, \"unit\")\n\ningredient ingredient ingredient \n  \"стакан\"    \"грамм\"   \"стакан\" \n\n\n\n\n5.2.5 Синтаксис XPath\nДобраться до узлов определенного уровня можно также при помощи синтаксиса XPath. XPath – это язык запросов к элементам XML-документа. С его помощью можно описать “путь” до нужного узла: абсолютный (начиная с корневого элемента) или относительный. В пакете XML синтаксис XPath поддерживает функция getNodeSet().\n\n# абсолютный путь\ningr_nodes &lt;- getNodeSet(rootnode, \"/recipe//composition//ingredient\")\n\ningr_nodes\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n\n[[3]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n# относительный путь\ningr_nodes &lt;- getNodeSet(rootnode, \"//composition//ingredient\")\n\ningr_nodes\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n\n[[3]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВ большинстве случаев функция getNodeSet() требует задать пространство имен (namespace), но в нашем случае оно не определено, поэтому пока передаем только дерево и путь до узла. С пространством имен встретимся чуть позже!\n\n\n\nСинтаксис XPath позволяет отбирать узлы с определенными атрибутами. Допустим, нам нужны только те узлы, где значение атрибута unit = “стакан”:\n\ngetNodeSet(rootnode, \"//composition//ingredient[@unit='стакан']\")\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n\n5.2.6 От дерева к таблице\nПри работе с xml в большинстве случаев наша задача – извлечь значения определеннных узлов или их атрибутов и сохранить их в прямоугольном формате. Один из способов выглядит так.\n\ntibble(title = xmlValue(rootnode[[\"title\"]]), \n       ingredients = map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlValue),\n       unit = map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlGetAttr, \"unit\"),\n       amount = map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlGetAttr, \"amount\"))\n\n\n  \n\n\n\n\n\n5.2.7 Разметка TEI\nБольшая часть размеченных литературных корпусов хранится именно в формате XML. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п.\nДобавлять и удалять разметку может любой пользователь в редакторе XML кода или даже в простом текстовом редакторе. При этом в качестве универсального языка разметки в гуманитарных дисциплинах используется язык TEI (Скоринкин 2016). Корневой элемент в документах TEI называется TEI, внутри него располагается элемент teiHeader с метаинформацией о документе и элемент text. Последний содержит текст документа с элементами, определяющими его структурное членение.\n&lt;TEI&gt;\n  &lt;teiHeader&gt;&lt;/teiHeader&gt;\n  &lt;text&gt;&lt;/text&gt;\n&lt;/TEI&gt;\nПример оформления документа можно посмотреть по ссылке.\nУ teiHeader есть четыре главных дочерних элемента:\n\nfileDesc (описание документа c библиографической информацией)\nencodingDesc (описание способа кодирование первоисточника)\nprofileDesc (“досье” на текст, например отправитель и получатель для писем, жанр, используемые языки, обстоятельства создания, место написания и т.п.)\nrevisionDesc (история изменений документа).\n\nВ самом тексте язык TEI дает возможность представлять разные варианты (авторские, редакторские, корректорские и др.) Основным средством параллельного представления является элемент choice. Например, в тексте Лукреция вы можете увидеть такое:\nsic calor atque &lt;choice&gt;&lt;reg&gt;aer&lt;/reg&gt;&lt;orig&gt;aër&lt;/orig&gt;&lt;/choice&gt; et venti caeca potestas\nЗдесь reg указывает на нормализованное написание, а orig – на оригинальное.\n\n\n5.2.8 Датасет: “Война и мир”\nВ качестве примера загрузим датасет “Пушкинского дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”.\n\nfilename = \"../files/War_and_Peace.xml\"\ndoc &lt;- xmlTreeParse(filename, useInternalNodes = T)\nrootnode &lt;- xmlRoot(doc)\n\nТеперь можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена:\n\nnames(xmlChildren(rootnode)) \n\n[1] \"teiHeader\" \"text\"     \n\n\nОчевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее:\n\nnames(xmlChildren(rootnode[[\"text\"]])) \n\n[1] \"div\" \"div\" \"div\" \"div\" \"div\"\n\n\nИтак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это четыре тома и эпилог.\n\n# это список\ndivs &lt;-  rootnode[[\"text\"]][\"div\"]\n\nsapply(divs, xmlGetAttr, \"type\")\n\n       div        div        div        div        div \n  \"volume\"   \"volume\"   \"volume\"   \"volume\" \"epilogue\" \n\n\nКак мы уже знаем, добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – при помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на два уровня ниже: там тоже будет тег div, но с другим атрибутом. Как легко убедиться, теперь это главы, всего их 358.\n\ndivs &lt;- getNodeSet(doc, \"/tei:TEI//tei:text//tei:div//tei:div//tei:div\",\n                     namespaces = c(tei = \"http://www.tei-c.org/ns/1.0\")) \n\nlength(divs)\n\n[1] 358\n\nunique(sapply(divs, xmlGetAttr, \"type\"))\n\n[1] \"chapter\"\n\n\nОбратите внимание, что в данном случае надо прямо прописать пространство имен (namespaces). Это можно посмотреть в самом xml, а можно воспользоваться специальной функцией:\n\nxmlNamespace(rootnode)\n\n[1] \"http://www.tei-c.org/ns/1.0\"\nattr(,\"class\")\n[1] \"XMLNamespace\"\n\n\nЗабрать конкретную главу можно путем индексации, но лучше – по значению соответствующего атрибута.\n\nidx &lt;- which(map(divs, xmlGetAttr, \"xml:id\") == \"chapter1part1Volume1\")\nch1 &lt;- divs[[idx]]\n\nЧтобы извлечь текст, понадобится функция xmlValue.\n\nchapter_1 &lt;- xmlValue(ch1)\n\nРаспечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на параграфы и выведем первый и последний:\n\nlibrary(stringr)\nchapter_lines &lt;- str_split(chapter_1, pattern = \"\\n\")\n\nchapter_lines[[1]][[5]]\n\n[1] \"        — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j'y crois) — je ne vous connais plus, vous n'êtes plus mon ami, vous n'êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\"\n\nchapter_lines[[1]][[838]]\n\n[1] \"       Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.\"\n\n\nПервая и последняя реплика по-французски: все правильно!\n\n\n\n\n\n\nЗадание\n\n\n\nСкачайте по ссылке “Горе от ума” Грибоедова и преобразуйте xml в прямоугольный формат таким образом, чтобы для каждой реплики был указан акт, сцена и действующее лицо.\n\n\nПодбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan и Lang 2014) и (Холзнер 2004).\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт: JSON & XML</span>"
    ]
  },
  {
    "objectID": "import.html#tei",
    "href": "import.html#tei",
    "title": "5  Импорт",
    "section": "5.3 TEI",
    "text": "5.3 TEI\nБольшая часть размеченных литературных корпусов хранится именно в формате XML. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п.\nДобавлять и удалять разметку может любой пользователь в редакторе XML кода или даже в простом текстовом редакторе. При этом в качестве универсального языка разметки в гуманитарных дисциплинах используется язык TEI (Скоринкин 2016). Корневой элемент в документах TEI называется TEI, внутри него располагается элемент teiHeader с метаинформацией о документе и элемент text. Последний содержит текст документа с элементами, определяющими его структурное членение.\n&lt;TEI&gt;\n  &lt;teiHeader&gt;&lt;/teiHeader&gt;\n  &lt;text&gt;&lt;/text&gt;\n&lt;/TEI&gt;\nПример оформления документа можно посмотреть по ссылке.\nУ teiHeader есть четыре главных дочерних элемента:\n\nfileDesc (описание документа c библиографической информацией)\nencodingDesc (описание способа кодирование первоисточника)\nprofileDesc (“досье” на текст, например отправитель и получатель для писем, жанр, используемые языки, обстоятельства создания, место написания и т.п.)\nrevisionDesc (история изменений документа).\n\nВ самом тексте язык TEI дает возможность представлять разные варианты (авторские, редакторские, корректорские и др.) Основным средством параллельного представления является элемент choice. Например, в тексте Лукреция вы можете увидеть такое:\nsic calor atque &lt;choice&gt;&lt;reg&gt;aer&lt;/reg&gt;&lt;orig&gt;aër&lt;/orig&gt;&lt;/choice&gt; et venti caeca potestas\nЗдесь reg указывает на нормализованное написание, а orig – на оригинальное.\nВ качестве примера загрузим датасет “Пушкинского дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”.\n\nfilename = \"../files/War_and_Peace.xml\"\ndoc &lt;- xmlTreeParse(filename, useInternalNodes = T)\nrootnode &lt;- xmlRoot(doc)\n\nТеперь можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена:\n\nnames(xmlChildren(rootnode)) \n\n[1] \"teiHeader\" \"text\"     \n\n\nОчевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее:\n\nnames(xmlChildren(rootnode[[\"text\"]])) \n\n[1] \"div\" \"div\" \"div\" \"div\" \"div\"\n\n\nИтак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это четыре тома и эпилог.\n\n# это список\ndivs &lt;-  rootnode[[\"text\"]][\"div\"]\n\nsapply(divs, xmlGetAttr, \"type\")\n\n       div        div        div        div        div \n  \"volume\"   \"volume\"   \"volume\"   \"volume\" \"epilogue\" \n\n\nКак мы уже знаем, добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – при помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на два уровня ниже: там тоже будет тег div, но с другим атрибутом. Как легко убедиться, теперь это главы, всего их 358.\n\ndivs &lt;- getNodeSet(doc, \"/tei:TEI//tei:text//tei:div//tei:div//tei:div\",\n                     namespaces = c(tei = \"http://www.tei-c.org/ns/1.0\")) \n\nlength(divs)\n\n[1] 358\n\nunique(sapply(divs, xmlGetAttr, \"type\"))\n\n[1] \"chapter\"\n\n\nОбратите внимание, что в данном случае надо прямо прописать пространство имен (namespaces). Это можно посмотреть в самом xml, а можно воспользоваться специальной функцией:\n\nxmlNamespace(rootnode)\n\n[1] \"http://www.tei-c.org/ns/1.0\"\nattr(,\"class\")\n[1] \"XMLNamespace\"\n\n\nЗабрать конкретную главу можно путем индексации, но лучше – по значению соответствующего атрибута.\n\nidx &lt;- which(map(divs, xmlGetAttr, \"xml:id\") == \"chapter1part1Volume1\")\nch1 &lt;- divs[[idx]]\n\nЧтобы извлечь текст, понадобится функция xmlValue.\n\nchapter_1 &lt;- xmlValue(ch1)\n\nРаспечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на параграфы и выведем первый и последний:\n\nlibrary(stringr)\nchapter_lines &lt;- str_split(chapter_1, pattern = \"\\n\")\n\nchapter_lines[[1]][[5]]\n\n[1] \"        — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j'y crois) — je ne vous connais plus, vous n'êtes plus mon ami, vous n'êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\"\n\nchapter_lines[[1]][[838]]\n\n[1] \"       Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.\"\n\n\nПервая и последняя реплика по-французски: все правильно!\n\n\n\n\n\n\nЗадание\n\n\n\nСкачайте по ссылке “Горе от ума” Грибоедова и преобразуйте xml в прямоугольный формат таким образом, чтобы для каждой реплики был указан акт, сцена и действующее лицо.\n\n\nПодбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan и Lang 2014) и (Холзнер 2004).\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#бонус-gutenbergr",
    "href": "import.html#бонус-gutenbergr",
    "title": "5  Импорт",
    "section": "5.4 Бонус: GutenbergR",
    "text": "5.4 Бонус: GutenbergR\nПакет GutenbergR поможет достать тексты из библиотеки Gutenberg, но будьте осторожны: распознаны они не всегда хорошо и порой содержат много разного шума, например примечания редактора, номера страниц и т.п. В билингвах источник и перевод могут идти вперемешку. И если в XML подобные элементы будут окружены соответствующими тегами, которые позволят их легко отбросить при анализе, то Gutenberg дает вам сырой текст. Часто его надо хорошенько чистить при помощи регулярных выражений или даже вручную.\nРаботать с метаданными GutenbergR вы уже научились, теперь можете пользоваться пакетом и для скачивания текстов. Сначала узнаем id нужных текстов^ [https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html]\n\nlibrary(gutenbergr)\n\ncaesar &lt;- gutenberg_works(author == \"Caesar, Julius\", languages = \"la\") \n\ncaesar \n\n\n  \n\n\n\nЧтобы извлечь отдельный текст (тексты):\n\nde_bello_gallico &lt;- gutenberg_download(218, meta_fields = \"title\", mirror = \"ftp://mirrors.xmission.com/gutenberg/\")\nde_bello_gallico\n\n\n  \n\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nСуществует несколько зеркал библиотеки Gutenberg, и, если при выполнении функции gutenberg_download() возникает ошибка “could not download a book at http://aleph.gutenberg.org/”, то следует использовать аргумент mirror. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL\n\n\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "share.html",
    "href": "share.html",
    "title": "6  Публикационная система Quarto",
    "section": "",
    "text": "6.1 О воспроизводимости\nПолученный в результате количественных исследований результат должен быть проверяем и воспроизводим. Это значит, что в большинстве случаев недостаточно просто рассказать, что вы проделали. Теоретически читатель должен иметь возможность проделать тот же путь, что и автор: вопроизвести его результаты, но в обратном направлении.\nДля этого должны выполняться три основных требования:\nУже на этапе планирования исследования очень важно продумать, как вы будете его документировать. Важно помнить, что код пишется не только для машин, но и для людей, поэтому стоит документировать не только то, что вы делали, но и почему. R дает для этого множество возможностей, главная из которых – это Markdown.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#о-воспроизводимости",
    "href": "share.html#о-воспроизводимости",
    "title": "6  Публикационная система Quarto",
    "section": "",
    "text": "На заметку\n\n\n\nВоспроизводимость (reproducibility) – это не то же, что повторяемость (replicability). Ученый, который повторяет исследование, проводит его заново на новых данных. Воспроизведение – гораздо более скромная задача, не требующая таких ресурсов, как повторение (Winter 2020, 47).\n\n\n\n\nдоступность данных и метаданных;\nдоступность компьютерного кода;\nдоступность программного обеспечения.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#markdown",
    "href": "share.html#markdown",
    "title": "6  Публикационная система Quarto",
    "section": "6.2 Markdown",
    "text": "6.2 Markdown\nMarkdown – это облегчённый язык разметки. Он позволяет создавать документы разного формата – не только HTML (веб-страницы), но и PDF и Word. Markdown дает возможность создания полностью воспроизводимых документов, сочетающих код и поясняющий текст. Этот язык используется для создания сайтов, статей, книг, презентаций, отчетов, дашбордов и т.п. Этот курс написан с использованием Markdown.\nЧтобы начать работать с документами .rmd, нужен пакет rmarkdown; в RStudio он уже предустановлен. Создание нового документа .rmd происходит из меню.\nПо умолчанию документ .rmd снабжен шапкой yaml. Она не обязательна. Здесь содержатся данные об авторе, времени создания, формате, сведения о файле с библиографией и т.п.\n---\ntitle: \"Demo\"\nauthor: \"My name\"\ndate: \"2025-03-11\"\noutput: html_document\n---\nТакже в документе .rmd скорее всего будет простой текст и блоки кода. Чтобы “сшить” html (pdf, doc), достаточно нажать кнопку knit либо запустить в консоли код: rmarkdown::render(\"Demo.Rmd\"). После этого в рабочей директории появится новый файл (html, pdf, или doc), которым можно поделиться с коллегами, грантодателями или друзьями.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#quarto",
    "href": "share.html#quarto",
    "title": "6  Публикационная система Quarto",
    "section": "6.3 Quarto",
    "text": "6.3 Quarto\nРаботать с маркдауном мы будем, используя издательскую систему Quarto с открытым исходным кодом. Она позволяет создавать и публиковать статьи, презентации, информационные панели, веб-сайты, блоги и книги в HTML, PDF, MS Word, ePub и других форматах. В общем, обычный Markdown тоже позволяет все это делать, но чуть сложнее. Quarto объединяет различные пакеты из экосистемы R Markdown воедино и значительно упрощает работу с ними. Подробнее см. практическое руководство “Quarto: The Definitive Guide”.\n\n\n\n\n\n\nЗадание\n\n\n\nСоздайте новый .qmd документ. Потренируйтесь запускать код и сшивать документ в .html, .pdf, .docx.\n\n\nДля .pdf может понадобиться установка LaTeX.\n\n# install.packages(\"tinytex\")\ntinytex::install_tinytex()\n# to uninstall TinyTeX, run\n# tinytex::uninstall_tinytex()\n\nМожно указать сразу несколько форматов для файла, как показано здесь, и “сшить” их одновременно:\n\nquarto::quarto_render(\n  \"untitled.qmd\", \n  output_format = c(\"pdf\", \"html\", \"docx\")\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#шапка-yaml",
    "href": "share.html#шапка-yaml",
    "title": "6  Публикационная система Quarto",
    "section": "6.4 Шапка YAML",
    "text": "6.4 Шапка YAML\nОсновные параметры документа хранятся в YAML-шапке. К ним относятся format, title, subtitle, date, date-format, author, abstract, lang, toc, number-sections и другие.\nПопробуйте изменить шапку своего .qmd-документа и заново его сшить. Сравните с предыдущей версией.\n---\ntitle: \"Заголовок\"\nsubtitle: \"Подзаголовок\"\nformat: html\nauthor: locusclassicus\ndate: today\ndate-format: D.MM.YYYY\nabstract: Значенье бублика нам непонятно.\nlang: ru\ntoc: true\nnumber-sections: true\n---\n\nПоле execute позволяет задать параметры всех фрагментов кода в документе, например:\n---\nexecute:\n  echo: false\n  fig-width: 9\n---\n  \nНо для отдельных кусков кода эти настройки можно поменять:\n```\n#| echo: true\n\nsqrt(16)\n```\nПараметр df-print позволяет выбрать один из возможных способов отображения датафреймов:\n\ndefault — стандартный, как в консоли;\ntibble — стандартный, как в консоли, но в формате tibble;\nkable — минималистичный вариант, подходит для всех видов документов;\npaged — интерактивная таблица, подходит для html страниц.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#синтаксис-markdown",
    "href": "share.html#синтаксис-markdown",
    "title": "6  Публикационная система Quarto",
    "section": "6.5 Синтаксис Markdown",
    "text": "6.5 Синтаксис Markdown\n\n6.5.1 Заголовки\nЗаголовки разного уровня задаются при помощи решетки:\n# Заголовок первого уровня\n## Заголовок второго уровня\n### Заголовок третьего уровня\n#### Заголовок четвёртого уровня\nПример заголовка третьего уровня:\n\n\n6.5.2 Форматирование\n*курсив*  \n_курсив_\n\n**полужирный**  \n__полужирный__\n\n***полужирный курсив***  \n___полужирный курсив___\n\n~~зачеркнутый~~\n\n&lt;mark&gt;выделение&lt;/mark&gt;\nПример:\nкурсив\nполужирный\nуж и не знаю как выделить\nзачеркнутый\nвыделение\n\n\n6.5.3 Списки\nНумерованный список\n1. Пункт первый\n2. Пункт второй\n3. Пункт третий\nПример:\n\nПункт первый\nПункт второй\nПункт третий\n\nМаркированный список\n- Пункт первый\n- Пункт второй\n- Пункт третий\nПример:\n\nПункт первый\nПункт второй\nПункт третий\n\nТакже Markdown позволяет делать вложенные списки:\n1. Пункт первый\n    - Подпункт первый\n    - Подпункт второй\n2. Пункт второй\nПример:\n\nПункт первый\n\nПодпункт первый\nПодпункт второй\n\nПункт второй\n\nСамое удобное, что элементы списка не обязательно нумеровать:\n(@) Пункт первый.\n(@) Пункт не знаю какой.\n\nПункт первый.\nПункт не знаю какой.\n\n\n\n6.5.4 Ссылки\n[Текст ссылки](http://antibarbari.ru/)\nПример:\nТекст ссылки\n\n\n6.5.5 Изображения\n![Текст описания](https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/Holbein-erasmus.jpg/548px-Holbein-erasmus.jpg)\nПример:\n\n\n\nМоя картинка\n\n\nДва нюанса:\n\nможно давать ссылки на локальные файлы (то есть такие файлы, которые хранятся на компьютере), но имейте в виду, что такой код не будет работать у другого пользователя;\nизображения можно вставлять, пользуясь непосредственно разметкой html.\n\n&lt;img src=\"images/my_image.jpg\" width=40%&gt;\n\n\n6.5.6 Блоки кода\nМожно вставлять непосредственно в текст; для этого код выделяют одинарным обратным апострофом (грависом). Но чаще код дают отдельным блоком. Эти блоки можно именовать; тогда в случае ошибки будет сразу понятно, где она случилась.\n```{}\nsome code here\n```\nВ фигурных скобках надо указать язык, например {r}, только в этом случае код будет подсвечиваться и выполняться.\nТам же в фигурных скобках можно задать следующие параметры:\n\neval = FALSE код будет показан, но не будет выполняться;\ninclude = FALSE код будет выполнен, но ни код, ни результат не будут показаны;\necho = FALSE код будет выполнен, но не показан, результаты при этом видны;\nmessage = FALSE или warning = FALSE прячет сообщения или предупреждения;\nresults = 'hide' не распечатывает результат, а fig.show = 'hide' прячет графики;\nerror = TRUE “сшивание” продолжается, даже если этот блок вернул ошибку.\n\n\n\n6.5.7 Цитаты\n&gt; Omnia praeclara rara.\nПример:\n\nOmnia praeclara rara.\n\nЦитата с подписью может быть оформлена так:\n&gt; Omnia praeclara rara.\n&gt;\n&gt; --- Cicero\nПример:\n\nOmnia praeclara rara.\n— Cicero\n\n\n\n6.5.8 Разделители\nЧтобы создать горизонтальную линию, можно использовать ---, *** или ___.\nПример:\n\n\n\n6.5.9 Таблицы\nТаблицы можно задать вручную при помощи дефисов - и вертикальных линий |; идеальная точность при этом не нужна. Перед таблицей обязательно оставляйте пустую строку, иначе волшебство не сработает.\n\n| Фрукты   | Калории  |\n| -----  | ---- |\n| Яблоко   | 52  |\n| Апельсин | 47  |\nПример:\n\n\n\nФрукты\nКалории\n\n\n\n\nЯблоко\n52\n\n\nАпельсин\n47\n\n\n\nПо умолчанию Markdown распечатывает таблицы так, как они бы выглядели в консоли.\n\ndata(\"iris\")\nhead(iris)\n\n\n  \n\n\n\nДля дополнительного форматирования можно использовать функцию knitr::kable():\n\nknitr::kable(iris[1:6, ], caption = \"Таблица knitr\")\n\n\nТаблица knitr\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\nИнтерактивную таблицу можно создать так:\n\nDT::datatable(iris[1:6,])\n\n\n\n\n\n\n\n6.5.10 Чек-листы\n- [x] Таблицы\n- [ ] Графики\nПример:\n\nТаблицы\nГрафики\n\n\n\n6.5.11 Внутренние ссылки\nУдобны для навигации по документу. К названию любого раздела можно добавить {#id}.\n[Вернуться к чек-листам](#id)\nПример:\nВернуться к чек-листам\n\n\n6.5.12 Графики\nMarkdown позволяет встраивать любые графики.\n\nlibrary(ggplot2)\nggplot(aes(x = Sepal.Length, y = Petal.Length, col = Species), data = iris) +\n  geom_point(show.legend = F)\n\n\n\n\n\n\n\n\nДля интерактивных графиков понадобится пакет plotly:\n\nlibrary(plotly)\nplot_ly(data=iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species)\n\n\n\n\n\nПодробное руководство по созданию интерактивных графиков можно найти на сайте https://plotly.com/r/.\n\n\n6.5.13 Математические формулы\nПишутся с использованием синтаксиса LaTeX, о котором можно прочитать подробнее здесь.\nФормулы заключаются в одинарный $, если пишутся в строку, и в двойной $$, если отдельным блоком.\n\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\nВот так это выглядит в тексте: \\(\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\).\nА вот так – блоком:\n\\[\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\]\n\n\n6.5.14 Смайлы\nУдобнее вставлять через визуальный редактор (“шестеренка” &gt; Use Visual Editor), но можно и без него:\n\n# devtools::install_github(\"hadley/emo\")\nlibrary(emo)\nemo::ji(\"apple\")\n\n🍎 \n\n\nКод можно записать в строку, тогда смайл появится в тексте: 💀.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#библиография",
    "href": "share.html#библиография",
    "title": "6  Публикационная система Quarto",
    "section": "6.6 Библиография",
    "text": "6.6 Библиография\nMarkdown позволяет добавлять библиографию в формате BibTeX. BibTeX — программное обеспечение для создания форматированных списков библиографии; обычно используется совместно с LaTeX’ом. Многие сайты, например GoogleScholar, позволяют экспортировать библиографические записи в формате BibTeX. При необходимости запись можно исправить вручную.\nКаждая запись имеет следующую форму.\n@book{winter2020,\n  author = {Bodo Winter},\n  title = \"{Statistics for Linguists: An Introduction Using R}\",\n  year = {2020},\n  publisher = {Routledge}\n}\nЗдесь book — тип записи («книга»), winter2020 — метка-идентификатор записи, дальше список полей со значениями.\nОдна запись описывает ровно одну публикацию статью, книгу, диссертацию, и т. д. Подробнее о типах записей можно посмотреть вот здесь.\nПодобные записи хранятся в текстовом файле с расширением .bib. Чтобы привязать библиографию, нужно указать имя файла в шапке yaml.\n---\nbibliography: bibliography.bib\n---\nДальше, чтобы добавить ссылку, достаточно ввести ключ публикации после @ (в квадратных скобках, чтобы публикация отражалась в круглых): [@wickham2016].\nПример:\n(Wickham и Grolemund 2016).\nМожно интегрировать BibTex с Zotero или другим менеджером библиографии. Для этого придется установить специальное расширение.\nЧтобы изменить стиль цитирования, необходимо добавить в шапку yaml название csl-файла (CSL - Citation Style Language), например:\n---\noutput: html_document\nbibliography: references.bib\ncsl: archiv-fur-geschichte-der-philosophie.csl\n---\nНайти необходимый csl-файл можно, например, в репозитории стилей Zotero.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "share.html#публикация-html",
    "href": "share.html#публикация-html",
    "title": "6  Публикационная система Quarto",
    "section": "6.7 Публикация html",
    "text": "6.7 Публикация html\nДля публикации на RPubs понадобится установить пакеты packrat, rsconnect.\nПри публикации страницы на https://rpubs.com/ следует добавить в шапку две строчки:\n\n---\nembed-resources: true\nstandalone: true\n---\n\nЭто позволит корректно отобразить локальные фото, графики и сохранит оформление.\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using R. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикационная система Quarto</span>"
    ]
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "7  Регулярные выражения",
    "section": "",
    "text": "7.1 Regex в базовом R\nВ базовом R за работу со строками отвечают, среди прочего, такие функции, как grep() и grepl(). При этом grepl() возвращает TRUE, если шаблон найден в соответствующей символьной строке, а grep() возвращает вектор индексов символьных строк, содержащих паттерн.\nОбеим функциям необходим аргумент pattern и аргумент x, где pattern - регулярное выражение, по которому производится поиск, а аргумент x - вектор символов, по которым следует искать совпадения.\nФункция gsub() позволяет производить замену и требует также аргумента replacement.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#литералы-и-классы",
    "href": "regex.html#литералы-и-классы",
    "title": "7  Регулярные выражения",
    "section": "7.2 Литералы и классы",
    "text": "7.2 Литералы и классы\nБуквальные символы – это то, что вы ожидаете увидеть (или не увидеть – для управляющих и пробельных символов); можно сказать, что это символы, которые ничего не “имеют в виду”. Их можно объединять в классы при помощи квадратных скобок, например, так: [abc].\n\nvec &lt;- c(\"a\", \"d\", \"c\")\ngrepl(\"[abc]\", vec)\n\n[1]  TRUE FALSE  TRUE\n\ngrep(\"[abc]\", vec)\n\n[1] 1 3\n\n\nДля некоторых классов есть специальные обозначения.\n\n\n\n\n\n\n\n\nКласс\nЭквивалент\nЗначение\n\n\n\n\n[:upper:]\n[A-Z]\nСимволы верхнего регистра\n\n\n[:lower:]\n[a-z]\nСимволы нижнего регистра\n\n\n[:alpha:]\n[[:upper:][:lower:]]\nБуквы\n\n\n[:digit:]\n[0-9], т. е. \\d\nЦифры\n\n\n[:alnum:]\n[[:alpha:][:digit:]]\nБуквы и цифры\n\n\n[:word:]\n[[:alnum:]_], т. е. \\w\nСимволы, образующие «слово»\n\n\n[:punct:]\n[-!“#$%&’()*+,./:;&lt;=&gt;?@[\\]_`{|}~]\nЗнаки пунктуации\n\n\n[:blank:]\n[\\s\\t]\nПробел и табуляция\n\n\n[:space:]\n[[:blank:]\\v\\r\\n\\f], т. е. \\s\nПробельные символы\n\n\n[:cntrl:]\n\nУправляющие символы (перевод строки, табуляция и т.п.)\n\n\n[:graph:]\n\nПечатные символы\n\n\n[:print:]\n\nПечатные символы с пробелом\n\n\n\nЭти классы тоже можно задавать в качестве паттерна.\n\nvec &lt;- c(\"жираф\", \"верблюд1\", \"0зебра\")\ngsub( \"[[:digit:]]\",  \"\", vec)\n\n[1] \"жираф\"   \"верблюд\" \"зебра\"  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nВ пакете stringr есть небольшой датасет words. Найдите все слова с последовательностью символов wh. Сколько слов содержат два гласных после w?\n\n\n\nВ качестве классов можно рассматривать и следующие обозначения:\n\n\n\n\n\n\n\n\nПредставление\nЭквивалент\nЗначение\n\n\n\n\n\\d\n[0-9]\nЦифра\n\n\n\\D\n[^\\\\d]\nЛюбой символ, кроме цифры\n\n\n\\w\n[A-Za-zА-Яа-я0-9_]\nСимволы, образующие «слово» (буквы, цифры и символ подчёркивания)\n\n\n\\W\n[^\\\\w]\nСимволы, не образующие «слово»\n\n\n\\s\n[ \\t\\v\\r\\n\\f]\nПробельный символ\n\n\n\\S\n[^\\\\s]\nНепробельный символ\n\n\n\n\ngsub( \"\\\\d\",  \"\", vec) # вторая косая черта \"экранирует\" первую\n\n[1] \"жираф\"   \"верблюд\" \"зебра\"  \n\n\nВнутри квадратных скобор знак ^ означает отрицание:\n\ngsub( \"[^[:digit:]]\",  \"\", vec) \n\n[1] \"\"  \"1\" \"0\"\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, в которых за w следует согласный. Замените всю пунктуацию в строке “tomorrow?and-tomorrow_and!tomorrow” на пробелы.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#якоря",
    "href": "regex.html#якоря",
    "title": "7  Регулярные выражения",
    "section": "7.3 Якоря",
    "text": "7.3 Якоря\nЯкоря позволяют искать последовательности символов в начале или в конце строки. Знак ^ (вне квадратных скобок!) означает начало строки, а знак $ – конец. Мнемоническое правило: First you get the power (^) and then you get the money ($).\n\nvec &lt;- c(\"The spring is a lovely time\", \n         \"Fall is a time of peace\")\ngrepl(\"time$\", vec)\n\n[1]  TRUE FALSE\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, которые заканчиваются на x. Найдите все слова, которые начинаются на b или на g.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#метасимволы",
    "href": "regex.html#метасимволы",
    "title": "7  Регулярные выражения",
    "section": "7.4 Метасимволы",
    "text": "7.4 Метасимволы\nВсе метасимволы представлены в таблице ниже.\n\n\n\nОписание\nСимвол\n\n\n\n\nоткрывающая квадратная скобка\n[\n\n\nзакрывающая квадратная скобка\n]\n\n\nобратная косая черта\n\\\n\n\nкарет\n^\n\n\nзнак доллара\n$\n\n\nточка\n.\n\n\nвертикальная черта\n|\n\n\nзнак вопроса\n?\n\n\nастериск\n*\n\n\nплюс\n+\n\n\nоткрывающая фигурная скобка\n{\n\n\nзакрывающая фигурная скобка\n}\n\n\nоткрывающая круглая скобка\n(\n\n\nзакрывающая круглая скобка\n)\n\n\n\nКвадратные скобки используются для создания классов, карет и знак доллара – это якоря, но карет внутри квадратных скобок может также быть отрицанием. Точка – это любой знак.\n\nvec &lt;- c(\"жираф\", \"верблюд1\", \"0зебра\")\ngrep(\".б\", vec) \n\n[1] 2 3\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, в которых есть любые два символа между b и k.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#экранирование",
    "href": "regex.html#экранирование",
    "title": "7  Регулярные выражения",
    "section": "7.5 Экранирование",
    "text": "7.5 Экранирование\nЕсли необходимо найти буквальную точку, буквальный знак вопроса и т.п., то используется экранирование: перед знаком ставится косая черта. Но так как сама косая черта – это метасимвол, но нужно две косые черты, первая из которых экранирует вторую.\n\nvec &lt;- c(\"жираф?\", \"верблюд.\", \"зебра\")\ngrep(\"\\\\?\", vec) \n\n[1] 1\n\ngrepl(\"\\\\.\", vec)\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУзнайте, все ли предложения в sentences (входит в stringr) кончаются на точку.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#квантификация",
    "href": "regex.html#квантификация",
    "title": "7  Регулярные выражения",
    "section": "7.6 Квантификация",
    "text": "7.6 Квантификация\nКвантификатор после символа, символьного класса или группы определяет, сколько раз предшествующее выражение может встречаться. Квантификатор может относиться более чем к одному символу в регулярном выражении, только если это символьный класс или группа.\n\n\n\nПредставление\nЧисло повторений\nЭквивалент\n\n\n\n\n?\nНоль или одно\n{0,1}\n\n\n*\nНоль или более\n{0,}\n\n\n+\nОдно или более\n{1,}\n\n\n\nПример:\n\nvec &lt;- c(\"color\", \"colour\", \"colouur\")\ngrepl(\"ou?r\", vec) # ноль или одно \n\n[1]  TRUE  TRUE FALSE\n\ngrepl(\"ou+r\", vec) # одно или больше\n\n[1] FALSE  TRUE  TRUE\n\ngrepl(\"ou*r\", vec) # ноль или больше\n\n[1] TRUE TRUE TRUE\n\n\nТочное число повторений (интервал) можно задать в фигурных скобках:\n\n\n\nПредставление\nЧисло повторений\n\n\n\n\n{n}\nРовно n раз\n\n\n{m,n}\nОт m до n включительно\n\n\n{m,}\nНе менее m\n\n\n{,n}\nНе более n\n\n\n\n\nvec &lt;- c(\"color\", \"colour\", \"colouur\", \"colouuuur\")\ngrepl(\"ou{1}r\", vec)\n\n[1] FALSE  TRUE FALSE FALSE\n\ngrepl(\"ou{1,2}r\", vec)\n\n[1] FALSE  TRUE  TRUE FALSE\n\ngrepl(\"ou{,2}r\", vec) # это включает и ноль!\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nЧасто используется последовательность .* для обозначения любого количества любых символов между двумя частями регулярного выражения.\n\n\n\n\n\n\nЗадание\n\n\n\nУзнайте, в каких предложениях в sentences за пробелом следует ровно три согласных.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#жадная-и-ленивая-квантификация",
    "href": "regex.html#жадная-и-ленивая-квантификация",
    "title": "7  Регулярные выражения",
    "section": "7.7 Жадная и ленивая квантификация",
    "text": "7.7 Жадная и ленивая квантификация\nВ регулярных выражениях квантификаторам соответствует максимально длинная строка из возможных (квантификаторы являются жадными, англ. greedy). Это может оказаться значительной проблемой. Например, часто ожидают, что выражение &lt;.*&gt; найдёт в тексте теги HTML. Однако если в тексте есть более одного HTML-тега, то этому выражению соответствует целиком строка, содержащая множество тегов.\n\nvec &lt;- c(\"&lt;p&gt;&lt;b&gt;Википедия&lt;/b&gt; — свободная энциклопедия, в которой &lt;i&gt;каждый&lt;/i&gt; может изменить или дополнить любую статью.&lt;/p&gt;\")\ngsub(\"&lt;.*&gt;\", \"\", vec) # все исчезло!\n\n[1] \"\"\n\n\nЧтобы этого избежать, надо поставить после квантификатора знак вопроса. Это сделает его ленивым.\n\n\n\nregex\nзначение\n\n\n\n\n??\n0 или 1, лучше 0\n\n\n*?\n0 или больше, как можно меньше\n\n\n+?\n1 или больше, как можно меньше\n\n\n{n,m}?\nот n до m, как можно меньше\n\n\n\nПример:\n\ngsub(\"&lt;.*?&gt;\", \"\", vec) # все получилось!\n\n[1] \"Википедия — свободная энциклопедия, в которой каждый может изменить или дополнить любую статью.\"\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nДана строка “tomorrow (and) tomorrow (and) tomorrow”. Необходимо удалить первые скобки с их содержанием.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#regex-в-stringr-основы",
    "href": "regex.html#regex-в-stringr-основы",
    "title": "7  Регулярные выражения",
    "section": "7.8 Regex в stringr: основы",
    "text": "7.8 Regex в stringr: основы\nПакет stringr является частью tidyverse1:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nЭто очень удобный инструмент для работы со строками. Вот так можно узнать длину строки или объединить ее с другими строками:\n\nvec &lt;- c(\"жираф\", \"верблюд\")\nstr_length(vec)\n\n[1] 5 7\n\nstr_c(\"красивый_\", vec)\n\n[1] \"красивый_жираф\"   \"красивый_верблюд\"\n\n\nЭлементы вектора можно объединить в одну строку:\n\nstr_c(vec, collapse = \", \") # теперь у них общие кавычки\n\n[1] \"жираф, верблюд\"\n\n\nС помощью str_sub() и str_sub_all() можно выбрать часть строки2.\n\nvec &lt;- c(\"жираф\", \"верблюд\")\nstr_sub(vec, 1, 3)\n\n[1] \"жир\" \"вер\"\n\nstr_sub(vec, 1, -2)\n\n[1] \"жира\"   \"верблю\"\n\n\nФункции ниже меняют начертание с прописного на строчное или наоборот:\n\nVEC &lt;- str_to_upper(vec)\nVEC\n\n[1] \"ЖИРАФ\"   \"ВЕРБЛЮД\"\n\nstr_to_lower(VEC)\n\n[1] \"жираф\"   \"верблюд\"\n\nstr_to_title(vec)\n\n[1] \"Жираф\"   \"Верблюд\"\n\n\nОдна из полезнейших функций в этом пакете – str_view(); она помогает увидеть, что поймало регулярное выражение – до того, как вы внесете какие-то изменения в строку.\n\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n\n[2] │ &lt;a.c&gt;\n\n\nНапример, с помощью этой функции можно убедиться, что вертикальная черта выступает как логический оператор “или”:\n\nstr_view(c(\"grey\", \"gray\"), \"gr(e|a)y\")\n\n[1] │ &lt;grey&gt;\n[2] │ &lt;gray&gt;\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСоздайте тиббл с двумя столбцами: letters и numbers (1:26). Преобразуйте, чтобы в третьем столбце появился результат соединения первых двух через подчеркивание, например a_1. Отфильтруйте, чтобы остались только ряды, где есть цифра 3 или буква x.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_detect-и-str_count",
    "href": "regex.html#str_detect-и-str_count",
    "title": "7  Регулярные выражения",
    "section": "7.9 str_detect() и str_count()",
    "text": "7.9 str_detect() и str_count()\nАналогом grepl() в stringr является функция str_detect()\n\nlibrary(rcorpora)\ndata(\"fruit\")\nhead(fruit)\n\n[1] \"apple\"       \"apricot\"     \"avocado\"     \"banana\"      \"bell pepper\"\n[6] \"bilberry\"   \n\nstr_detect(head(fruit), \"[aeiou]$\")\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n\n# какая доля слов заканчивается на гласный?\nmean(str_detect(fruit, \"[aeiou]$\"))\n\n[1] 0.35\n\n# сколько всего слов заканчивается на гласный?\nsum(str_detect(fruit, \"[aeiou]$\"))\n\n[1] 28\n\n\nОтрицание можно задать двумя способами:\n\ndata(\"words\")\n\nno_vowels1 &lt;- !str_detect(words, \"[aeiou]\") # слова без гласных\n\nno_vowels2 &lt;- str_detect(words, \"^[^aeiou]+$\") # слова без гласных\n\nsum(no_vowels1 != no_vowels2)\n\n[1] 0\n\n\nЛогический вектор можно использовать для индексирования:\n\nwords[!str_detect(words, \"[aeiou]\")]\n\n[1] \"by\"  \"dry\" \"fly\" \"mrs\" \"try\" \"why\"\n\n\nЭту функцию можно применять вместе с функцией filter() из пакета dplyr:\n\nlibrary(dplyr)\ngods &lt;- corpora(which = \"mythology/greek_gods\")\n\ndf &lt;- tibble(god = as.character(gods$greek_gods), \n             i = seq_along(god)\n             )\n\ndf |&gt; \n  filter(str_detect(god, \"s$\"))\n\n\n  \n\n\n\nВариацией этой функции является str_count():\n\nstr_count(as.character(gods$greek_gods), \"[Aa]\")\n\n [1] 1 1 1 1 2 0 0 1 1 1 0 1 0 0 1 2 1 0 0 0 0 1 2 1 0 2 3 2 1 0 0\n\n\nЭту функцию удобно использовать вместе с mutate() из dplyr:\n\ndf |&gt; \n  mutate(\n    vowels = str_count(god, \"[AEIOYaeiou]\"),\n    consonants = str_count(god, \"[^AEIOYaeiou]\")\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПреобразуйте sentences из пакета stringr в тиббл; в новом столбце сохраните количество пробелов в каждом предложении.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_extract-str_subset-и-str_match",
    "href": "regex.html#str_extract-str_subset-и-str_match",
    "title": "7  Регулярные выражения",
    "section": "7.10 str_extract(), str_subset() и str_match()",
    "text": "7.10 str_extract(), str_subset() и str_match()\nФункция str_extract() извлекает совпадения3.\nСначала зададим паттерн для поиска.\n\ncolours &lt;- c(\" red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\")\ncolour_match &lt;- str_c(colours, collapse = \"|\")\ncolour_match\n\n[1] \" red|orange|yellow|green|blue|purple\"\n\n\nИ применим к предложениями. Используем str_extract_all(), т.к. str_extract() возвращает только первое вхождение.\n\nhas_colour &lt;- str_subset(sentences, colour_match)\nmatches &lt;- str_extract_all(has_colour, colour_match)\nhead(unlist(matches))\n\n[1] \"blue\"   \"blue\"   \"blue\"   \"yellow\" \"green\"  \" red\"  \n\n\nКруглые скобки используются для группировки. Например, мы можем задать шаблон для поиска существительного или прилагательного с артиклем.\n\nnoun &lt;- \"(a|the) ([^ ]+)\" # как минимум один непробельный символ после пробела\n\nhas_noun &lt;- sentences |&gt;\n  str_subset(noun) |&gt;\n  head(10)\nhas_noun\n\n [1] \"The birch canoe slid on the smooth planks.\"       \n [2] \"Glue the sheet to the dark blue background.\"      \n [3] \"It's easy to tell the depth of a well.\"           \n [4] \"These days a chicken leg is a rare dish.\"         \n [5] \"The box was thrown beside the parked truck.\"      \n [6] \"The boy was there when the sun rose.\"             \n [7] \"The source of the huge river is the clear spring.\"\n [8] \"Kick the ball straight and follow through.\"       \n [9] \"Help the woman get back to her feet.\"             \n[10] \"A pot of tea helps to pass the evening.\"          \n\n\nДальше можно воспользоваться уже известной функцией str_extract() или применить str_match. Результат будет немного отличаться: вторая функция вернет матрицу, в которой хранится не только сочетание слов, но и каждый компонент отдельно.\n\nhas_noun |&gt; \n  str_extract(noun)\n\n [1] \"the smooth\" \"the sheet\"  \"the depth\"  \"a chicken\"  \"the parked\"\n [6] \"the sun\"    \"the huge\"   \"the ball\"   \"the woman\"  \"a helps\"   \n\nhas_noun |&gt; \n  str_match(noun)\n\n      [,1]         [,2]  [,3]     \n [1,] \"the smooth\" \"the\" \"smooth\" \n [2,] \"the sheet\"  \"the\" \"sheet\"  \n [3,] \"the depth\"  \"the\" \"depth\"  \n [4,] \"a chicken\"  \"a\"   \"chicken\"\n [5,] \"the parked\" \"the\" \"parked\" \n [6,] \"the sun\"    \"the\" \"sun\"    \n [7,] \"the huge\"   \"the\" \"huge\"   \n [8,] \"the ball\"   \"the\" \"ball\"   \n [9,] \"the woman\"  \"the\" \"woman\"  \n[10,] \"a helps\"    \"a\"   \"helps\"  \n\n\nФункция tidyr::extract() работает похожим образом, но требует дать имена для каждого элемента группы. Этим удобно пользоваться, если ваши данные хранятся в виде тиббла.\n\ntibble(sentence = sentences) |&gt; \n  tidyr::extract(\n    sentence, c(\"article\", \"noun\"), \"(a|the) ([^ ]+)\", \n    remove = FALSE\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите в sentences все предложения, где есть to, и выберите следующее за этим слово. Переведите в нижний регистр. Узнайте, сколько всего уникальных сочетаний.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_replace",
    "href": "regex.html#str_replace",
    "title": "7  Регулярные выражения",
    "section": "7.11 str_replace",
    "text": "7.11 str_replace\nФункции str_replace() и str_replace_all() позволяют заменять совпадения на новые символы.\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_replace(x, \"[aeiou]\", \"-\")\n\n[1] \"-pple\"  \"p-ar\"   \"b-nana\"\n\nstr_replace_all(x, \"[aeiou]\", \"-\")\n\n[1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\n\nЭтим можно воспользоваться, если вы хотите, например, удалить из текста все греческие символы. Для стандартного греческого алфавита хватит [Α-Ωα-ω], но для древнегреческого этого, например, не хватит. Попробуем на отрывке из письма Цицерона Аттику, которое содержит греческий текст.\n\ncicero &lt;- \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀπεραντολογίας ἀηδοῦς! \"\n\nstr_replace_all(cicero, \"[Α-Ωα-ω]\", \"\")\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀί ἀῦ! \"\n\n\nὢ ἀί ἀῦ! Не все у нас получилось гладко. Попробуем иначе:\n\nstr_replace_all(cicero, \"[\\u0370-\\u03FF]\", \"\")\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀ ἀῦ! \"\n\n\nУдалилась (буквально была заменена на пустое место) та диакритика, которая есть в новогреческом (ί). Но остались еще буквы со сложной диакритикой, которой современные греки не пользуются.\n\nno_greek &lt;- str_replace_all(cicero, \"[[\\u0370-\\u03FF][\\U1F00-\\U1FFF]]\", \"\")\nno_greek\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius.   ! \"\n\n\n! Мы молодцы. Избавились от этого непонятного греческого.\nНа самом деле, конечно, str_replace хорош тем, что он позволяет производить осмысленные замены. Например, мы можем в оставшемся латинском текст заменить гласные с макроном (черточка, означающая долготу) на обычные гласные.\n\nstr_replace_all(no_greek, c(\"ā\" = \"a\", \"ū\" = \"u\", \"ī\" = \"i\", \"ō\" = \"o\"))\n\n[1] \"nihil hac solitudine iucundius, nisi paulum interpellasset Amyntae filius.   ! \"\n\n\nКрасота. О более сложных заменах с перемещением групп можно посмотреть видео здесь и здесь. Это помогает даже в таком скорбном деле, как переоформление библиографии.\n\n\n\n\n\n\nЗадание\n\n\n\nДана библиографическая запись:\nAst, Friedrich. 1816. Platon’s Leben und Schriften. Leipzig, Weidmann.\nИспользуя регулярные выражения, замените полное имя на инициал. Запятую перед инициалом удалите. Уберите название издательства. Год поставьте в круглые скобки.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_split",
    "href": "regex.html#str_split",
    "title": "7  Регулярные выражения",
    "section": "7.12 str_split",
    "text": "7.12 str_split\nФункция str_split() помогает разбить текст на предложения, слова или просто на бессмысленные наборы символов. Это важный этап подготовки текста для анализа, и проводится он нередко именно с применением регулярных выражений.\n\nsentences |&gt;\n  head(2) |&gt; \n  str_split(\" \")\n\n[[1]]\n[1] \"The\"     \"birch\"   \"canoe\"   \"slid\"    \"on\"      \"the\"     \"smooth\" \n[8] \"planks.\"\n\n[[2]]\n[1] \"Glue\"        \"the\"         \"sheet\"       \"to\"          \"the\"        \n[6] \"dark\"        \"blue\"        \"background.\"\n\n\nНо можно обойтись и без регулярных выражений.\n\nx &lt;- \"This is a sentence.  This is another sentence.\"\nstr_view_all(x, boundary(\"word\"))\n\n[1] │ &lt;This&gt; &lt;is&gt; &lt;a&gt; &lt;sentence&gt;.  &lt;This&gt; &lt;is&gt; &lt;another&gt; &lt;sentence&gt;.\n\nstr_view_all(x, boundary(\"sentence\"))\n\n[1] │ &lt;This is a sentence.  &gt;&lt;This is another sentence.&gt;\n\n\nОчень удобно, но убедитесь, что в вашем языке границы слов и предложения выглядят как у людей. С древнегреческим эта штука не справится (как делить на предложения греческие и латинские тексты, я рассказывала здесь):\n\napology &lt;- c(\"νῦν δ' ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ' ὅς. “Τίς,” ἦν δ' ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;\")\n\nstr_view_all(apology, boundary(\"sentence\"))\n\n[1] │ &lt;νῦν δ' ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ' ὅς. &gt;&lt;“Τίς,” ἦν δ' ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&gt;\n\n\nПолный крах 💩",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#footnotes",
    "href": "regex.html#footnotes",
    "title": "7  Регулярные выражения",
    "section": "",
    "text": "https://r4ds.had.co.nz/strings.html↩︎\nhttps://stringr.tidyverse.org/reference/str_sub.html↩︎\nhttps://r4ds.had.co.nz/strings.html#extract-matches↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "scrape.html",
    "href": "scrape.html",
    "title": "8  Веб-скрапинг",
    "section": "",
    "text": "8.1 Структура html\nДокументы html (HyperText Markup Language) имеют иерархическую структуру, состоящую из элементов. В каждом элементе есть открывающий тег (&lt;tag&gt;), опциональные атрибуты (id='first') и закрывающий тег (&lt;/tag&gt;). Все, что находится между открывающим и закрывающим тегом, называется содержанием элемента.\nВажнейшие теги, о которых стоит знать:\nЧтобы увидеть структуру веб-страницы, надо нажать правую кнопку мыши и выбрать View Source (это работает и для тех html, которые хранятся у вас на компьютере).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#структура-html",
    "href": "scrape.html#структура-html",
    "title": "8  Веб-скрапинг",
    "section": "",
    "text": "&lt;html&gt; (есть всегда), с двумя детьми (дочерними элементами): &lt;head&gt; и &lt;body&gt;;\nэлементы, отвечающие за структуру: &lt;h1&gt; (заголовок), &lt;section&gt;, &lt;p&gt; (параграф), &lt;ol&gt; (упорядоченный список);\nэлементы, отвечающие за оформление: &lt;b&gt; (bold), &lt;i&gt; (italics), &lt;a&gt; (ссылка).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#каскадные-таблицы-стилей",
    "href": "scrape.html#каскадные-таблицы-стилей",
    "title": "8  Веб-скрапинг",
    "section": "8.2 Каскадные таблицы стилей",
    "text": "8.2 Каскадные таблицы стилей\nУ тегов могут быть именованные атрибуты; важнейшие из них – это id и class, которые в сочетании с CSS контролируют внешний вид страницы.\n\n\n\n\n\n\nНа заметку\n\n\n\nCSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML).\n\n\nПример css-правила (такие инфобоксы использованы в предыдущей версии курса):\n\n.infobox {\n  padding: 1em 1em 1em 4em;\n  background: aliceblue 5px center/3em no-repeat;\n  color: black;\n}\n\n\nПроще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — селектор и блок объявлений. Селектор, расположенный в левой части правила до знака {, определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;».\nСелекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так:\n\np выберет все элементы &lt;p&gt;\n.title выберет элементы с классом “title”\n#title выберет все элементы с атрибутом id=‘title’\n\nВажно: если изменится структура страницы, откуда вы скрапили информацию, то и код придется переписывать.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#чтение-html",
    "href": "scrape.html#чтение-html",
    "title": "8  Веб-скрапинг",
    "section": "8.3 Чтение html",
    "text": "8.3 Чтение html\nЧтобы прочесть файл html, используем одноименную функцию.\n\nlibrary(rvest)\nantibarbari_files &lt;- list.files(\"../files/antibarbari_2024-08-18\", pattern = \"html\", full.names = TRUE)\n\nИспользуем пакет purrr, чтобы прочитать сразу три файла из архива.\n\nlibrary(tidyverse)\nantibarbari_archive &lt;- map(antibarbari_files, read_html)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#парсинг-html-отдельные-элементы",
    "href": "scrape.html#парсинг-html-отдельные-элементы",
    "title": "8  Веб-скрапинг",
    "section": "8.4 Парсинг html: отдельные элементы",
    "text": "8.4 Парсинг html: отдельные элементы\nНа следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельный html; rvest позволяет это сделать (но внутри двойных кавычек должны быть только одинарные):\n\nexample_html &lt;-  minimal_html(\"\n&lt;div class='message default clearfix' id='message83'&gt;\n      &lt;div class='pull_left userpic_wrap'&gt;\n       &lt;div class='userpic userpic2' style='width: 42px; height: 42px'&gt;\n        &lt;div class='initials' style='line-height: 42px'&gt;\nA\n        &lt;/div&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class='body'&gt;\n       &lt;div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'&gt;\n11:18\n       &lt;/div&gt;\n       &lt;div class='from_name'&gt;\nAntibarbari HSE \n       &lt;/div&gt;\n       &lt;div class='text'&gt;\nЭтот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. &lt;br&gt;&lt;br&gt;«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).&lt;br&gt;&lt;a href='http://antibarbari.ru/2022/05/19/digest_1/'&gt;http://antibarbari.ru/2022/05/19/digest_1/&lt;/a&gt;\n       &lt;/div&gt;\n       &lt;div class='signature details'&gt;\nOlga Alieva\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n\")\n\nИз всего этого мне может быть интересно id сообщения (\\&lt;div class='message default clearfix' id='message83'\\&gt;), текст сообщения (\\&lt;div class='text'\\&gt;), дата публикации (\\&lt;div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'\\&gt;), а также, если указан, автор сообщения (\\&lt;div class='signature details'\\&gt;). Извлекаем текст (для этого рекомендуется использовать функцию html_text2()):\n\nexample_html |&gt;\n  html_element(\".text\") |&gt; \n  html_text2()\n\n[1] \"Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора.\\n\\n«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).\\nhttp://antibarbari.ru/2022/05/19/digest_1/\"\n\n\nВ классе signature details есть пробел, достаточно на его месте поставить точку:\n\nexample_html |&gt;\n  html_element(\".signature.details\") |&gt; \n  html_text2()\n\n[1] \"Olga Alieva\"\n\n\nОсталось добыть дату и message id:\n\nexample_html |&gt; \n  html_element(\".pull_right.date.details\") |&gt; \n  html_attr(\"title\")\n\n[1] \"19.05.2022 11:18:07 UTC+03:00\"\n\n\n\nexample_html |&gt;\n  html_element(\"div\") |&gt; \n  html_attr(\"id\")\n\n[1] \"message83\"\n\n\nТеперь мы можем сохранить все нужные нам данные в таблицу.\n\ntibble(id = example_html |&gt; \n         html_element(\"div\") |&gt; \n         html_attr(\"id\"),\n       date = example_html |&gt; \n         html_element(\".pull_right.date.details\") |&gt; \n         html_attr(\"title\"),\n       signature = example_html |&gt;\n         html_element(\".signature.details\") |&gt; \n         html_text2(),\n       text = example_html |&gt; \n         html_element(\".text\") |&gt;\n         html_text2()\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#парсинг-html-вложенные-элементы",
    "href": "scrape.html#парсинг-html-вложенные-элементы",
    "title": "8  Веб-скрапинг",
    "section": "8.5 Парсинг html: вложенные элементы",
    "text": "8.5 Парсинг html: вложенные элементы\nДо сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, и нам надо извлечь только такие div, которым соответствует определенный класс. Также не будем забывать, что архив выгрузился в виде трех html-файлов, так что понадобится наше знание итераций в purrr. Пока пробуем на одном из них:\n\narchive_1 &lt;- antibarbari_archive[[1]]\n\narchive_1 |&gt;\n  html_elements(\"div.message.default\") |&gt; \n  head()\n\n{xml_nodeset (6)}\n[1] &lt;div class=\"message default clearfix\" id=\"message3\"&gt;\\n\\n      &lt;div class= ...\n[2] &lt;div class=\"message default clearfix\" id=\"message5\"&gt;\\n\\n      &lt;div class= ...\n[3] &lt;div class=\"message default clearfix\" id=\"message6\"&gt;\\n\\n      &lt;div class= ...\n[4] &lt;div class=\"message default clearfix\" id=\"message7\"&gt;\\n\\n      &lt;div class= ...\n[5] &lt;div class=\"message default clearfix\" id=\"message8\"&gt;\\n\\n      &lt;div class= ...\n[6] &lt;div class=\"message default clearfix\" id=\"message9\"&gt;\\n\\n      &lt;div class= ...\n\n\nУже из этого набора узлов можем доставать все остальное.\n\narchive_1_tbl &lt;- tibble(id = archive_1 |&gt; \n         html_elements(\"div.message.default\") |&gt; \n         html_attr(\"id\"),\n       date = archive_1 |&gt; \n         html_elements(\"div.message.default\") |&gt; \n         html_element(\".pull_right.date.details\") |&gt; \n         html_attr(\"title\"),\n       signature = archive_1 |&gt;\n         html_elements(\"div.message.default\") |&gt; \n         html_element(\".signature.details\") |&gt; \n         html_text2(),\n       text = archive_1 |&gt; \n         html_elements(\"div.message.default\") |&gt; \n         html_element(\".text\") |&gt;\n         html_text2()\n)\n\narchive_1_tbl\n\n\n  \n\n\n\nОбратите внимание, что мы сначала извлекаем нужные элементы при помощи html_elements(), а потом применяем к каждому из них html_element(). Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений, т.к. функция html_element(), если она не может найти, например, подпись, возвращает NA.\nКак вы уже поняли, теперь нам надо проделать то же самое для двух других файлов из архива антиварваров, а значит пришло время превратить наш код в функцию.\n\nscrape_antibarbari &lt;- function(html_file){\n  messages_tbl &lt;- tibble(\n    id = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_attr(\"id\"),\n    date = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".pull_right.date.details\") |&gt;\n      html_attr(\"title\"),\n    signature = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".signature.details\") |&gt;\n      html_text2(),\n    text = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".text\") |&gt;\n      html_text2()\n  )\n  messages_tbl\n}\n\n\nmessages_tbl &lt;- map_df(antibarbari_archive, scrape_antibarbari)\n\nВот что у нас получилось.\n\nmessages_tbl",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#разведывательный-анализ",
    "href": "scrape.html#разведывательный-анализ",
    "title": "8  Веб-скрапинг",
    "section": "8.6 Разведывательный анализ",
    "text": "8.6 Разведывательный анализ\nСоздатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. Кроме того, в некоторых постах только фото, для них в столбце text – NA, их можно сразу отсеять.\n\nmessages_tbl &lt;- messages_tbl |&gt;\n  filter(!is.na(text))\n\nmessages_tbl\n\n\n  \n\n\n\nТакже преобразуем столбец, в котором хранится дата и время. Разделим его на два и выясним, в какое время и день недели чаще всего публикуются сообщения.\n\n\n\n\n\n\nЗадание\n\n\n\nИз курса Getting and Cleaning Data в swirl будет полезно пройти урок Dates and Times with lubridate.\n\n\n\nmessages_tbl2 &lt;- messages_tbl |&gt; \n  separate(date, into = c(\"date\", \"time\", NA), sep = \" \") |&gt; \n  mutate(date = dmy(date), \n         time = hms(time)) |&gt; \n  mutate(year = year(date), \n        month = month(date, label = TRUE),\n        wday = wday(date, label = TRUE),\n        hour = hour(time),\n        length = str_count(text, \" \") + 1) |&gt; \n  mutate(wday = factor(wday, levels = c(\"Sun\", \"Sat\", \"Fri\", \"Thu\", \"Wed\", \"Tue\", \"Mon\")))\n\n\nmessages_tbl2\n\n\n  \n\n\n\n\nsummary1 &lt;- messages_tbl2 |&gt; \n  group_by(year, month) |&gt; \n  summarise(n = n()) \n\nsummary1\n\n\n  \n\n\nsummary2 &lt;- messages_tbl2 |&gt; \n  group_by(year, hour) |&gt; \n  summarise(n = n()) |&gt; \n  mutate(hour = case_when(hour == 0 ~ 24,\n                          .default = hour))\n\nsummary2\n\n\n  \n\n\nsummary3 &lt;- messages_tbl2 |&gt; \n   group_by(wday) |&gt; \n   summarise(n = n())\n\nsummary3\n\n\n  \n\n\n\n\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"khroma::okabeitoblack\")\n\np1 &lt;- summary1 |&gt; \n  ggplot(aes(month, n, color = as.factor(year), group = year)) +\n  geom_line(show.legend = FALSE, linewidth = 1.2, alpha = 0.8) +\n  labs(title = \"Число постов в месяц\") +\n  theme(legend.title = element_blank(), \n        legend.position = c(0.8, 0.3),\n        title = element_text(face=\"italic\")) +\n  labs(x = NULL, y = NULL) +\n  scale_color_manual(values = cols[1:3])\n\n\np2 &lt;- summary2 |&gt; \n  ggplot(aes(hour, n, color = as.factor(year), group = year)) + \n  geom_line(linewidth = 1.2, alpha = 0.8) +\n  scale_x_continuous(breaks = seq(1,24,1)) +\n  labs(x = NULL, y = NULL, title = \"Время публикации поста\") + \n  theme(legend.title = element_blank(), \n        legend.position = \"left\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        title = element_text(face=\"italic\")\n        ) +\n  coord_polar(start = 0) +\n  scale_color_manual(values = cols[1:3])\n\n\np3 &lt;- summary3 |&gt; \n  ggplot(aes(wday, n, fill = wday)) + \n  geom_bar(stat = \"identity\", \n           show.legend = FALSE) + \n  coord_flip() + \n  labs(x = NULL, y = NULL, title  = \"Публикации по дням недели\") +\n  scale_fill_manual(values = cols) +\n  theme(title = element_text(face=\"italic\"))\n\n\np4 &lt;- messages_tbl2 |&gt; \n  ggplot(aes(as.factor(year), length, fill = as.factor(year))) +\n  geom_boxplot(show.legend = FALSE) +\n  labs(title = \"Длина поста по годам\") + \n  labs(x = NULL, y = NULL) + \n  scale_fill_manual(values = cols[1:3]) + \n  theme(title = element_text(face=\"italic\"))\n\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2,\n             top =  textGrob(\"Телеграм-канал Antibarbari HSE\",\n                    gp=gpar(fontsize=16)),\n             bottom = textGrob(\"@Rantiquity\",\n                    gp = gpar(fontface = 3, fontsize = 9), hjust = 1, x = 1))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#html-таблицы",
    "href": "scrape.html#html-таблицы",
    "title": "8  Веб-скрапинг",
    "section": "8.7 Html таблицы",
    "text": "8.7 Html таблицы\nЕсли вам повезет, то ваши данные уже будут храниться в HTML-таблице, и их можно будет просто считать из этой таблицы. Распознать таблицу в браузере обычно несложно: она имеет прямоугольную структуру из строк и столбцов, и ее можно скопировать и вставить в такой инструмент, как Excel.\nТаблицы HTML строятся из четырех основных элементов: &lt;table&gt;, &lt;tr&gt; (строка таблицы), &lt;th&gt; (заголовок таблицы) и &lt;td&gt; (данные таблицы). Мы достанем программу курса “Количественные методы в гуманитарных науках: критическое введение” (2023/2024).\n\nhtml &lt;- read_html(\"http://criticaldh.ru/program/\")\nmy_table &lt;- html |&gt;  \n  html_table() |&gt; \n  pluck(1)\n\nmy_table\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nС сайта Новой философской энциклопедии извлеките список слов на букву П. Используйте map_df() для объединения таблиц.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько всего слов на букву П в НФЭ?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#selector-gadget",
    "href": "scrape.html#selector-gadget",
    "title": "8  Веб-скрапинг",
    "section": "8.8 Selector Gadget",
    "text": "8.8 Selector Gadget\nМногие тексты доступны на сайте &lt;wikisource.org&gt;. Попробуем извлечь латинский текст “Записок о Галльской войне” Цезаря: он пригодится нам в следующем уроке.\n\nurl &lt;- \"https://la.wikisource.org/wiki/Commentarii_de_bello_Gallico\"\nhtml = read_html(url)\n\nДля того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот тут можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни.\n\ntoc &lt;- html |&gt; \n  html_elements(\"td, #toc a\")\n\ntoc\n\n{xml_nodeset (11)}\n [1] &lt;td class=\"fr-text\" style=\"vertical-align: middle;\"&gt;Accuracy&lt;/td&gt;\\n\n [2] &lt;td class=\"fr-value40\" style=\"vertical-align: middle;\"&gt;Spot checked&lt;/td&gt;\n [3] &lt;td align=\"center\" style=\"background: #efefef\"&gt;\\n&lt;a href=\"/wiki/Commenta ...\n [4] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_I\" title=\"Commentarii  ...\n [5] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_II\" title=\"Commentarii ...\n [6] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_III\" title=\"Commentari ...\n [7] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_IV\" title=\"Commentarii ...\n [8] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_V\" title=\"Commentarii  ...\n [9] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VI\" title=\"Commentarii ...\n[10] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VII\" title=\"Commentari ...\n[11] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VIII\" title=\"Commentar ...\n\n\nИзвлекаем путь и имя файла для web-страниц.\n\nlibri &lt;- tibble(\n  title = toc |&gt;\n    html_attr(\"title\"),\n  href = toc |&gt; \n    html_attr(\"href\")\n) |&gt; \n  filter(!is.na(title))\n\nlibri\n\n\n  \n\n\n\nТеперь добавляем протокол доступа и доменное имя для каждой страницы.\n\nlibri &lt;- libri |&gt; \n  mutate(link = paste0(\"https://la.wikisource.org\", href)) |&gt; \n  select(-href)\n\nlibri\n\n\n  \n\n\n\nДальше необходимо достать текст для каждой книги. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила.\n\nurls &lt;- libri |&gt; \n  pull(link)\n\ntext &lt;- read_html(urls[1]) |&gt; \n  html_elements(\".mw-heading3+ p\") |&gt; \n  html_text2() \n\ntext[1]\n\n[1] \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt. Gallos ab Aquitanis Garumna flumen, a Belgis Matrona et Sequana dividit. Horum omnium fortissimi sunt Belgae, propterea quod a cultu atque humanitate provinciae longissime absunt, minimeque ad eos mercatores saepe commeant atque ea quae ad effeminandos animos pertinent important, proximique sunt Germanis, qui trans Rhenum incolunt, quibuscum continenter bellum gerunt. Qua de causa Helvetii quoque reliquos Gallos virtute praecedunt, quod fere cotidianis proeliis cum Germanis contendunt, cum aut suis finibus eos prohibent aut ipsi in eorum finibus bellum gerunt. Eorum una pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones. Belgae ab extremis Galliae finibus oriuntur, pertinent ad inferiorem partem fluminis Rheni, spectant in septentrionem et orientem solem. Aquitania a Garumna flumine ad Pyrenaeos montes et eam partem Oceani quae est ad Hispaniam pertinet; spectat inter occasum solis et septentriones.\"\n\n\nУбедившись, что параграфы извлечены верно, обобщаем: пишем функцию для извлечения текстов и применяем ее ко всем книгам.\n\nget_text &lt;- function(url) {\n  # Sys.sleep(1)\n  read_html(url) |&gt; \n  html_elements(\".mw-heading3+ p\") |&gt; \n  html_text2() |&gt; \n  paste(collapse= \" \")\n}\n\nЭто займет некоторое время.\n\nlibri_text &lt;- map(urls, get_text)\n\nСоединим две таблицы.\n\nlibri_text &lt;- libri_text |&gt;\n  flatten_chr() |&gt; \n  as_tibble()\n\ncaesar &lt;- libri |&gt; \n  bind_cols(libri_text) |&gt; \n  mutate(title = str_remove(title, \"Commentarii de bello Gallico/\"))\n\ncaesar\n\n\n  \n\n\n\nСохраним подготовленный датасет для дальнейшего анализа.\n\nsave(caesar, file = \"../data/caesar.Rdata\")\n\n\nПоздравляем, на этом закончился первый большой раздел нашего курса “Основы работы в R” 🎁. За восемь уроков вы познакомились с основными структурами данных в R, научились собирать и трансформировать данные, строить графики, писать функции и циклы, а также готовить html-отчеты о своих исследованиях. Впереди нас ждут методы анализа текстовых данных.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html",
    "href": "tokenize.html",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "",
    "text": "9.1 Токенизация\nТокенизация — процесс разделения текста на составляющие (их называют «токенами»). Токенами могут быть слова, символьные или словесные энграмы (n-grams), то есть сочетания символов или слов, даже предложения или параграфы.\nТокенизировать можно в базовом R с использованием регулярных выражений, и Jockers (2014) прекрасно показывает, как это можно делать. Но мы воспользуемся двумя пакетами, которые предназначены специально для работы с текстовыми данными и разделяют идеологию tidyverse: tidytext (Silge и Robinson 2017) и tokenizers (Hvitfeldt и Silge 2022).\nlibrary(tidyverse) \nlibrary(tidytext)\nlibrary(tokenizers)\nДля анализа воспользуемся датасетом c латинским текстом “Записок о Галльской войне”, который мы подготовили в предыдущем уроке. Его можно забрать отсюда.\nload(\"../data/caesar.RData\")\ncaesar &lt;- caesar |&gt; \n  rename(text = value) |&gt; \n  select(-link)\n\ncaesar\nФункция unnest_tokens() из пакета tidytext принимает на входе тиббл, название столбца, в котором хранится текст для токенизации, а также название нового столбца, куда будут “сложены” отдельные токены (зачастую это слова, но не обязательно).\nАргумент token принимает следующие значения:\nИспользуя уже знакомую функцию map, можно запустить unnest_tokens() с разными аргументами:\ntest &lt;- tibble(text = \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt.\")\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token,\n  test, \"word\", \"text\", \"words\", \n  test, \"sentence\", \"text\", \"sentences\",\n  test, \"char\", \"text\", \"characters\", \n)\n\nparams\nparams |&gt; \n  pmap(unnest_tokens) \n\n[[1]]\n# A tibble: 29 × 1\n   word    \n   &lt;chr&gt;   \n 1 gallia  \n 2 est     \n 3 omnis   \n 4 divisa  \n 5 in      \n 6 partes  \n 7 tres    \n 8 quarum  \n 9 unam    \n10 incolunt\n# ℹ 19 more rows\n\n[[2]]\n# A tibble: 2 × 1\n  sentence                                                                      \n  &lt;chr&gt;                                                                         \n1 gallia est omnis divisa in partes tres, quarum unam incolunt belgae, aliam aq…\n2 hi omnes lingua, institutis, legibus inter se differunt.                      \n\n[[3]]\n# A tibble: 166 × 1\n   char \n   &lt;chr&gt;\n 1 g    \n 2 a    \n 3 l    \n 4 l    \n 5 i    \n 6 a    \n 7 e    \n 8 s    \n 9 t    \n10 o    \n# ℹ 156 more rows\nСледующие значения аргумента token требуют также аргумента n:\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token, ~n,\n  test, \"ngram\", \"text\", \"ngrams\", 3,\n  test, \"shingles\", \"text\", \"character_shingles\", 3\n)\n\nparams  |&gt; \n  pmap(unnest_tokens)  |&gt; \n  head()\n\n[[1]]\n# A tibble: 27 × 1\n   ngram                \n   &lt;chr&gt;                \n 1 gallia est omnis     \n 2 est omnis divisa     \n 3 omnis divisa in      \n 4 divisa in partes     \n 5 in partes tres       \n 6 partes tres quarum   \n 7 tres quarum unam     \n 8 quarum unam incolunt \n 9 unam incolunt belgae \n10 incolunt belgae aliam\n# ℹ 17 more rows\n\n[[2]]\n# A tibble: 164 × 1\n   shingles\n   &lt;chr&gt;   \n 1 gal     \n 2 all     \n 3 lli     \n 4 lia     \n 5 iae     \n 6 aes     \n 7 est     \n 8 sto     \n 9 tom     \n10 omn     \n# ℹ 154 more rows\nДальше мы будем работать со словами, поэтому сохраним токенизированный текст “Записок” в виде “опрятного” датасета (одно наблюдение - один ряд).\ncaesar_tokens &lt;- caesar |&gt; \n  unnest_tokens(\"word\", \"text\")\n\ncaesar_tokens\nПри работе с данными в текстовом формате unnest_tokens() опирается на пакет tokenizers, из которого в нашем случае подтягивает функцию tokenize_words. У этой функции есть несколько полезных аргументов: strip_non_alphanum (удаляет пробельные символы и пунктуацию), strip_punct (удаляет пунктуацию), strip_numeric (удаляет числа).\nЭти аргументы мы тоже можем задать через unnest_tokens(), поскольку у функции есть аргумент ... (загляните в документацию, чтобы убедиться).\ncaesar |&gt; \n  unnest_tokens(\"word\", \"text\", strip_punct = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#токенизация",
    "href": "tokenize.html#токенизация",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "",
    "text": "unnest_tokens(\n  tbl,\n  output,\n  input,\n  token = \"words\",\n  format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"),\n  to_lower = TRUE,\n  drop = TRUE,\n  collapse = NULL,\n  ...\n)\n\n\n“words” (default),\n“characters”,\n“character_shingles”,\n“ngrams”,\n“skip_ngrams”,\n“sentences”,\n“lines”,\n“paragraphs”,\n“regex”,\n“ptb” (Penn Treebank).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#лемматизация-и-частеречная-разметка",
    "href": "tokenize.html#лемматизация-и-частеречная-разметка",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.2 Лемматизация и частеречная разметка",
    "text": "9.2 Лемматизация и частеречная разметка\nЛемматизация – приведение слов к начальной форме (лемме). Как правило, она проводится одновременно с частеречной разметкой (POS-tagging). Все это умеет делать UDPipe – обучаемый конвейер (trainable pipeline), для которого существует одноименный пакет в R.\nОсновным форматом файла для него является CoNLL-U. Файлы в таком формате хранятся в так называемых трибанках, то есть коллекциях уже размеченных текстов (название объясняется тем, что синтаксическая структура предложений представлена в них в виде древовидных графов). Файлы CoNLL-U используются для обучения нейросетей, но для большинства языков доступны хорошие предобученные модели, работать с которыми достаточно просто.\nПакет udpipe позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных трибанках. Среди этих языков есть и латинский.\nПрежде всего нужно выбрать и загрузить модель (список); в нашем случае это модель Perseus, но можно попробовать и другие доступные на сайте https://universaldependencies.org/.\n\nlibrary(udpipe)\n\n#  скачиваем модель в рабочую директорию\nudpipe_download_model(language = \"latin-perseus\")\n\n# загружаем модель\nlatin_perseus &lt;- udpipe_load_model(file = \"latin-perseus-ud-2.5-191206.udpipe\")\n\n# аннотируем\ncaesar_annotate &lt;- udpipe_annotate(latin_perseus, caesar$text)\n\nРезультат возвращается в формате CoNLL-U; это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. Вот пример разбора предложения:\n\nCтроки слов содержат следующие поля:\n\nID: индекс слова, целое число, начиная с 1 для каждого нового предложения; может быть диапазоном токенов с несколькими словами.\nFORM: словоформа или знак препинания.\nLEMMA: Лемма или основа словоформы.\nUPOSTAG: тег части речи из универсального набора проекта UD, который создавался для того, чтобы аннотации разных языков были сравнимы между собой.\nXPOSTAG: тег части речи, который выбрали исследователи под конкретные нужды языка\nFEATS: список морфологических характеристик.\nHEAD: идентификатор (номер) синтаксической вершины текущего токена. Если такой вершины нет, то ставят ноль (0).\nDEPREL: характер синтаксической зависимости.\nDEPS: Список вторичных зависимостей.\nMISC: любая другая аннотация.\n\nДля работы данные удобнее трансформировать в прямоугольный формат.\n\ncaesar_pos &lt;- as_tibble(caesar_annotate) |&gt; \n  select(-paragraph_id)\n\ncaesar_pos",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#обучение-модели",
    "href": "tokenize.html#обучение-модели",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.3 Обучение модели",
    "text": "9.3 Обучение модели\nМожно заметить, что модель Perseus 2.5 справилась не безупречно: все бельги оказались женского рода, а кельты и вовсе признаны глаголом. Есть ошибки в падежах и числах: например, “provinciae” в четвертом предложении, конечно, не именительный, а родительный падеж. Множество топонимов не опознано в качестве имен собственных.\nЗдесь есть два пути. Первый: пробовать другие модели, доступные в пакете udpipe. Например, для латыни это PROIEl, обученная не только на классических авторах, но и на Вульгате, или ITTB, обученная на сочинениях Фомы. Но если тексты в трибанках не очень похожи на ваш корпус, то это вряд ли сработает.\nВторой путь - обучить модель самостоятельно. Например, для трибанка Perseus доступны более свежие версии (2.13 на момент написания этой главы) на GitHub. Вот некоторые изменения:\n\nпоявилась метка dep_rel для ablativus absolutus (advcl:abs);\nисправлены аннотации для супина (VerbForm=Conv, Aspect=Prosp), а также герундия и герундива (VerbForm=Part, Aspect=Prosp);\nдобавлен тип для местоимения (PronType) и вид для глагола (Aspect) и др.\n\nИнструкцию по обучению модели при помощи udpipe можно найти здесь. Следуя этой инструкции и используя трибанк Perseus 2.13, мы обучили новую модель (это заняло около 8 часов на персональном компьютере), которую можно загрузить и использовать для аннотации.\nНадо иметь в виду, что само по себе обновление трибанка еще не гарантирует того, что модель будет лучше справляться с парсингом: многое зависит от параметров обучения. В нашем случае, впрочем, некоторые улучшения есть: например, “provinciae” корректно опознано как родительный падеж. Но есть и потери: “fortissimi” в том же предложении выше - это nominativus pluralis, который ошибочно опознан как генитив единственного числа.\n\nlatin_perseus_new &lt;- udpipe_load_model(\"../latin_model/la_perseus-2.13-20231115.udpipe\")\n\ncaesar_annotate2 &lt;- udpipe_annotate(latin_perseus_new, caesar$text[1])\n\ncaesar_pos2 &lt;- as_tibble(caesar_annotate2) |&gt; \n  select(-paragraph_id)\n\n\ncaesar_pos2\n\n\n  \n\n\n\nДля многих задач достигнутой точности хватит, но есть способы ее повысить (часто за пределами R). Например, для латинского языка разработан пайплайн под названием LatinPipe, в 2024 г. победивший в конкурсе как лучший парсер для латинского языка. Это сложная конфигурация из различных нейросетей, которые учатся не на одном, а сразу на нескольких трибанках, что позволяет достичь большой точности. Мы обучили подобную модель и передали ей “Записки Цезаря”. Результат возвращается в формате CoNLL-U: прочитаем его в окружение и посмотрим, что получилось (скачать можно здесь).\n\nlibrary(udpipe)\ncaesar_pos3 &lt;- udpipe_read_conllu(\"../files/bg_latinpipe.conllu\") |&gt; \n  select(-paragraph_id)\ncaesar_pos3\n\n\n  \n\n\n\nКельты признаны существительным, бельги мужского рода (в поле FEATS), а provinciae – генитив.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-upos",
    "href": "tokenize.html#поле-upos",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.4 Поле UPOS",
    "text": "9.4 Поле UPOS\nМорфологическая аннотация, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, местоимения.\n\ncaesar_pos3 |&gt; \n  filter(upos == \"PRON\") |&gt; \n  select(token, lemma, upos, xpos)\n\n\n  \n\n\n\nПосчитать части речи можно так:\n\nupos_counts &lt;- caesar_pos3 |&gt; \n  group_by(upos) |&gt; \n  count() |&gt; \n  arrange(-n)\n\nupos_counts\n\n\n  \n\n\n\nСтолбиковая диаграмма позволяет наглядно представить результаты подсчетов:\n\nupos_counts |&gt; \n  ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) +\n  geom_bar(stat = \"identity\", show.legend = F) +\n  coord_flip() +\n  labs(x = NULL) +\n  theme_bw() \n\n\n\n\n\n\n\n\nОтберем наиболее частотные имена и имена собственные.\n\nnouns &lt;- caesar_pos3  |&gt; \n  filter(upos %in% c(\"NOUN\", \"PROPN\")) |&gt; \n  count(lemma) |&gt; \n  arrange(-n) \n\nnouns\n\n\n  \n\n\n\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(RColorBrewer)\n\npal &lt;- RColorBrewer::brewer.pal(8, \"Dark2\")\n\nwordcloud(nouns$lemma, nouns$n, colors = pal, max.words = 130)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-feats",
    "href": "tokenize.html#поле-feats",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.5 Поле FEATS",
    "text": "9.5 Поле FEATS\nДопустим, нам нужны не все местоимения, а лишь определенные их формы: например, относительные.\n\nrel_pron &lt;- caesar_pos3  |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  as_tibble()\n\nrel_pron \n\n\n  \n\n\n\nПосмотрим на некоторые местоимения в контексте.\nrel_pron |&gt; \n  filter(row_number() %in% c(1, 7)) |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.”\n[2] “Eorum una pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones.”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-xpos",
    "href": "tokenize.html#поле-xpos",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.6 Поле XPOS",
    "text": "9.6 Поле XPOS\nЧтение xpos требует сноровки: например причастие sublata там описывается так: v-srppfb-, где\n\nv = verbum;\n- на месте лица;\ns = singularis;\nr = perfectum (не p, потому что p = praesens);\np = participium;\np = passivum;\nf = femininum;\nb = ablativus (не a, потому что a = accusativus).\n\nСравним с описанием личной формы глагола differunt v3ppia---:\n\nv = verbum;\n3 = 3. persona;\np = pluralis;\np = praesens;\ni = indicativus;\na = activum;\n-- на месте рода и падежа, т.к. форма личная.\n\nПоследнее “место” (Degree) у глаголов всегда свободно; в первой книге там стоит s (superlativus) лишь у florentissimis, что явно ошибка, потому что это не глагол.\n\n\n\n\n\n\nНа заметку\n\n\n\nСпецификацию всех xpos-тегов для латинского языка можно найти по ссылке.\n\n\nДля удобства разобьем xpos на 9 столбцов.\n\ncaesar_pos3_sep &lt;- caesar_pos3 |&gt; \n  separate(xpos, into = c(\"POS\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"persona\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"numerus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"tempus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"modus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"vox\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"genus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"casus\", \"gradus\"), sep = 1)\n\ncaesar_pos3_sep\n\n\n  \n\n\n\nЭти столбцы тоже можно использовать для поиска конкретных признаков. Посмотрим, например, в каком числе и падеже чаще всего стоит относительное местоимения.\n\npron_rel_sum &lt;- caesar_pos3_sep  |&gt; \n  filter(upos == \"PRON\") |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  group_by(numerus, casus) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\npron_rel_sum\n\n\n  \n\n\n\nДля удобства преобразуем сокращения.\n\npron_rel_sum &lt;- pron_rel_sum |&gt; \n  filter(casus != \"-\") |&gt; \n  mutate(casus = case_when(casus == \"n\" ~ \"nom\",\n                           casus == \"g\" ~ \"gen\",\n                           casus == \"d\" ~ \"dat\",\n                           casus == \"a\" ~ \"acc\",\n                           casus == \"b\" ~ \"abl\")) |&gt; \n  mutate(numerus = case_when(numerus == \"s\" ~ \"sing\",\n                              numerus == \"p\" ~ \"plur\"))\n\npron_rel_sum\n\n\n  \n\n\n\nФункция facet_wrap позволяет разбить график на две части на основании значения переменной numerus.\n\npron_rel_sum |&gt; \n  ggplot(aes(casus, n, fill = casus)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  coord_flip() +\n  theme_light() +\n  facet_wrap(~numerus) +\n  labs(x = NULL, y = NULL, title = \"Относительные местоимения в BG 1-7\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-dep_rel",
    "href": "tokenize.html#поле-dep_rel",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.7 Поле DEP_REL",
    "text": "9.7 Поле DEP_REL\nАналогичным образом можно отбирать синтаксические признаки и их комбинации, а также визуализировать деревья зависимостей для отдельных предложений.\nДерево зависимостей – это направленный граф, который имеет единственную корневую вершину (сказуемое главного предложения) без входящих дуг (рёбер), при этом все остальные вершины имеют ровно одну входящую дугу. Иными словами, каждое слово зависит от другого, но только от одного. Это выглядит примерно так:\n\nlibrary(textplot)\n\nsent &lt;- caesar_pos3 |&gt; \n  filter(doc_id == \"doc1\", sentence_id == 10) \n\nsent |&gt; \n  distinct(sentence) |&gt; \n  pull(sentence) \n\n[1] \"Apud Helvetios longe nobilissimus fuit et ditissimus Orgetorix.\"\n\ntextplot_dependencyparser(sent, size = 3)\n\n\n\n\n\n\n\n\nПрилагательные “nobilissiumus” и “ditissimus” верно опознаны в качестве именной части сказуемого при подлежащем “Оргеториг”. Информация, которая на графе представлена стрелками, хранится в таблице в полях token_id и head_token_id и dep_rel. Корень синтаксического дерева всегда имеет значение 0, то есть ни от чего не зависит.\n\nsent |&gt; \n  select(token_id, token, head_token_id, dep_rel)\n\n\n  \n\n\n\n\nПравила синтаксической разметки для латинского языка доступны по ссылке, а расшифровку сокращений (для всех языков) надо смотреть здесь.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#совместная-встречаемость-слов",
    "href": "tokenize.html#совместная-встречаемость-слов",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.9 Совместная встречаемость слов",
    "text": "9.9 Совместная встречаемость слов\nФункция cooccurence() из пакета udpipe позволяет выяснить, сколько раз некий термин встречается совместно с другим термином, например:\n\nслова встречаются в одном и том же документе/предложении/параграфе;\nслова следуют за другим словом;\nслова находятся по соседству с другим словом на расстоянии n слов.\n\nКод ниже позволяет выяснить, какие существительные встречаются в одном предложении:\n\ncaesar_subset &lt;-  subset(caesar_pos3, upos == \"NOUN\")\ncooc &lt;- cooccurrence(caesar_subset, term = \"lemma\", group = c(\"doc_id\", \"sentence_id\")) |&gt;\n  as_tibble() |&gt; \n  filter(cooc &gt; 25)\n\ncooc\n\n\n  \n\n\n\nЭтот результат легко визуализировать, используя пакет ggraph (подробнее о нем мы будем говорить в следующих уроках):\n\nlibrary(igraph)\nlibrary(ggraph)\n\nwordnetwork &lt;- graph_from_data_frame(cooc)\nggraph(wordnetwork, layout = \"fr\") +\n  geom_edge_link(aes(width = cooc), alpha = 0.8, edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость существительных\", subtitle = \"De Bello Gallico 1-7\")\n\n\n\n\n\n\n\n\nЧтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но с другими аргументами:\n\ncooc2 &lt;- cooccurrence(caesar_subset$lemma, relevant = caesar_subset$upos %in% c(\"NOUN\", \"ADJ\"), skipgram = 1) |&gt; \n  as_tibble() |&gt; \n  filter(cooc &gt; 10)\n\ncooc2\n\n\n  \n\n\n\n\nwordnetwork &lt;- graph_from_data_frame(cooc2)\n\nggraph(wordnetwork, layout = \"fr\") +\n  geom_edge_link(aes(width = cooc), edge_colour = \"grey90\", edge_alpha=0.8, show.legend = F) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  labs(title = \"Слова, стоящие рядом в тексте\", subtitle = \"De Bello Gallico 1-7\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nHvitfeldt, Emil, и Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. Taylor; Francis.\n\n\nJockers, Matthew L. 2014. Text Analysis with R for Students of Literature. Springer.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "count.html",
    "href": "count.html",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "",
    "text": "10.1 Подготовка данных\nЗа основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты:\nИсточники для этого урока доступны в библиотеке Gutengerg; чтобы их извлечь, следует выяснить gutenberg_id. Пример ниже; таким же образом можно найти id для трактатов Локка и Беркли.\n# install.packages(\"gutenbergr\")\nlibrary(gutenbergr)\nlibrary(tidyverse)\nlibrary(stringr)\n\ngutenberg_works(str_detect(author, \"Hume\"), languages = \"en\")\nКогда id найдены, gutenbergr позволяет загрузить сочинения; на этом этапе часто возникают ошибки – в таком случае надо воспользоваться одним из зеркал. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL.\nmy_corpus &lt;- gutenberg_download(meta_fields = c(\"author\", \"title\"), c(9662, 4723, 10615), mirror = \"https://www.gutenberg.org/dirs/\")\n\nmy_corpus\nВ этом тиббле хранятся все три текста, которые нам нужны. Уточнить уникальные называния можно при помощи функции distinct() из tidyverse.\nmy_corpus |&gt; \n  distinct(author)\nПрежде чем приступать к анализу, придется немного прибраться. Для этого используем инструменты tidyverse, о которых шла речь в главе про опрятные данные.\nmy_corpus &lt;- my_corpus |&gt; \n  select(-gutenberg_id) |&gt; \n  select(-title) |&gt; \n  relocate(text, .after = author) |&gt; \n  mutate(author = str_remove(author, \",.+$\")) |&gt; \n  filter(text != \"\")\n\nmy_corpus\nВ случае с Юмом отрезаем предисловия, оглавление и индексы, а также номера разделов (везде прописными). Многие слова, которые в оригинале были выделены курсивом, окружены знаками подчеркивания (_), их тоже удаляем.\nHume &lt;- my_corpus |&gt; \n  filter(author == \"Hume\")|&gt; \n  filter(!row_number() %in% c(1:25),\n         !row_number() %in% c(4814:nrow(my_corpus))) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"_\", \" \")) |&gt; \n  filter(!str_detect(text, \"SECTION .{1,4}\"))\nВ случае с Беркли отрезаем метаданные и посвящение в самом начале, а также удаляем нумерацию параграфов. Кроме того, текст содержит примечания следующего вида: Note: Vide Hobbes’ Tripos, ch. v. sect. 6., от них тоже следует избавиться.\nBerkeley &lt;- my_corpus |&gt; \n  filter(author == \"Berkeley\") |&gt; \n  filter(!row_number() %in% c(1:38)) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]+?\\\\.\", \" \"))  |&gt; \n  mutate(text = str_replace_all(text, \"\\\\[.+?\\\\]\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]+\", \" \"))\nЧто касается Локка, то здесь удаляем метаданные и оглавление в самом начале, а также посвящение и подчеркивания вокруг слов. “Письмо к читателю” уже содержит некоторые философские положения, и его можно оставить.\nLocke &lt;- my_corpus  |&gt;  \n  filter(author == \"Locke\") |&gt; \n  filter(!row_number() %in% c(1:135)) |&gt; \n  mutate(text = str_replace_all(text, \"_\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]\", \" \"))\nСоединив обратно все три текста, замечаем некоторые орфографические нерегулярности; исправляем.\ncorpus_clean &lt;- bind_rows(Hume, Berkeley, Locke) |&gt; \n  mutate(text = str_replace_all(text, c(\"[Mm]an’s\" = \"man's\", \"[mM]en’s\" = \"men's\", \"[hH]ath\" = \"has\")))\n\ncorpus_clean\nПосле этого делим корпус на слова.\nlibrary(tidytext)\n\ncorpus_words &lt;- corpus_clean |&gt; \n  unnest_tokens(word, text)\n\ncorpus_words",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#подготовка-данных",
    "href": "count.html#подготовка-данных",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "",
    "text": "“Опыт о человеческом разумении” Джона Локка (1690), первые две книги;\n“Трактат о принципах человеческого знания” Джорджа Беркли (1710);\n“Исследование о человеческом разумении” Дэвида Юма (1748).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nСкачать подготовленный таким образом корпус можно здесь.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#cтоп-слова",
    "href": "count.html#cтоп-слова",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.2 Cтоп-слова",
    "text": "10.2 Cтоп-слова\nБольшая часть слов, которые мы сейчас видим в корпусе, нам пока не интересна – это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. Функция anti_join() позволяет от них избавиться; в случае с английским языком список стоп-слов уже доступен в пакете tidytext; в других случаях их следует загружать отдельно.\nДля многих языков стоп-слова доступны в пакете stopwords. Пример удаления стопслов на русском языке можно посмотреть здесь.\nФункция anti_join() работает так:\n\n\nother &lt;- c(\"section\", \"chapter\", 0:40, \"edit\", 1710, \"v.g\", \"v.g.a\")\n\ncorpus_words_tidy &lt;- corpus_words  |&gt;  \n  anti_join(stop_words) |&gt; \n  filter(!word %in% other)\n\ncorpus_words_tidy\n\n\n  \n\n\n\nУборка закончена, мы готовы к подсчетам.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#абсолютная-частотность",
    "href": "count.html#абсолютная-частотность",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.3 Абсолютная частотность",
    "text": "10.3 Абсолютная частотность\nДля начала посмотрим на самые частотные слова во всем корпусе.\n\nlibrary(ggplot2)\n\ncorpus_words_tidy |&gt; \n  count(word, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = F) + \n  coord_flip() \n\n\n\n\n\n\n\n\nЭтот график уже дает общее представление о тематике нашего корпуса: это теория познания, в центре которой для всех трех авторов стоит понятие “idea”.\nОднако можно заподозрить, что высокие показатели для слов “simple”, “distinct” и “powers” – это заслуга прежде всего Локка, который вводит понятия “простой идеи” и “отчетливой идеи”, а также говорит о “силах” вещей, благодаря которым они воздействуют как друг на друга, так и на разум. Силы для Локка – это причины идей, и как таковые они часто упоминаются в его тексте. Понятие врожденности (“innate”) также занимает в первую очередь его: вся первая книга “Опыта” – это опровержение теории врожденных идей. Беркли о врожденности не говорит вообще, а Юм – очень кратко.\nКроме того, хотя мы взяли только две книги из “Опыта” Локка – это самый длинный текст в нашем корпусе, что создает значительный перекос:\n\ncorpus_words_tidy |&gt; \n  group_by(author) |&gt; \n  summarise(sum = n())\n\n\n  \n\n\n\nПосмотрим статистику по отдельным авторам.\n\ncorpus_words_tidy  |&gt; \n  group_by(author) |&gt; \n  count(word, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder_within(word, n, author), n, fill = word)) +\n  geom_col(show.legend = F) + \n  facet_wrap(~author, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nНаиболее частотные слова (при условии удаления стоп-слов) дают вполне адекватное представление о тематике каждого из трех трактатов.\nСогласно Локку, объектом мышления является идея (желательно отчетливая, но тут уж как получится). Все идеи приобретены умом из опыта, направленного на либо на внешние предметы (ощущения, или чувства), либо на внутренние действия разума (рефлексия, или внутреннее чувство). Никаких врожденных идей у человека нет, изначально его душа похожа на чистый лист (антинативизм). Идеи могут быть простыми и сложными; они делятся на модусы, субстанции и отношения. К числу простых модусов относятся пространство, в котором находятся тела, а также продолжительность; измеренная продолжительность представляет собой время.\nБеркли спорит с мнением, согласно котором ум способен образовывать абстрактные идеи. В том числе, утверждает он, невозможна абстрактная идея движения, отличная от движущегося тела. Он пытается устранить заблуждение Локка, согласно которому слова являются знаками абстрактных общих идей. В мыслящей душе (которую он также называет умом и духом) существуют не абстрактные идеи, а ощущения, и существование немыслящих вещей безотносительно к их воспринимаемости совершенно невозможно. Нет иной субстанции, кроме духа; немыслящие вещи ее совершенно лишены. По этой причине нельзя допустить, что существует невоспринимающая протяженная субстанция, то есть материя. Идеи ощущений возникают в нас согласно с некоторыми правилами, которые мы называем законами природы. Действительные вещи – это комбинации ощущений, запечатлеваемые в нас могущественным духом.\nСогласно Юму, все объекты, доступные человеческому разуму, могут быть разделены на два вида, а именно: на отношения между идеями и факты. К суждениям об отношениях можно прийти благодаря одной только мыслительной деятельности, в то время как все заключения о фактах основаны на отношениях причины и действия. В свою очередь знание о причинности возникает всецело из опыта: только привычка заставляет нас ожидать наступления одного события при наступлении другого. Прояснение этого позволяет добиться большей ясности и точности в философии.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#стемминг",
    "href": "count.html#стемминг",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.4 Стемминг",
    "text": "10.4 Стемминг\nПоскольку мы не лемматизировали текст, то единственное и множественное число слова idea рассматриваются как разные токены. Один из способов справиться с этим - стемминг.\nСтемминг (англ. stemming “поиск происхождения”) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. Стемминг применяется в поисковых системах для расширения поискового запроса пользователя, является частью процесса нормализации текста. Один из наиболее популярных алгоритмов стемминга был написан Мартином Портером и опубликован в 1980 году.\nВ R стеммер Портера доступен в пакете snowball. К сожалению, он поддерживает не все языки, но русский, французский, немецкий и др. там есть. Не для всех языков, впрочем, и не для всех задач стемминг – это хорошая идея. Но попробуем применить его к нашему корпусу.\n\nlibrary(SnowballC)\n\ncorpus_stems &lt;- corpus_words_tidy |&gt; \n  mutate(stem = wordStem(word)) \n\ncorpus_stems |&gt; \n  count(stem, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder(stem, n), n, fill = stem)) +\n  geom_col(show.legend = F) + \n  coord_flip() \n\n\n\n\n\n\n\n\nВсе слова немного покромсаны, но вполне узнаваемы. При этом общее количество уникальных токенов стало значительно меньше:\n\n# до стемминга\nn_distinct(corpus_words_tidy$word)\n\n[1] 8132\n\n# после стемминга\nn_distinct(corpus_stems$stem)\n\n[1] 5229\n\n\nСтемминг применяется в некоторых алгоритмах машинного обучения, но сегодня - все реже, потому что современные компьютеры прекрасно справляются с лемматизацией.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#относительная-частотность",
    "href": "count.html#относительная-частотность",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.5 Относительная частотность",
    "text": "10.5 Относительная частотность\nАбсолютная частотность – плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте.\nCначала считаем частотность для всех токенов по авторам.\n\nauthor_word_counts &lt;- corpus_words  |&gt; \n  count(author, word, sort = T) |&gt; \n  filter(!word %in% other) |&gt; \n  ungroup()\n\nauthor_word_counts\n\n\n  \n\n\n\nЗатем - число токенов в каждой книге.\n\ntotal_counts &lt;- author_word_counts |&gt; \n  group_by(author) |&gt; \n  summarise(total = sum(n))\n\ntotal_counts\n\n\n  \n\n\n\nСоединяем два тиббла:\n\nauthor_word_counts &lt;- author_word_counts |&gt; \n  left_join(total_counts)\n\nauthor_word_counts\n\n\n  \n\n\n\nСчитаем относительную частотность:\n\nauthor_word_tf &lt;- author_word_counts |&gt; \n  mutate(tf = round((n / total), 5))\n\nauthor_word_tf\n\n\n  \n\n\n\nНаиболее частотные слова – это служебные части речи. На графике видно, что подавляющее большинство слов встречается очень редко, а слов с высокой частотностью - мало.\n\nauthor_word_tf |&gt; \n  ggplot(aes(tf, fill = author)) +\n  geom_histogram(show.legend = FALSE) +\n  facet_wrap(~author, scales = \"free_y\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#закон-ципфа",
    "href": "count.html#закон-ципфа",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.6 Закон Ципфа",
    "text": "10.6 Закон Ципфа\nПодобная картина характерна для естественных языков. Распределения слов в них подчиняются закону Ципфа. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) и утверждает следующее: если все слова языка или длинного текста упорядочить по убыванию частоты использования, частота (tf) n-го слова в списке окажется обратно пропорциональной его рангу (r) в степени α. Это значит (в самом общем случае), что если ранг увеличится в n раз, то частотность во столько же раз должна упасть: второе слово в корпусе встречается примерно в два раза реже, чем первое (Savoy 2020, 24).\n\\[tf_{r_i} = \\frac{c}{r^α_i}\\]\nЗдесь c - это константа, которая оценивается для каждого случая отдельно, как и параметр α. Иначе говоря:\n\\[ tf_{r_i} \\times r^α_i = c \\] Посмотрим на ранги и частотность первых 50 слов.\n\nauthor_word_tf_rank &lt;- author_word_tf |&gt; \n  group_by(author) |&gt; \n  mutate(rank = row_number()) \n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(linewidth = 1.1, alpha = 0.7) +\n  coord_cartesian(xlim = c(NA, 50)) +\n  scale_x_continuous(breaks = seq(0,50,5))\n\n\n\n\n\n\n\n\nВспомнив, что логарифм дроби равен разности логарифмов числителя и знаменателя, запишем:\n\\[log(tf_{r_i}) = c - α \\times log(r_i) \\] Таким образом, мы получаем близкую к линейность зависимость, где константа c определяет точку пересечения оси y, a коэффициентα - угол наклона прямой. Графически это выглядит так:\n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЧтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом поговорим подробнее в следующих уроках):\n\nlm_zipf &lt;- lm(data = author_word_tf_rank, \n              formula = log10(tf) ~ log10(rank))\n\ncoefficients(lm_zipf)\n\n(Intercept) log10(rank) \n -0.2501025  -1.2728170 \n\n\nМы получили коэффициент наклона α чуть больше -1 (на практике точно -1 встречается редко). Добавим линию регрессии на график:\n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  geom_abline(intercept = coefficients(lm_zipf)[1],\n              slope = coefficients(lm_zipf)[2], \n              linetype = 2, \n              color = \"grey50\") +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЗдесь видно, что отклонения наиболее заметны в “хвостах” графика. Это характерно для многих корпусов: как очень редких, так и самых частотных слов не так много, как предсказывает закон Ципфа. Кроме того, внизу кривая почти всегда приобретает ступенчатый вид, потому что слова встречаются в корпусе дискретное число раз: ранг у них разный, а частотности одинаковые.\n\n\n\n\n\n\nВопрос\n\n\n\nДальше всего вправо уходит кривая Локка. Почему?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#меры-лексического-разнообразия",
    "href": "count.html#меры-лексического-разнообразия",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.7 Меры лексического разнообразия",
    "text": "10.7 Меры лексического разнообразия\nКоэффициент наклона α для кривой Ципфа колеблется в достаточно узком диапазоне между 0.7 и 2 и, как полагают, может быть связан с “когнитивным усилием” говорящего: например, для устной речи α чуть больше, для письменной - ниже. Однако рассматривать этот наклон как индивидуальную характеристику стиля не стоит: как и другие меры лексического разнообразия, он сильно коррелирует с длиной текста.\nДело в том, что редкие слова (события) встречаются очень часто; это явление известно под названием Large Number of Rare Events (LNRE). И чем длиннее текст, тем больше в нем будет редких слов, но скорость их прибавления постепенно уменьшается: чем дальше, тем сложнее встретить слово, которого еще не было.\nЧтобы в этом убедиться, взглянем на наиболее известную мера лексического разнообразия под названием type-token ratio (TTR).\n\\[ TTR(T) = \\frac{Voc(T)}{n} \\] Здесь n - общее число токенов, а Voc (т.е. словарь) - число уникальных токенов (типов).\nВ пакете languageR, написанном лингвистом Гаральдом Баайеном, есть функция, позволяющая быстро производить такие вычисления. Она требует на входе вектор, а не тиббл, поэтому для эксперимента извлечем один из текстов.\n\nlocke_words &lt;- corpus_words %&gt;% \n  filter(author == \"Locke\") %&gt;% \n  pull(word)\n\nlength(locke_words)\n\n[1] 148171\n\n\nФункция считает различные меры лексического разнообразия, из которых нас сейчас будет интересовать наклон Ципфа и TTR.\n\nlibrary(languageR)\nlocke.growth = growth.fnc(text = locke_words, size = 1000, nchunks = 40)\n\n........................................\n\ngrowth_df &lt;- locke.growth@data$data \ngrowth_df\n\n\n  \n\n\n\nБыстро визуализировать результат можно при помощи plot(locke.growth), но мы воспользуемся ggplot2.\n\nlibrary(gridExtra)\n\np1 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, Types)) + \n  geom_point(color = \"steelblue\")\n\np2 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, Zipf)) + \n  geom_point(color = \"#B44682\") +\n  ylab(\"Zipf Slope\")\n\np3 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, TypeTokenRatio)) + \n  geom_point(color = \"#81B446\")\n\np4 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, HapaxLegomena / Tokens)) + \n  geom_point(color = \"#B47846\") +\n  ylab(\"Growth Rate\")\n\ngrid.arrange(p1, p2, p3, p4, nrow=2)\n\n\n\n\n\n\n\n\nПодробнее о различных мерах лексического разнообразия см.: (Baayen 2008, 222–36) и (Savoy 2020).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#tf-idf",
    "href": "count.html#tf-idf",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.8 TF-IDF",
    "text": "10.8 TF-IDF\nНаиболее частотные слова (с низким рангом) наименее подвержены влиянию тематики, поэтому их используют для стилометрического анализа. Если отобрать наиболее частотные после удаления стоп-слов, то мы получим достаточно адекватное отражение тематики документов. Если же мы необходимо найти наиболее характерные для документов токены, то применяется другая мера, которая называется tf-idf (term frequency - inverse document frequency).\n\nЛогарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для документа. При этом относительная частотность тоже учитывается! Например, Беркли один раз упоминает “сахарные бобы”, а Локк – “миндаль”, но из-за редкой частотности tf-idf для подобных слов будет низкой.\nФункция bind_tf_idf() принимает на входе тиббл с абсолютной частотностью для каждого слова.\n\nauthor_word_tfidf &lt;- author_word_tf |&gt; \n  bind_tf_idf(word, author, n)\n\nauthor_word_tfidf\n\n\n  \n\n\n\nПосмотрим на слова с высокой tf-idf:\n\nauthor_word_tfidf |&gt; \n  select(-total) |&gt; \n  arrange(desc(tf_idf))\n\n\n  \n\n\n\nСнова визуализируем.\n\nauthor_word_tfidf |&gt; \n  arrange(-tf_idf) |&gt; \n  group_by(author) |&gt; \n  top_n(15) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) +\n  geom_col(show.legend = F) +\n  labs(x = NULL, y = \"tf-idf\") +\n  facet_wrap(~author, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip()\n\n\n\n\n\n\n\n\nНа такой диаграмме авторы совсем не похожи друг на друга, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#сравнение-при-помощи-диаграммы-рассеяния",
    "href": "count.html#сравнение-при-помощи-диаграммы-рассеяния",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.9 Сравнение при помощи диаграммы рассеяния",
    "text": "10.9 Сравнение при помощи диаграммы рассеяния\nСтолбиковая диаграмма – не единственный способ сравнить частотности слов. Еще один наглядный метод – это диаграмма рассеяния с относительными частотностями. Сначала “расширим” наш тиббл.\n\nspread_freq &lt;- author_word_tf  |&gt; \n  anti_join(stop_words) |&gt; \n  filter(!word %in% other) |&gt; \n  filter(tf &gt; 0.0001) |&gt; \n  select(-n, -total) |&gt; \n  pivot_wider(names_from = author, values_from = tf, values_fill = 0) \n\nspread_freq\n\n\n  \n\n\n\nТеперь “удлиним”.\n\nlong_freq &lt;- spread_freq |&gt; \n  pivot_longer(c(\"Hume\", \"Locke\"), names_to = \"author\", values_to = \"tf\")\n\nlong_freq\n\n\n  \n\n\n\nМожно визуализировать. Обратите внимание, что частотности для Юма и Лока упаковываются в один столбец (ось X), по оси Y на обеих панелях даны частотности для Беркли (ось y).\n\nlibrary(scales)\n\nlong_freq |&gt; \n  ggplot(aes(x = tf, y = Berkeley)) +\n  geom_abline(color = \"grey40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, \n              height = 0.3, color = \"darkblue\") +\n  geom_text(aes(label = word), check_overlap = TRUE, \n            vjust = 1.5, color = \"grey30\") +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position = \"none\") +\n  theme_minimal() +\n  labs(y = \"Berkeley\", x = NULL)\n\n\n\n\n\n\n\n\nСлова, расположенные ближе к линии, примерно одинаково представлены в обоих текстах (например, “ум” и “душа” у Беркли и Локка); слова, которые находятся дальше от линии, более свойственны одному из двух авторов: например, у Беркли чаще встречается “абстрактный” по сравнению с первой книгой Локка, а у Локка чаще используется слово “простой”.\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press.\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "sentiment.html",
    "href": "sentiment.html",
    "title": "11  Эмоциональная тональность",
    "section": "",
    "text": "11.1 Анализ тональности\nАнализ тональности текста (англ. Sentiment analysis) — задача компьютерной лингвистики, заключающаяся в определении эмоциональной окраски (тональности) текста и, в частности, в выявлении эмоциональной оценки авторов по отношению к объектам, описываемым в тексте.\nАнализ тональности – это частный случай бинарной (позитивная / негативная) или многоклассовой (радость / гнев / обида и т.п.) классификации, хотя иногда бывает также необходимо оценить эмоциональную окрашенность текста по заданной шкале.\nО том, как в решении подобных задач могут быть полезны методы машинного обучения, мы поговорим в следующих уроках, а здесь речь пойдет о достаточно простом и в то же време эффективном подходе, основанном на тональных словарях (англ. affective lexicons). Тональный словарь представляет из себя список слов со значением тональности для каждого слова.\nСравнивая текст (или отрывок текста) со словарем, мы можем вычислить тональность для всего текста (или отрывка). Словари эмоциональной тональности размечаются вручную, полуавтоматически или автоматически на основании уже существующих тезаурусов, при этом используются различные шкалы:\nВ некоторых случаях дополнительно вводятся различия между оценочной лексикой (“неряшливый”) и негативным фактом (“кража”) и т.п.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#анализ-тональности",
    "href": "sentiment.html#анализ-тональности",
    "title": "11  Эмоциональная тональность",
    "section": "",
    "text": "бинарная: negative / positive (-1 / 1)\nтринарная: бинарная + 0 (neutral)\nранжированная: например, от -5 до 5",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#лексиконы-для-русского-языка",
    "href": "sentiment.html#лексиконы-для-русского-языка",
    "title": "11  Эмоциональная тональность",
    "section": "11.2 Лексиконы для русского языка",
    "text": "11.2 Лексиконы для русского языка\nПакет с лексиконами устанавливается напрямую из GitHub.\n\nremotes::install_github(\"dmafanasyev/rulexicon\")\n\nНачало работы.\n\nlibrary(rulexicon)\nlibrary(tidyverse)\nlibrary(tidytext)\n\nРусский язык входит в языков, для которых Й. Чен и С. Скиена собрали оценочную лексику (Chen и Skiena 2014). Их лексикон построен на основе графа знаний, связывающего слова на разных языках (на основе Wiktionary, Google Translate, транслитерационных ссылок и WordNet). Слова оцениваются по бинарной шкале ( -1 / 1).\n\nset.seed(0211)\nchen_skiena &lt;- hash_sentiment_chen_skiena\nsample_n(chen_skiena, 10)\n\n\n  \n\n\n\nДля русского языка в свободном доступе находится “РуСентиЛекс” («Создание лексикона оценочных слов русского языка РуСентилекс» 2016). Он содержит около 15000 уникальных слов или фраз, среди которых оценочные слова, а также слова и выражения, не передающие оценочное отношения автора, но имеющие положительную или отрицательную ассоциацию (коннотацию). Возможные значения переменной sentiment: neutral, positive, negative, positive/negative.\n\nset.seed(1102)\nrusenti2017 &lt;- hash_rusentilex_2017\nsample_n(rusenti2017, 10) |&gt; \n  select(-source, -token)\n\n\n  \n\n\n\nПри работе с этим лексиконом следует учитывать, что для отдельных слов он содержит несколько вхождений, как положительных, так и отрицательных, например:\n\nrusenti2017 %&gt;% \n  filter(token == \"нежный\") |&gt; \n  select(-source, -token)\n\n\n  \n\n\n\nСловарь AFINN содержит 7268 оценочных слов. Их тональность оценивается по шкале от -5 (крайне негативная) до 5 (в высшей степени положительная). Например, слово “адский” имеет оценку -5, а слово “ангельский” – +5.\n\nset.seed(0211)\nafinn &lt;- hash_sentiment_afinn_ru\nsample_n(afinn, 10)\n\n\n  \n\n\n\nNRC** для русского языка – это переведенная версия списка положительных и отрицательных слов Mohammad & Turney (2010). Таблица содержит 5179 слов с не нейтральными оценками. Бинарная шкала: -1 / 1.\n\nset.seed(1102)\nnrc &lt;- hash_sentiment_nrc_emolex_ru\nsample_n(nrc, 10)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#анализ-тональности-опрятный-подход",
    "href": "sentiment.html#анализ-тональности-опрятный-подход",
    "title": "11  Эмоциональная тональность",
    "section": "11.3 Анализ тональности: опрятный подход",
    "text": "11.3 Анализ тональности: опрятный подход\nСогласно Silge и Robinson (2017), анализе эмоциональной тональности в духе tidy data предполагает следующий алгоритм работы:\n\nПрежде всего текст делится на токены (или лемматизируется), затем каждому токену присваивается некое значение тональности, после чего эти значения суммируются и визуализируются.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#подготовка-текста",
    "href": "sentiment.html#подготовка-текста",
    "title": "11  Эмоциональная тональность",
    "section": "11.4 Подготовка текста",
    "text": "11.4 Подготовка текста\nПрежде всего текст необходимо токенизировать, лемматизировать и привести в опрятный формат. Можно загрузить уже подготовленные данные по ссылке.\n\nload(\"../data/liza_tbl.Rdata\")\n\nРазделим весь текст “Лизы” на отрывки по 100 слов: это позволит понять, как меняется эмоциональная тональность произведения по мере развития сюжета.\n\nliza_tbl &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  select(lemma) |&gt;  \n  rename(token = lemma)  |&gt;  \n  mutate(chunk = round(((row_number() + 50) / 100), 0))\n\nliza_tbl\n\n\n  \n\n\n\nВ тексте чуть более 5000 слов, у нас получился 51 отрывок.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#модификация-лексикона",
    "href": "sentiment.html#модификация-лексикона",
    "title": "11  Эмоциональная тональность",
    "section": "11.5 Модификация лексикона",
    "text": "11.5 Модификация лексикона\nСовременные лексиконы могут не очень подходят для анализа классической литературы. Например, в лексиконе AFINN, доступном в пакете rulexicon, слово “старый” имеет отрицательную оценку, как и слово “чувствительный”.\nКод ниже показывает, как можно удалить слово или поменять его знак в R. Разумеется, все то же самое можно сделать вручную, сохранив лексикон локально в виде файла.\n\nlex &lt;- hash_sentiment_afinn_ru |&gt; \n  filter(token != \"старый\")\n\nlex &lt;- lex  |&gt; \n  mutate_at(vars(score), ~ case_when(token == \"чувствительный\" ~  1.7,\n                 TRUE ~ .))\n\nlex |&gt; \n  filter(str_detect(token, \"чувств\"))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#соединение-лексикона-с-документом",
    "href": "sentiment.html#соединение-лексикона-с-документом",
    "title": "11  Эмоциональная тональность",
    "section": "11.6 Соединение лексикона с документом",
    "text": "11.6 Соединение лексикона с документом\nСтоп-слова, то есть слова, не несущие никакой смысловой нагрузки, нам не нужны, но удалять их отдельно нет смысла: мы соединим, при помощи функции inner_join(), документ с одним из лексиконов, в котором не будет стоп-слов. Функция inner_join() работает так:\n\n\nliza_sent &lt;- liza_tbl |&gt; \n  inner_join(lex)\n\nliza_sent\n\n\n  \n\n\n\nЗдесь “горе” – ошибка лемматизации (“стоя на сей горе…”).\nСложив положительно и отрицательно окрашенную лексику для каждого отрывка, получаем значение, позволяющее судить о доминирующей тональности:\n\nliza_chunk_sent &lt;- liza_sent |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  arrange(sum)\n\nliza_chunk_sent\n\n\n  \n\n\n\nДовольно неожиданно, что самый негативный отрывок находится не в конце повести, ближе к трагической развязке, а почти в начале (отрывок 5, ср. отрывки 3 и 4 рядом). Представим эмоционально окрашенную лексику отрывков 3-5 в виде сравнительного облака слов. Палитру берем отсюда.\n\nlibrary(reshape2)\nlibrary(wordcloud)\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\n# добавляем новый столбец для удобства визуализации\nliza_sent_class &lt;- liza_sent |&gt; \n  mutate(tone = case_when( score &gt;= 0 ~ \"pos\",\n                           score &lt; 0 ~ \"neg\"))\nset.seed(0211)\nliza_sent_class |&gt; \n  filter(chunk  %in%  c(3, 4, 5)) |&gt; \n  count(token, tone, sort = T) |&gt; \n  acast(token ~ tone, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = c(pal[1], pal[5]),\n                   max.words = 99)\n\n\n\n\n\n\n\n\nЗдесь видно, что негативная тональность в этой части не связана с судьбой героев: об этом говорят такие слова, как “лютый”, “враг”, “свирепый”. Рассказчик, глядя на заброшенный Симонов монастырь, вспоминает о “печальной истории” Москвы. Если верить нашей модели, самый мрачный фрагмент повести посвящен не судьбе бедной девушки, а “глухому стону времен”:\n\nИногда на вратах храма рассматриваю изображение чудес, в сем монастыре случившихся, там рыбы падают с неба для насыщения жителей монастыря, осажденного многочисленными врагами; тут образ богоматери обращает неприятелей в бегство. Все сие обновляет в моей памяти историю нашего отечества — печальную историю тех времен, когда свирепые татары и литовцы огнем и мечом опустошали окрестности российской столицы и когда несчастная Москва, как беззащитная вдовица, от одного бога ожидала помощи в лютых своих бедствиях.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#тональность-на-оси-времени",
    "href": "sentiment.html#тональность-на-оси-времени",
    "title": "11  Эмоциональная тональность",
    "section": "11.7 Тональность на оси времени",
    "text": "11.7 Тональность на оси времени\nТаблица, которую мы подготовили, позволяет наглядно показать, как меняется тональность во времени – разумеется, речь идет о повествовательном времени, которое измеряется не в минутах, а в словах.\nОбозначим как положительный или отрицательный каждый из отрывков, как мы это делали для слов.\n\nliza_chunk_sent &lt;- liza_chunk_sent |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_chunk_sent\n\n\n  \n\n\n\nПалитра у нас уже сохранена.\n\nlibrary(showtext)\nfont_add(family = \"vibes\", \"GreatVibes-Regular.ttf\")\nshowtext_auto()\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\np1 &lt;- liza_chunk_sent |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (без учета отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\np1\n\n\n\n\n\n\n\n\nВ целом график получился осмысленным. Мы уже сказали выше про отрывки 3-4. Дальше немного скорби в отрывке 8 посвящено покойному отцу Лизы. В 11-м отрывке отразилась тревога матери за судьбу дочери: “коварно”, “обидеть”, “дурной” вносят вклад в настроение этого фрагмента. Это достаточно характерно для сентиментальной прозы с ее противопоставлением пороков городской жизни и пасторальных добродетелей.\n\nУ меня всегда сердце бывает не на своем месте, когда ты ходишь в город; я всегда ставлю свечу перед образ и молю господа бога, чтобы он сохранил тебя от всякой беды и напасти.\n\nЕще два минимума: отрывки 31 и 34. В первом из них Лиза встревожена вестью о возможном замужестве с сыном крестьянина. Отрывок 34 – это падение Лизы:\n\nГрозно шумела буря, дождь лился из черных облаков — казалось, что натура сетовала о потерянной Лизиной невинности.\n\nНа графике видно, что это место гораздо более эмоционально, чем эпизод самоубийства Лизы: именно после знаменитых карамзинских многоточий и тире события устремляются к трагическому финалу. О самой смерти девушки Карамзин говорит, конечно, с грустью, но без надрыва: “Тут она бросилась в воду”.\nОтрывки 38, 39, 42 – Эраст отправляется на войну. Все, как положено, плачут, что зафиксировал и наш график.\nНаконец, в отрывках 49-51 доминирует тема смерти, причем часть этих слов относится не к самой девушке, а к ее матери.\n\nliza_sent_class %&gt;% \n  filter(chunk %in% c(49:51)) %&gt;% \n  filter(tone == \"neg\") %&gt;% \n  count(token, sort = T) %&gt;% \n  with(wordcloud(token, n, max.words = 100, colors = pal[2]))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#отрицания",
    "href": "sentiment.html#отрицания",
    "title": "11  Эмоциональная тональность",
    "section": "11.8 Отрицания",
    "text": "11.8 Отрицания\nВ отрывке 15 несколько негативных слов имеют перед собой отрицания (“не подозревая”, “никакого худого намерения” и т.п.), поэтому к числу отрицательно окрашенных он отнесен ошибочно. К сожалению, это недостаток подхода, основанного на словарях, не принимающего в учет синтаксические связи в предложении.\nОдно из самых простых решений заключается в том, что бы соединить отрицание и следующее за ним слово (или добавить отрицание ко всем словам до следующего знака препинания).\n\nneg_sent &lt;- \"Старушка с охотою приняла сие предложение, не подозревая в нем никакого худого намерения.\"\n\nstr_replace_all(neg_sent, \"( не | никакого )(\\\\w+)\", \" NEG_\\\\2\")\n\n[1] \"Старушка с охотою приняла сие предложение, NEG_подозревая в нем NEG_худого намерения.\"\n\n\nЧтобы систематически применить этот подход ко всему документу (или коллекции документов), необходим список отрицаний для выбранного языка. Список ниже не претендует на полноту, но иллюстрирует общий принцип.\n\nnegations &lt;- c(\"никто\", \"никого\", \"никем\", \"ничто\", \"ничем\", \"ничего\", \"ни\", \"никакой\", \"никакого\", \"никаких\", \"никаким\", \"никак\", \"ничей\", \"ничьих\", \"нисколько\", \"никогда\", \"нигде\", \"никуда\", \"некого\", \"нельзя\", \"нечего\", \"незачем\", \"нет\", \"едва\", \"не\", \"ничуть\")\n\nregex &lt;- str_c(negations, collapse = \" | \")\nregex &lt;- paste0(\"( \", regex, \"  )(\\\\w+)\")\nregex\n\n[1] \"( никто | никого | никем | ничто | ничем | ничего | ни | никакой | никакого | никаких | никаким | никак | ничей | ничьих | нисколько | никогда | нигде | никуда | некого | нельзя | нечего | незачем | нет | едва | не | ничуть  )(\\\\w+)\"\n\n\n\nload(\"../data/liza_tbl.Rdata\") \ntext &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  pull(lemma) |&gt; \n  str_c(collapse = \" \")\n\nЗаменяем отрицания и считаем статистику по отрывкам.\n\ntext &lt;-  str_replace_all(text, regex, \" NEG_\\\\2\")\n\n\nliza_NEG &lt;- tibble(text = text) |&gt; \n  unnest_tokens(token, text) |&gt; \n  mutate(chunk = round(((row_number() + 50) / 100), 0)) |&gt; \n  inner_join(lex) \n\nJoining with `by = join_by(token)`\n\nliza_NEG_chunk &lt;- liza_NEG |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_NEG_chunk \n\n\n  \n\n\n\nОсталось заново построить график. Для сравнения оставим рядом старую версию.\n\nlibrary(gridExtra)\n\np2 &lt;- liza_NEG_chunk |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (с учетом отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nИз-за изменения числа токенов отрывки сдвинулись, но незначительно. Бывший отрывок 15, как мы и ожидали, перешел в число положительно окрашенных (несмотря на ошибочную оценку слова “левый”).\n\n\nБыло:\n\n\n\n  \n\n\n\n\nСтало:\n\n\n\n  \n\n\n\n\n\nПомимо этого, повысилось абсолютное значение негативной тональности в последних отрывках, хотя на это повлияли не столько отрицания, сколько изменение числа слов и перераспределение их по отрывкам.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#пакет-ggpage",
    "href": "sentiment.html#пакет-ggpage",
    "title": "11  Эмоциональная тональность",
    "section": "11.9 Пакет ggpage",
    "text": "11.9 Пакет ggpage\n\nlibrary(ggpage)\n\npage_data &lt;- liza_tbl |&gt; \n  select(lemma) |&gt; \n  rename(text = lemma)  # required by ggpage_build()\n\npage_data |&gt; \n  ggpage_build(lpp = 22, character_height = 3) |&gt; \n  rename(token = word) |&gt; # required by join\n  left_join(lex) |&gt; \n  rename(text = token) |&gt; \n  mutate(neg = case_when(score &lt; 0 ~ TRUE,\n                         .default = FALSE)) |&gt; \n  ggpage_plot(aes(fill = neg), page.number = \"top-left\") +\n  labs(title = \"Негативная лексика в «Бедной Лизе»\", x = NULL, y = NULL) +\n  scale_fill_manual(values = c(pal[5], pal[1]),\n                    labels = c(\"другая\", \"негативная\"),\n                    name = NULL) + \n  theme(axis.title = element_blank(), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_blank(),\n        text = element_text(family = \"vibes\", size = 12, color = \"grey30\"),\n        )",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#p.s.-для-других-языков",
    "href": "sentiment.html#p.s.-для-других-языков",
    "title": "11  Эмоциональная тональность",
    "section": "11.10 P.S.: Для других языков",
    "text": "11.10 P.S.: Для других языков\nДля языков, которые используют латиницу, в R есть отличный пакет под названием syuzhet, разработанный Мэтью Джокерсом. Название пакета, как говорит его разработчик, подсмотрено у русских формалистов Виктора Шкловского и Владимира Проппа. Возможности и ограничения этого пакета обсуждались в специальной литературе.\n\n\n\n\nChen, Y., и S. Skiena. 2014. «Building Sentiment Lexicons for All Major Languages». Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, 383–89.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.\n\n\n«Создание лексикона оценочных слов русского языка РуСентилекс». 2016. Труды конференции OSTIS-2016, 377–82.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "lsa.html",
    "href": "lsa.html",
    "title": "12  Латентно-семантический анализ",
    "section": "",
    "text": "12.1 Векторы в лингвистике\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Есть несколько возможных решений.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравнивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход – зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности.\nВ этом уроке мы рассмотрим алгоритм LSA, а в следующем – векторные модели на основе PMI. В первом случае анализируется матрица термин-документ, во втором – матрица термин-термин. Оба подхода предполагают использование SVD.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#векторы",
    "href": "lsa.html#векторы",
    "title": "12  Латентно-семантический анализ",
    "section": "",
    "text": "As You Like It\nTwelfth Night\nJulius Caesar\nHenry V\n\n\n\n\nbattle\n1\n1\n8\n15\n\n\nsoldier\n2\n2\n12\n36\n\n\nfool\n37\n58\n1\n5\n\n\nclown\n6\n117\n0\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#латентно-семантический-анализ",
    "href": "lsa.html#латентно-семантический-анализ",
    "title": "12  Латентно-семантический анализ",
    "section": "12.3 Латентно-семантический анализ",
    "text": "12.3 Латентно-семантический анализ\nLSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) – это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками). Слово “latent” (англ. “скрытый”) в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить.\nСоздатели метода LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов представляет собой своего рода сумму (или среднее) отдельных слов. Этот принцип является общим для всех векторных моделей.\nНа входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово “дорожное”, например, служит лучшим показателем темы, чем слово “происшествие”, которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции. Общий принцип действия алгоритма подробно объясняется на очень простом примере по ссылке; мы же перейдем сразу к анализу реальных данных.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#lsa-на-простом-примере",
    "href": "lsa.html#lsa-на-простом-примере",
    "title": "12  Латентно-семантический анализ",
    "section": "12.3 LSA на простом примере",
    "text": "12.3 LSA на простом примере\nДан “корпус” из пяти документов.\n\n\n\ndoc\ntext\n\n\n\n\nd1\nRomeo and Juliet.\n\n\nd2\nJuliet: O happy dagger!\n\n\nd3\nRomeo died by dagger.\n\n\nd4\n“Live free or die”, that’s the New-Hampshire’s motto.\n\n\nd5\nDid you know, New Hampshire is in New-England.\n\n\n\nПосле удаления стоп-слов термдокументная матрица выглядит так.\n\n\n\n  \n\n\n\nПо этой матрице пока нельзя сделать вывод о том, с какими темами связаны, с одной стороны, слова, а с другой - документы. Ее необходимо “переупорядочить” так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем. Примерно так.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#сингулярное-разложение-матрицы",
    "href": "lsa.html#сингулярное-разложение-матрицы",
    "title": "12  Латентно-семантический анализ",
    "section": "12.4 Сингулярное разложение матрицы",
    "text": "12.4 Сингулярное разложение матрицы\nПри сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы.\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. Мы пока не будем пытаться понять, что такое сингулярные векторы с математической точки зрения; достаточно думать о них как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-в-базовом-r",
    "href": "lsa.html#svd-в-базовом-r",
    "title": "12  Латентно-семантический анализ",
    "section": "12.5 SVD в базовом R",
    "text": "12.5 SVD в базовом R\nПрименим SVD к игрушечной термдокументной матрице, которую мы создали выше. В R для этого есть специальная функция (и не одна).\n\nmy_svd = svd(df)\n\nmy_svd\n\n$d\n[1] 2.2852979 2.0102582 1.3606993 1.1181404 0.7965768\n\n$u\n           [,1]       [,2]        [,3]        [,4]        [,5]\n[1,] -0.3961528  0.2800574  0.57117132  0.44968498  0.10183880\n[2,] -0.3142681  0.4495321 -0.41059055  0.51301824 -0.20390607\n[3,] -0.1782395  0.2689915 -0.49732052 -0.25699778 -0.04305233\n[4,] -0.4383638  0.3685083 -0.01287918 -0.57732882  0.21964021\n[5,] -0.2638806 -0.3459214 -0.14578908  0.04748488 -0.41748402\n[6,] -0.5240048 -0.2464047  0.33865227 -0.27284616 -0.15479149\n[7,] -0.2638806 -0.3459214 -0.14578908  0.04748488 -0.41748402\n[8,] -0.3263732 -0.4596688 -0.31700297  0.23724380  0.72485145\n\n$v\n           [,1]       [,2]       [,3]        [,4]        [,5]\n[1,] -0.3108657  0.3629332  0.1180134  0.86098600 -0.12813236\n[2,] -0.4073304  0.5407425 -0.6767037 -0.28735960 -0.03429449\n[3,] -0.5944614  0.2000544  0.6591790 -0.35817507  0.20925479\n[4,] -0.6030458 -0.6953914 -0.1983751  0.05309476 -0.33255810\n[5,] -0.1428143 -0.2286616 -0.2329706  0.21217712  0.90995798\n\n\nСингулярные значения меньше двух отсекаем, остается два значения. Это позволит нам визуализировать результат; в реальном исследовании используется больше измерений (от 50 до 1000 в зависимости от корпуса).\n\nmy_svd$d[3:5] &lt;- 0\n\ns &lt;- diag(my_svd$d) \n\ns\n\n         [,1]     [,2] [,3] [,4] [,5]\n[1,] 2.285298 0.000000    0    0    0\n[2,] 0.000000 2.010258    0    0    0\n[3,] 0.000000 0.000000    0    0    0\n[4,] 0.000000 0.000000    0    0    0\n[5,] 0.000000 0.000000    0    0    0\n\n\nМатрицу правых сингулярных векторов транспонируем.\n\nvt &lt;- t(my_svd$v)\n\nvt\n\n           [,1]        [,2]       [,3]        [,4]       [,5]\n[1,] -0.3108657 -0.40733041 -0.5944614 -0.60304575 -0.1428143\n[2,]  0.3629332  0.54074246  0.2000544 -0.69539140 -0.2286616\n[3,]  0.1180134 -0.67670369  0.6591790 -0.19837510 -0.2329706\n[4,]  0.8609860 -0.28735960 -0.3581751  0.05309476  0.2121771\n[5,] -0.1281324 -0.03429449  0.2092548 -0.33255810  0.9099580\n\n\nТеперь перемножим матрицы, чтобы получить эмбеддинги.\n\n# эмбеддинги слов\nu &lt;- my_svd$u\nword_emb &lt;- u %*% s |&gt; \n  round(3)\n\nrownames(word_emb) &lt;- rownames(df)\n\nword_emb\n\n                [,1]   [,2] [,3] [,4] [,5]\nromeo         -0.905  0.563    0    0    0\njuliet        -0.718  0.904    0    0    0\nhappy         -0.407  0.541    0    0    0\ndagger        -1.002  0.741    0    0    0\nlive          -0.603 -0.695    0    0    0\ndie           -1.198 -0.495    0    0    0\nfree          -0.603 -0.695    0    0    0\nnew-hampshire -0.746 -0.924    0    0    0\n\n\n\n# эмбеддинги документов\ndoc_emb &lt;- s %*% vt |&gt; \n  round(3)\n\ncolnames(doc_emb) &lt;- colnames(df)\n\ndoc_emb \n\n        d1     d2     d3     d4     d5\n[1,] -0.71 -0.931 -1.359 -1.378 -0.326\n[2,]  0.73  1.087  0.402 -1.398 -0.460\n[3,]  0.00  0.000  0.000  0.000  0.000\n[4,]  0.00  0.000  0.000  0.000  0.000\n[5,]  0.00  0.000  0.000  0.000  0.000\n\n\nДобавим условный поисковый запрос: dies, dagger. Очевидно, ближе всего к документы d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 – ни одного. Координаты поискового запроса (который рассматриваем как псевдодокумент) считаем как среднее арифметическое координат:\n\nq = c(\"die\", \"dagger\")\nq_doc &lt;-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2\nq_doc\n\n[1] -1.100  0.123  0.000  0.000  0.000\n\n\nОбъединив все в единый датафрейм, можем визуализировать.\n\nlibrary(tidyverse)\n\nplot_tbl &lt;- rbind(word_emb, t(doc_emb), q_doc) |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"item\") |&gt; \n  rename(dim1 = V1, dim2 = V2) |&gt; \n  mutate(type = c(rep(\"word\", 8), rep(\"doc\", 6))) |&gt; \n  select(!starts_with(\"V\"))\n\nplot_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nИтак, “поисковый запрос” оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#межвекторное-расстояние",
    "href": "lsa.html#межвекторное-расстояние",
    "title": "12  Латентно-семантический анализ",
    "section": "12.6 Межвекторное расстояние",
    "text": "12.6 Межвекторное расстояние\nМы исследовали наш небольшой корпус визуально, но там, где число измерений больше двух, это просто невозможно. На практике расстояние или сходство между векторами слов (или документов) вычисляется алгебраически. Наиболее известны манхэттенское и евклидово расстояние, а также косинусное сходство. Для анализа текстовых данных как правило применяется косинусное сходство.\n\nВсе их можно посчитать в R для заданной пары векторов.\n\ndoc_mx &lt;- plot_tbl |&gt; \n  filter(row_number() &gt; 8 ) |&gt; \n  column_to_rownames(\"item\") |&gt; \n  select(dim1, dim2) |&gt; \n  as.matrix()\n\ndoc_mx\n\n        dim1   dim2\nd1    -0.710  0.730\nd2    -0.931  1.087\nd3    -1.359  0.402\nd4    -1.378 -1.398\nd5    -0.326 -0.460\nq_doc -1.100  0.123\n\n\n\ndist_mx &lt;- doc_mx |&gt; \n  philentropy::distance(method = \"cosine\", use.row.names = T) \n\ndist_mx\n\n               d1         d2        d3          d4         d5     q_doc\nd1     1.00000000  0.9979996 0.8719223 -0.02109092 -0.1817325 0.7725616\nd2     0.99799957  1.0000000 0.8392224 -0.08425530 -0.2435368 0.7308749\nd3     0.87192229  0.8392224 1.0000000  0.47114573  0.3230341 0.9845083\nd4    -0.02109092 -0.0842553 0.4711457  1.00000000  0.9869622 0.6185045\nd5    -0.18173249 -0.2435368 0.3230341  0.98696218  1.0000000 0.4839672\nq_doc  0.77256165  0.7308749 0.9845083  0.61850449  0.4839672 1.0000000\n\n\nЧтобы получить расстояние (а не сходство), вычитаем результат из единицы.\n\nround(1 - dist_mx, 3)\n\n         d1    d2    d3    d4    d5 q_doc\nd1    0.000 0.002 0.128 1.021 1.182 0.227\nd2    0.002 0.000 0.161 1.084 1.244 0.269\nd3    0.128 0.161 0.000 0.529 0.677 0.015\nd4    1.021 1.084 0.529 0.000 0.013 0.381\nd5    1.182 1.244 0.677 0.013 0.000 0.516\nq_doc 0.227 0.269 0.015 0.381 0.516 0.000\n\n\nАналогично вычисляются расстояния между словами. При желании все косинусы можно пересчитать в градусы, чтобы узнать точный угол между векторами.\n\nacos(dist_mx[3,1])  # acos для d3 и d1 (cos = 0.872)\n\n[1] 0.5116817\n\n180 * acos(dist_mx[3,1]) / pi # переводим из радиан в градусы\n\n[1] 29.3172",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#подготовка-данных",
    "href": "lsa.html#подготовка-данных",
    "title": "12  Латентно-семантический анализ",
    "section": "12.4 Подготовка данных",
    "text": "12.4 Подготовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для простоты возьмем из него лишь один год). Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nlibrary(tidyverse)\nload(\"../data/news.Rdata\")\n\nnews_2019 |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\nДобавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\nПодготовленные данные можно забрать по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-опрятный-подход",
    "href": "lsa.html#svd-опрятный-подход",
    "title": "12  Векторные представления слов",
    "section": "12.8 SVD: опрятный подход",
    "text": "12.8 SVD: опрятный подход",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "index.html#дополнительные-материалы",
    "href": "index.html#дополнительные-материалы",
    "title": "Компьютерный анализ текста",
    "section": "Дополнительные материалы",
    "text": "Дополнительные материалы\nЭтот курс опирается на четыре книги, к которым можно обращаться за дополнительной информацией. Все они находятся в открытом доступе.\n\n\n\n\n\n\n\n\n\n1.\n\n\n\n\n\n\n\n2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.\n\n\n\n\n\n\n\n4.\n\n\n\n\n\nЭти книги объединяет общий подход, основанный на идеологии tidy data, и использовать описанные там инструменты можно, не опасаясь проблем совместимости.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "lsa.html#подгтовка-данных",
    "href": "lsa.html#подгтовка-данных",
    "title": "12  Латентно-семантический анализ",
    "section": "12.4 Подгтовка данных",
    "text": "12.4 Подгтовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для простоты возьмем из него лишь один год). Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nlibrary(tidyverse)\nload(\"../data/news.Rdata\")\n\nnews_2019 |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\nДобавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\nПодготовленные данные можно забрать по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#tf-idf-опрятный-подход",
    "href": "lsa.html#tf-idf-опрятный-подход",
    "title": "12  Латентно-семантический анализ",
    "section": "12.5 TF-IDF: опрятный подход",
    "text": "12.5 TF-IDF: опрятный подход\nВместо показателей абсолютной встречаемости при анализе больших текстовых данных применяется tf-idf. Эта статистическая мера не используется, если дана матрица термин-термин, но она хорошо работает с матрицами термин-документ, позволяя повысить веса для тех слов, которые служат хорошими дискриминаторами. Например, “заявил” и “отметил”, хотя это не стоп-слова, могут встречаться в разных темах.\n\nnews_counts &lt;- news_tokens_pruned |&gt;\n  count(token, id)\n\nnews_counts\n\n\nnews_counts |&gt; \n  arrange(id)\n\n\n  \n\n\n\nДобавляем tf_idf.\n\nnews_tf_idf &lt;- news_counts |&gt; \n  bind_tf_idf(token, id, n) |&gt; \n  arrange(tf_idf) |&gt; \n  select(-n, -tf, -idf)\n\nnews_tf_idf",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#documenttermmatrix",
    "href": "lsa.html#documenttermmatrix",
    "title": "12  Латентно-семантический анализ",
    "section": "12.6 DocumentTermMatrix",
    "text": "12.6 DocumentTermMatrix\nПосмотрим на размер получившейся таблицы.\n\nobject.size(news_tf_idf)\n\n5369712 bytes\n\nformat(object.size(news_tf_idf), units = \"auto\")\n\n[1] \"5.1 Mb\"\n\n\nЧтобы вычислить SVD, такую таблицу необходимо преобразовать в матрицу термин-документ. Оценим ее размер:\n\n# число уникальных токенов\nm &lt;- unique(news_tf_idf$token) |&gt; \n  length()\nm\n\n[1] 6299\n\n# число уникальных документов\nn &lt;- unique(news_tf_idf$id) |&gt; \n  length()  \nn\n\n[1] 3407\n\n# число элементов в матрице \nm * n\n\n[1] 21460693\n\n\nИспользуем специальный формат для хранения разреженных матриц.\n\ndtm &lt;- news_tf_idf |&gt; \n  cast_sparse(token, id, tf_idf)\n\n\n# первые 10 рядов и 5 столбцов\ndtm[1:10, 1:5]\n\n10 x 5 sparse Matrix of class \"dgCMatrix\"\n               doc608     doc1670     doc2170     doc2184     doc2219\nранее     0.003530193 .           .           0.005002585 .          \nроссии    0.010127611 0.003675658 0.004689633 0.004783897 0.005471238\nсловам    .           .           0.011776384 .           0.006869557\nрублей    0.006686328 .           0.027865190 .           .          \nрассказал 0.007250151 .           .           0.010274083 .          \nданным    0.007406320 .           .           .           0.012003345\nиздание   0.007759457 .           .           .           0.012575671\nходе      0.008675929 .           .           .           .          \nзаявил    0.016729327 .           .           .           .          \nчастности 0.008860985 .           0.012309349 .           .          \n\n\nСнова уточним размер матрицы.\n\nformat(object.size(dtm), units = \"auto\")\n\n[1] \"3 Mb\"",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-с-пакетом-irlba",
    "href": "lsa.html#svd-с-пакетом-irlba",
    "title": "12  Латентно-семантический анализ",
    "section": "12.7 SVD с пакетом irlba",
    "text": "12.7 SVD с пакетом irlba\nМетод для эффективного вычисления усеченного SVD на больших матрицах реализован в пакете irlba. Возможно, придется подождать ⏳.\n\nlibrary(irlba)\nlsa_space&lt;- irlba::irlba(dtm, 50) \n\nФункция вернет список из трех элементов:\n\nd: k аппроксимированных сингулярных значений;\nu: k аппроксимированных левых сингулярных векторов;\nv: k аппроксимированных правых сингулярных векторов.\n\nПолученную LSA-модель можно использовать для поиска наиболее близких слов и документов или для изучения тематики корпуса – в последнем случае нас может интересовать, какие топики доминируют в тех или иных документах и какими словами они в первую очередь представлены.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#эмбеддинги-слов",
    "href": "lsa.html#эмбеддинги-слов",
    "title": "12  Латентно-семантический анализ",
    "section": "12.8 Эмбеддинги слов",
    "text": "12.8 Эмбеддинги слов\nВернем имена рядов матрице левых сингулярных векторов и добавим имена столбцов.\n\nrownames(lsa_space$u) &lt;- rownames(dtm)\ncolnames(lsa_space$u) &lt;- paste0(\"dim\", 1:50)\n\nТеперь посмотрим на эмбеддинги слов.\n\nword_emb &lt;- lsa_space$u |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"word\") |&gt; \n  as_tibble()\n\nword_emb\n\n\n  \n\n\n\nПреобразуем наши данные в длинный формат.\n\nword_emb_long &lt;- word_emb |&gt; \n  pivot_longer(-word, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\nword_emb_long",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#визуализация-топиков",
    "href": "lsa.html#визуализация-топиков",
    "title": "12  Латентно-семантический анализ",
    "section": "12.9 Визуализация топиков",
    "text": "12.9 Визуализация топиков\nВизуализируем несколько топиков, чтобы понять, насколько они осмыслены.\n\nword_emb_long |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#ближайшие-соседи",
    "href": "lsa.html#ближайшие-соседи",
    "title": "12  Латентно-семантический анализ",
    "section": "12.10 Ближайшие соседи",
    "text": "12.10 Ближайшие соседи\nЭмбеддинги можно использовать для поиска ближайших соседей.\n\nlibrary(widyr)\n\nnearest_neighbors &lt;- function(df, feat, doc=F) {\n  inner_f &lt;- function() {\n    widely(\n        ~ {\n          y &lt;- .[rep(feat, nrow(.)), ]\n          res &lt;- rowSums(. * y) / \n            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))\n          \n          matrix(res, ncol = 1, dimnames = list(x = names(res)))\n        },\n        sort = TRUE\n    )}\n  if (doc) {\n    df |&gt; inner_f()(doc, dimension, value) }\n  else {\n    df |&gt; inner_f()(word, dimension, value)\n  } |&gt; \n    select(-item2)\n}\n\n\nnearest_neighbors(word_emb_long, \"сборная\")\n\n\n  \n\n\nnearest_neighbors(word_emb_long, \"завод\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#похожие-документы",
    "href": "lsa.html#похожие-документы",
    "title": "12  Латентно-семантический анализ",
    "section": "12.11 Похожие документы",
    "text": "12.11 Похожие документы\nИнформация о документах хранится в матрице правых сингулярных векторов.\n\nrownames(lsa_space$v) &lt;- colnames(dtm)\ncolnames(lsa_space$v) &lt;- paste0(\"dim\", 1:50)\n\nПосмотрим на эмбеддинги документов.\n\ndoc_emb &lt;- lsa_space$v |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  as_tibble()\n\ndoc_emb\n\n\n  \n\n\n\nПреобразуем в длинный формат.\n\ndoc_emb_long &lt;- doc_emb |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\ndoc_emb_long\n\n\n  \n\n\n\nИ найдем соседей для произвольного документа.\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nВыведем документ 14 вместе с его соседями.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc14\", \"doc392\", \"doc2043\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nПоздравляем, вы построили свою первую рекомендательную систему 🥛.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#сложные-запросы",
    "href": "tokenize.html#сложные-запросы",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.8 Сложные запросы",
    "text": "9.8 Сложные запросы\nДобудем все сложные сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nacl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-id, -sentence, -lemma, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;\\\\\\\\mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Erant omnino itinera duo, quibus&lt;\\mark&gt; itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua&lt;\\mark&gt; die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius&lt;\\mark&gt; legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:”\n[4] “Ita sive casu sive consilio deorum immortalium quae&lt;\\mark&gt; pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius&lt;\\mark&gt; legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\n[6] “Ubi se diutius duci intellexit et diem instare quo&lt;\\mark&gt; die frumentum militibus metiri oporteret, convocatis eorum principibus, quorum magnam copiam in castris habebat, in his Diviciaco et Lisco, qui summo magistratui praeerat, quem vergobretum appellant Haedui, qui creatur annuus et vitae necisque in suos habet potestatem, graviter eos accusat, quod, cum neque emi neque ex agris sumi possit, tam necessario tempore, tam propinquis hostibus ab iis non sublevetur, praesertim cum magna ex parte eorum precibus adductus bellum susceperit [;”\n[7] “In castris Helvetiorum tabulae repertae sunt litteris Graecis confectae et ad Caesarem relatae, quibus in tabulis nominatim ratio confecta erat, qui numerus domo exisset eorum qui arma ferre possent, et item separatim, quot&lt;\\mark&gt; pueri, senes mulieresque.”\n[8] “Quibus&lt;\\mark&gt; proeliis calamitatibusque fractos, qui et sua virtute et populi Romani hospitio atque amicitia plurimum ante in Gallia potuissent, coactos esse Sequanis obsides dare nobilissimos civitatis et iure iurando civitatem obstringere sese neque obsides repetituros neque auxilium a populo Romano imploraturos neque recusaturos quo minus perpetuo sub illorum dicione atque imperio essent.”\n[9] “Quod si decessisset et liberam possessionem Galliae sibi tradidisset, magno se illum praemio remuneraturum et quaecumque&lt;\\mark&gt; bella geri vellet sine ullo eius labore et periculo confecturum.”\n[10] “Eo circiter hominum XVI milia expedita cum omni equitatu Ariovistus misit, quae&lt;\\mark&gt; copiae nostros terrerent et munitione prohiberent.”\n[11] “Ubi prima impedimenta nostri exercitus ab iis qui in silvis abditi latebant visa sunt, quod&lt;\\mark&gt; tempus inter eos committendi proelii convenerat, ut intra silvas aciem ordinesque constituerant atque ipsi sese confirmaverant, subito omnibus copiis provolaverunt impetumque in nostros equites fecerunt.”\n[12] “quibusnam manibus aut quibus viribus praesertim homines tantulae staturae (nam plerumque omnibus Gallis prae magnitudine corporum quorum&lt;\\mark&gt; brevitas nostra contemptui est) tanti oneris turrim in muro sese posse conlocare confiderent?”\n[13] “quarum&lt;\\mark&gt; rerum a nostris propter paucitatem fieri nihil poterat, ac non modo defesso ex pugna excedendi, sed ne saucio quidem eius loci ubi constiterat relinquendi ac sui recipiendi facultas dabatur.”\n[14] “Ita commutata fortuna eos qui in spem potiundorum castrorum venerant undique circumventos intercipiunt, et ex hominum milibus amplius XXX, quem&lt;\\mark&gt; numerum barbarorum ad castra venisse constabat, plus tertia parte interfecta reliquos perterritos in fugam coiciunt ac ne in locis quidem superioribus consistere patiuntur.”\n[15] “superiorum dierum Sabini cunctatio, perfugae confirmatio, inopia cibariorum, cui&lt;\\mark&gt; rei parum diligenter ab iis erat provisum, spes Venetici belli, et quod fere libenter homines id quod volunt credunt.”\n[16] “Qua&lt;\\mark&gt; re concessa laeti, ut explorata victoria, sarmentis virgultisque collectis, quibus fossas Romanorum compleant, ad castra pergunt.”\n[17] “qui complures annos Sueborum vim sustinuerunt, ad extremum tamen agris expulsi et multis locis Germaniae triennium vagati ad Rhenum pervenerunt, quas&lt;\\mark&gt; regiones Menapii incolebant.”\n[18] “Qua&lt;\\mark&gt; spe adducti Germani latius iam vagabantur et in fines Eburonum et Condrusorum, qui sunt Treverorum clientes, pervenerant.”\n[19] “Quod ubi Caesar comperit, omnibus iis rebus confectis, quarum&lt;\\mark&gt; rerum causa exercitum traducere constituerat, ut Germanis metum iniceret, ut Sugambros ulcisceretur, ut Ubios obsidione liberaret, diebus omnino XVIII trans Rhenum consumptis, satis et ad laudem et ad utilitatem profectum arbitratus se in Galliam recepit pontemque rescidit.”\n[20] “Quibus&lt;\\mark&gt; rebus nostri perterriti atque huius omnino generis pugnae imperiti, non eadem alacritate ac studio quo in pedestribus uti proeliis consuerant utebantur.”\n[21] “Nostri tamen, quod neque ordines servare neque firmiter insistere neque signa subsequi poterant atque alius alia ex navi quibuscumque&lt;\\mark&gt; signis occurrerat se adgregabat, magnopere perturbabantur;”\n[22] “Eadem nocte accidit ut esset luna plena, qui&lt;\\mark&gt; dies maritimos aestus maximos in Oceano efficere consuevit, nostrisque id erat incognitum.”\n[23] “Quibus&lt;\\mark&gt; rebus cognitis, principes Britanniae, qui post proelium ad Caesarem convenerant, inter se conlocuti, cum et equites et naves et frumentum Romanis deesse intellegerent et paucitatem militum ex castrorum exiguitate cognoscerent, quae hoc erant etiam angustior quod sine impedimentis Caesar legiones transportaverat, optimum factu esse duxerunt rebellione facta frumento commeatuque nostros prohibere et rem in hiemem producere, quod his superatis aut reditu interclusis neminem postea belli inferendi causa in Britanniam transiturum confidebant.”\n[24] “Qui cum propter siccitates paludum quo&lt;\\mark&gt; se reciperent non haberent, quo perfugio superiore anno erant usi, omnes fere in potestatem Labieni venerunt.”\n[25] “Qua&lt;\\mark&gt; re nuntiata Pirustae legatos ad eum mittunt qui doceant nihil earum rerum publico factum consilio, seseque paratos esse demonstrant omnibus rationibus de iniuriis satisfacere.”\n[26] “complures praeterea minores subiectae insulae existimantur, de quibus&lt;\\mark&gt; insulis nonnulli scripserunt dies continuos triginta sub bruma esse noctem.”\n[27] “Ex his omnibus longe sunt humanissimi qui Cantium incolunt, quae&lt;\\mark&gt; regio est maritima omnis, neque multum a Gallica differunt consuetudine.”\n[28] “Dum haec in his locis geruntur, Cassivellaunus ad Cantium, quod esse ad mare supra demonstravimus, quibus&lt;\\mark&gt; regionibus quattuor reges praeerant, Cingetorix, Carvilius, Taximagulus, Segovax, nuntios mittit atque eis imperat uti coactis omnibus copiis castra navalia de improviso adoriantur atque oppugnent.”\n[29] “habere sese, quae de re communi dicere vellent, quibus&lt;\\mark&gt; rebus controversias minui posse sperarent.”\n[30] “Interim ad Labienum per Remos incredibili celeritate de victoria Caesaris fama perfertur, ut, cum ab hibernis Ciceronis milia passuum abesset circiter LX, eoque post horam nonam diei Caesar pervenisset, ante mediam noctem ad portas castrorum clamor oreretur, quo&lt;\\mark&gt; clamore significatio victoriae gratulatioque ab Remis Labieno fieret.”\n[31] “Hi certo anni tempore in finibus Carnutum, quae&lt;\\mark&gt; regio totius Galliae media habetur, considunt in loco consecrato.”\n[32] “Viri, quantas&lt;\\mark&gt; pecunias ab uxoribus dotis nomine acceperunt, tantas ex suis bonis aestimatione facta cum dotibus communicant.”\n[33] “Quae&lt;\\mark&gt; civitates commodius suam rem publicam administrare existimantur, habent legibus sanctum, si quis quid de re publica a finitimis rumore aut fama acceperit, uti ad magistratum deferat neve cum quo alio communicet, quod saepe homines temerarios atque imperitos falsis rumoribus terreri et ad facinus impelli et de summis rebus consilium capere cognitum est.”\n[34] “sed magistratus ac principes in annos singulos gentibus cognationibusque hominum, qui una coierunt, quantum et quo&lt;\\mark&gt; loco visum est agri attribuunt atque anno post alio transire cogunt.”\n[35] “His rebus agitatis profitentur Carnutes se nullum periculum communis salutis causa recusare principesque ex omnibus bellum facturos pollicentur et, quo&lt;\\mark&gt;niam in praesentia obsidibus cavere inter se non possint ne res efferatur, ut iureiurando ac fide sanciatur, petunt, collatis militaribus signis, quo more eorum gravissima caerimonia continetur, ne facto initio belli ab reliquis deserantur.”\n[36] “Nam quae Cenabi oriente sole gesta essent, ante primam confectam vigiliam in finibus Arvernorum audita sunt, quod&lt;\\mark&gt; spatium est milium passuum circiter centum LX. Simili ratione ibi Vercingetorix, Celtilli filius, Arveruus, summae potentiae adulescens, cuius pater principatum Galliae totius obtinuerat et ob eam causam, quod regnum appetebat, ab civitate erat interfectus, convocatis suis clientibus facile incendit.”\n[37] “Eo cum venisset, timentes confirmat, praesidia in Rutenis provincialibus, Volcis Arecomicis, Tolosatibus circumque Narbonem, quae&lt;\\mark&gt; loca hostibus erant finitima, constituit;”\n[38] “Qua&lt;\\mark&gt; re per exploratores nuntiata Caesar legiones quas expeditas esse iusserat portis incensis intromittit atque oppido potitur, perpaucis ex hostium numero desideratis quin cuncti caperentur, quod pontis atque itinerum angustiae multitudinis fugam intercluserant.”\n[39] “Cum is murum hostium paene contingeret, et Caesar ad opus consuetudine excubaret militesque hortaretur, ne quod omnino tempus ab opere intermitteretur, paulo ante tertiam vigiliam est animadversum fumare aggerem, quem cuniculo hostes succenderant, eodemque tempore toto muro clamore sublato duabus portis ab utroque latere turrium eruptio fiebat, alii faces atque aridam materiem de muro in aggerem eminus iaciebant, picem reliquasque res, quibus ignis excitari potest, fundebant, ut quo primum curreretur aut cui&lt;\\mark&gt; rei ferretur auxilium vix ratio iniri posset.” [40] “Non virtute neque in acie vicisse Romanos, sed artificio quodam et scientia oppugnationis, cuius&lt;\\mark&gt; rei fuerint ipsi imperiti.”\n[41] “Sibi numquam placuisse Avaricum defendi, cuius&lt;\\mark&gt; rei testes ipsos haberet;”\n[42] “Hi similitudine armorum vehementer nostros perterruerunt, ac tametsi dextris humeris ex sertis animadvertebantur, quod&lt;\\mark&gt; insigne pactum esse consuerat, tamen id ipsum sui fallendi causa milites ab hostibus factum existimabant.”\n[43] “Quanto&lt;\\mark&gt; opere eorum animi magnitudinem admiraretur, quos non castrorum munitiones, non altitudo montis, non murus oppidi tardare potuisset, tanto opere licentiam arrogantiamque reprehendere, quod plus se quam imperatorem de victoria atque exitu rerum sentire existimarent;”\n[44] “Namque altera ex parte Bellovaci, quae&lt;\\mark&gt; civitas in Gallia maximam habet opinionem virtutis, instabant, alteram Camulogenus parato atque instructo exercitu tenebat;”\n[45] “ab sinistro, quem&lt;\\mark&gt; locum duodecima legio tenebat, cum primi ordines hostium transfixi telis concidissent, tamen acerrime reliqui resistebant, nec dabat suspicionem fugae quisquam.”\n[46] “Sub muro, quae&lt;\\mark&gt; pars collis ad orientem solem spectabat, hunc omnem locum copiae Gallorum compleverant fossamque et maceriam sex in altitudinem pedum praeduxerant.”\n[47] “Subito clamore sublato, qua&lt;\\mark&gt; significatione qui in oppido obsidebantur de suo adventu cognoscere possent, crates proicere, fundis, sagittis, lapidibus nostros de vallo proturbare reliquaque quae ad oppugnationem pertinent parant administrare.”\n[48] “Eius adventu ex colore vestitus cognito, quo&lt;\\mark&gt; insigni in proeliis uti consuerat, turmisque equitum et cohortibus visis quas se sequi iusserat, ut de locis superioribus haec declivia et devexa cernebantur, hostes proelium committunt.”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#сочетание-условий",
    "href": "tokenize.html#сочетание-условий",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.8 Сочетание условий",
    "text": "9.8 Сочетание условий\nДобудем все сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nacl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-id, -sentence, -lemma, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent)\n[1] “Erant omnino itinera duo, quibus itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:”\n[4] “Ita sive casu sive consilio deorum immortalium quae pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\n[6] “Ubi se diutius duci intellexit et diem instare quo die frumentum militibus metiri oporteret, convocatis eorum principibus, quorum magnam copiam in castris habebat, in his Diviciaco et Lisco, qui summo magistratui praeerat, quem vergobretum appellant Haedui, qui creatur annuus et vitae necisque in suos habet potestatem, graviter eos accusat, quod, cum neque emi neque ex agris sumi possit, tam necessario tempore, tam propinquis hostibus ab iis non sublevetur, praesertim cum magna ex parte eorum precibus adductus bellum susceperit [;”\n[7] “In castris Helvetiorum tabulae repertae sunt litteris Graecis confectae et ad Caesarem relatae, quibus in tabulis nominatim ratio confecta erat, qui numerus domo exisset eorum qui arma ferre possent, et item separatim, quot pueri, senes mulieresque.”\n[8] “Quibus proeliis calamitatibusque fractos, qui et sua virtute et populi Romani hospitio atque amicitia plurimum ante in Gallia potuissent, coactos esse Sequanis obsides dare nobilissimos civitatis et iure iurando civitatem obstringere sese neque obsides repetituros neque auxilium a populo Romano imploraturos neque recusaturos quo minus perpetuo sub illorum dicione atque imperio essent.”\n[9] “Quod si decessisset et liberam possessionem Galliae sibi tradidisset, magno se illum praemio remuneraturum et quaecumque bella geri vellet sine ullo eius labore et periculo confecturum.”\n[10] “Eo circiter hominum XVI milia expedita cum omni equitatu Ariovistus misit, quae copiae nostros terrerent et munitione prohiberent.”\n[11] “Ubi prima impedimenta nostri exercitus ab iis qui in silvis abditi latebant visa sunt, quod tempus inter eos committendi proelii convenerat, ut intra silvas aciem ordinesque constituerant atque ipsi sese confirmaverant, subito omnibus copiis provolaverunt impetumque in nostros equites fecerunt.”\n[12] “quibusnam manibus aut quibus viribus praesertim homines tantulae staturae (nam plerumque omnibus Gallis prae magnitudine corporum quorum brevitas nostra contemptui est) tanti oneris turrim in muro sese posse conlocare confiderent?”\n[13] “quarum rerum a nostris propter paucitatem fieri nihil poterat, ac non modo defesso ex pugna excedendi, sed ne saucio quidem eius loci ubi constiterat relinquendi ac sui recipiendi facultas dabatur.”\n[14] “Ita commutata fortuna eos qui in spem potiundorum castrorum venerant undique circumventos intercipiunt, et ex hominum milibus amplius XXX, quem numerum barbarorum ad castra venisse constabat, plus tertia parte interfecta reliquos perterritos in fugam coiciunt ac ne in locis quidem superioribus consistere patiuntur.”\n[15] “superiorum dierum Sabini cunctatio, perfugae confirmatio, inopia cibariorum, cui rei parum diligenter ab iis erat provisum, spes Venetici belli, et quod fere libenter homines id quod volunt credunt.”\n[16] “Qua re concessa laeti, ut explorata victoria, sarmentis virgultisque collectis, quibus fossas Romanorum compleant, ad castra pergunt.”\n[17] “qui complures annos Sueborum vim sustinuerunt, ad extremum tamen agris expulsi et multis locis Germaniae triennium vagati ad Rhenum pervenerunt, quas regiones Menapii incolebant.”\n[18] “Qua spe adducti Germani latius iam vagabantur et in fines Eburonum et Condrusorum, qui sunt Treverorum clientes, pervenerant.”\n[19] “Quod ubi Caesar comperit, omnibus iis rebus confectis, quarum rerum causa exercitum traducere constituerat, ut Germanis metum iniceret, ut Sugambros ulcisceretur, ut Ubios obsidione liberaret, diebus omnino XVIII trans Rhenum consumptis, satis et ad laudem et ad utilitatem profectum arbitratus se in Galliam recepit pontemque rescidit.”\n[20] “Quibus rebus nostri perterriti atque huius omnino generis pugnae imperiti, non eadem alacritate ac studio quo in pedestribus uti proeliis consuerant utebantur.”\n[21] “Nostri tamen, quod neque ordines servare neque firmiter insistere neque signa subsequi poterant atque alius alia ex navi quibuscumque signis occurrerat se adgregabat, magnopere perturbabantur;”\n[22] “Eadem nocte accidit ut esset luna plena, qui dies maritimos aestus maximos in Oceano efficere consuevit, nostrisque id erat incognitum.”\n[23] “Quibus rebus cognitis, principes Britanniae, qui post proelium ad Caesarem convenerant, inter se conlocuti, cum et equites et naves et frumentum Romanis deesse intellegerent et paucitatem militum ex castrorum exiguitate cognoscerent, quae hoc erant etiam angustior quod sine impedimentis Caesar legiones transportaverat, optimum factu esse duxerunt rebellione facta frumento commeatuque nostros prohibere et rem in hiemem producere, quod his superatis aut reditu interclusis neminem postea belli inferendi causa in Britanniam transiturum confidebant.”\n[24] “Qui cum propter siccitates paludum quo se reciperent non haberent, quo perfugio superiore anno erant usi, omnes fere in potestatem Labieni venerunt.”\n[25] “Qua re nuntiata Pirustae legatos ad eum mittunt qui doceant nihil earum rerum publico factum consilio, seseque paratos esse demonstrant omnibus rationibus de iniuriis satisfacere.”\n[26] “complures praeterea minores subiectae insulae existimantur, de quibus insulis nonnulli scripserunt dies continuos triginta sub bruma esse noctem.”\n[27] “Ex his omnibus longe sunt humanissimi qui Cantium incolunt, quae regio est maritima omnis, neque multum a Gallica differunt consuetudine.”\n[28] “Dum haec in his locis geruntur, Cassivellaunus ad Cantium, quod esse ad mare supra demonstravimus, quibus regionibus quattuor reges praeerant, Cingetorix, Carvilius, Taximagulus, Segovax, nuntios mittit atque eis imperat uti coactis omnibus copiis castra navalia de improviso adoriantur atque oppugnent.”\n[29] “habere sese, quae de re communi dicere vellent, quibus rebus controversias minui posse sperarent.”\n[30] “Interim ad Labienum per Remos incredibili celeritate de victoria Caesaris fama perfertur, ut, cum ab hibernis Ciceronis milia passuum abesset circiter LX, eoque post horam nonam diei Caesar pervenisset, ante mediam noctem ad portas castrorum clamor oreretur, quo clamore significatio victoriae gratulatioque ab Remis Labieno fieret.”\n[31] “Hi certo anni tempore in finibus Carnutum, quae regio totius Galliae media habetur, considunt in loco consecrato.”\n[32] “Viri, quantas pecunias ab uxoribus dotis nomine acceperunt, tantas ex suis bonis aestimatione facta cum dotibus communicant.”\n[33] “Quae civitates commodius suam rem publicam administrare existimantur, habent legibus sanctum, si quis quid de re publica a finitimis rumore aut fama acceperit, uti ad magistratum deferat neve cum quo alio communicet, quod saepe homines temerarios atque imperitos falsis rumoribus terreri et ad facinus impelli et de summis rebus consilium capere cognitum est.”\n[34] “sed magistratus ac principes in annos singulos gentibus cognationibusque hominum, qui una coierunt, quantum et quo loco visum est agri attribuunt atque anno post alio transire cogunt.”\n[35] “His rebus agitatis profitentur Carnutes se nullum periculum communis salutis causa recusare principesque ex omnibus bellum facturos pollicentur et, quoniam in praesentia obsidibus cavere inter se non possint ne res efferatur, ut iureiurando ac fide sanciatur, petunt, collatis militaribus signis, quo more eorum gravissima caerimonia continetur, ne facto initio belli ab reliquis deserantur.”\n[36] “Nam quae Cenabi oriente sole gesta essent, ante primam confectam vigiliam in finibus Arvernorum audita sunt, quod spatium est milium passuum circiter centum LX. Simili ratione ibi Vercingetorix, Celtilli filius, Arveruus, summae potentiae adulescens, cuius pater principatum Galliae totius obtinuerat et ob eam causam, quod regnum appetebat, ab civitate erat interfectus, convocatis suis clientibus facile incendit.”\n[37] “Eo cum venisset, timentes confirmat, praesidia in Rutenis provincialibus, Volcis Arecomicis, Tolosatibus circumque Narbonem, quae loca hostibus erant finitima, constituit;”\n[38] “Qua re per exploratores nuntiata Caesar legiones quas expeditas esse iusserat portis incensis intromittit atque oppido potitur, perpaucis ex hostium numero desideratis quin cuncti caperentur, quod pontis atque itinerum angustiae multitudinis fugam intercluserant.”\n[39] “Cum is murum hostium paene contingeret, et Caesar ad opus consuetudine excubaret militesque hortaretur, ne quod omnino tempus ab opere intermitteretur, paulo ante tertiam vigiliam est animadversum fumare aggerem, quem cuniculo hostes succenderant, eodemque tempore toto muro clamore sublato duabus portis ab utroque latere turrium eruptio fiebat, alii faces atque aridam materiem de muro in aggerem eminus iaciebant, picem reliquasque res, quibus ignis excitari potest, fundebant, ut quo primum curreretur aut cui rei ferretur auxilium vix ratio iniri posset.” [40] “Non virtute neque in acie vicisse Romanos, sed artificio quodam et scientia oppugnationis, cuius rei fuerint ipsi imperiti.”\n[41] “Sibi numquam placuisse Avaricum defendi, cuius rei testes ipsos haberet;”\n[42] “Hi similitudine armorum vehementer nostros perterruerunt, ac tametsi dextris humeris ex sertis animadvertebantur, quod insigne pactum esse consuerat, tamen id ipsum sui fallendi causa milites ab hostibus factum existimabant.”\n[43] “Quanto opere eorum animi magnitudinem admiraretur, quos non castrorum munitiones, non altitudo montis, non murus oppidi tardare potuisset, tanto opere licentiam arrogantiamque reprehendere, quod plus se quam imperatorem de victoria atque exitu rerum sentire existimarent;”\n[44] “Namque altera ex parte Bellovaci, quae civitas in Gallia maximam habet opinionem virtutis, instabant, alteram Camulogenus parato atque instructo exercitu tenebat;”\n[45] “ab sinistro, quem locum duodecima legio tenebat, cum primi ordines hostium transfixi telis concidissent, tamen acerrime reliqui resistebant, nec dabat suspicionem fugae quisquam.”\n[46] “Sub muro, quae pars collis ad orientem solem spectabat, hunc omnem locum copiae Gallorum compleverant fossamque et maceriam sex in altitudinem pedum praeduxerant.”\n[47] “Subito clamore sublato, qua significatione qui in oppido obsidebantur de suo adventu cognoscere possent, crates proicere, fundis, sagittis, lapidibus nostros de vallo proturbare reliquaque quae ad oppugnationem pertinent parant administrare.”\n[48] “Eius adventu ex colore vestitus cognito, quo insigni in proeliis uti consuerat, turmisque equitum et cohortibus visis quas se sequi iusserat, ut de locis superioribus haec declivia et devexa cernebantur, hostes proelium committunt.”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "tokenize.html#сочетания-признаков",
    "href": "tokenize.html#сочетания-признаков",
    "title": "9  Токенизация, лемматизация, POS-тэггинг и синтаксический анализ",
    "section": "9.8 Сочетания признаков",
    "text": "9.8 Сочетания признаков\nДобудем все сложные предложения, в состав которых входят придаточные относительные (адноминальные).\n\n# адноминальные предложения\nacl_ids &lt;- caesar_pos3 |&gt; \n  filter(str_detect(dep_rel, \"acl:relcl\")) |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  pull(id)\n\n\nacl &lt;- caesar_pos3 |&gt; \n  unite(id, c(\"doc_id\", \"sentence_id\")) |&gt; \n  filter(id %in% acl_ids) |&gt; \n  as_tibble() |&gt; \n  mutate(token_id = as.numeric(token_id), \n        head_token_id = as.numeric(head_token_id))\n\nacl\n\n\n  \n\n\n\nПосмотрим на одно из таких предложений, в котором проявилась характерная для Цезаря черта: повторять антецедент относительного местоимения в придаточном. Например, вместо “было два пути, которыми…” он говорит “было два пути, каковыми путями…”.\n\nexample_sentence &lt;- acl |&gt; \n  filter(id == \"doc1_43\") |&gt; \n  select(-sentence, -deps, -misc) |&gt; \n  relocate(dep_rel, .before = upos) |&gt; \n  relocate(head_token_id, .before = upos)\n\nexample_sentence\n\n\n  \n\n\n\nТакие случаи можно попробовать выловить при помощи условия или нескольких условий, например достать такие относительные местоимения, сразу за которыми стоит их вершина:\n\nout &lt;- acl |&gt; \n  filter(str_detect(feats, \"PronType=Rel\") & \n        dep_rel == \"det\" & \n        head_token_id == (token_id + 1)) |&gt; \n  select(id, token_id, token, sentence) \n\nout |&gt; \n  mutate(html_token = paste0(\"&lt;mark&gt;\", token, \"&lt;/mark&gt;\")) |&gt; \n  mutate(html_sent = str_replace(sentence, token, html_token)) |&gt; \n  pull(html_sent) |&gt; \n  head(5)\n[1] “Erant omnino itinera duo, quibus itineribus domo exire possent:”\n[2] “Omnibus rebus ad profectionem comparatis diem dicunt, qua die ad ripam Rhodani omnes conveniant.”\n[3] “Ubi de eius adventu Helvetii certiores facti sunt, legatos ad eum mittunt nobilissimos civitatis, cuius legationis Nammeius et Verucloetius principem locum obtinebant, qui dicerent sibi esse in animo sine ullo maleficio iter per provinciam facere, propterea quod aliud iter haberent nullum:” [4] “Ita sive casu sive consilio deorum immortalium quae pars civitatis Helvetiae insignem calamitatem populo Romano intulerat, ea princeps poenam persolvit.”\n[5] “cuius legationis Divico princeps fuit, qui bello Cassiano dux Helvetiorum fuerat.”\n\nТак мы кое-что полезное поймали, но не все, потому что между местоимением и его антецедентом возможны другие слова (например, “каковыми опасными путями”). С другой стороны, есть и кое-что лишнее, а именно случаи инкорпорации антецедента в придаточное предложение (“quae pars …, ea” вместо “ea pars, quae…” ). В общем, условие можно дальше дорабатывать, но мы пока не будем этого делать.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Токенизация, лемматизация, POS-тэггинг и синтаксический анализ</span>"
    ]
  },
  {
    "objectID": "pmi.html",
    "href": "pmi.html",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "",
    "text": "13.1 Скользящее окно\nПрежде всего разделим новости на контекстные окна фиксированной величины. Чем меньше окно, тем больше синтаксической информации оно хранит.\nload(\"../data/news_tokens_pruned.Rdata\")\n\nnested_news &lt;- news_tokens_pruned |&gt; \n  dplyr::select(-topic) |&gt; \n  nest(tokens = c(token))\n\nnested_news\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out  |&gt; \n    transpose()  |&gt; \n    pluck(\"result\")  |&gt; \n    compact()  |&gt; \n    bind_rows()\n}\nДеление на окна может потребовать нескольких минут. Чем больше окно, тем больше потребуется времени и тем больше будет размер таблицы.\nnews_windows &lt;- nested_news |&gt; \n  mutate(tokens = map(tokens, slide_windows, 10L)) %&gt;% \n  unnest(tokens) %&gt;% \n  unite(window_id, id, window_id)\n\nnews_windows\nload(\"../data/news_windows.Rdata\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#что-такое-pmi",
    "href": "pmi.html#что-такое-pmi",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.2 Что такое PMI",
    "text": "13.2 Что такое PMI\nОбычная мера ассоциации между словами, которой пользуются лингвисты, — точечная взаимная информация, или PMI (pointwise mutual information). Она рассчитывается по формуле:\n\\[PMI\\left(x;y\\right)=\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}}\\]\nВ числителе — вероятность встретить два слова вместе (например, в пределах одного документа или одного «окна» длинной n слов). В знаменателе — произведение вероятностей встретить каждое из слов отдельно. Если слова чаще встречаются вместе, логарифм будет положительным; если по отдельности — отрицательным.\nПосчитаем PMI на наших данных, воспользовавшись подходящей функцией из пакета widyr.\n\nlibrary(widyr)\nnews_pmi  &lt;- news_windows  |&gt; \n  pairwise_pmi(token, window_id)\n\n\nnews_pmi |&gt; \n  arrange(-abs(pmi))",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#почему-ppmi",
    "href": "pmi.html#почему-ppmi",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.3 Почему PPMI",
    "text": "13.3 Почему PPMI\nВ отличие от коэффициента корреляции, например, PMI может варьироваться от \\(-\\infty\\) до \\(+\\infty\\), но негативные значения проблематичны. Они означают, что вероятность встретить эти два слова вместе меньше, чем мы бы ожидали в результате случайного совпадения. Проверить это без огромного корпуса невозможно: если у нас есть \\(w_1\\) и \\(w_2\\), каждое из которых встречается с вероятностью \\(10^{-6}\\), то трудно удостовериться в том, что \\(p(w_1, w_2)\\) значимо отличается от \\(10^{-12}\\). Поэтому негативные значения PMI принято заменять нулями. В таком случае формула выглядит так:\n\\[ PMI\\left(x;y\\right)=max(\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}},0) \\] Для подобной замены подойдет векторизованное условие.\n\nnews_ppmi &lt;- news_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi)) \n\nnews_ppmi |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\nЕсли мы развернем такую матрицу вширь, то она получится очень разреженной; чтобы получить плотные векторы слов, необходимо прибегнуть к SVD.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#svd-на-матрице-с-ppmi",
    "href": "pmi.html#svd-на-матрице-с-ppmi",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.4 SVD на матрице с PPMI",
    "text": "13.4 SVD на матрице с PPMI\nДля этого можно передать тиббл фунции widely_svd() для вычисления сингулярного разложения. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов:\n\nword_emb &lt;- news_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n\n\nword_emb",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#визуализация-топиков",
    "href": "pmi.html#визуализация-топиков",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.5 Визуализация топиков",
    "text": "13.5 Визуализация топиков\nСнова визуализируем главные компоненты нашего векторного пространства.\n\nword_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#ближайшие-соседи",
    "href": "pmi.html#ближайшие-соседи",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.6 Ближайшие соседи",
    "text": "13.6 Ближайшие соседи\nИсследуем наши эмбеддинги, используя уже знакомую функцию, которая считает косинусное сходство между словами.\n\nsource(\"../helper_scripts/nearest_neighbors.R\")\n\n\nword_emb |&gt; \n  nearest_neighbors(\"сборная\")\n\n\n  \n\n\nword_emb |&gt; \n  nearest_neighbors(\"завод\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#от-эмбеддингов-слов-к-эмбеддингам-документов",
    "href": "pmi.html#от-эмбеддингов-слов-к-эмбеддингам-документов",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.7 От эмбеддингов слов к эмбеддингам документов",
    "text": "13.7 От эмбеддингов слов к эмбеддингам документов\nИспользуя документы как суммы входящих в них слов, мы может использовать эмбеддинги для нахождения ближайших документов в корпусе.\nПервая матрица хранит информацию о встречаемости слов в документах; в рядах здесь документы; в столбцах – слова (всего 6299).\n\ncounts_mx &lt;- news_tokens_pruned %&gt;%\n  count(id, token) %&gt;%\n  cast_sparse(id, token, n)\ndim(counts_mx)\n\n[1] 3407 6299\n\n\nВторая матрица – это наши эмбеддинги в разреженном виде. Число столбцов соответствует числу сингулярных векторов, которое мы задали при факторизации.\n\nembedding_mx &lt;- word_emb %&gt;%\n  cast_sparse(word, dimension, value)\ndim(embedding_mx)\n\n[1] 6299  100\n\n\nПеремножение матрицы \\((3407 \\times 6299)\\) на \\((6299 \\times 20)\\) вернет матрицу размером \\((3407 \\times 20)\\), то есть мы получим эмбеддинги для документов.\n\ndoc_mx &lt;- counts_mx %*% embedding_mx\ndim(doc_mx)\n\n[1] 3407  100",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "pmi.html#похожие-документы",
    "href": "pmi.html#похожие-документы",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.8 Похожие документы",
    "text": "13.8 Похожие документы\nПревратим разреженную матрицу обратно в датафрейм.\n\ndoc_emb_long &lt;- doc_mx |&gt; \n  as.matrix() |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\")\n\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nПознакомимся с соседями.\n\nload(\"../data/news.Rdata\")\nnews_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number())) |&gt; \n  filter(id %in% c(\"doc14\", \"doc1\", \"doc1000\", \"doc1142\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nРелевантная выдача здесь – только первый документ. С этой задачей LSA в нашем случае справилась лучше. Посмотрим теперь на семантическое пространство слов. Для этого придется снизить размерность со 100 до 2 измерений.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "pmi.html#d-визуализации-пространства-слов",
    "href": "pmi.html#d-визуализации-пространства-слов",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.7 2D-визуализации пространства слов",
    "text": "13.7 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы снова используем алгоритм UMAP.\n\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🏈 Теперь попробуем построить векторное пространство с использованием поверхностных нейросетей.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "pmi.html#математическое-волшебство",
    "href": "pmi.html#математическое-волшебство",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.10 Математическое волшебство",
    "text": "13.10 Математическое волшебство\nЕсли слова – это векторы, то их можно складывать и вычитать, как мы это делали в школе. Согласно известному примеру, если из вектора слова “король” вычесть вектор “мужчина” и прибавить вектор “женщина”, получатся числа, соответствующие слову “королева”. Правда, у нас слишком небольшой датасет, так что на озарения лучше не рассчитывать.\n\nlibrary(plyr)\n\nmystery_word &lt;- unrowname(word_emb_mx[\"спортсмен\",] + word_emb_mx[\"мяч\",]) \n\nhead(mystery_word)\n\n          1           2           3           4           5           6 \n 0.02899943 -0.01946437  0.12604935  0.02450570 -0.03578517 -0.01520623 \n\n\n\nmystery_tbl &lt;- tibble(\n  word = \"mystery\",\n  dimension = as.numeric(names(mystery_word)),\n  value = as.numeric(mystery_word)\n)\n\n\nword_emb_new &lt;- word_emb |&gt; \n  bind_rows(mystery_tbl)\n\nnearest_neighbors(word_emb_new, \"mystery\")\n\n\n  \n\n\n\nОтличная работа 🏈",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "pmi.html#сглаженная-pmi-dpf",
    "href": "pmi.html#сглаженная-pmi-dpf",
    "title": "13  Эмбеддинги на основе PMI-матрицы",
    "section": "13.11 Сглаженная PMI (DPF)",
    "text": "13.11 Сглаженная PMI (DPF)\nКак уже было сказано, для редких слов PMI оказывается завышена: т.к. вероятность встретить их в корпусе очень мала, знаменатель в формуле PMI тоже уменьшается. Иными словами, существует негативная корреляция между частотностью слова и его PMI.\nЕсть несколько способов если не совсем избавиться от негативной корреляции, то по крайней мере уменьшить ее абсолютное значение. Один из них заключается в том, чтобы чуть завысить знаменатель в формуле PMI, например, возведя его в дробную степень. Такой подход применялся в недавнем исследовании по цифровой истории идей, авторы которого предлагают использовать меру под названием DPF (Distributional Probability Factor). Показатель степени α при этом устанавливается в районе α = 0.75.\n\\[ DPF\\ \\left(x;y\\right)=\\frac{P\\left(x,y\\right)}{(P\\left(x\\right)P{\\left(y\\right))}^\\alpha} \\]\nВторой способ известен как сглаживание Лапласа, оно же аддитивное сглаживание. В этом случае мы прибавляем единицу к частоте каждого слова, как будто видели его на один раз больше (этот способ задействуется и в некоторых алгоритмах машинного обучения).\nПрименять сглаживание функция pairwise_pmi() не умеет, поэтому на этот раз мы посчитаем взаимную информацию чуть иначе.\n\n# вероятность для каждого слова\nunigram_probs &lt;- news_tokens_pruned  |&gt; \n  dplyr::count(token, sort = TRUE)  |&gt; \n  mutate(p = n / sum(n))\n\nunigram_probs\n\n\n  \n\n\n# вероятность встретить слова вместе\nbigram_probs &lt;- news_tokens_pruned  |&gt; \n  pairwise_count(token, id, diag = TRUE, sort = TRUE)  |&gt; \n  mutate(p = n / sum(n))\n\nbigram_probs |&gt; \n  arrange(-n) # на этом этапе можно отфильтровать n &lt; 2\n\n\n  \n\n\n\nПосчитаем нормализованную вероятность: это вероятность встретить два слова вместе, деленная на произведение вероятностей встретить каждое из них отдельно.\n\n# взаимная информация\nnormalized_probs &lt;- bigram_probs  |&gt; \n  # filter(n &gt; 4)  |&gt; \n  dplyr::rename(word1 = item1, word2 = item2)  |&gt; \n  left_join(unigram_probs  |&gt; \n              select(word1 = token, p1 = p),\n            by = \"word1\")  |&gt; \n  left_join(unigram_probs %&gt;%\n              select(word2 = token, p2 = p),\n            by = \"word2\")  |&gt; \n  mutate(p_together = p / p1 / p2)\n\nnormalized_probs\n\n\n  \n\n\n\nРассчитаем заново PMI и добавим сглаживание.\n\n# pmi & dpf\npmi_data &lt;- normalized_probs |&gt; \n  mutate(pmi = log(p_together)) |&gt; \n  mutate(dpf = p / (p1 * p2)^0.75) |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi))\n\npmi_data |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\nСравним зависимость от частотности в трех случаях.\n\nlibrary(ggpubr)\n# корреляция\npmi_data |&gt;\n  filter(word1 == \"футболист\") |&gt; \n  filter(!word1 == word2) |&gt; \n  select(word2, pmi, ppmi, dpf, p2) |&gt; \n  pivot_longer(cols = c(pmi, dpf, ppmi), names_to = \"measure\", values_to = \"value\") |&gt; \n  ggplot(aes(p2, value, color = value)) +\n   geom_jitter(width = 0.0002, height = 0.0002, \n              show.legend = FALSE, alpha = 0.5) +\n  xlim(NA, 0.0045) + # zoom in\n  facet_wrap(~ measure, scales = \"free_y\") +\n  stat_cor(aes(label = ..r.label..),\n           method = \"pearson\",\n           label.x = 0.002,\n           color = \"tomato\",\n           geom = \"label\"\n  ) +\n  theme_bw() +\n  labs(title = \"Корреляция между частотностью слова и PMI / DPF\", \n       y = NULL)\n\n\n\n\n\n\n\n\nАналогично тому, что мы делали выше, можно снизить размерность DPF-матрицы при помощи SVD или же сохранить ее в разреженном виде для изучения совместной встречаемости слов.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Эмбеддинги на основе PMI-матрицы</span>"
    ]
  },
  {
    "objectID": "embeddings.html",
    "href": "embeddings.html",
    "title": "12  Векторные представления слов",
    "section": "",
    "text": "12.1 Векторы в лингвистике\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Есть несколько возможных решений.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравнивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход – зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности.\nВ этом уроке мы рассмотрим алгоритм LSA и векторные модели на основе PMI. В первом случае анализируется матрица термин-документ, во втором – матрица термин-термин. Оба подхода предполагают использование SVD.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#svd",
    "href": "embeddings.html#svd",
    "title": "12  Векторные представления слов",
    "section": "12.2 SVD",
    "text": "12.2 SVD\nДля любых текстовых данных и матрица термин-термин и матрица термин-документ будет очень разряженной (то есть большая часть значений будет равна нулю). Необходимо “переупорядочить” ее так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD). При сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы (К. Маннинг, П. Рагхаван, Х. Шютце 2020, 407).\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. О сингулярных векторах можно думать как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.\nПосмотрим теперь, как SVD применяется при анализе текста.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#латентно-семантический-анализ",
    "href": "embeddings.html#латентно-семантический-анализ",
    "title": "12  Векторные представления слов",
    "section": "12.3 Латентно-семантический анализ",
    "text": "12.3 Латентно-семантический анализ\nLSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) – это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками). Слово “latent” (англ. “скрытый”) в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить.\nСоздатели метода LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов представляет собой своего рода сумму (или среднее) отдельных слов. Этот принцип является общим для всех векторных моделей.\nНа входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово “дорожное”, например, служит лучшим показателем темы, чем слово “происшествие”, которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции. Общий принцип действия алгоритма подробно объясняется на очень простом примере по ссылке; мы же перейдем сразу к анализу реальных данных.\n\n12.3.1 Подгтовка данных\nМы воспользуемся датасетом с подборкой новостей на русском языке (для простоты возьмем из него лишь один год). Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nlibrary(tidyverse)\nload(\"../data/news.Rdata\")\n\nnews_2019 |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\nДобавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nМногие слова встречаются всего несколько раз и для тематического моделирования бесполезны. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\n\nWarning in rm(news_tokens): object 'news_tokens' not found\n\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.\n\n\n12.3.2 TF-IDF: опрятный подход\nВместо показателей абсолютной встречаемости при анализе больших текстовых данных применяется tf-idf. Эта статистическая мера не используется, если дана матрица термин-термин, но она хорошо работает с матрицами термин-документ, позволяя повысить веса для тех слов, которые служат хорошими дискриминаторами. Например, “заявил” и “отметил”, хотя это не стоп-слова, могут встречаться в разных темах.\n\nnews_counts &lt;- news_tokens_pruned |&gt;\n  count(token, id)\n\nnews_counts\n\n\nnews_counts |&gt; \n  arrange(id)\n\n\n  \n\n\n\nДобавляем tf_idf.\n\nnews_tf_idf &lt;- news_counts |&gt; \n  bind_tf_idf(token, id, n) |&gt; \n  arrange(tf_idf) |&gt; \n  select(-n, -tf, -idf)\n\nnews_tf_idf\n\n\n\n\n  \n\n\n\n\n\n12.3.3 DocumentTermMatrix\nПосмотрим на размер получившейся таблицы.\n\nobject.size(news_tf_idf)\n\n5369712 bytes\n\nformat(object.size(news_tf_idf), units = \"auto\")\n\n[1] \"5.1 Mb\"\n\n\nЧтобы вычислить SVD, такую таблицу необходимо преобразовать в матрицу термин-документ. Оценим ее размер:\n\n# число уникальных токенов\nm &lt;- unique(news_tf_idf$token) |&gt; \n  length()\nm\n\n[1] 6299\n\n# число уникальных документов\nn &lt;- unique(news_tf_idf$id) |&gt; \n  length()  \nn\n\n[1] 3407\n\n# число элементов в матрице \nm * n\n\n[1] 21460693\n\n\nИспользуем специальный формат для хранения разреженных матриц.\n\ndtm &lt;- news_tf_idf |&gt; \n  cast_sparse(token, id, tf_idf)\n\n\n# первые 10 рядов и 5 столбцов\ndtm[1:10, 1:5]\n\n10 x 5 sparse Matrix of class \"dgCMatrix\"\n               doc608     doc1670     doc2170     doc2184     doc2219\nранее     0.003530193 .           .           0.005002585 .          \nроссии    0.010127611 0.003675658 0.004689633 0.004783897 0.005471238\nсловам    .           .           0.011776384 .           0.006869557\nрублей    0.006686328 .           0.027865190 .           .          \nрассказал 0.007250151 .           .           0.010274083 .          \nданным    0.007406320 .           .           .           0.012003345\nиздание   0.007759457 .           .           .           0.012575671\nходе      0.008675929 .           .           .           .          \nзаявил    0.016729327 .           .           .           .          \nчастности 0.008860985 .           0.012309349 .           .          \n\n\nСнова уточним размер матрицы.\n\nformat(object.size(dtm), units = \"auto\")\n\n[1] \"3 Mb\"\n\n\n\n\n12.3.4 SVD с пакетом irlba\nМетод для эффективного вычисления усеченного SVD на больших матрицах реализован в пакете irlba. Возможно, придется подождать ⏳.\n\nlibrary(irlba)\nlsa_space&lt;- irlba::irlba(dtm, 50) \n\nФункция вернет список из трех элементов:\n\nd: k аппроксимированных сингулярных значений;\nu: k аппроксимированных левых сингулярных векторов;\nv: k аппроксимированных правых сингулярных векторов.\n\nПолученную LSA-модель можно использовать для поиска наиболее близких слов и документов или для изучения тематики корпуса – в последнем случае нас может интересовать, какие топики доминируют в тех или иных документах и какими словами они в первую очередь представлены.\n\n\n12.3.5 Эмбеддинги слов\nВернем имена рядов матрице левых сингулярных векторов и добавим имена столбцов.\n\nrownames(lsa_space$u) &lt;- rownames(dtm)\ncolnames(lsa_space$u) &lt;- paste0(\"dim\", 1:50)\n\nТеперь посмотрим на эмбеддинги слов.\n\nword_emb &lt;- lsa_space$u |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"word\") |&gt; \n  as_tibble()\n\nword_emb\n\n\n  \n\n\n\nПреобразуем наши данные в длинный формат.\n\nword_emb_long &lt;- word_emb |&gt; \n  pivot_longer(-word, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\nword_emb_long\n\n\n  \n\n\n\n\n\n12.3.6 Визуализация топиков\nВизуализируем несколько топиков, чтобы понять, насколько они осмыслены.\n\nword_emb_long |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n12.3.7 Ближайшие соседи\nЭмбеддинги можно использовать для поиска ближайших соседей.\n\nlibrary(widyr)\n\nnearest_neighbors &lt;- function(df, feat, doc=F) {\n  inner_f &lt;- function() {\n    widely(\n        ~ {\n          y &lt;- .[rep(feat, nrow(.)), ]\n          res &lt;- rowSums(. * y) / \n            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))\n          \n          matrix(res, ncol = 1, dimnames = list(x = names(res)))\n        },\n        sort = TRUE\n    )}\n  if (doc) {\n    df |&gt; inner_f()(doc, dimension, value) }\n  else {\n    df |&gt; inner_f()(word, dimension, value)\n  } |&gt; \n    select(-item2)\n}\n\n\nnearest_neighbors(word_emb_long, \"сборная\")\n\n\n  \n\n\nnearest_neighbors(word_emb_long, \"завод\")\n\n\n  \n\n\n\n\n\n12.3.8 Похожие документы\nИнформация о документах хранится в матрице правых сингулярных векторов.\n\nrownames(lsa_space$v) &lt;- colnames(dtm)\ncolnames(lsa_space$v) &lt;- paste0(\"dim\", 1:50)\n\nПосмотрим на эмбеддинги документов.\n\ndoc_emb &lt;- lsa_space$v |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  as_tibble()\n\ndoc_emb\n\n\n  \n\n\n\nПреобразуем в длинный формат.\n\ndoc_emb_long &lt;- doc_emb |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\ndoc_emb_long\n\n\n  \n\n\n\nИ найдем соседей для произвольного документа.\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nВыведем документ 14 вместе с его соседями.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc14\", \"doc392\", \"doc2043\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nПоздравляем, вы построили свою первую рекомендательную систему 🍸.\n\n\n12.3.9 2D-визуализация пространства документов\nДля снижения размерности мы используем алгоритм UMAP. В отличие от PCA, он снижает размерность нелинейно, и в этом отношении похож на t-SNE.\n\nlibrary(uwot)\nset.seed(07062024)\nviz_lsa &lt;- umap(lsa_space$v ,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все документы вложены теперь в двумерное пространство.\n\ndim(viz_lsa)\n\n[1] 3407    2\n\n\nЗакодировав цветом рубрики новостного сайта, нанесем документы на “карту”.\n\n\ntibble(doc = rownames(viz_lsa),\n       topic = news_2019$topic,\n       V1 = viz_lsa[, 1], \n       V2 = viz_lsa[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = doc, color = topic)) + \n  geom_text(size = 2, alpha = 0.8, position = position_jitter(width = 1.5, height = 1.5)) +\n  annotate(geom = \"rect\", ymin = -10, ymax = 0, xmin = -20, xmax = -10, alpha = 0.2, color = \"tomato\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nВот несколько новостей из небольшого тематического кластера, выделенного на карте квадратом.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc718\", \"doc2437\", \"doc2918\")) |&gt; \n  mutate(text = str_trunc(text, 70))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#эмбеддинги-на-основе-pmi-матрицы",
    "href": "embeddings.html#эмбеддинги-на-основе-pmi-матрицы",
    "title": "12  Векторные представления слов",
    "section": "12.4 Эмбеддинги на основе PMI-матрицы",
    "text": "12.4 Эмбеддинги на основе PMI-матрицы\nТеперь рассмотрим второй способ построения эмбеддингов, когда за основу берется матрица термин-термин.\n\n12.4.1 Скользящее окно\nПрежде всего разделим новости на контекстные окна фиксированной величины. Чем меньше окно, тем больше синтаксической информации оно хранит.\n\nlibrary(tidyr)\n\nnested_news &lt;- news_tokens_pruned |&gt; \n  dplyr::select(-topic) |&gt; \n  nest(tokens = c(token))\n\nnested_news\n\n\n  \n\n\n\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %&gt;%\n    transpose() %&gt;%\n    pluck(\"result\") %&gt;%\n    compact() %&gt;%\n    bind_rows()\n}\n\nДеление на окна может потребовать нескольких минут. Чем больше окно, тем больше потребуется времени и тем больше будет размер таблицы.\n\nnews_windows &lt;- nested_news |&gt; \n  mutate(tokens = map(tokens, slide_windows, 10L)) %&gt;% \n  unnest(tokens) %&gt;% \n  unite(window_id, id, window_id)\n\nnews_windows\n\n\nload(\"../data/news_windows.Rdata\")\n\n\n\n12.4.2 Что такое PMI\nОбычная мера ассоциации между словами, которой пользуются лингвисты, — точечная взаимная информация, или PMI (pointwise mutual information). Она рассчитывается по формуле:\n\\[PMI\\left(x;y\\right)=\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}}\\]\nВ числителе — вероятность встретить два слова вместе (например, в пределах одного документа или одного «окна» длинной n слов). В знаменателе — произведение вероятностей встретить каждое из слов отдельно. Если слова чаще встречаются вместе, логарифм будет положительным; если по отдельности — отрицательным.\nПосчитаем PMI на наших данных, воспользовавшись подходящей функцией из пакета widyr.\n\nlibrary(widyr)\nnews_pmi  &lt;- news_windows  |&gt; \n  pairwise_pmi(token, window_id)\n\n\nnews_pmi |&gt; \n  arrange(-abs(pmi))\n\n\n  \n\n\n\n\n\n12.4.3 Почему PPMI\nВ отличие от коэффициента корреляции, например, PMI может варьироваться от \\(-\\infty\\) до \\(+\\infty\\), но негативные значения проблематичны. Они означают, что вероятность встретить эти два слова вместе меньше, чем мы бы ожидали в результате случайного совпадения. Проверить это без огромного корпуса невозможно: если у нас есть \\(w_1\\) и \\(w_2\\), каждое из которых встречается с вероятностью \\(10^{-6}\\), то трудно удостовериться в том, что \\(p(w_1, w_2)\\) значимо отличается от \\(10^{-12}\\). Поэтому негативные значения PMI принято заменять нулями. В таком случае формула выглядит так:\n\\[ PMI\\left(x;y\\right)=max(\\log{\\frac{P\\left(x,y\\right)}{P\\left(x\\right)P\\left(y\\right)}},0) \\] Для подобной замены подойдет векторизованное условие.\n\nnews_ppmi &lt;- news_pmi |&gt; \n  mutate(ppmi = case_when(pmi &lt; 0 ~ 0, \n                          .default = pmi)) \n\nnews_ppmi |&gt; \n  arrange(pmi)\n\n\n  \n\n\n\nЕсли мы развернем такую матрицу вширь, то она получится очень разреженной; чтобы получить плотные векторы слов, необходимо прибегнуть к SVD.\n\n\n12.4.4 SVD на матрице с PPMI\nДля этого можно передать тиббл фунции widely_svd() для вычисления сингулярного разложения. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов:\n\nword_emb &lt;- news_ppmi |&gt; \n  widely_svd(item1, item2, ppmi,\n             weight_d = FALSE, nv = 100) |&gt; \n  rename(word = item1) # иначе nearest_neighbors() будет жаловаться\n\n\nword_emb\n\n\n  \n\n\n\n\n\n12.4.5 Визуализация топиков\nСнова визуализируем главные компоненты нашего векторного пространства.\n\nword_emb |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\n\n12.4.6 Ближайшие соседи\nИсследуем наши эмбеддинги, используя уже знакомую функцию, которая считает косинусное сходство между словами.\n\nsource(\"../helper_scripts/nearest_neighbors.R\")\n\n\nword_emb |&gt; \n  nearest_neighbors(\"сборная\")\n\n\n  \n\n\nword_emb |&gt; \n  nearest_neighbors(\"завод\")\n\n\n  \n\n\n\n\n\n12.4.7 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы снова используем алгоритм UMAP.\n\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🏈 Позже мы научимся строить векторное пространство с использованием поверхностных нейросетей.\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "embeddings.html#d-визуализации-пространства-слов",
    "href": "embeddings.html#d-визуализации-пространства-слов",
    "title": "12  Векторные представления слов",
    "section": "12.5 2D-визуализации пространства слов",
    "text": "12.5 2D-визуализации пространства слов\n\nword_emb_mx &lt;- word_emb  |&gt; \n  cast_sparse(word, dimension, value) |&gt; \n  as.matrix()\n\nДля снижения размерности мы используем алгоритм UMAP. В отличие от PCA, он снижает размерность нелинейно, и в этом отношении похож на t-SNE.\n\nlibrary(uwot)\nset.seed(02062024)\nviz &lt;- umap(word_emb_mx,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все наши слова вложены теперь в двумерное пространство.\n\ndim(viz)\n\n[1] 6299    2\n\n\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n   annotate(geom = \"rect\", ymin = 2.5, ymax = 7, xmin = 1.5, xmax = 6.5, alpha = 0.2, color = \"tomato\")+\n  theme_light()\n\n\n\n\n\n\n\n\nПосмотрим на выделенный фрагмент этой карты.\n\ntibble(word = rownames(word_emb_mx), \n       V1 = viz[, 1], \n       V2 = viz[, 2]) |&gt; \n  filter(V1 &gt; 1.5 & V1 &lt; 6.5) |&gt; \n  filter(V2 &gt; 2.5 & V2 &lt; 7) |&gt; \n  ggplot(aes(x = V1, y = V2, label = word)) + \n  geom_text(size = 2, alpha = 0.4, position = position_jitter(width = 0.5, height = 0.5)) +\n  theme_light()\n\n\n\n\n\n\n\n\nОтличная работа 🏈 Позже мы научимся строить векторное пространство с использованием поверхностных нейросетей.\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lda.html",
    "href": "lda.html",
    "title": "14  Тематическое моделирование c LDA",
    "section": "",
    "text": "14.1 Что такое LDA\nТематическое моделирование — семейство методов обработки больших коллекций текстовых документов. Эти методы позволяют определить, к каким темам относится каждый документ и какие слова образуют каждую тему.\nОдним из таких методов является Латентное размещение Дирихле (Latent Dirichlet Allocation, LDA). Это вероятностная модель, которая позволяет выявить заданное количество тем в корпусе. В основе метода лежит предположение о том, что каждый документ представляет собой комбинацию ограниченного числа топиков (тем), а каждый топик — это распределение вероятностей для слов. При этом, как и в естественном языке, документы могут перекрывать друг друга по темам, а темы — по словам. Например, слово «мяч» может быть связано не только со спортивным топиком, но и, например, с политическим («клятва в зале для игры в мяч»).\nСоздатели метода поясняют это на примере публикации из журнала Science.\nНа картинке голубым выделена тема «анализ данных»; розовым — «эволюционная биология», а желтым — «генетика». Если разметить все слова в тексте (за исключением «шумовых», таких как союзы, артикли и т.п.), то можно увидеть, что документ представляет собой сочетание нескольких тем. Цветные «окошки» слева — это распределение вероятностей для слов в теме. Гистограмма справа — это распределение вероятностей для тем в документе. Все документы в коллекции представляют собой сочетание одних и тех же тем — но в разной пропорции. Например, в этом примере почти нет зеленого «текстовыделителя», что хорошо видно на гистограмме.\nАссоциацию тем с документами, с одной стороны, и слов с темами, с другой, и рассчитывает алгоритм. При этом LDA относится к числу методов обучения без учителя (unsupervised), то есть не требует предварительной разметки корпуса: машина сама «находит» скрытые в корпусе темы и аннотирует каждый документ. Это делает метод востребованным в тех случаях, когда мы сами точно не знаем, что ищем — например, в исследованиях электронных архивов.\nСложность при построении модели обычно заключается в том, чтобы установить оптимальное число тем: для этого предлагались различные количественные метрики, но важнейшим условием является также интерпретируемость результата. Единственно правильного решения здесь нет: например, моделируя архив газетных публикаций, мы можем подобрать темы так, чтобы они примерно соответствовали рубрикам («спорт», «политика», «культура» и т.п.), но в некоторых случаях бывает полезно сделать zoom in, чтобы разглядеть отдельные сюжеты (например, «фигурное катание» и «баскетбол» в спортивной рубрике…)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#что-такое-lda",
    "href": "lda.html#что-такое-lda",
    "title": "14  Тематическое моделирование c LDA",
    "section": "",
    "text": "Источник: Blei, D. M. (2012), Probabilistic topic models",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#распределение-дирихле",
    "href": "lda.html#распределение-дирихле",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.2 Распределение Дирихле",
    "text": "14.2 Распределение Дирихле\nМатематические и статистические основания LDA достаточно хитроумны; общие принципы на русском языке хорошо изложены в статье “Как понять, о чем текст, не читая его” на сайте “Системный блок”, а лучшее объяснение на английском языке можно найти здесь и здесь.\n\nАльфа и бета на этой схеме - гиперпараметры распределения Дирихле. Гиперпараметры регулируют распределения документов по темам и тем по словам. Наглядно это можно представить так (при числе тем &gt; 3 треугольник превращается в n-мерный тетраэдр):\n\n\n\n\n\nПри α = 1 получается равномерное распределение: темы распределены равномерно (поэтому α называют “параметром концентрации”). При значениях α &gt; 1 выборки начинают концентрироваться в центре треугольника, представляя собой равномерную смесь всех тем. При низких значениях альфа α &lt; 1 большинство наблюдений находится в углах – скорее всего, в в этом случае в документах будет меньше смешения тем.\nРаспределение документов по топикам θ зависит от значения α; из θ выбирается конкретная тема Z. Аналогичным образом гиперпараметр β определяет связь тем со словами. Чем выше бета, тем с большим числом слов связаны темы. При меньших значениях беты темы меньше похожи друг на друга. Конкретное слово W “выбирается” из распределения слов φ в теме Z.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#подтоговка-данных",
    "href": "lda.html#подтоговка-данных",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.3 Подтоговка данных",
    "text": "14.3 Подтоговка данных\nЧтобы понять возможности алгоритма, мы попробуем передать ему тот же новостной архив: на новостях сразу видно адекватность модели; но это не значит, что применение LDA ограничено подобными сюжетами. Этот метод с успехом применяется, например, в историко-научных или литературоведческих исследованиях. Он хорошо подходит, если необходимо на основе журнального архива описать развитие некоторой области знания. Но сейчас нам подойдет пример попроще 👶\n\nlibrary(tidyverse)\nload(\"../data/news_tokens_pruned.Rdata\")\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПоскольку LDA – вероятностная модель, то на входе она принимает целые числа. В самом деле, не имеет смысла говорить о том, что некое распределение породило 0.5 слов или того меньше. Поэтому мы считаем абсолютную, а не относительную встречаемость – и не tf_idf.\n\nnews_counts &lt;- news_tokens_pruned |&gt; \n  count(token, id)\n\nnews_counts",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#матрица-встречаемости",
    "href": "lda.html#матрица-встречаемости",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.4 Матрица встречаемости",
    "text": "14.4 Матрица встречаемости\nДля работы с LDA в R устанавливаем пакет topicmodels. На входе нужная нам функция этого пакета принимает такую структуру данных, как document-term matrix (dtm), которая используется для хранения сильно разреженных данных и происходит из популярного пакета для текст-майнинга tm.\nПоэтому “тайдифицированный” текст придется для моделирования преобразовать в этот формат, а полученный результат вернуть в опрятный формат для визуализаций.\nДля преобразования подготовленного корпуса в формат dtm воспользуемся возможностями пакета tidytext:\n\nlibrary(tidytext)\n\nnews_dtm &lt;- news_counts |&gt; \n  cast_dtm(id, term = token, value = n)\n\nnews_dtm\n\n&lt;&lt;DocumentTermMatrix (documents: 3407, terms: 6299)&gt;&gt;\nNon-/sparse entries: 196774/21263919\nSparsity           : 99%\nMaximal term length: 20\nWeighting          : term frequency (tf)\n\n\nУбеждаемся, что почти все ячейки в нашей матрице – нули (99-процентная разреженность).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#оценка-perplexity",
    "href": "lda.html#оценка-perplexity",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.5 Оценка perplexity",
    "text": "14.5 Оценка perplexity\nКоличество тем для модели LDA задается вручную. Здесь на помощь приходит функция perplexity() из topicmodels. Она показывает, насколько подогнанная модель не соответствует данным – поэтому чем значение меньше, тем лучше.\nПодгоним сразу несколько моделей с разным количеством тем и посмотрим, какая из них покажет себя лучше. Чтобы ускорить дело, попробуем запараллелить вычисления.\n\nlibrary(topicmodels)\nlibrary(furrr)\n\nplan(multisession, workers = 6)\n\nn_topics &lt;- c(2, 4, 8, 16, 32, 64)\nnews_lda_models &lt;- n_topics  |&gt; \n  future_map(topicmodels::LDA, x = news_dtm, \n      control = list(seed = 0211), .progress = TRUE)\n\n\ndata_frame(k = n_topics,\n           perplex = map_dbl(news_lda_models, perplexity))  |&gt; \n  ggplot(aes(k, perplex)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Оценка LDA модели\",\n       x = \"Число топиков\",\n       y = \"Perplexity\")\n\n\nЕсли верить графику, предпочтительны 32 темы или больше. Посмотрим, сколько тем задано редакторами вручную.\n\nnews_tokens_pruned |&gt; \n  count(topic) |&gt; \n  arrange(-n)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#выбор-числа-тем-с-ldatuning",
    "href": "lda.html#выбор-числа-тем-с-ldatuning",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.6 Выбор числа тем с ldatuning",
    "text": "14.6 Выбор числа тем с ldatuning\nЕще одну возможность подобрать оптимальное число тем предлагает пакет ldatuning. Снова придется подождать.\n\nlibrary(ldatuning)\n\nresult &lt;- FindTopicsNumber(\n  news_dtm,\n  topics = n_topics,\n  metrics = c(\"Griffiths2004\", \"CaoJuan2009\", \"Arun2010\", \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 05092024),\n  verbose = TRUE\n)\n\n\nresult\n\n\n  \n\n\n\n\nFindTopicsNumber_plot(result)\n\n\n\n\n\n\n\n\nЭтот график тоже говорит о том, что модель требует не меньше 32 тем.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#модель-lda",
    "href": "lda.html#модель-lda",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.7 Модель LDA",
    "text": "14.7 Модель LDA\n\nnews_lda &lt;- topicmodels::LDA(news_dtm, k = 32, control = list(seed = 05092024))\n\nТеперь наша тематическая модель готова. Ее можно скачать в формате .Rdata отсюда; это примерно 2.5 Mb.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#слова-и-темы",
    "href": "lda.html#слова-и-темы",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.8 Слова и темы",
    "text": "14.8 Слова и темы\nПакет tidytext дает возможность “тайдифицировать” объект lda с использованием разных методов. Метод β (“бета”) показывает связь топиков с отдельными словами.\n\nnews_topics &lt;- tidy(news_lda, matrix = \"beta\")\n\nnews_topics |&gt; \n  filter(term == \"чай\")  |&gt;  \n  arrange(-beta)\n\n\n  \n\n\n\nНапример, слово “чай” с большей вероятностью порождено темой 22, чем остальными темами 🍵\nПосмотрим на главные термины в первых девяти топиках.\n\nnews_top_terms &lt;- news_topics |&gt; \n  filter(topic &lt; 10) |&gt; \n  group_by(topic) |&gt; \n  arrange(-beta) |&gt; \n  slice_head(n = 12) |&gt; \n  ungroup()\n\nnews_top_terms\n\n\n  \n\n\n\n\nnews_top_terms |&gt; \n  mutate(term = reorder(term, beta)) |&gt; \n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) + \n  facet_wrap(~ topic, scales = \"free\", ncol=3) +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#сравнение-топиков",
    "href": "lda.html#сравнение-топиков",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.9 Сравнение топиков",
    "text": "14.9 Сравнение топиков\nСравним два топика по формуле: \\(log_2\\left(\\frac{β_2}{β_1}\\right)\\). Если \\(β_2\\) в 2 раза больше \\(β_1\\), то логарифм будет равен 1; если наоборот, то -1. На всякий случай напомним: \\(\\frac{1}{2} = 2^{-1}\\).\nДля подсчетов снова придется трансформировать данные.\n\nbeta_wide &lt;- news_topics |&gt; \n  filter(topic %in% c(5, 7)) |&gt; \n  mutate(topic = paste0(\"topic_\", topic)) |&gt; \n  pivot_wider(names_from = topic, values_from = beta) |&gt; \n  filter(topic_5 &gt; 0.001  | topic_7 &gt; 0.001)  |&gt; \n  mutate(log_ratio = log2(topic_7 / topic_5))\n\nbeta_wide\n\n\n  \n\n\n\nНа графике выглядит понятнее:\n\nbeta_log_ratio &lt;- beta_wide  |&gt; \n  filter(!log_ratio %in% c(-Inf, Inf, 0)) |&gt; \n  mutate(sign = case_when(log_ratio &gt; 0 ~ \"pos\",\n                          log_ratio &lt; 0 ~ \"neg\"))  |&gt; \n  select(-topic_5, -topic_7) |&gt; \n  group_by(sign) |&gt; \n  arrange(desc(abs(log_ratio))) |&gt; \n  slice_head(n = 12)\n\n\nbeta_log_ratio |&gt; \n  ggplot(aes(reorder(term, log_ratio), log_ratio, fill = sign)) +\n  geom_col(show.legend = FALSE) +\n  xlab(\"термин\") +\n  ylab(\"log2 (beta_7 / beta_5)\") +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#темы-и-документы",
    "href": "lda.html#темы-и-документы",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.10 Темы и документы",
    "text": "14.10 Темы и документы\nРаспределение тем по документам хранит матрица gamma.\n\nnews_documents &lt;- tidy(news_lda, matrix = \"gamma\")\n\nnews_documents |&gt; \n  filter(topic == 1) |&gt; \n  arrange(-gamma)\n\n\n  \n\n\n\nЗначение gamma можно понимать как долю слов в документе, происходящую из данного топика, при этом каждый документ в рамках LDA рассматривается как собрание всех тем. Значит, сумма всех гамм для текста должна быть равна единице. Проверим.\n\nnews_documents |&gt; \n  group_by(document) |&gt; \n  summarise(sum = sum(gamma))\n\n\n  \n\n\n\nВсе верно!\nТеперь отберем несколько новостей и посмотрим, какие топики в них представлены.\n\nlong_news_id &lt;- news_tokens_pruned  |&gt; \n  group_by(id) |&gt; \n  summarise(nwords = n()) |&gt; \n  arrange(-nwords) |&gt; \n  slice_head(n = 4) |&gt; \n  pull(id)\n\nlong_news_id\n\n[1] \"doc608\"  \"doc1670\" \"doc389\"  \"doc2200\"\n\n\n\nnews_documents |&gt; \n  filter(document  %in%  long_news_id) |&gt; \n  arrange(-gamma) |&gt; \n  ggplot(aes(as.factor(topic), gamma, color = document)) + \n  geom_boxplot(show.legend = F) +\n  facet_wrap(~document, nrow = 2) +\n  xlab(NULL)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#распределения-вероятности-для-топиков",
    "href": "lda.html#распределения-вероятности-для-топиков",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.11 Распределения вероятности для топиков",
    "text": "14.11 Распределения вероятности для топиков\n\nnews_documents  |&gt;  \n  filter(topic &lt; 10) |&gt; \n  ggplot(aes(gamma, fill = as.factor(topic))) +\n  geom_histogram(show.legend = F) +\n  facet_wrap(~ topic, ncol = 3) + \n  scale_y_log10() +\n  labs(title = \"Распределение вероятностей для каждого топика\",\n       y = \"Число документов\")\n\n\n\n\n\n\n\n\nПочти ни одна тема не распределена равномерно: гамма чаще всего принимает значения либо около нуля, либо в районе единицы.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#интерактивные-визуализации",
    "href": "lda.html#интерактивные-визуализации",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.12 Интерактивные визуализации",
    "text": "14.12 Интерактивные визуализации\nБолее подробно изучить полученную модель можно при помощи интерактивной визуализации. Пакет LDAvis установим из репозитория.\n\ndevtools::install_github(\"cpsievert/LDAvis\")\n\nЭта функция поможет преобразовать объект lda в файл json.\n\ntopicmodels2LDAvis &lt;- function(x, ...){\n  svd_tsne &lt;- function(x) tsne(svd(x)$u)\n  post &lt;- topicmodels::posterior(x)\n  if (ncol(post[[\"topics\"]]) &lt; 3) stop(\"The model must contain &gt; 2 topics\")\n  mat &lt;- x@wordassignments\n  \n  LDAvis::createJSON(\n    phi = post[[\"terms\"]], \n    theta = post[[\"topics\"]],\n    vocab = colnames(post[[\"terms\"]]),\n    doc.length = slam::row_sums(mat, na.rm = TRUE),\n    term.frequency = slam::col_sums(mat, na.rm = TRUE),\n    mds.method = svd_tsne,\n    reorder.topics = FALSE\n  )\n}\n\nИнтерактивная визуализация сохранится в отдельной папке.\n\nlibrary(LDAvis)\nLDAvis::serVis(topicmodels2LDAvis(news_lda), out.dir = \"ldavis\")\n\nЭто приложение можно опубликовать на GitHub Pages.\n\n\n\nОб этом приложении см. здесь.\nЗначения лямбды, очень близкие к нулю, показывают термины, наиболее специфичные для выбранной темы. Это означает, что вы увидите термины, которые “важны” для данной конкретной темы, но не обязательно “важны” для всего корпуса.\nЗначения лямбды, близкие к единице, показывают те термины, которые имеют наибольшее соотношение между частотой терминов по данной теме и общей частотой терминов из корпуса.\nСами разработчики советуют выставлять значение лямбды в районе 0.6.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "lda.html#footnotes",
    "href": "lda.html#footnotes",
    "title": "13  Тематическое моделирование",
    "section": "",
    "text": "https://stackoverflow.com/questions/50726713/meaning-of-bar-width-for-pyldavis-for-lambda-0↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Тематическое моделирование</span>"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "",
    "text": "15.1 Виды кластерного анализа\nВсе методы машинного обучения делятся на методы обучения с учителем и методы обучения без учителя. В первом случае у нас есть некоторое количество признаков X, измеренных у n объектов, и некоторый отклик Y. Задача заключается в предсказании Y по X. Например, мы измерили вес и пушистость у сотни котов известных пород, и хотим предсказать породу других котов, зная их вес и пушистость.\nОбучение без учителя предназначено для случаев, когда у нас есть только некоторый набор признаков X, но нет значения отклика. Например, есть группа котов, для которых мы измерили вес и пушистость, но мы не знаем, на какие породы они делятся.\nКластеризация относится к числу методов для обнаружения неизвестных групп (кластеров) в данных. Точнее, это целый набор методов. Мы рассмотрим два из них:\nВ случае с кластеризацией по методу K средних мы пытаемся разбить наблюдения на некоторое заранее заданное число кластеров. Иерархическая кластеризация возвращает результат в виде дерева (дендрограммы), которая позволяет увидеть все возможные кластеры.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#виды-кластерного-анализа",
    "href": "cluster.html#виды-кластерного-анализа",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "",
    "text": "кластеризация по методу K средних\nиерархическая кластеризация",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#кластеризация-по-методу-k-средних",
    "href": "cluster.html#кластеризация-по-методу-k-средних",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.2 Кластеризация по методу K средних",
    "text": "15.2 Кластеризация по методу K средних\nАлгоритм кластеризации:\n\nКаждому наблюдению присваивается случайно выбранное число из интервала от 1 до K (число кластеров). Это исходные метки.\n\n\n\nВычисляется центроид для каждого из кластеров. Центроид k-го класса – вектор из p средних значений признаков, описывающих наблюдения из этого кластера.\nКаждому наблюдению присваивается метка того кластера, чей центроид находится ближе всего к этому наблюдению (удаленность выражается обычно в виде евклидова расстояния).\nШаги 2-3 до тех пор, пока метки классов не станут изменяться.\n\nЭто дает возможность минимизировать внутрикластерный разброс: хорошей считается такая кластеризация, при которой такой разброс минимален.\nКогда центроиды двигаются, кластеры приобретают и теряют документы.\n\n\nВнутрикластерный разброс в кластере k – это сумма квадратов евклидовых расстояний между всеми парами наблюдений в этом кластере, разделенная на общее число входящих в него наблюдений.\n\n\n15.2.1 K-means в R\nРассмотрим это сначала на симулированных, а затем на реальных данных.\n\nset.seed(07092024)\nx = matrix(rnorm(50 * 2), ncol = 2)\nx[1:25, 1:2] = x[1:25, 1:2] + 3\nx[26:50, 1:2] = x[1:25, 1:2] - 4\n\n\nkm.out &lt;- kmeans(x, centers = 2, nstart = 20)\n\nkm.out$cluster\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nНаблюдения разделились идеально. Вот так выглядят наши центроиды:\n\nlibrary(tidyverse)\n\nas_tibble(x)  |&gt; \n  ggplot(aes(V1, V2, color = as.factor(km.out$cluster))) +\n  geom_point(show.legend = F) +\n  geom_point(data = as.data.frame(km.out$centers), color = \"grey40\", size = 3, alpha = 0.7) +\n  theme_light()\n\n\n\n\n\n\n\n\nАргумент nstart позволяет запустить алгоритм функции несколько раз с разными начальными метками кластеров; функция вернет наилучший результат.\n\n\n15.2.2 Кластеризация текстов\nЯ воспользуюсь датасетом из пакета stylo, в котором хранятся частотности 3000 наиболее частотных слов для 26 книг 5 авторов. Один из этих авторов – таинственный Роберт Гэлбрейт, как выяснилось – псевдоним Джоан Роулинг.\n\nlibrary(stylo)\ndata(\"galbraith\")\n\ngalbraith &lt;- as.data.frame.matrix(galbraith) |&gt; \n  select(1:150)\n\ngalbraith[1:10, 1:10]\n\n\n  \n\n\n\nЕсли одни признаки имеют больший разброс значений, чем другие, то при вычислении расстояний будут преобладать элементы с более широкими диапазонами. Поэтому перед применением алгоритма в некоторых случаях рекомендуется нормализовать данные по Z-оценке: из значения признака Х вычитается среднее арифметическое, а результат разделить на стандартное отклонение Х. Это делает функция scale().\n\\[ X_{new} = \\frac{X - Mean(X)}{StDev(X)}\\]\n\nset.seed(07092024)\nkm.out &lt;- kmeans(scale(galbraith), centers = 5, nstart = 20)\n\nkm.out$cluster\n\n       coben_breaker       coben_dropshot       coben_fadeaway \n                   3                    3                    3 \n     coben_falsemove    coben_goneforgood coben_nosecondchance \n                   3                    3                    3 \n     coben_tellnoone    galbraith_cuckoos         lewis_battle \n                   3                    4                    5 \n       lewis_caspian          lewis_chair          lewis_horse \n                   5                    5                    5 \n          lewis_lion         lewis_nephew         lewis_voyage \n                   5                    5                    5 \n      rowling_casual      rowling_chamber       rowling_goblet \n                   4                    2                    2 \n     rowling_hallows        rowling_order       rowling_prince \n                   2                    2                    2 \n    rowling_prisoner        rowling_stone        tolkien_lord1 \n                   2                    2                    1 \n       tolkien_lord2        tolkien_lord3 \n                   1                    1 \n\n\n\nexpected &lt;- str_remove_all(names(km.out$cluster), \"_.*\")\n\ntibble(expected = expected, \n       predicted = km.out$cluster)  |&gt;  \n  group_by(expected) |&gt; \n  count(predicted)\n\n\n  \n\n\n\nПочти все авторы разошлись по разным кластерам (кроме Роулинг), при этом Гэлбрейт в одном кластере с Роулинг. Результат кластеризации по методу k-средних можно визуализировать в двумерном пространстве, прибегнув к методу главных компонент.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#метод-главных-компонент",
    "href": "cluster.html#метод-главных-компонент",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.3 Метод главных компонент",
    "text": "15.3 Метод главных компонент\n\n15.3.1 PCA: общий смысл\nМетод главных компонент (англ. principal component analysis, PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. Этот метод привлекается, в частности, когда надо визуализировать многомерные данные.\nОбщий принцип хорошо объясняет Гаральд Баайен (Baayen 2008, 119).\n\nСерый цвет верхнего левого куба означает, что точки распределены равномерно – нужны все три измерения для того, чтобы описать положение точки в кубе. Куб справа сверху по-прежнему имеет три измерения, но нам достаточно только двух, вдоль которых рассеяны данные. Куб слева снизу тоже имеет два измерения, но вдоль оси y разброс данных меньше, чем вдоль x. Наконец, для куба справа снизу достаточно только одного измерения.\nМетод главных компонент ищет такие измерения, вдоль которых наблюдается наибольший разброс данных, причем каждая следующая компонента будет объяснять меньше разброса.\n\n\n15.3.2 PCA в базовом R\n\npca_fit &lt;- prcomp(galbraith, scale = T, center = T)\n\nnames(pca_fit)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nПервый элемент хранит данные о стандартном отклонении, соответствующем каждой компоненте.\n\nround(pca_fit$sdev, 3)\n\n [1] 7.100 5.586 4.055 3.147 2.891 2.318 1.799 1.720 1.691 1.653 1.385 1.345\n[13] 1.293 1.259 1.230 1.137 1.074 1.034 0.927 0.904 0.833 0.812 0.753 0.738\n[25] 0.612 0.000\n\n\nЭто можно узнать также, вызвав функцию summary.\n\nsummary(pca_fit)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     7.1000 5.5857 4.0551 3.14673 2.89110 2.31817 1.79909\nProportion of Variance 0.3361 0.2080 0.1096 0.06601 0.05572 0.03583 0.02158\nCumulative Proportion  0.3361 0.5441 0.6537 0.71971 0.77543 0.81126 0.83283\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.71973 1.69124 1.65255 1.38483 1.34501 1.29297 1.25850\nProportion of Variance 0.01972 0.01907 0.01821 0.01279 0.01206 0.01115 0.01056\nCumulative Proportion  0.85255 0.87162 0.88983 0.90261 0.91467 0.92582 0.93637\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     1.22957 1.13749 1.07351 1.03397 0.92701 0.90422 0.83317\nProportion of Variance 0.01008 0.00863 0.00768 0.00713 0.00573 0.00545 0.00463\nCumulative Proportion  0.94645 0.95508 0.96276 0.96989 0.97562 0.98107 0.98570\n                         PC22    PC23    PC24   PC25      PC26\nStandard deviation     0.8121 0.75291 0.73778 0.6122 6.838e-15\nProportion of Variance 0.0044 0.00378 0.00363 0.0025 0.000e+00\nCumulative Proportion  0.9901 0.99387 0.99750 1.0000 1.000e+00\n\n\nТаким образом, первые две компоненты объясняют почти половину дисперсии, а последняя почти не имеет объяснительной ценности.\n\nplot(pca_fit)\n\n\n\n\n\n\n\n\nКоординаты текстов в новом двумерном пространстве, определяемом первыми двумя компонентами, хранятся в элементе под названием x.\n\npca_fit$x[,1:2]\n\n                           PC1       PC2\ncoben_breaker        -8.757336  4.352396\ncoben_dropshot       -9.459904  5.276560\ncoben_fadeaway       -8.964170  4.572739\ncoben_falsemove      -8.738214  4.707646\ncoben_goneforgood    -8.113474  7.115332\ncoben_nosecondchance -6.783907  7.946904\ncoben_tellnoone      -7.779098  5.535273\ngalbraith_cuckoos    -3.828020 -5.113776\nlewis_battle          8.179671  2.898561\nlewis_caspian         6.991680  3.005847\nlewis_chair           6.743976  3.867256\nlewis_horse           6.451278  2.783159\nlewis_lion            6.281176  2.401030\nlewis_nephew          5.682845  4.443366\nlewis_voyage          7.947284  3.612055\nrowling_casual       -1.807538 -6.273038\nrowling_chamber      -3.168879 -7.780693\nrowling_goblet       -2.249544 -8.491719\nrowling_hallows      -1.350855 -7.684606\nrowling_order        -2.364382 -7.381456\nrowling_prince       -1.366061 -6.124957\nrowling_prisoner     -3.373330 -9.270778\nrowling_stone        -3.152846 -5.291951\ntolkien_lord1        10.392543  0.650889\ntolkien_lord2        10.799804  1.429537\ntolkien_lord3        11.787300 -1.185577\n\n\n\n\n15.3.3 PCA и кластеры K-means\nФункция augment() из пакета broom позволяет соединить результат анализа с исходными данными.\n\nlibrary(broom)\n\npca_fit  |&gt; \n  augment(galbraith) |&gt; \n  mutate(expected = str_remove_all(.rownames, \"_.+\")) |&gt; \n  ggplot(aes(.fittedPC1, .fittedPC2,\n             color = expected, \n             shape = as.factor(km.out$cluster))) +\n  geom_point(size = 3, alpha = 0.7) +\n  scale_color_discrete(name = \"автор\") +\n  scale_shape_discrete(name = \"кластер\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nЕще один способ представить наблюдения.\n\n# install.packages(\"FactoMineR\")\n# install.packages(\"factoextra\")\nlibrary(FactoMineR)\nlibrary(factoextra)\n\nfviz_pca_ind(pca_fit, geom = c(\"text\"),\n             habillage = as.factor(km.out$cluster),\n             addEllipses = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nАналогично можно представить и нагрузки компонент.\n\nfviz_pca_var(pca_fit, col.var=\"contrib\",\n             select.var = list(contrib = 40),\n             repel = TRUE)+\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nПри интерпретации этого графика следует учитывать, что положительно коррелированные переменные находятся рядом, а отрицательно коррелированные переменные находятся в противоположных квадрантах. Например, для первого измерения “his” и “as” коррелированы отрицательно. Это можно проверить, достав матрицу c нагрузками компонент из объекта pca_fit (в качестве координат используются коэффициенты корреляции между переменными и компонентами):\n\npca_fit$rotation[c(\"his\", \"as\"),1:2]\n\n            PC1         PC2\nhis -0.04033987 -0.15077917\nas   0.11665675 -0.07410514\n\n\nТеперь - наблюдения и переменные на одном графике.\n\nfviz_pca_biplot(pca_fit,  geom = c(\"text\"),\n                select.var = list(cos2 = 40),\n                habillage = as.factor(km.out$cluster),\n                col.var = \"steelblue\",\n                alpha.var = 0.3,\n                repel = TRUE,\n                ggtheme = theme_minimal()) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nПоработать над оформлением такого графика вы сможете в домашнем задании.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#иерархическая-кластеризация",
    "href": "cluster.html#иерархическая-кластеризация",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.4 Иерархическая кластеризация",
    "text": "15.4 Иерархическая кластеризация\n\n15.4.1 Интерпретация дендрограммы\nОдним из недостатков кластеризации по методу k-средних является то, что она требует предварительно указать число кластеров. Этого недостатка лишена иерархическая кластеризация. Если такая кластеризация происходит “снизу вверх”, она называется агломеративной. При этом построение дендрограммы начинается с “листьев” и продолжается вплоть до самого “ствола”.\n\nПри интерпретации дерева надо иметь в виду, что существует \\(2^{n-1}\\) способов упорядочения ветвей дендрограммы, где n – это число листьев. В каждой из точек слияния можно поменять местами наблюдения, не изменяя смысла дендрограммы. Поэтому выводы о сходстве двух наблюдений нельзя делать на основе из близости по горизонтальной оси. См. рис. из книги (Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани 2017, 423)). На рисунке видно, что наблюдение 9 похоже на наблюдение 2 не больше, чем оно похоже на наблюдения 8, 5 и 7. Выводы делаются, исходя из положения на вертикальной оси той точки, где происходит слияние наблюдений.\nКоличество кластеров определяется высотой, на которой мы разрезаем дендрограмму. Из этого следует, что одну и ту же дендрограмму можно использовать для получения разного числа кластеров.\n\n\n15.4.2 Алгоритм кластеризации\n\nВычислить меру различия для всех пар наблюдений. На первом шаге все наблюдения рассматриваются как отдельный кластер.\nНайти пару наиболее похожих кластеров и объединить их. Различие между кластерами соответствует высоте, на которой происходит их слияние в дендрограмме.\nПовторить шаги 1-2, пока не останется 1 кластер.\n\n\n\n\n15.4.3 Тип присоединения\nВид дерева будет зависеть от типа присоединения. На рисунке ниже представлено три способа: полное, одиночное, среднее.\n\nОбычно предпочитают среднее и полное, т.к. они приводят к более сбалансированным дендрограммам.\n\nДля функции hclust() в R по умолчанию выставлено значение аргумента method = \"complete\".\n\n\n15.4.4 Иерархическая кластеризация в R\nПрименим алгоритм к симулированным данным, которые мы создали выше. Функция dist() по умолчанию считает евклидово расстояние.\n\nhc.complete &lt;- hclust(dist(x), method = \"complete\")\nplot(hc.complete)\n\n\n\n\n\n\n\n\nНа картинке видно, что наблюдения из верхих и нижних рядов расходятся на два больших кластера.\n\n\n15.4.5 Иерархическая кластеризация текстов\nДля вычисления расстояния между текстами лучше подойдет не евклидово, а косинусное расстояние на нормализованных данных. В базовой dist() его нет, поэтому воспользуемся пакетом philentropy.\n\ndist_mx &lt;- galbraith  |&gt; \n  scale() |&gt; \n  philentropy::distance(method = \"cosine\", use.row.names = T) \n\nMetric: 'cosine'; comparing: 26 vectors.\n\n\nПреобразуем меру сходства в меру расстояния и передадим на кластеризацию.\n\ndist_mx &lt;- as.dist(1 - dist_mx)\nhc &lt;- hclust(dist_mx)\n\nplot(hc)\n\n\n\n\n\n\n\n\nДля получения меток кластеров, возникающих в результате рассечения дендрограммы на той или иной высоте, можно воспользоваться функцией cutree().\n\ncutree(hc, 5)\n\n       coben_breaker       coben_dropshot       coben_fadeaway \n                   1                    1                    1 \n     coben_falsemove    coben_goneforgood coben_nosecondchance \n                   1                    1                    1 \n     coben_tellnoone    galbraith_cuckoos         lewis_battle \n                   1                    2                    3 \n       lewis_caspian          lewis_chair          lewis_horse \n                   3                    3                    3 \n          lewis_lion         lewis_nephew         lewis_voyage \n                   3                    3                    3 \n      rowling_casual      rowling_chamber       rowling_goblet \n                   2                    4                    4 \n     rowling_hallows        rowling_order       rowling_prince \n                   4                    4                    4 \n    rowling_prisoner        rowling_stone        tolkien_lord1 \n                   4                    4                    5 \n       tolkien_lord2        tolkien_lord3 \n                   5                    5 \n\n\nЭтим меткам можно назначить свой цвет.\n\nlibrary(dendextend)\nhcd &lt;- as.dendrogram(hc)\npar(mar=c(2,2,2,7))\nhcd |&gt; \n  set(\"branches_k_color\", k = 5) |&gt; \n  set(\"labels_col\", k=5) |&gt; \n  plot(horiz = TRUE)\nabline(v=0.8, col=\"pink4\",lty=2)\n\n\n\n\n\n\n\n\n\n\n15.4.6 PCA и иерархическая кластеризация\nКод почти как выше, но надо указать, на сколько кластеров мы разрезаем дерево.\n\npca_fit |&gt; \n  augment(galbraith) |&gt; \n  ggplot(aes(.fittedPC1, .fittedPC2, \n             color = expected, shape = as.factor(cutree(hc, 5)))) +\n  geom_point(size = 3, alpha = 0.7)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "cluster.html#многомерное-шкалирование",
    "href": "cluster.html#многомерное-шкалирование",
    "title": "15  Кластеризация и метод главных компонент",
    "section": "15.5 Многомерное шкалирование",
    "text": "15.5 Многомерное шкалирование\nКроме этого, для визуализации многомерных данных применяют многомерное шкалирование (cmd = classical multidimensional scaling). Функция получает на входе матрицу расстояний.\n\ncmd_fit &lt;- cmdscale(dist_mx)  |&gt; \n  as_tibble()\n\ncmd_fit\n\n\n  \n\n\n\n\ncmd_fit  |&gt; \n  ggplot(aes(V1, V2, \n             color = expected, \n             shape = as.factor(cutree(hc, 5)))) +\n  geom_point(size = 3, alpha = 0.7)\n\n\n\n\n\n\n\n\nМногомерное шкалирование стремится отразить расстояния между наблюдениями.\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press.\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Кластеризация и метод главных компонент</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd",
    "href": "lsa.html#svd",
    "title": "12  Латентно-семантический анализ",
    "section": "12.2 SVD",
    "text": "12.2 SVD\nДля любых текстовых данных и матрица термин-термин и матрица термин-документ будет очень разряженной (то есть большая часть значений будет равна нулю). Необходимо “переупорядочить” ее так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD). При сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы (К. Маннинг, П. Рагхаван, Х. Шютце 2020, 407).\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. О сингулярных векторах можно думать как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.\nПосмотрим теперь, как SVD применяется при анализе текста.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "lsa.html#d-визуализация-пространства-документов",
    "href": "lsa.html#d-визуализация-пространства-документов",
    "title": "12  Латентно-семантический анализ",
    "section": "12.12 2D-визуализация пространства документов",
    "text": "12.12 2D-визуализация пространства документов\nДля снижения размерности мы используем алгоритм UMAP. В отличие от PCA, он снижает размерность нелинейно, и в этом отношении похож на t-SNE.\n\nlibrary(uwot)\nset.seed(07062024)\nviz_lsa &lt;- umap(lsa_space$v ,  n_neighbors = 15, n_threads = 2)\n\nКак видно по размерности матрицы, все документы вложены теперь в двумерное пространство.\n\ndim(viz_lsa)\n\n[1] 3407    2\n\n\nЗакодировав цветом рубрики новостного сайта, нанесем документы на “карту”.\n\n\ntibble(doc = rownames(viz_lsa),\n       topic = news_2019$topic,\n       V1 = viz_lsa[, 1], \n       V2 = viz_lsa[, 2]) |&gt; \n  ggplot(aes(x = V1, y = V2, label = doc, color = topic)) + \n  geom_text(size = 2, alpha = 0.8, position = position_jitter(width = 1.5, height = 1.5)) +\n  annotate(geom = \"rect\", ymin = -10, ymax = 0, xmin = -20, xmax = -10, alpha = 0.2, color = \"tomato\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nВот несколько новостей из небольшого тематического кластера, выделенного на карте квадратом.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc718\", \"doc2437\", \"doc2918\")) |&gt; \n  mutate(text = str_trunc(text, 70))\n\n\n  \n\n\n\n\n\n\n\nК. Маннинг, П. Рагхаван, Х. Шютце. 2020. Введение в информационный поиск. Диалектика.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Латентно-семантический анализ</span>"
    ]
  },
  {
    "objectID": "stylo.html",
    "href": "stylo.html",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "",
    "text": "16.1 Установка stylo\nВ этом уроке мы рассмотрим возможности стилометрического анализа с использованием пакета stylo. К пакету прилагается внятный HOWTO.\nlibrary(stylo)\n\n\n### stylo version: 0.7.5 ###\n\nIf you plan to cite this software (please do!), use the following reference:\n    Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R:\n    a package for computational text analysis. R Journal 8(1): 107-121.\n    &lt;https://journal.r-project.org/archive/2016/RJ-2016-007/index.html&gt;\n\nTo get full BibTeX entry, type: citation(\"stylo\")\nВозможности этого инструмента мы исследуем на корпусе древнегреческой литературы, подробнее о котором можно прочитать в препринте (upd.: опубликованная версия). Для этого эксперимента корпус был немного урезан и разложен по папкам. Корпус в формате .zip надо забрать по ссылке и сделать рабочей директорией.\nТексты могут быть на любом языке, но обязательно в кодировке Unicode.\nНа Mac может потребоваться поставить XQuartz.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#stylo",
    "href": "stylo.html#stylo",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.2 stylo()",
    "text": "16.2 stylo()\nГлавная рабочая лошадка этого пакета – функция stylo(). Если вызвать ее без аргументов, то запустится GUI (который можно отключить).\n\nstylo()\n\nНа вкладке Input & Language выбираете формат файла и язык.\n\nНа вкладке Features указываете, как разбивать текст: на слова, символы, словесные или символьные энграмы. Также можно уточнить, что делать с прописными буквами (в нашем случае это нерелевантно). Параметр MFW указывает, сколько слов использовать для анализа. CULLING задает порог отсечения для слов: 20 означает, что будут использованы слова, которые встречаются как минимум в 20% текстов, 0 – все слова, 100 - только те, которые есть во всех текстах корпуса.\n\nСледующая вкладка определяет метод, который будет использоваться для анализа.\n\nМожно также уточнить метод выборки.\n\nИ, наконец, формат, в котором следует вернуть результат.\n\nБез графического интерфейса команда будет выглядеть так.\n\nstylo(corpus.dir = \"corpus\", \n      analysis.type = \"CA\",\n      analyzed.features=\"w\", \n      ngram.size=1,\n      culling.min=20,\n      culling.max=20,\n      mfw.min = 100,\n      mfw.max = 100,\n      mfw.incr = 0,\n      distance.measure = \"dist.delta\",\n      write.png.file = FALSE,\n      gui = FALSE,\n      corpus.lang = \"Other\",\n      plot.custom.height=8,\n      plot.custom.width=7\n      )\n\n\nПодписи и цвета функция подбирает автоматически. Попробуйте использовать другие меры расстояния и другие статистические методы, и сравните результат (пока можно не обращать внимания на Consensus Tree – об этом поговорим на следующем занятии). После каждого запуска функции в рабочей директории сохраняются файлы с конфигурацией, признаками, которые использовались для анализа и, опционально, визуализация.\n\nstylo(corpus.dir = \"corpus\", \n      analysis.type = \"PCR\",\n      analyzed.features=\"w\", \n      ngram.size=1,\n      mfw.min = 100,\n      mfw.max = 100,\n      mfw.incr = 0,\n      #pca.visual.flavour=\"loadings\",\n      write.png.file = FALSE,\n      gui = FALSE,\n      corpus.lang = \"Other\",\n      plot.custom.height=8,\n      plot.custom.width=8,\n      save.analyzed.freqs=FALSE\n      )\n\n\nЗаглянуть внутрь функции stylo() можно здесь.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#section",
    "href": "stylo.html#section",
    "title": "16  Деревья и сети в stylo",
    "section": "16.3 ",
    "text": "16.3",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Деревья и сети в `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#classify",
    "href": "stylo.html#classify",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.4 classify()",
    "text": "16.4 classify()\nЕсли stylo() возвращает результат, который должен интерпретировать человек, то classify() используется для машинного обучения с учителем. Вызов функции без аргументов вернет GUI, похожий на тот, что мы видели выше. Отличие будет на вкладке “Статистика”.\n\nСреди доступных методов классификации: Delta, k-NN, SVM, Наивный Байес, метод ближайших центроидов. Подробнее о них мы будем говорить позже, а пока можно поэкспериментировать с Delta.\nПеред запуском функции необходимо создать в рабочей директории две папки: primary_set и secondary_set (они есть в архиве, который вы уже скачали). В первой находится так называемые обучающие данные, во второй - тестовые (контрольные) данные. Обычно это тексты неизвестного авторства, но к ним можно добавить и несколько произведений известного авторства для дополнительного контроля. Мы примем за спорные отрывок из “Греческой истории” Ксенофонта, диалог “Софист” Платона, “Наблюдателей” Лукиана и “Против софистов” Исократа.\n\nclassify_result &lt;- classify(\n      training.corpus.dir = \"primary_set\",\n      test.corpus.dir = \"secondary_set\",\n      classification.method = \"delta\",\n      culling.of.all.samples = FALSE,\n      analyzed.features=\"w\", \n      culling.max=20,\n      culling.min=20,\n      ngram.size=1,\n      mfw.min = 100,\n      mfw.max = 100,\n      mfw.incr = 0,\n      gui = FALSE,\n      corpus.lang = \"Other\")\n\nПосле того, как функция вернет управление, в рабочей директории появится несколько файлов, среди них – final_results.txt. В нашем случае успех 100%, но не стоит переоценивать этот результат: пример был совсем игрушечный. О подводных камнях поговорим в модуле про машинное обучение.\n\nclassify_result$expected == classify_result$predicted\n\n[1] TRUE TRUE TRUE TRUE\n\n\nТеперь попробуйте поэкспериментировать с разными методами и настройками.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#rolling.delta",
    "href": "stylo.html#rolling.delta",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.6 rolling.delta()",
    "text": "16.6 rolling.delta()\nЕще одна “фирменная” функция stylo называется rolling.delta(). Она подходит для тех случаев, когда текст написан в соавторстве (или мы предполагаем, что это так). Delta “прокатится” по всему тексту и для каждого его отрывка оценит вероятность того, что он создан тем или иным автором. Разумеется, это имеет смысл лишь в том случае, если у нас, во-первых, достаточно длинный спорный текст, а, во-вторых, есть понятные кандидаты.\nДля демонстрации работы функции мы составили “монстра” из “Бусириса” Исократа и “Софиста” Платона: первая тысяча слов из Исократа, потом две тысячи из Платона, потом еще тысяча из Исократа и тысяча из Платона. Монстр лежит в папке test_set. Обучающие данные находятся в папке reference_set.\n\nrolling.classify(training.corpus.dir = \"reference_set\",\n                 test.corpus.dir = \"test_set\",\n                 write.png.file = FALSE, \n                 classification.method = \"delta\", \n                 mfw = 150, \n                 corpus.lang=\"Other\", \n                 slice.size = 500, \n                 slice.overlap = 200,\n                 plot.legend = FALSE,\n                 milestone.points = seq(0, 5000, 500),\n                 shading = TRUE\n                 )\npar(mar=c(0,0,0,0))\nlegend('top', c(\"Isocrates\",\"Plato\"), \n       col=c('red', 'green'), lty = 2)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#oppose",
    "href": "stylo.html#oppose",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.7 oppose()",
    "text": "16.7 oppose()\nФункция oppose() реализует контрастивный анализ, помогая понять, каких слов авторы избегают, а какие – предпочитают. Функция возвращает два файла: words-preferred.txt и words-avoided.txt. Она тоже поддерживает графический интерфейс, но с древнегреческим бывают трудности токенизации, поэтому прописываем правило при помощи регулярных выражений.\nДля сравнения возьмем Платона и Исократа (они тоже есть в архиве).\n\noppose(corpus.format = \"plain\",\n       corpus.lang = \"Other\",\n       primary.corpus.dir = \"Plato\" , \n       secondary.corpus.dir = \"Isocrates\", \n       splitting.rule = \"[ \\t\\n]+\",\n       text.slice.length = 1000,\n       text.slice.overlap = 0,\n       rare.occurrences.threshold = 3,\n       zeta.filter.threshold = 0.05,\n       oppose.method = \"craig.zeta\",\n       display.on.screen = TRUE,\n       gui = FALSE)\n\n\nИзменим настройки визуализации и проверим, к кому из двух ближе Демосфен.\n\noppose(corpus.format = \"plain\",\n       corpus.lang = \"Other\",\n       primary.corpus.dir = \"Plato\" , \n       secondary.corpus.dir = \"Isocrates\", \n       test.corpus.dir = \"Demosthenes\",\n       splitting.rule = \"[ \\t\\n]+\",\n       text.slice.length = 1000,\n       text.slice.overlap = 0,\n       rare.occurrences.threshold = 3,\n       visualization=\"markers\", # изменение тут\n       zeta.filter.threshold = 0.05,\n       oppose.method = \"craig.zeta\",\n       display.on.screen = TRUE,\n       gui = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#imposters",
    "href": "stylo.html#imposters",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.9 imposters()",
    "text": "16.9 imposters()\nФункция imposters() реализует метод верификации авторства, предложенный в статье М. Коппеля и Я. Винтера и апробированный на корпусе Цезаря.\nНазвание метода отражает его суть: вместо того чтобы сравнивать текст неизвестного автора с текстами предполагаемых авторов, метод использует “импостеров” — случайно выбранные тексты, не принадлежащие ни одному из кандидатов, — для создания фона, на котором оценивается вероятность принадлежности текста конкретному автору.\nОсновные идеи метода imposters:\n\nСоздание обучающей выборки. Для проверки гипотезы о принадлежности текста определенному автору берется его текст, тексты кандидатов и большое количество “самозванцев” (то есть случайные тексты, которые заведомо не принадлежат ни одному из кандидатов).\nБустреп-подход. Метод много раз случайным образом выбирает подмножества признаков и случайные наборы “импостеров”, а затем выполняет этап классификации.\nПроверка гипотезы. Если текст подозреваемого автора чаще всего классифицируется как принадлежащий этому автору, можно с высокой вероятностью утверждать, что это действительно так. Если же нет, значит, автором с большей долей вероятности является кто-то другой.\n\nПочему такой метод эффективен? Классические методы (например, Delta-классификатор Бэрроуза) могут быть чувствительны к дисбалансу классов или менее устойчивы к вариативности текстов. Кроме того, использование “самозванцев” позволяет создать “естественный уровень шумова”, на фоне которого можно оценить значимость конкретной атрибуции.\nАвтор считается установленным, если атрибуция одному автору превышает некий установленный порог; значение этого порога устанавливается в зависимости от того, какова цена ошибки, то есть что для нас важнее – точность, precision (доля объектов, названными классификатором положительными и при этом действительно являющимися положительными) или полнота, recall (доля объектов положительного класса из всех объектов положительного класса). В качестве подмоги можно использовать функцию imposters.optimize().\nОб этой функции см. подробнее виньетку и документацию. На входе она требует уже подготовленные таблицы с частотностями. Обратите внимание: если не задать значение аргументу candidate.set, функция проверит на авторство все доступные в reference.set классы.\n\ndata(\"galbraith\")\n\n# забираем 8-й ряд из датасета:\nmy_text_to_be_tested = galbraith[8,]\n\n# исключаем 8-й ряд из датасета\nmy_frequency_table = galbraith[-c(8),]\n\n# поехали:\nimposters(reference.set = my_frequency_table, \n          test = my_text_to_be_tested,\n          iterations = 100,\n          features = 0.5)\n\n  coben   lewis rowling tolkien \n   0.34    0.00    1.00    0.00 \n\n\nФункция возвращает вектор вероятностей, где значения, близкие к 1, соответствуют наиболее правдоподобным кандидатам на авторство.\n\nlibrary(stringr)\n\n# кандидаты\nidx = str_detect(rownames(my_frequency_table), \"rowling\")\nmy_candidates &lt;- my_frequency_table[idx, ]\nmy_imposters &lt;- my_frequency_table[-idx, ]\n\n# поехали:\nimposters(reference.set = my_imposters, \n          test = my_text_to_be_tested,\n          # вот тут задаем кандидатов\n          candidate.set = my_candidates,\n          iterations = 100,\n          features = 0.5,\n          # доля импостеров для каждой итерации\n          imposters = 1)\n\nTesting a given candidate against imposters...\n\n\nrowling      1\n\n\nrowling \n      1 \n\n\n\n\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#что-такое-delta",
    "href": "stylo.html#что-такое-delta",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.3 Что такое Delta",
    "text": "16.3 Что такое Delta\nDelta Берроуза – это мера стилистической близости между текстами. Метод был предложен в 2001 году австралийским филологом Джоном Бёрроузом. С тех пор дельту используют во многих исследованиях, большая часть которых посвящена установлению авторства различных произведений.\nСуть метода заключается в том, что для корпуса текстов рассчитывается частотность ряда признаков; это могут быть слова (словоформы) или так называемые n-граммы, то есть последовательности n символов подряд. Для сравнения берутся самые частотные слова, среди которых будет значительная доля служебных, в наименьшей степени связанных с тематикой текста (предлоги, союзы, частицы и т.п.). Поскольку сравниваемые тексты, как правило, имеют разную длину, в стилометрических исследованиях принято брать для сравнения относительную, а не абсолютную частотность; Берроуз идет еще дальше, предлагая использовать так называемые z-scores, то есть стандартизированные оценки, показывающие разброс значений относительно средних. Z-score вычисляется по формуле:\n\\[Ζ =\\frac{x-\\mu}{sd}\\]\nЗдесь случайная величина x — это значение частотности, μ — математическое ожидание (среднее), а sd — стандартное отклонение. Иными словами, z-score показывает, на сколько стандартных отклонений x отстоит от ожидаемого. Зная z-scores для заданных слов у известных авторов/текстов, можно сравнить их с z-scores спорного текста; искомая дистанция Delta вычисляется как сумма взятых по модулю разниц между z-scores у двух сравниваемых текстов, поделенная на количество слов:\n\\[ΔB = \\frac{1}{n}\\times\\ \\sum_{i}^{n}{|z_{i,\\ A-}}z_{i,\\ B}|\\],\nгде i – конкретное слово, n – общее число слов, а A и B – сравниваемые авторы (знак | указывает, что суммируется абсолютное значение разницы). Чем больше дистанция, тем менее вероятно авторство.\nПростота метода позволяет использовать его в традиционных методах обучения без учителя, таких как кластерный анализ, так и с машинно-обучаемыми классификаторами, когда для каждого значения предиктора \\(x_i\\) имеется значение отклика \\(y_i\\). Это позволяет, имея показатели предикторов, прогнозировать отклик, то есть, в нашем примере, определять наиболее вероятного автора. Количество классов формально не ограничено: мы можем сравнивать спорные тексты (test set) как с двумя, так и с двадцатью кандидатами, которые включаются в обучающую выборку (training set).\nПакет stylo дает возможность работать не только с классической Delta, но и с ее вариациями, из которых заслуживает внимание т.н. “вюрцбургская Delta”. В отличие от Delta Берроуза, она использует не манхэттенское, а косинусное расстояние, что во многих случаях позволяет повысить точность классификации. Подробнее о разных расстояниях (на примере древнегреческого корпуса) см. наш препринт.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#size.penalize",
    "href": "stylo.html#size.penalize",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.5 size.penalize()",
    "text": "16.5 size.penalize()\nФункция size.penalize() позволяет проверить эффективность метода на отрывках разной длины при работе с различными машинно-обучаемыми классификаторами, в том числе Delta.\nФункция извлекает из текста случайные выборки все большей и большей длины и сравнивает их с обучающей выборкой для классификации с применением разного числа mfw; по умолчанию для каждой заданной длины отрывка проводится 100 итераций. На выходе функция возвращает матрицы с указанием количество успешных классификаций для каждой длины отрывка и заданного количества mfw, а также матрицы смешения, позволяющие судить о том, между какими авторами чаще возникала путаница.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#что-такое-zeta",
    "href": "stylo.html#что-такое-zeta",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.8 Что такое Zeta",
    "text": "16.8 Что такое Zeta\nКонтрастивный анализ, который проводит функция oppose(), тоже основан на методе, предложенном Д. Берроузом. Этот метод был описан и доработан рядом других исследователей, в том числе Хью Крейгом.\nЕго смысл подробно объясняет Savoy (2020), а более популярное объяснение (со ссылками на специальную литературу) можно найти на сайте https://zeta-project.eu. Общий смысл такой. Берутся два корпуса, которые необходимо сравнить. Это может быть корпус мужской и женской прозы, корпус Шекспира и других драматургов его времени, корпус американских и британских детективов… you name it. Один из корпусов (назовем его primary set) принимается за основу сравнения.\nВсе тексты делятся на фрагменты фиксированной длины, обычно от 900 до 6000 слов (Savoy 2020, 154). Дальше считается, в какой доле фрагментов из primary set слово встретилось и в какой доле фрагментов из secondary set оно не встретилось. Затем доли суммируются (тогда \\(0 \\leqslant  z \\leqslant  2\\)). Допустим, мы сравниваем Шекспира и Марлоу. Если у Шекспира слово есть во всех фрагментах, а у Марлоу – ни в одном, то \\(1 + 1 = 2\\). Если наоборот, то \\(0 + 0 = 0\\). На практике крайние значения встречаются очень редко.\nДругой вариант с примерно тем же смыслом. Считаем долю документов, в которых слово встречается у Шекспира и у Марлоу. Например, у Шекспира в 100%, а у Марло - ни в одном. Вторая доля вычитается из первой: \\(1 - 0 = 1\\). Если наоборот, то Zeta равна \\(-1\\). Таким образом, \\(-1 \\leqslant  z \\leqslant  1\\).\nДостоинство этого метода в том, что результат легко интерпретировать: мы сразу видим слова-дискриминаторы. Но надо помнить, что Zeta работает не с самыми частотными словами (точнее, не только с ними), а значит подвержена влиянию тематики и жанра.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#кто-такие-самозванцы",
    "href": "stylo.html#кто-такие-самозванцы",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.10 Кто такие самозванцы",
    "text": "16.10 Кто такие самозванцы\nФункция imposters() реализует метод верификации авторства, предложенный в статье М. Коппеля и Я. Винтера и апробированный на корпусе Цезаря.\nМетод подходит для тех случаев, когда кандидатов много, но мы не можем быть уверены в том, что среди них есть нужный. Задача заключается в том, чтобы заставить машину сказать “я не знаю”. Для этого используется следующий алгоритм.\nВыбирается часть признаков (например 0.5), на основании которых для спорного текста вычисляются его “ближайшие соседи” в корпусе; для этого может использоваться любая мера расстояния, например Delta, косинусное сходство или др. Потом берется еще одна случайная выборка признаков, снова ищется сосед и так k раз (обычно 100).\nАвтор считается установленным, если атрибуция одному автору превышает некий установленный порог; значение этого порога устанавливается в зависимости от того, какова цена ошибки, то есть что для нас важнее – точность, precision (доля объектов, названными классификатором положительными и при этом действительно являющимися положительными) или полнота, recall (доля объектов положительного класса из всех объектов положительного класса). В качестве подмоги можно использовать функцию imposters.optimize().\nНо подробнее об этом речь пойдет в разделе про машинное обучение.\n\n\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "stylo.html#samplesize.penalize",
    "href": "stylo.html#samplesize.penalize",
    "title": "16  Стилометрический анализ с пакетом stylo",
    "section": "16.5 samplesize.penalize()",
    "text": "16.5 samplesize.penalize()\nОдна из известных проблем стилометрии связана с тем, что любые метрики плохо работают на небольших отрывках. Но какого размера должен быть текст, чтобы мы могли установить его автора?\nФункция samplesize.penalize() позволяет проверить эффективность метода на отрывках разной длины при работе с различными машинно-обучаемыми классификаторами, в том числе Delta.\nФункция извлекает из текста случайные выборки все большей и большей длины и сравнивает их с обучающей выборкой для классификации с применением разного числа mfw; по умолчанию для каждой заданной длины отрывка проводится 100 итераций. На выходе функция возвращает матрицы с указанием количества успешных классификаций для каждой длины отрывка и заданного количества mfw, а также матрицы смешения, позволяющие судить о том, между какими авторами чаще возникала путаница.\n\npenalize_result &lt;- samplesize.penalize(mfw = c(100, 200, 500), \n                    features = NULL,\n                    path = NULL, corpus.dir = \"corpus\",\n                    sample.size.coverage = seq(100, 5000, 100),\n                    sample.with.replacement = TRUE,\n                    iterations = 100, \n                    classification.method = \"delta\")\n\nФункция вернет список с показателями точность и матрицами смешения и некоторыми другими показателями. Подробнее о матрицах смешения мы будем говорить в разделе про машинное обучение, а пока просто посмотрим на то, как это выглядит.\n\nlibrary(tidyverse)\n\nhelen_confusion_mfw100 &lt;- penalize_result$confusion.matrices$Isocrates_Hel$mfw_100 |&gt; \n   as.data.frame() |&gt; \n   rownames_to_column(\"predicted\") |&gt; \n   as_tibble()\n\nhelen_confusion_mfw100\n\n\n  \n\n\n\nВытянем в длину и визуализируем.\n\nhelen_confusion_mfw100 |&gt; \n  filter(predicted == \"Isocrates\") |&gt; \n  pivot_longer(-predicted, \n               names_to = \"sample_size\", \n               values_to = \"n\") |&gt; \n  ggplot(aes(as.numeric(sample_size), n)) +\n   geom_point(color = \"steelblue\") + \n   scale_x_continuous(breaks = seq(100, 5000, 300)) +\n   labs(x = NULL)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Стилометрический анализ с пакетом `stylo`</span>"
    ]
  },
  {
    "objectID": "pmi.html#word2vec",
    "href": "pmi.html#word2vec",
    "title": "13  Векторные представления слов на основе PMI. Word2Vec.",
    "section": "13.8 Word2vec",
    "text": "13.8 Word2vec\nWord2vec – это полносвязаная нейросеть с одним скрытым слоем. Такое обучение называется не глубоким, а поверхностным (shallow).\n\nlibrary(word2vec)\n\ncorpus &lt;- news_tokens_pruned |&gt; \n  group_by(id) |&gt; \n  mutate(text = str_c(token, collapse = \" \")) |&gt; \n  distinct(id, text)\n\n\n# устанавливаем зерно, т.к. начальные веса устанавливаются произвольно\nset.seed(02062024) \nmodel &lt;- word2vec(x = corpus$text, \n                  type = \"skip-gram\",\n                  dim = 50,\n                  window = 5,\n                  iter = 20,\n                  hs = TRUE,\n                  min_count = 5,\n                  threads = 6)\n\nНаша модель содержит эмбеддинги для слов; посмотрим на матрицу.\n\nemb &lt;- as.matrix(model)\ndim(emb)\n\n[1] 6305   50\n\n\n\npredict(model, c(\"погода\", \"спорт\"), type = \"nearest\", top_n = 10) |&gt; \n  bind_rows()\n\n\n  \n\n\n\nПолучившуюся модель можно визуализировать, как мы это делали выше.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Векторные представления слов на основе PMI. Word2Vec.</span>"
    ]
  },
  {
    "objectID": "consensus.html",
    "href": "consensus.html",
    "title": "17  Консенсусные деревья и сети",
    "section": "",
    "text": "17.1 Танглграммы\nПоэтому бывает необходимо сравнить разные деревья. Один из способов это сделать визуально – построить tanglegram, например, при помощи пакета dendextend.\nlibrary(stylo)\nlibrary(dendextend)\n\ndata(\"galbraith\")\ngalbraith &lt;- as.data.frame.matrix(galbraith)\ndist_mx &lt;- dist(scale(galbraith))\n\nd1 &lt;- as.dendrogram(hclust(dist_mx, method =\"average\"))  |&gt;  \n  set(\"labels_col\", value = c(\"skyblue\", \"orange\", \"grey40\"), k=3)  |&gt; \n  set(\"branches_k_color\", value = c(\"skyblue\", \"orange\", \"grey40\"), k = 3)\n\nd2 &lt;- as.dendrogram(hclust(dist_mx, method =\"ward.D2\")) |&gt; \n  set(\"labels_col\", value = c(\"skyblue\", \"orange\", \"grey40\"), k=3)  |&gt; \n  set(\"branches_k_color\", value = c(\"skyblue\", \"orange\", \"grey40\"), k = 3)\n\ndlist &lt;- dendlist(d1, d2)\n\npar(family = \"Arial Bold\")\ntanglegram(dlist, common_subtrees_color_lines = FALSE, \n           highlight_distinct_edges  = TRUE, \n           highlight_branches_lwd=FALSE, \n           margin_inner=10, \n           lwd=2, \n           axes=FALSE, \n           main_left = \"Cредняя\", \n           main_right = \"Уорд\", \n           lab.cex = 1.3)\nНа картинке видно, что книги группируются немного по-разному в зависимости от метода связи, хотя для кластеризации использовалась одна и та же матрица расстояний.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#танглграммы",
    "href": "consensus.html#танглграммы",
    "title": "17  Консенсусные деревья и сети",
    "section": "",
    "text": "число предикторов (например, наиболее частотных слов для разных произведений);\nрасстояние, которое используется для попарных сравнений (евклидово, косинусное, др.)\nметод связи (метод полной связи, метод средней связи, метод Уорда и др.);",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусные-деревья-в-stylo",
    "href": "consensus.html#консенсусные-деревья-в-stylo",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.2 Консенсусные деревья в stylo",
    "text": "17.2 Консенсусные деревья в stylo\nКонсенсусное дерево позволяет “обобщить” произвольное число дендрограмм. В stylo за консенсусные деревья отвечает метод BCT (Bootstrap Consensus Tree), к которому можно обратиться через GUI (но здесь мы показываем решение без него).\n\nbct_result &lt;- stylo(gui = FALSE, \n                    frequencies = galbraith,\n                    analysis.type = \"BCT\",\n                    mfw.min = 100,\n                    mfw.max = 500,\n                    mfw.incr = 100,\n                    distance.measure = \"wurzburg\",\n                    write.png.file = FALSE,\n                    consensus.strength = 0.5,\n                    plot.custom.width = 8, \n                    plot.custom.height = 8\n                    )\n\n\nРаботать через GUI удобно, но есть нюансы. Во-первых, не получится кастомизировать внешний вид дерева, а, во-вторых, в Stylo реализована достаточно специфическая процедура бутстрепа (повторных выборок).\nВот что пишут разработчики:\n\nUnder the FEATURES tab, users can define the minutes of the MFW division and sampling procedure, using the increment, the minimum and maximum parameters. For minimum = 100, maximum = 3000, and increment = 50, stylo will run subsequent analyses for the following frequency bands: 100 MFW, 50–150 MFW, 100–200 MFW, …, 2900–2950 MFW, 2950–3000 MFW.\n\nДля консенсуса нужно много деревьев, и Stylo будет строить эти деревья в заданном интервале. Это значит, что последние деревья будут построены уже не на основе самой частотной лексики, т.е. скорее всего на них отразится тематика текстов, входящих в корпус.\nВ некоторых случаях это работает неплохо, но, возможно, у вас есть другие идеи для консенсуса. Разные расстояния. Разные методы кластеризации. Случайные выборки из первых двух сотен слов или еще что-то. Тогда придется самим строить сразу множество деревьев.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#целый-лес-c-purrr",
    "href": "consensus.html#целый-лес-c-purrr",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.3 Целый лес c purrr",
    "text": "17.3 Целый лес c purrr\nЕсли изучить изнанку функции stylo(), которая вызывает GUI в одноименном пакете, то можно заметить, что за консенсусное дерево там отвечает пакет для работы с филогенетическими данными под названием Ape 🦍\nЧто делает машина, когда вы заказываете у нее консенсусное дерево? Принимает на входе матрицу с 1 … n столбцами, в которых хранится частотность для слов. Потом отбирает первые сколько-то слов (скажем, сотню или сколько скажете), считает расстояние, строит на основе матрицы расстояний дерево, складывает его в корзинку. Потом берет следующую сотню слов, считает расстояние, строит дерево, складывает в корзинку… Ну вы поняли. Получается целый лес.\nЗвучит как итерация, а такие задачи в R решаются при помощи цикла for или пакета purrr. Функции map() из пакета purrr надо вручить другую функцию – у нас это будет пользовательская get_tree(). Она берет случайные 100 столбцов в таблице с частотностями galbraith из пакета Stylо, считает расстояние городских кварталов между документами и строит дерево.\n\n\n\n\n\n\nВопрос\n\n\n\nКак называется метод, использующий расстояние городских кварталов на стандартизированных показателях частотности? Ответ найдете в предыдущем уроке.\n\n\n\nlibrary(ape)\n\nget_tree &lt;- function(df) {\n  X &lt;- df[ , sample(3000, replace = F, size = 100)]\n  # стандартизация\n  distmx &lt;- dist(scale(X), method = \"manhattan\")\n  tr &lt;- as.phylo(hclust(distmx))\n  tr\n}\n\nЗапускаем функцию несколько раз при помощи map(), получаем список деревьев. Если хочется на них посмотреть по отдельности, то функцией walk() печатаем сразу несколько деревьев одной строчкой кода 🧙🪄\n\nlibrary(purrr)\nset.seed(123)\n\n\ntrees_result &lt;- map(1:100, ~get_tree(galbraith))\n\n# отдельные деревья\npar(mfrow = c(2, 2), mar = c(1,1,1,1))\nwalk(trees_result[1:4], plot)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nПодумайте, как можно модифицировать функцию, чтобы посчитать косинусное расстояние? Ответ найдете чуть ниже.\n\n\nТак можно построить и 100, и 1000 деревьев. Но сравнивать их вручную мы не будем, а вместо этого посчитаем консенсус. Но сначала разберемся, что это такое.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусное-дерево-c-ape-простой-пример",
    "href": "consensus.html#консенсусное-дерево-c-ape-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.4 Консенсусное дерево c ape: простой пример",
    "text": "17.4 Консенсусное дерево c ape: простой пример\nДопустим, у нас есть три дерева. Создадим их с использованием формата Ньюика, т.е. просто-напросто комбинации скобок и запятых.\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,2),(3,4));\")\n\n\npar(mfrow = c(1, 3), mar = c(5,1,5,1), cex = 1)\nwalk(list(tr1, tr2, tr3), plot.phylo, tip.color = 2)\n\n\n\n\n\n\n\n\nКластеры 1-2, 3-4 встречаются в двух деревьях, остальные лишь в одном. Задача — найти наиболее устойчивые кластеры методом простого большинства. Для этого считаем консенсус, причем аргумент p указывает, что кластер должен быть представлен не менее, чем в половине деревьев. Также уточняем, что наши деревья укоренены.\n\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = TRUE)\n\nЗначение p не может быть меньше 0.5, потому что конфликтующие сплиты не могут быть представлены вместе в одном дереве.\nТеперь изобразим консенсус в виде дерева; дополнительно для узлов укажем силу консенсуса (2/3 = 0.67):\n\npar(mfrow = c(1,1), mar = c(5,5,5,5))\nplot.phylo(cons, tip.color = 2)\nnodelabels(round(cons$node.label[3],2), 7, frame = \"c\", cex = 0.7)\nnodelabels(round(cons$node.label[2],2), 6, bg = \"yellow\")\n\n\n\n\n\n\n\n\nЭто очень простое консенсусное дерево, построенное по методу простого большинства.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусное-дерево-c-ape-galbraith",
    "href": "consensus.html#консенсусное-дерево-c-ape-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.5 Консенсусное дерево c  ape: galbraith",
    "text": "17.5 Консенсусное дерево c  ape: galbraith\nТеперь попробуем сделать такое же дерево для текстовых данных.\n\nlibrary(tidyverse)\nlibrary(paletteer)\n# добавим красоты \npal &lt;- paletteer_d(\"ghibli::PonyoMedium\")[c(1:4,6)]\n\ncons &lt;- consensus(trees_result, p = 0.5, rooted = FALSE)\n\n# назначаем авторам цвета\ncols &lt;- tibble(author = str_remove(cons$tip.label, \"_.+\")) |&gt; \n  mutate(color = case_when(author == \"coben\"  ~ pal[1],\n                           author == \"galbraith\" ~ pal[2],\n                           author == \"lewis\"  ~ pal[3],\n                           author == \"rowling\" ~ pal[4],\n                           author == \"tolkien\"  ~ pal[5]))\n \n# строим дерево\npar(mar = c(0,0,0,0))\nplot.phylo(cons, \n           type = \"fan\", \n           use.edge.length = FALSE,\n           edge.width = 1.5, \n           node.color = \"grey30\",\n           font = 1, \n           no.margin = TRUE, \n           label.offset = 0.1,\n           direction = \"rightwards\", \n           plot = TRUE, \n           lab4ut = \"a\",\n           node.depth = 1, \n           tip.color = cols$color)\n\n# подписываем узлы\nnodelabels(text=sprintf(\"%.2f\", cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"#E8C4A2FF\",\n           cex = 0.5, \n           )",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусная-сеть-c-phangorn-простой-пример",
    "href": "consensus.html#консенсусная-сеть-c-phangorn-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.6 Консенсусная сеть c phangorn: простой пример",
    "text": "17.6 Консенсусная сеть c phangorn: простой пример\nУ консенсусного дерева есть одно очевидное ограничение: оно плохо передает конфликтующие сигналы. Допустим, у нас есть три неукоренённых дерева.\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,4),(2,3));\")\n\npar(mfrow = c(1, 3), mar = c(0,0,0,0), cex=1)\nwalk(list(tr1, tr2, tr3), \n     plot.phylo, \n     tip.color=2, \n     type=\"unrooted\",\n     label.offset = 0.1)\n\n\n\n\n\n\n\n\nКонсенсусное дерево в таком случае никак не поможет: оно не допускает значений p &lt; 0.5. Проверьте сами: код ниже вернет садовые вилы 🔱\n\npar(mfrow = c(1,1))\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = F)\nplot.phylo(cons)\n\nnodelabels(text=sprintf(\"%.2f\", cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"#E8C4A2FF\",\n           cex = 0.5, \n           )\n\n\n\n\n\n\n\n\nВ таких случаях на помощь приходит консенсусная сеть. Построим сеть с использованием пакета phangorn. На входе отдаем объект класса multiPhylo, это по сути просто три дерева в одном “букете”.\n\nlibrary(phangorn)\nlibrary(TreeTools)\nmph &lt;- as.multiPhylo(list(tr1, tr2, tr3))\n\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\nclass(cons.nw)\n\n[1] \"networx\" \"phylo\"  \n\n\nОбъект cons.nw относится к классу networx. Его можно изобразить как в двух, так и в трех измерениях. Вот 2D.\n\nset.seed(16092024)\npar(mar = c(0,0,0,0))\nplot(cons.nw, type = \"2D\")\n\n\n\n\n\n\n\n\nА вот 3D.\n\nlibrary(rgl) \nplot(cons.nw, \"3D\")\n# create animated gif file \nmovie3d(spin3d(axis=c(0,1,0), rpm=3), \n        duration=10, \n        dir = \".\",  \n        type = \"gif\")\n\n\nТеперь попробуем понять, что это значит (иллюстрация и объяснение отсюда).\n\nРассмотрим неукорененные деревья в середине: их внутренние ветви определяют расщепления (splits), а именно 12|34, 13|24 и 14|23, которые явно не могут наблюдаться в одном дереве и, следовательно, все они несовместимы. Сеть в левом верхнем углу представляет одновременно два первых дерева с прямоугольником, символизирующим две внутренние ветви. Чтобы представить все три расщепления, нам нужен куб, как показано справа.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусная-сеть-c-phangorn-galbraith",
    "href": "consensus.html#консенсусная-сеть-c-phangorn-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.7 Консенсусная сеть c phangorn: galbraith",
    "text": "17.7 Консенсусная сеть c phangorn: galbraith\nИтак, у нас есть 13 деревьев для данных galbraith.\n\n# вычисляем консенсус\nmph &lt;- as.multiPhylo(trees_result)\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\n\nПридется немного поколдовать, чтобы раскрасить сеть.\n\nlibrary(tidyverse)\ncons.nw$col &lt;- str_remove_all(cons.nw$tip.label, \"_.+\")\n\ncol_tbl &lt;- tibble(label = unique(cons.nw$col),\n                  col = pal)\n\ncolor_group &lt;- tibble(label = cons.nw$col) |&gt; \n  left_join(col_tbl)\n\nJoining with `by = join_by(label)`\n\ncons.nw$col &lt;- color_group$col\n\n\n# рисуем\nset.seed(16092024)\npar(mar = c(0,0,0,0))\nplot(cons.nw, type = \"2D\", \n     direction = \"axial\",\n     tip.color = cons.nw$col,\n     edge.color = \"grey30\",\n     edge.width = 1,)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#neighbornet-galbraith",
    "href": "consensus.html#neighbornet-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.10 neighborNet(): galbraith",
    "text": "17.10 neighborNet(): galbraith\nТеперь применим алгоритм к реальным данным.\n\npar(mar = c(0,0,0,0), cex = 0.8)\nnnet &lt;- neighborNet(dist_mx)\n\npal &lt;- pal_d3()(5)\n\n\n# назначаем авторам цвета\ncols &lt;- tibble(author = str_remove(nnet$tip.label, \"_.+\")) |&gt; \n  mutate(color = case_when(author == \"coben\"  ~ pal[1],\n                           author == \"galbraith\" ~ pal[2],\n                           author == \"lewis\"  ~ pal[3],\n                           author == \"rowling\" ~ pal[4],\n                           author == \"tolkien\"  ~ pal[5]))\n \n\nplot(nnet, \n     direction = \"axial\",\n     edge.color = \"grey30\",\n     use.edge.length = TRUE, # попробуйте FALSE\n     edge.width = 1,\n     tip.color = cols$color)\n\n\n\n\n\n\n\n\nВ статье “Untangling Our Past: Languages, Trees, Splits and Networks” создатели алгоритма NeighborNet объясняют, как правильно интерпретировать подобный граф на примере дерева германских языков.\n\nКонфликтующие сигналы передаются за счет “ретикулярной структуры” (квадратиков, проще говоря). Там, где конфликта нет, мы видим дерево.\nКаждый сплит представлен несколькими параллельными линиями, и если эти параллели удалить, то граф распадется на два. Чем длиннее ребро, тем “весомее” сплит.\nНа графе видно, что креольский язык сранан-тонго обладает сходством и с английским, и с нидерландским (граф можно разрезать по зеленым линиям двояко).\nБолее слабый конфликтующий сигнал прослеживается между немецким, нидерландским и фламандским, с одной стороны, и пенсильванским немецким, с другой (синий разрез).\nРассмотренные филогенетические методы (консенсусные сети, консенсусные деревья и neighborNet) ничего не говорят о происхождении одного текста от другого. Филограмма, полученная дистанционными методами, не отражает эволюционный процесс, а показывает степень дивергенции таксонов.\nЭто значит, что модель NeighborNet не делает никаких допущений о происхождении, однако в каком-то смысле она вполне способна показывать то, что называют «конфликтующими сигналами». В биологии это рекомбинация, гибридизация и т.п., а в гуманитарных науках — жанровые и диалектные особенности, отношения подражания, заимствования и все то, что способно влиять на результат классификации текстов, помимо авторства.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#neighbornet-простой-пример",
    "href": "consensus.html#neighbornet-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.9 neighborNet(): простой пример",
    "text": "17.9 neighborNet(): простой пример\nЕще один алгоритм для визуализации неоднозначных филогенетических отношений в R назвается neighborNet. Он подходит для тех случаев, когда мы подозреваем нарушения в древовидной структуре (в генетике это может быть горизонтальный перенос генов, а в литературе – например, отношения подражания или т.п.).\nКлючевое различие по сравнению с consensusNet() заключается в том, что neighborNet() строит сеть непосредственно из данных, а не на основе набора деревьев.\nNeighborNet работает в два шага:\n\nСначала строит круговую раскладку для таксонов таким образом, чтобы минимизировать расстояния между парами кластеров, каждый из которых включает в себя 1 или 2 таксона.\nПотом считает веса для сплитов. На этом этапе некоторые ребра удаляются, а другие вытягиваются сообразно весам. Чем длиннее ребро, тем больше вес сплита.\n\nРассмотрим это на простом примере. Представьте, что у нас есть следующая матрица расстояний.\n\nmx &lt;- matrix(data = c(0, 0.07, 0.12, 0.12, 0.07, 0, 0.13, 0.09, 0.12, 0.13, 0, 0.06, 0.12, 0.09, 0.06, 0), nrow = 4)\ncolnames(mx) &lt;- c(\"a\", \"b\", \"c\", \"d\")\nrownames(mx) &lt;- colnames(mx)\nmx\n\n     a    b    c    d\na 0.00 0.07 0.12 0.12\nb 0.07 0.00 0.13 0.09\nc 0.12 0.13 0.00 0.06\nd 0.12 0.09 0.06 0.00\n\nD &lt;- as.dist(mx)\nD\n\n     a    b    c\nb 0.07          \nc 0.12 0.13     \nd 0.12 0.09 0.06\n\n\nОт матрицы расстояний можно перейти к длине ребер. Для нашей простой матрицы длина горизонтальных ребер, например, считается по формуле:\n\\(1/2 (max(D[a,d]+D[b,c], D[a,c]+D[b,d])-D[a,b] – D[d,c])\\)\n\\(1/2 (max(0.12+0.13, 0.12+0.09) – 0.07 – 0.06) = 0.06\\)\n\nnnet &lt;- neighborNet(D)\npar(mar = c(0,0,0,0))\nplot(nnet, show.edge.label = T, \n     edge.label = nnet$edge.length, \n     edge.color = \"grey\", \n     col.edge.label = \"navy\")\n\n\n\n\n\n\n\n\nАналогичным образом считается длина вертикальных ребер. Формула сработает максимум для четырех таксонов, для более сложных структур понадобится метод наименьших квадратов. Все вычисления делает функция neighborNet из пакета phangorn.\nЕсли аргументу edge.label оставить значение по умолчанию, то на картинке увидите номер сплита.\n\npar(mar = c(0,0,0,0))\nplot(nnet, show.edge.label = T, \n     edge.color = \"grey\", \n     col.edge.label = \"firebrick\")\n\n\n\n\n\n\n\n\nУ каждого сплита есть свой вес (рассчитанный методом наименьших квадратов). Его можно достать из объекта nnet.\n\nw = attr(nnet$splits, \"weights\")\nw\n\n[1] 0.01 0.03 0.03 0.02 0.06 0.02\n\n\nЭто можно понять так: чтобы попасть из пунка b в пункт d, нужно сложить веса для сплитов 4, 5 и 1:\n\nw[4] + w[5] + w[1]\n\n[1] 0.09\n\n\nЭто вернет нам 0.09. Сверяемся с матрицей расстояний — все верно!\nСплит — это разбиение совокупности таксонов на два непустых множества. Посмотрим, какие сплиты возможны для 4 таксонов из нашего примера:\n\nas.matrix(nnet$splits)\n\n     d c a b\n[1,] 1 0 0 0\n[2,] 0 1 0 0\n[3,] 0 0 1 0\n[4,] 0 0 0 1\n[5,] 1 1 0 0\n[6,] 1 0 0 1\n\n\nПервые четыре сплита довольно заурядны: мы просто откусываем по одному углу от нашего прямоугольника. Пятый сплит делит прямоугольник поперек, а шестой — вдоль. Дальше алгоритм для каждого сплита считает, какие пары таксонов оказались с разных сторон сплита. На основе матрицы сплитов А и исходной матрицы расстояний D рассчитывается длина ребра таким образом, чтобы кратчайшие пути между таксонами были максимально приближены к исходной матрице расстояний.\nКак уже говорилось, для 4-х таксонов соответствие может быть полным. Это легко проверить, достав атрибут RSS (Residual Sum of Squares, остаточная сумма квадратов) из объекта nnet, который мы создали.\n\nround(attr(nnet$splits, \"RSS\"), 3)\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#консенсусная-сеть-stylo",
    "href": "consensus.html#консенсусная-сеть-stylo",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.10 Консенсусная сеть stylo",
    "text": "17.10 Консенсусная сеть stylo\nПакет stylo не создает сетей как таковых, однако он генерирует таблицы ребер/узлов (или только ребер), используя два алгоритма Эдера для установления связей между узлами. Таблицу можно загрузить в Gephi (https://gephi.org) или прочитать в R (что мы сделаем дальше). Чтобы получить такую таблицу, вызовите функцию stylo() с аргументом network=TRUE и, по желанию, с некоторыми другими аргументами.\n\nstylo(network = TRUE, \n      frequencies = galbraith, \n      network.type=\"undirected\",\n      network.tables=\"both\",\n      linked.neighbors=3,\n      edge.weights=\"linear\",\n      gui=FALSE)\n\nТеперь в рабочей директории должны были появиться два файла .csv.\n\nmy_csv &lt;- list.files(pattern = \"csv\")\nmy_csv\n\n[1] \"book_CA_100_MFWs_Culled_0__Classic Delta_EDGES.csv\"\n[2] \"book_CA_100_MFWs_Culled_0__Classic Delta_NODES.csv\"\n\n\n\ngalbraith_edges &lt;- read_csv(my_csv[1])\ngalbraith_edges\n\n\n  \n\n\ngalbraith_nodes &lt;- read_csv(my_csv[2])\ngalbraith_nodes\n\n\n  \n\n\n\nСоединим две таблицы.\n\nnet_data &lt;- galbraith_edges |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Source == Id)) |&gt; \n  select(-Source) |&gt; \n  rename(Source = Label) |&gt; \n  relocate(Source, .before = Target) |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Target == Id)) |&gt; \n  select(-Target) |&gt; \n  rename(Target = Label) |&gt; \n  relocate(Target, .after = Source) |&gt; \n  select(Source, Target, Weight)\n\nnet_data\n\n\n  \n\n\n\n\nlibrary(igraph)\n\ngalbraith_graph &lt;- graph_from_data_frame(net_data, directed = FALSE)\ngalbraith_graph\n\nIGRAPH 1a39d85 UN-- 26 104 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 1a39d85 (vertex names):\n [1] coben_breaker  --coben_dropshot       coben_breaker  --coben_fadeaway      \n [3] coben_breaker  --coben_falsemove      coben_breaker  --coben_dropshot      \n [5] coben_dropshot --coben_fadeaway       coben_dropshot --coben_falsemove     \n [7] coben_breaker  --coben_fadeaway       coben_dropshot --coben_fadeaway      \n [9] coben_fadeaway --coben_falsemove      coben_breaker  --coben_falsemove     \n[11] coben_dropshot --coben_falsemove      coben_fadeaway --coben_falsemove     \n[13] coben_falsemove--coben_goneforgood    coben_falsemove--coben_nosecondchance\n[15] coben_falsemove--coben_tellnoone      coben_falsemove--coben_goneforgood   \n+ ... omitted several edges\n\n\n\nlibrary(ggraph)\n\ncols &lt;- gryffindor_theme_colors[-c(1:2)]\n\nggraph(galbraith_graph, layout = \"auto\") +\n  geom_edge_link(color = cols[1]) +\n  geom_node_point(color = cols[1]) +\n  geom_node_label(aes(label = name), vjust = 1, hjust = 1, \n                 color = cols[5]) +\n  labs(x = NULL, y = NULL) +\n  theme_gryffindor()\n\n\n\n\n\n\n\n\nУ нас получились три не связанные между собой подсети. О том, как работать с такого рода объектами в R, и как их интерпретировать, мы поговорим уже в следующий раз 🧙‍♂️.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#plot.phylo-простой-пример",
    "href": "consensus.html#plot.phylo-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.4 plot.phylo(): простой пример",
    "text": "17.4 plot.phylo(): простой пример\nДопустим, у нас есть три дерева. Создадим их с использованием формата Ньюика, т.е. просто-напросто комбинации скобок и запятых.\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,2),(3,4));\")\n\n\npar(mfrow = c(1, 3), mar = c(5,1,5,1), cex = 1)\nwalk(list(tr1, tr2, tr3), plot.phylo, tip.color = \"firebrick\", font = 2, edge.width = 1.5)\n\n\n\n\n\n\n\n\nКластеры 1-2, 3-4 встречаются в двух деревьях, остальные лишь в одном. Задача — найти наиболее устойчивые кластеры методом простого большинства. Для этого считаем консенсус, причем аргумент p указывает, что кластер должен быть представлен не менее, чем в половине деревьев. Также уточняем, что наши деревья укоренены.\n\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = TRUE)\n\nЗначение p не может быть меньше 0.5, потому что конфликтующие сплиты не могут быть представлены вместе в одном дереве.\nТеперь изобразим консенсус в виде дерева; дополнительно для узлов укажем силу консенсуса (2/3 = 0.67). Обратите внимание, как менять форму и цвет меток.\n\npar(mfrow = c(1,1), mar = c(5,5,5,5))\nplot.phylo(cons, tip.color = \"firebrick\", \n           edge.width = 1.5, font = 2)\nnodelabels(round(cons$node.label[3],2), 7, \n           frame = \"c\", cex = 0.7)\nnodelabels(round(cons$node.label[2],2), 6, \n           bg = \"darkolivegreen\", col = \"white\")\n\n\n\n\n\n\n\n\nЭто очень простое консенсусное дерево, построенное по методу простого большинства.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#plot.phylo-galbraith",
    "href": "consensus.html#plot.phylo-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.5 plot.phylo(): galbraith",
    "text": "17.5 plot.phylo(): galbraith\nТеперь попробуем сделать такое же дерево для текcтовых данных. Для выбора палитры обратимся к пакету {ggsci} (виньетка).\n\nlibrary(tidyverse)\nlibrary(ggsci) \n\n# добавим красоты \ncols &lt;- pal_igv()(5)\n\ncons &lt;- consensus(trees_result, p = 0.5, rooted = FALSE)\n\n# назначаем авторам цвета\ncols &lt;- tibble(author = str_remove(cons$tip.label, \"_.+\")) |&gt; \n  mutate(color = case_when(author == \"coben\"  ~ cols[1],\n                           author == \"galbraith\" ~ cols[2],\n                           author == \"lewis\"  ~ cols[3],\n                           author == \"rowling\" ~ cols[4],\n                           author == \"tolkien\"  ~ cols[5]))\n \n# строим дерево\npar(mar = c(0,0,0,0))\nplot.phylo(cons, \n           type = \"fan\", \n           use.edge.length = TRUE,\n           edge.width = 1.5, \n           node.color = \"grey30\",\n           font = 2, \n           no.margin = TRUE, \n           label.offset = 0.1,\n           direction = \"rightwards\", \n           plot = TRUE, \n           lab4ut = \"a\",\n           node.depth = 1, \n           tip.color = cols$color,\n           cex = 1.2)\n\n# подписываем узлы\nnodelabels(text=sprintf(\"%.2f\", cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"white\",\n           cex = 1, \n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nО чем вам говорит это дерево? Поменяйте тип дерева с fan на что-то иное.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#consensusnet-простой-пример",
    "href": "consensus.html#consensusnet-простой-пример",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.6 consensusNet(): простой пример",
    "text": "17.6 consensusNet(): простой пример\nУ консенсусного дерева есть одно очевидное ограничение: оно плохо передает конфликтующие сигналы. Допустим, у нас есть три неукоренённых дерева.\n\nlibrary(ape)\nlibrary(purrr)\n\ntr1 &lt;- read.tree(text = \"((1,2),(3,4));\")\ntr2 &lt;- read.tree(text = \"((1,3),(2,4));\")\ntr3 &lt;- read.tree(text = \"((1,4),(2,3));\")\n\n# Настраиваем область графика\npar(mfrow = c(1, 3), \n    mar = c(2, 2, 2, 2), \n    oma = c(1, 1, 1, 1))\n\n# Функция для рисования с увеличенными границами\nplot_tree_with_space &lt;- function(tree) {\n  # Используем в вашем случае больший отступ\n  plot.phylo(tree, \n             tip.color = \"firebrick\",  \n             font = 2,\n             edge.width = 1.5,\n             type = \"unrooted\",\n             label.offset = 0.5,\n             cex = 1,\n             # Добавляем параметр, дающий больше места\n             x.lim = c(-2, 2),  # Увеличенные границы по X\n             y.lim = c(-2, 2))  # Увеличенные границы по Y\n}\n\n# Применяем функцию к каждому дереву\ninvisible(lapply(list(tr1, tr2, tr3), plot_tree_with_space))\n\n\n\n\n\n\n\n\nКонсенсусное дерево в таком случае никак не поможет: оно не допускает значений p &lt; 0.5. Проверьте сами: код ниже вернет садовые вилы 🔱\n\npar(mfrow = c(1,1))\ncons &lt;- consensus(list(tr1, tr2, tr3), p = 0.5, rooted = F)\nplot.phylo(cons, tip.color = \"firebrick\", \n           font =2, label.offset = 0.1)\n\nnodelabels(text=as.character(cons$node.label),\n           node=1:cons$Nnode+Ntip(cons),\n           frame=\"circle\",\n           bg = \"darkolivegreen\",\n           col = \"white\"\n           )\n\n\n\n\n\n\n\n\nВ таких случаях на помощь приходит консенсусная сеть. Построим сеть с использованием пакета phangorn. На входе отдаем объект класса multiPhylo, это по сути просто три дерева в одном “букете”.\n\nlibrary(phangorn)\nlibrary(TreeTools)\nmph &lt;- as.multiPhylo(list(tr1, tr2, tr3))\n\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\nclass(cons.nw)\n\n[1] \"networx\" \"phylo\"  \n\n\nОбъект cons.nw относится к классу networx. Его можно изобразить как в двух, так и в трех измерениях. Вот 2D.\n\nset.seed(16092024)\npar(mar = c(0,0,0,0))\nplot(cons.nw, type = \"2D\", \n     tip.color = \"firebrick\", font = 2)\n\n\n\n\n\n\n\n\nА вот 3D.\n\nlibrary(rgl) \nplot(cons.nw, \"3D\")\n# create animated gif file \nmovie3d(spin3d(axis=c(0,1,0), rpm=3), \n        duration=10, \n        dir = \".\",  \n        type = \"gif\")\n\n\nТеперь попробуем понять, что это значит (иллюстрация и объяснение отсюда).\n\nРассмотрим неукорененные деревья в середине: их внутренние ветви определяют расщепления (splits), а именно 12|34, 13|24 и 14|23, которые явно не могут наблюдаться в одном дереве и, следовательно, все они несовместимы. Сеть в левом верхнем углу представляет одновременно два первых дерева с прямоугольником, символизирующим две внутренние ветви. Чтобы представить все три расщепления, нам нужен куб, как показано справа.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#consensusnet-galbraith",
    "href": "consensus.html#consensusnet-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.7 consensusNet(): galbraith",
    "text": "17.7 consensusNet(): galbraith\nИтак, у нас есть сто деревьев для данных galbraith.\n\n# вычисляем консенсус\nmph &lt;- as.multiPhylo(trees_result)\ncons.nw &lt;- consensusNet(mph, prob = 0.3, rooted = FALSE)\n\nПридется немного поколдовать, чтобы раскрасить сеть.\n\nlibrary(tidyverse)\ncons.nw$author &lt;- str_remove_all(cons.nw$tip.label, \"_.+\")\n\ncol_tbl &lt;- tibble(label = unique(cons.nw$author),\n                  col = pal_d3()(5))\n\ncolor_group &lt;- tibble(label = cons.nw$author) |&gt; \n  left_join(col_tbl)\n\nJoining with `by = join_by(label)`\n\ncons.nw$col &lt;- color_group$col\n\n\nset.seed(04032024)\npar(mar = c(0,0,0,0), oma = c(0,0,0,0), cex = 1.2)\nplot(cons.nw, type = \"2D\", \n     direction = \"axial\",\n     use.edge.length = FALSE,\n     font = 2,\n     tip.color = cons.nw$col,\n     edge.color = \"grey30\",\n     edge.width = 1.2, \n     label.offset = 0.1)\n\n\n\n\n\n\n\n\nТаким образом, consensusNet() строит консенсусную сеть на основе набора деревьев: это позволяет визуализировать степень поддержки различных связей, найденных в наборе деревьев. Подход полезен для выявления областей неопределенности в филогенетических отношениях, когда несколько разных деревьев одинаково хорошо соответствуют данным.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "consensus.html#сетевой-анализ-в-stylo",
    "href": "consensus.html#сетевой-анализ-в-stylo",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.11 Сетевой анализ в stylo",
    "text": "17.11 Сетевой анализ в stylo\nПакет stylo не создает сетей как таковых, однако он генерирует таблицы ребер/узлов (или только ребер), используя два алгоритма Эдера для установления связей между узлами. Таблицу можно загрузить в Gephi (https://gephi.org) или прочитать в R (что мы сделаем дальше). Чтобы получить такую таблицу, вызовите функцию stylo() с аргументом network=TRUE и, по желанию, с некоторыми другими аргументами.\n\nstylo(network = TRUE, \n      frequencies = galbraith, \n      network.type=\"undirected\",\n      network.tables=\"both\",\n      linked.neighbors=3,\n      edge.weights=\"linear\",\n      gui=FALSE)\n\nТеперь в рабочей директории должны были появиться два файла .csv.\n\nmy_csv &lt;- list.files(pattern = \"csv\")\nmy_csv\n\n[1] \"book_CA_100_MFWs_Culled_0__Classic Delta_EDGES.csv\"\n[2] \"book_CA_100_MFWs_Culled_0__Classic Delta_NODES.csv\"\n\n\n\ngalbraith_edges &lt;- read_csv(my_csv[1])\ngalbraith_edges\n\n\n  \n\n\ngalbraith_nodes &lt;- read_csv(my_csv[2])\ngalbraith_nodes\n\n\n  \n\n\n\nСоединим две таблицы.\n\nnet_data &lt;- galbraith_edges |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Source == Id)) |&gt; \n  select(-Source) |&gt; \n  rename(Source = Label) |&gt; \n  relocate(Source, .before = Target) |&gt; \n  left_join(galbraith_nodes, \n            by = join_by(Target == Id)) |&gt; \n  select(-Target) |&gt; \n  rename(Target = Label) |&gt; \n  relocate(Target, .after = Source) |&gt; \n  select(Source, Target, Weight)\n\nnet_data\n\n\n  \n\n\n\n\nlibrary(igraph)\n\ngalbraith_graph &lt;- graph_from_data_frame(net_data, directed = FALSE)\ngalbraith_graph\n\nIGRAPH aa6e9d6 UN-- 26 104 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from aa6e9d6 (vertex names):\n [1] coben_breaker  --coben_dropshot       coben_breaker  --coben_fadeaway      \n [3] coben_breaker  --coben_falsemove      coben_breaker  --coben_dropshot      \n [5] coben_dropshot --coben_fadeaway       coben_dropshot --coben_falsemove     \n [7] coben_breaker  --coben_fadeaway       coben_dropshot --coben_fadeaway      \n [9] coben_fadeaway --coben_falsemove      coben_breaker  --coben_falsemove     \n[11] coben_dropshot --coben_falsemove      coben_fadeaway --coben_falsemove     \n[13] coben_falsemove--coben_goneforgood    coben_falsemove--coben_nosecondchance\n[15] coben_falsemove--coben_tellnoone      coben_falsemove--coben_goneforgood   \n+ ... omitted several edges\n\n\n\nlibrary(ggraph)\n\n# нормализация весов\nweights &lt;- (E(galbraith_graph)$Weight - min(E(galbraith_graph)$Weight)) / (max(E(galbraith_graph)$Weight) - min(E(galbraith_graph)$Weight))\nE(galbraith_graph)$Weight &lt;- weights\n\n\n# атрибут с именем автора\nlabels = str_remove(V(galbraith_graph)$name, \"_.+$\")\nV(galbraith_graph)$label &lt;- labels\n\n# граф\nggraph(galbraith_graph, layout = \"kk\") +\n  geom_edge_link(aes(alpha = Weight), \n                 linewidth = 1.1,\n                 show.legend = FALSE, \n                 color = \"grey70\") +\n  geom_node_point(aes(color = label),\n                  size = 3, shape = 21, \n                  fill = \"white\", \n                  show.legend = FALSE) +\n  geom_node_label(aes(label = name, color = label), \n                 vjust = -1, cex = 2,\n                 show.legend = FALSE) +\n  labs(x = NULL, y = NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nУ нас получились три не связанные между собой подсети. О том, как работать с такого рода объектами в R, и как их интерпретировать, мы поговорим уже в следующий раз 🧙‍♂️.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "igraph.html",
    "href": "igraph.html",
    "title": "18  Сетевые данные в igraph",
    "section": "",
    "text": "18.1 Создание графа",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#создание-графа",
    "href": "igraph.html#создание-графа",
    "title": "18  Сетевые данные в igraph",
    "section": "",
    "text": "18.1.1 Функция make_graph()\nigraph предлагает много способов создания графа. Наиболее распространенный способ создания графа - make_graph(), который строит сеть на основе указанных ребер. Например, чтобы создать граф с 10 узлами (пронумерованными от 1 до 10) и двумя ребрами, соединяющими узлы 1-2 и 1-5:\n\ng &lt;- make_graph(edges = c(1, 2, 1, 5), n = 10, directed = FALSE)\n\nplot(g)\n\n\n\n\n\n\n\n\nТакже можно использовать буквальную запись с помощью формульной нотации igraph. Выражения состоят из имен вершин и операторов ребер. Оператор ребра - это последовательность символов - и +, где первый используется для ребер, а второй - для стрелок. Ребра могут быть произвольно длинными, то есть вы можете использовать столько символов -, сколько нужно для “рисования” их. Если все операторы ребер состоят только из символов -, тогда граф будет ненаправленным, в то время как хотя бы один символ + подразумевает направленный граф:\n\ng &lt;- make_graph(~ 1--2, 1--5, 3, 4, 5, 6, 7, 8, 9, 10)\n\nМы можем напечатать граф, чтобы получить сводку его узлов и ребер:\n\ng\n\nIGRAPH 64c3c01 UN-- 10 2 -- \n+ attr: name (v/c)\n+ edges from 64c3c01 (vertex names):\n[1] 1--2 1--5\n\n\nЭто означает: ненаправленный граф с 10 вершинами и 2 ребрами.\nНаправленность также можно узнать при помощи специальной функции.\n\nis_directed(g)\n\n[1] FALSE\n\n\n\ng &lt;- graph_from_literal(Sam-+Mary, Sam-+Tom, Mary++Tom)\ng\n\nIGRAPH c0db80d DN-- 3 4 -- \n+ attr: name (v/c)\n+ edges from c0db80d (vertex names):\n[1] Sam -&gt;Mary Sam -&gt;Tom  Mary-&gt;Tom  Tom -&gt;Mary\n\nplot(g)\n\n\n\n\n\n\n\n\nФункция make_graph() также может создавать некоторые графы по названию. Например, вы можете создать граф, представляющий социальную сеть клуба каратэ Захарии, который показывает дружбу между 34 членами клуба каратэ в университете США в 1970-х годах:\n\ng &lt;- make_graph(\"Zachary\")\n\nВызовите документацию к функции, чтобы узнать, какие еще графы можно создать таким способом.\nВот еще несколько возможностей.\n\ng.full = make_full_graph(7)\ng.ring = make_ring(7)\ng.tree = make_tree(7, children = 2, mode=\"undirected\")\ng.star = make_star(7, mode = \"undirected\")\n\npar(mfrow = c(2,2), mai = rep(0.2, 4))\nplot(g.full)\nplot(g.ring)\nplot(g.tree)\nplot(g.star)\n\n\n\n\n\n\n\n\n\n\n18.1.2 Cоциоматрица\nЕще один способ – социоматрица, т.е. матрица, хранящая информацию о сети. Ее можно создать вручную.\n\nnetmat1 &lt;- rbind(c(0,1,1,0,0),\n                c(0,0,1,1,0),\n                c(0,1,0,0,0),\n                c(0,0,0,0,0),\n                c(0,0,1,0,0))\nrownames(netmat1) &lt;- letters[1:5]\ncolnames(netmat1) &lt;- letters[1:5]\n\ng &lt;- graph_from_adjacency_matrix(netmat1)\nplot(g)\n\n\n\n\n\n\n\nclass(g)\n\n[1] \"igraph\"\n\nsummary(g)\n\nIGRAPH 89619a9 DN-- 5 6 -- \n+ attr: name (v/c)\n\n\n\n\n18.1.3 Список ребер\nТакже матрицу можно построить при помощи списка ребер. Списки ребер меньше по размеру, и собирать сетевые данные в таком формате проще.\n\nnetmat2 &lt;- rbind(c(1,2),\n                 c(1,3),\n                 c(2,3),\n                 c(2,4),\n                 c(3,2),\n                 c(5,3))\ng &lt;- graph_from_edgelist(netmat2)\nV(g)$name &lt;- letters[1:5]\nplot(g)\n\n\n\n\n\n\n\nsummary(g)\n\nIGRAPH 4ecc601 DN-- 5 6 -- \n+ attr: name (v/c)\n\n\n\n\n18.1.4 Таблица\nВоспользуемся датасетом, подготовленным Б.В. Ореховым и опубликованном на сайте Пушкинского дома, “Словарь русских писателей XVIII века: сеть персоналий”.\n\nДатасет представляет собой осмысленные в терминах сетевого анализа междустатейные ссылки в Словаре русских писателей XVIII века (1988–2010. Вып. 1–3). Узлами сети выступают посвященные персоналиям статьи словаря, а ребрами — ссылки на другие статьи в том же словаре.\n\n\nurl &lt;- \"https://dataverse.pushdom.ru/api/access/datafile/3646\"\ndownload.file(url, destfile = \"Persons_EDGES.csv\")\n\n\nlibrary(readr)\n# у вас будет другой путь\nwriters_data &lt;- read_tsv(file = \"../files/Persons_EDGES.csv\")\n\nwriters_data\n\n\n  \n\n\n\nЭту таблицу можно преобразовать в сеть несколькими способами. Можно использовать функцию graph_from_edgelist(), которая ожидает на входе матрицу с двумя столбцами или же создать граф напрямую из датафрейма.\n\nlibrary(dplyr)\n\nwriters &lt;- writers_data |&gt; \n              select(-Type) |&gt; \n              graph_from_data_frame()\n\nwriters\n\nIGRAPH 1eac482 DN-- 780 4440 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 1eac482 (vertex names):\n [1] Н.И.Ахвердов -&gt;П.И.Богданович    А.Д.Байбаков -&gt;А.А.Барсов       \n [3] А.Д.Кантемир -&gt;А.К.Барсов        А.Д.Кантемир -&gt;С.С.Волчков      \n [5] А.Д.Кантемир -&gt;И.И.Ильинский     А.Д.Кантемир -&gt;Ф.Кролик         \n [7] А.Д.Кантемир -&gt;М.В.Ломоносов     А.Д.Кантемир -&gt;Е.Прокопович     \n [9] А.Д.Кантемир -&gt;А.П.Сумароков     А.Д.Кантемир -&gt;В.К.Тредиаковский\n[11] П.М.Карабанов-&gt;А.А.Барсов        П.М.Карабанов-&gt;И.И.Виноградов   \n[13] П.М.Карабанов-&gt;Д.П.Горчаков      П.М.Карабанов-&gt;Г.Р.Державин     \n[15] П.М.Карабанов-&gt;С.Е.Десницкий     П.М.Карабанов-&gt;И.И.Дмитриев     \n+ ... omitted several edges\n\n\nОписание позволяет понять, что граф является направленным (D), а его узлы имеют имена (N). Всего в графе 780 вершин и 4440 связей.\n\nДемонстрационная версия интерактивного приложения, построенного на сетевых данных, размещена здесь.\n\nРазумеется, таблицу не обязательно импортировать, но можно создать самим. Например, на основе совместной встречаемости слов, которую вы уже умеете считать.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#вершины-и-ребра",
    "href": "igraph.html#вершины-и-ребра",
    "title": "18  Сетевые данные в igraph",
    "section": "18.2 Вершины и ребра",
    "text": "18.2 Вершины и ребра\nСамая главная характеристика сети – это ее размер. Размер – это количество участников (members), которые называются узлами (nodes), вершинами (vertices) или акторами.\n\nV(writers)\n\n+ 780/780 vertices, named, from 1eac482:\n  [1] Н.И.Ахвердов                 А.Д.Байбаков                \n  [3] А.Д.Кантемир                 П.М.Карабанов               \n  [5] Н.Карандашов                 Ф.В.Каржавин                \n  [7] Н.Г.Карин                    П.Кохановский               \n  [9] Н.А.Краевич                  П.Крайский                  \n [11] В.И.Крамаренков              И.Краснопольский            \n [13] Н.С.Краснопольский           С.П.Крашенинников           \n [15] П.Н.Крекшин                  И.Кременецкий               \n [17] В.В.Крестинин                Ф.В.Кречетов                \n [19] И.Кречетовский               Г.А.Криновский              \n+ ... omitted several vertices\n\n\n\nvcount(writers)\n\n[1] 780\n\n\nУзнать число ребер и характер связей можно так\n\nE(writers)\n\n+ 4440/4440 edges from 1eac482 (vertex names):\n [1] Н.И.Ахвердов -&gt;П.И.Богданович    А.Д.Байбаков -&gt;А.А.Барсов       \n [3] А.Д.Кантемир -&gt;А.К.Барсов        А.Д.Кантемир -&gt;С.С.Волчков      \n [5] А.Д.Кантемир -&gt;И.И.Ильинский     А.Д.Кантемир -&gt;Ф.Кролик         \n [7] А.Д.Кантемир -&gt;М.В.Ломоносов     А.Д.Кантемир -&gt;Е.Прокопович     \n [9] А.Д.Кантемир -&gt;А.П.Сумароков     А.Д.Кантемир -&gt;В.К.Тредиаковский\n[11] П.М.Карабанов-&gt;А.А.Барсов        П.М.Карабанов-&gt;И.И.Виноградов   \n[13] П.М.Карабанов-&gt;Д.П.Горчаков      П.М.Карабанов-&gt;Г.Р.Державин     \n[15] П.М.Карабанов-&gt;С.Е.Десницкий     П.М.Карабанов-&gt;И.И.Дмитриев     \n[17] П.М.Карабанов-&gt;ЕкатеринаII       П.М.Карабанов-&gt;Н.М.Карамзин     \n[19] П.М.Карабанов-&gt;П.Г.Левшин        П.М.Карабанов-&gt;А.А.Нартов       \n+ ... omitted several edges\n\necount(writers)\n\n[1] 4440",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#плотность",
    "href": "igraph.html#плотность",
    "title": "18  Сетевые данные в igraph",
    "section": "18.3 Плотность",
    "text": "18.3 Плотность\nЕще одна важная характеристика сети – это ее плотность.\n\nedge_density(writers)\n\n[1] 0.007307199\n\n\nПлотность – это доля имеющихся связей по отношению к максимально возможному количеству связей. Формула плотности будет отличаться для направленных и ненаправленных сетей.\n\n\nНаправленный граф\n\\(\\frac{L}{k\\times(k - 1)}\\)\n\nНенаправленный граф\n\\(\\frac{2L}{k\\times(k-1)}\\)\n\n\nЗдесь \\(k\\times(k-1)\\) – это максимально возможное число связей, а k - число акторов.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#компоненты",
    "href": "igraph.html#компоненты",
    "title": "18  Сетевые данные в igraph",
    "section": "18.4 Компоненты",
    "text": "18.4 Компоненты\nКомпонента сети – это подгруппа, где все акторы связаны между собой прямо или косвенно.\n\ncomponents(writers)$no\n\n[1] 3\n\ncomponents(writers)$csize\n\n[1] 776   2   2\n\n\nОбратим внимание: в нашем графе 4 писателя, которые не связаны с главной компонентой.\n\nwhich(components(writers)$membership !=1)\n\nИ.В.Паузе И.Выродов    Э.Глюк А.Выродов \n      154       727       760       779 \n\n\nИнтересная компания (точнее, две компании) из XVIII в. Иоганн-Вернер Паузе был переводчиком Эразма Роттердамского в начале XVIII века, и его также обычно причисляют к создателям «Зерцала». Братья Иван Выродов и Андрей Выродов – выпускники Московского благородного пансиона (ныне МГУ), а Эрнст Глюк – один из переводчиков Библии на русский язык.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#диаметр",
    "href": "igraph.html#диаметр",
    "title": "18  Сетевые данные в igraph",
    "section": "18.5 Диаметр",
    "text": "18.5 Диаметр\nДиаметр сети – количество шагов, которые нужно пройти, чтобы попасть из узла А в узел B; для сетей с несколькими компонентами учитывается та, что больше. Сначала вычисляются кратчайшие пути (геодезическое расстояние) между каждой парой узлов, затем из них берется максимальный.\n\nlgc &lt;- largest_component(writers)\ndiameter(lgc, directed = TRUE)\n\n[1] 11\n\n\n\nget_diameter(lgc)\n\n+ 12/776 vertices, named, from d64ec00:\n [1] Н.Карандашов         И.Г.Бакмейстер       М.М.Щербатов        \n [4] Н.И.Новиков          П.И.Богданович       Н.Ф.Эмин            \n [7] Н.Р.Судовщиков       А.М.Ченыхаев         И.В.Нехачин         \n[10] В.Д.Голицын          М.И.Прокудин-Горский Ф.П.Печерин         \n\n\n\nfarthest_vertices(lgc)\n\n$vertices\n+ 2/776 vertices, named, from d64ec00:\n[1] Н.Карандашов Ф.П.Печерин \n\n$distance\n[1] 11\n\n\nПосмотрим на кратчайшие пути.\n\nshortest_paths(lgc, from = \"Н.Карандашов\", to = \"Ф.П.Печерин\")\n\n$vpath\n$vpath[[1]]\n+ 12/776 vertices, named, from d64ec00:\n [1] Н.Карандашов         И.Г.Бакмейстер       М.М.Щербатов        \n [4] Н.И.Новиков          П.И.Богданович       Н.Ф.Эмин            \n [7] Н.Р.Судовщиков       А.М.Ченыхаев         И.В.Нехачин         \n[10] В.Д.Голицын          М.И.Прокудин-Горский Ф.П.Печерин         \n\n\n$epath\nNULL\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\n\nПочему Николай Карандашов оказался так далеко от Федора Печерина, нам решительно не известно.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#транзитивность",
    "href": "igraph.html#транзитивность",
    "title": "18  Сетевые данные в igraph",
    "section": "18.6 Транзитивность",
    "text": "18.6 Транзитивность\nКоэффициент кластеризации, или транзитивность, отражает тенденцию к созданию закрытых треугольников, т.е. к замыканию. Транзитивность определяется как доля закрытых треугольников по отношению к общему количеству открытых и закрытых треугольников.\n\ntransitivity(writers)\n\n[1] 0.127146",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#атрибуты-вершин",
    "href": "igraph.html#атрибуты-вершин",
    "title": "18  Сетевые данные в igraph",
    "section": "18.7 Атрибуты вершин",
    "text": "18.7 Атрибуты вершин\nВ датасете “Словарь…” в качестве атрибута вершины хранятся данные об имени автора:\n\nnames &lt;-vertex_attr(writers)$name\nnames[1:12]\n\n [1] \"Н.И.Ахвердов\"     \"А.Д.Байбаков\"     \"А.Д.Кантемир\"     \"П.М.Карабанов\"   \n [5] \"Н.Карандашов\"     \"Ф.В.Каржавин\"     \"Н.Г.Карин\"        \"П.Кохановский\"   \n [9] \"Н.А.Краевич\"      \"П.Крайский\"       \"В.И.Крамаренков\"  \"И.Краснопольский\"",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#фильтрация-на-основе-значений-атрибутов-вершин",
    "href": "igraph.html#фильтрация-на-основе-значений-атрибутов-вершин",
    "title": "18  Сетевые данные в igraph",
    "section": "18.8 Фильтрация на основе значений атрибутов вершин",
    "text": "18.8 Фильтрация на основе значений атрибутов вершин\nАтрибуты вершин можно использовать для того, чтобы задать новую подсеть для анализа.\n\nvert &lt;- which(vertex_attr(writers)$name == \"М.С.Пахомов\")\n\np &lt;- induced_subgraph(writers, vids = vert)\n\np\n\nIGRAPH 11b20f4 DN-- 1 0 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 11b20f4 (vertex names):\n\n\n\nПАХОМОВ Матвей Сергеевич [1745—1792], преподаватель Смольного ин-та, совм. с И. И. Сидоровским перевел с греч. языка «Разговоры Лукиана Самосатского» (1775—1784. Ч. 1—3), «Творения велемудрого Платона» (1780—1785. Ч. 1—3; с кратким изложением содержания перед текстом каждого рассуждения), «Павсания, или Павсаниево описание Еллады, то есть Греции» (1788—1789. Ч. 1—3) и «Землеописание» Страбона (последний перевод остался неизданным).\n\nМы отобрали всего один узел, что не очень интересно. Вот его соседи.\n\nneighbors(writers, \"М.С.Пахомов\")\n\n+ 3/780 vertices, named, from 1eac482:\n[1] И.И.Сидоровский      И.Ф.Янковичдемириево А.А.Барсов          \n\n\n\nСИДОРОВСКИЙ Иван Иванович [1748-1795], преподаватель Смольного ин-та. Совместно с М. С. Пахомовым перевел с греч. языка: «Разговоры Лукиана Самосатского» (1775—1784. Ч. 1—3), «Творения велемудрого Платона» (1780—1785. Ч. 1—3), «Павсаний, или Павсаниево описание Еллады, то есть Греции» (1788—1789. Ч. 1—3). Ч. 3 сочинений Платона и Ч. 3 «Разговоров…» переведены одним С. Также С. перевел с греч. сб. «Поучительных разных слов и бесед Св. Иоанна Златоустого» (1787—1791. Ч. 1—2), в котором мн. из произведений Златоуста были впервые представлены на рус. языке, и «Деяния церковные и гражданские от Рождества Христова до половины XV столетия, собранные Георгием Кедрином с продолжением других» (1794. Ч. 1—3). В конце жизни С. начал переводить «Толкование св. Кирилла Александрийского на 12 Малых Пророков».\n\n\np2 &lt;- induced_subgraph(writers, vids = c(vert, neighbors(writers, \"М.С.Пахомов\")))\n\n\nplot(p2)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#функция-make_ego_graph",
    "href": "igraph.html#функция-make_ego_graph",
    "title": "18  Сетевые данные в igraph",
    "section": "18.9 Функция make_ego_graph()",
    "text": "18.9 Функция make_ego_graph()\nMake_ego_graph() создает (под)графы из всех соседей заданных вершин. Аргумент o можно мыслить как число “рукопожатий”: порядок 0 - это всегда сама v, порядок 1 - это v плюс ее ближайшие соседи, порядок 2 - это порядок 1 плюс ближайшие соседи вершин из порядка 1 и т.д. Попробуем найти “друзей друзей” М.С. Пахомова.\n\np3 &lt;- make_ego_graph(\n  writers,\n  order = 2,\n  nodes = \"М.С.Пахомов\",\n  mode = \"all\"\n)[[1]]\n\n# функция позволит распечатать все ребра\n# print_all(p3)\n\np3\n\nIGRAPH 5312832 DN-- 48 266 -- \n+ attr: name (v/c), Weight (e/n)\n+ edges from 5312832 (vertex names):\n [1] А.Д.Байбаков -&gt;А.А.Барсов           П.М.Карабанов-&gt;Н.М.Карамзин        \n [3] П.М.Карабанов-&gt;А.А.Барсов           П.М.Карабанов-&gt;ЕкатеринаII         \n [5] И.И.Мелиссино-&gt;Н.И.Новиков          И.И.Мелиссино-&gt;И.И.Бецкой          \n [7] И.И.Мелиссино-&gt;М.Н.Муравьев         И.И.Мелиссино-&gt;А.А.Барсов          \n [9] И.И.Мелиссино-&gt;ЕкатеринаII          И.И.Мелиссино-&gt;И.И.Шувалов         \n[11] И.И.Мелиссино-&gt;М.В.Ломоносов        И.И.Мелиссино-&gt;А.П.Сумароков       \n[13] Ф.И.Миллер   -&gt;Н.И.Новиков          Ф.И.Миллер   -&gt;Н.Н.Поповский       \n[15] Ф.И.Миллер   -&gt;Н.Н.Бантыш-Каменский Ф.И.Миллер   -&gt;В.Н.Татищев         \n+ ... omitted several edges\n\n\nЧтобы изобразить такой граф, придется немного поправить настройки.\n\npar(mar = rep(0,4), cex = 0.7)\nlayout_p3 &lt;- layout_with_kk(p3)\n\nplot(p3, vertex.size=6, \n     edge.arrow.size = 0.5, \n     vertex.label.dist = 1,\n     edge.curved = 0.2,\n     edge.color = \"grey80\",\n     vertex.color = \"plum\",\n     layout = layout_p3)\n\n\n\n\n\n\n\n\nМ.С. Пахомов связан с Екатериной II через И.Ф. Янковича де Мириево, камерпажа императрицы, впоследствии (при Павле I) – генерал-майора и участника походов против Наполеона в 1805 и 1807 г., отличившегося в сражении под Аустерлицем.\nЭто можно подтвердить и другим, уже известным нам способом:\n\nshortest_paths(writers, from = \"М.С.Пахомов\", to = \"ЕкатеринаII\")$vpath[[1]]\n\n+ 3/780 vertices, named, from 1eac482:\n[1] М.С.Пахомов          И.Ф.Янковичдемириево ЕкатеринаII         \n\n\nУзнать размер ego-графа можно при помощи специальной функции.\n\n# размер подграфа\nego_size(writers,\n  order = 2,\n  nodes = \"М.С.Пахомов\",\n  mode = \"all\")\n\n[1] 48\n\n\nПосмотрим, как растет размер сети при увеличении порядка.\n\nlibrary(purrr)\n\n\nAttaching package: 'purrr'\n\n\nThe following objects are masked from 'package:igraph':\n\n    compose, simplify\n\nf &lt;- function(x) {\n  ego_size(writers, \n  order = x,\n  nodes = \"М.С.Пахомов\", \n  mode = \"all\")\n}\n\norder &lt;- 1:10\nout &lt;- map_dbl(order, f)\nplot(order, out, \"o\")",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#атрибуты-ребер",
    "href": "igraph.html#атрибуты-ребер",
    "title": "18  Сетевые данные в igraph",
    "section": "18.10 Атрибуты ребер",
    "text": "18.10 Атрибуты ребер\nУ ребер в данных “Словаря…” есть атрибут, но он везде одинаковый.\n\noptions(max.print=35)\n\nedge_attr(writers)\n\n$Weight\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [ reached getOption(\"max.print\") -- omitted 4405 entries ]",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#преобразование-направленной-сети-в-ненаправленную",
    "href": "igraph.html#преобразование-направленной-сети-в-ненаправленную",
    "title": "18  Сетевые данные в igraph",
    "section": "18.11 Преобразование направленной сети в ненаправленную",
    "text": "18.11 Преобразование направленной сети в ненаправленную\nНаправленный граф бывает необходимо преобразовать в неправленный. Возьмем подграф “соседей” М.С. Пахомова и создадим симметричную сеть, оставив только те связи, где ссылки взаимны.\n\np4 &lt;- as.undirected(p3, mode = \"mutual\")\n\nWarning: `as.undirected()` was deprecated in igraph 2.1.0.\nℹ Please use `as_undirected()` instead.\n\np4\n\nIGRAPH 712f520 UN-- 48 58 -- \n+ attr: name (v/c)\n+ edges from 712f520 (vertex names):\n [1] И.И.Мелиссино       --Н.И.Новиков         \n [2] Н.И.Новиков         --М.И.Багрянский      \n [3] Ф.И.Миллер          --Н.Н.Бантыш-Каменский\n [4] Н.И.Новиков         --Н.Н.Бантыш-Каменский\n [5] Н.Н.Поповский       --Ф.Я.Яремский        \n [6] М.С.Пахомов         --И.И.Сидоровский     \n [7] И.И.Мелиссино       --М.Н.Муравьев        \n [8] Н.И.Новиков         --М.Н.Муравьев        \n+ ... omitted several edges\n\n\nВ новой сети по-прежнему 48 узлов, но количество связей стало меньше (58 вместо 266).\n\npar(mar = rep(0,4), cex = 0.7)\nplot(p4, vertex.size=6, \n     edge.arrow.size = 0.5, \n     vertex.label.dist = 1,\n     edge.curved = 0.2,\n     edge.color = \"grey80\",\n     vertex.color = \"plum\",\n     layout = layout_p3)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#удаление-изолированных-узлов",
    "href": "igraph.html#удаление-изолированных-узлов",
    "title": "18  Сетевые данные в igraph",
    "section": "18.12 Удаление изолированных узлов",
    "text": "18.12 Удаление изолированных узлов\nПосле симметризации некоторые узлы оказались изолированы. Удалим их. Для этого сначала необходимо найти узлы, степень которых равна 0.\n\nd &lt;- unname(degree(p4))\np4 &lt;- set_vertex_attr(p4, name = \"degree\", value = d)\n\nПроверим.\n\nvertex_attr(p4)$degree\n\n [1]  1  0  3  2  7  0  0  2  0  0  0  8  2  3  1  0  1  0  1  2  0  0  0  0  6\n[26]  6  0  4  0  4  0  9  1  0  0  0  9  1  7  0  1  1  3  9  0 11  2  9\n\n\n\np5 &lt;- delete_vertices(p4, vertex_attr(p4)$degree == 0)\np5\n\nIGRAPH e9f5e4f UN-- 28 58 -- \n+ attr: name (v/c), degree (v/n)\n+ edges from e9f5e4f (vertex names):\n [1] И.И.Мелиссино       --Н.И.Новиков         \n [2] Н.И.Новиков         --М.И.Багрянский      \n [3] Ф.И.Миллер          --Н.Н.Бантыш-Каменский\n [4] Н.И.Новиков         --Н.Н.Бантыш-Каменский\n [5] Н.Н.Поповский       --Ф.Я.Яремский        \n [6] М.С.Пахомов         --И.И.Сидоровский     \n [7] И.И.Мелиссино       --М.Н.Муравьев        \n [8] Н.И.Новиков         --М.Н.Муравьев        \n+ ... omitted several edges\n\n\n\npar(mar = rep(0,4), cex = 0.7)\nlayout_p5 &lt;- layout_with_kk(p5)\n\nplot(p5, vertex.size=6, \n     edge.arrow.size = 0.6, \n     vertex.label.dist = 1,\n     edge.curved = 0.2,\n     edge.color = \"grey80\",\n     vertex.color = \"plum\",\n     layout = layout_p5)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "igraph.html#сравнение-графов",
    "href": "igraph.html#сравнение-графов",
    "title": "18  Сетевые данные в igraph",
    "section": "18.13 Сравнение графов",
    "text": "18.13 Сравнение графов\nУбедимся, что графы отличаются.\n\nidentical_graphs(p4, p5)\n\n[1] FALSE\n\n\nДва графа называются изоморфными, если у них одинаковое число вершин (обозначим его n) и вершины каждого из них можно занумеровать так числами от 1 до n, что в первом графе две вершины соединены ребром тогда и только тогда, когда вершины с такими же номерами во втором графе соединены.\nПроверим подграфы на изоморфность.\n\nisomorphic(p4, p5)\n\n[1] FALSE\n\n\nТеперь произвольным образом переименуем узлы в p5, удалим один из атрибутов вершины (атрибуты ребер “потерялись” при симметризации) и снова проверим “двойника” на изоморфность.\n\np6 &lt;- p5\nV(p6)$name &lt;- c(letters, \"aa\", \"bb\")\np6 &lt;- delete_vertex_attr(p6, \"degree\")\n\np6\n\nIGRAPH e9f5e4f UN-- 28 58 -- \n+ attr: name (v/c)\n+ edges from e9f5e4f (vertex names):\n [1] b--d  d--g  c--h  d--h  f--i  e--k  b--m  d--m  f--m  d--n  g--n  h--n \n[13] m--n  a--q  e--q  f--q  l--q  m--q  d--s  l--s  o--s  p--s  q--s  f--u \n[25] p--u  r--u  s--u  q--v  f--w  s--x  c--y  f--y  n--y  o--y  q--y  u--y \n[37] x--y  f--z  j--z  m--z  n--z  o--z  p--z  s--z  u--z  x--z  y--z  q--aa\n[49] t--aa b--bb d--bb f--bb o--bb p--bb s--bb u--bb y--bb z--bb\n\n\nСнова сравним графы двумя способами.\n\nidentical_graphs(p5, p6)\n\n[1] FALSE\n\n\n\nisomorphic(p5, p6)\n\n[1] TRUE\n\n\n\n\n\n\nЛюк, Дуглас. 2017. Анализ сетей (графов) в среде R: Руководство пользователя. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Сетевые данные в `igraph`</span>"
    ]
  },
  {
    "objectID": "plot.html#диаграмма-рассеяния-с-geom_point",
    "href": "plot.html#диаграмма-рассеяния-с-geom_point",
    "title": "3  Визуализации",
    "section": "3.3 Диаграмма рассеяния с geom_point()",
    "text": "3.3 Диаграмма рассеяния с geom_point()\nФункция ggplot() имеет два основных аргумента: data и mapping. Аргумент mapping задает эстетические атрибуты геометрических объектов. Обычно используется в виде mapping = aes(x, y), где aes() означает aesthetics.\nПод “эстетикой” подразумеваются графические атрибуты, такие как размер, форма или цвет. Вы не увидите их на графике, пока не добавите какие-нибудь “геомы” – геометрические объекты (точки, линии, столбики и т.п.). Эти объекты могут слоями накладываться друг на друга (Wickham и Grolemund 2016).\nДиаграмма рассеяния, которая подходит для отражения связи между двумя переменными, делается при помощи geom_point(). Попробуем настройки по умолчанию.\n\nnoveltm |&gt; \n  ggplot(aes(inferreddate, n_words)) + \n  geom_point()\n\n\n\n\n\n\n\n\nУпс. Точек очень много, и они накладываются друг на друга, так как число слов – дискретная величина. Поступим так же, как Моретти, который отразил на графике среднее для каждого года.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#среднее-со-stat_summary",
    "href": "plot.html#среднее-со-stat_summary",
    "title": "3  Визуализации",
    "section": "3.4 Среднее со stat_summary()",
    "text": "3.4 Среднее со stat_summary()\nДля этого у нас есть два пути. Первый: обобщить данные при помощи group_by() и summarise(), как мы делали в прошлом уроке. Второй: воспользоваться возможностями stat_summary() в самом ggplot2.\n\nnoveltm |&gt; \n  filter(!is.na(n_words)) |&gt; \n  ggplot(aes(inferreddate, n_words)) +\n  geom_point(color = \"grey80\") +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"steelblue\")\n\n\n\n\n\n\n\n\nОставим только среднее и добавим линию тренда, а также уберем подпись оси X.\n\nnoveltm |&gt; \n  filter(!is.na(n_words)) |&gt; \n  ggplot(aes(inferreddate, n_words)) +\n  stat_summary(fun.y = \"mean\", geom = \"point\", color = \"steelblue\") +\n  geom_smooth(color = \"tomato\") +\n  labs(x = NULL)\n\n\n\n\n\n\n\n\nНисходящая тенденция, о которой писал Моретти, хорошо прослеживается. Но, возможно, она характерна не для всех стран?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#просто-украшательство",
    "href": "plot.html#просто-украшательство",
    "title": "3  Визуализации",
    "section": "3.10 Просто украшательство",
    "text": "3.10 Просто украшательство\nПоскольку нас интересует доля женщин, логично поменять группы местами.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  # меняем местами группы\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  coord_flip() +\n  # разные мелочи\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nТакже поменяем порядок, в котором идут декады (от меньшей к большей).\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  # меняем порядок лет\n  scale_x_reverse() +\n  coord_flip() +\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nУбавим цвет в мужской части диаграммы и добавим заголовки.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           # обводим столбики \n           color = \"darkred\", \n           # убираем легенду\n           show.legend = FALSE) +\n  scale_x_reverse() +\n  # беремся за палитру\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  # меняем цвет и шрифт текста\n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"))\n\n\n\n\n\n\n\n\nСтоит подвинуть заголовок и убрать просветы между столбцами.\n\n# почти ничего нового!\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           color = \"darkred\", \n           show.legend = FALSE,\n           # столбик во всю ширину\n           width = 10\n) +\n  # добавляем делений на оси\n  scale_x_reverse(breaks = seq(1800, 2000, 10)) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"),\n        # выравниваем заголовок\n        plot.title.position = \"plot\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#подписи-с-geom_text",
    "href": "plot.html#подписи-с-geom_text",
    "title": "3  Визуализации",
    "section": "3.11 Подписи с geom_text()",
    "text": "3.11 Подписи с geom_text()\nФункции geom_text() можно передать таблицу, которую мы сделали выше и которая хранит сведения о доле женщин по декадам. Обратите внимание: у геомов могут быть разные данные!\n\nlabel_data &lt;- noveltm_new_prop |&gt; \n                          filter(gender == \"f\")\n\n# тут все старое\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           color = \"darkred\", \n           show.legend = FALSE,\n           width = 10\n) +\n  scale_x_reverse(breaks = seq(1800, 2000, 10)) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"),\n        plot.title.position = \"plot\") + \n  # тут чуть-чуть нового\n  geom_text(data = label_data, \n            aes(label = round(share, 2),\n                y = share),\n            family = \"serif\", \n            hjust = 1.2, \n            color = \"darkred\")\n\n\n\n\n\n\n\n\nОтличная работа! Все сестры Бронте вами гордятся.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "ggraph.html",
    "href": "ggraph.html",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "",
    "text": "19.1 Дизайн узлов\nДля визуализации используем библиотеку ggraph. Минимум необходимых усилий уже даст нам что-то осмысленное, но это только начало.\nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point() +\n  geom_node_text(aes(label = name)) +\n  theme_graph() \n\nUsing \"sugiyama\" as default layout\nПри работе с узлами мы можем закодировать несколько переменных при помощи размера, цвета или, например, формы. Здесь мы ограничимся двумя способами: при помощи размера отразим степень узла (количество связей с другими участниками), а при помощи цвета – гендер.\nДля этого сначала считаем степень узлов; как это делать, мы обсуждали в предыдущем уроке.\nd &lt;- as.numeric(degree(tudors_g))\nV(tudors_g)$degree &lt;- d\ntudors_g\n\nIGRAPH 2b5cd7f DN-- 25 35 -- \n+ attr: name (v/c), degree (v/n), relationship (e/c)\n+ edges from 2b5cd7f (vertex names):\n [1] Henry VII          -&gt;Elizabeth of York     \n [2] Arthur Tudor       -&gt;Catharine of Aragon   \n [3] Henry VIII         -&gt;Catharine of Aragon   \n [4] Henry VIII         -&gt;Anne Boleyn           \n [5] Henry VIII         -&gt;Jane Seymour          \n [6] Henry VIII         -&gt;Anne of Cleves        \n [7] Henry VIII         -&gt;Katherine Howard      \n [8] Henry VIII         -&gt;Catherine Parr        \n+ ... omitted several edges\nТеперь в код выше вносим несколько изменений.\nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree)) +\n  geom_node_text(aes(label = name)) +\n  theme_graph() \n\nUsing \"sugiyama\" as default layout",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#добавление-атрибутов-узлов",
    "href": "ggraph.html#добавление-атрибутов-узлов",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.2 Добавление атрибутов узлов",
    "text": "19.2 Добавление атрибутов узлов\nДанных о гендере в датасете нет, но их несложно добавить.\n\ngender_tbl &lt;- tibble(name = V(tudors_g)$name) |&gt; \n  mutate(gender = case_when(\n    str_detect(name, \"(Margaret|Mary|Elizabeth|[CK]ath[ae]rine|Anne|Jane)\") ~ \"f\",\n    .default = \"m\"))\n\ngender_tbl\n\n\n  \n\n\n\n\nV(tudors_g)$gender &lt;- gender_tbl$gender\n\nvertex_attr(tudors_g)\n\n$name\n [1] \"Henry VII\"              \"Arthur Tudor\"           \"Henry VIII\"            \n [4] \"Margaret Tudor\"         \"Mary Tudor\"             \"James V\"               \n [7] \"Mary Queen of Scots\"    \"Mary I\"                 \"James VI/I\"            \n[10] \"Elizabeth I\"            \"Edward VI\"              \"Elizabeth of York\"     \n[13] \"Catharine of Aragon\"    \"Anne Boleyn\"            \"Jane Seymour\"          \n[16] \"Anne of Cleves\"         \"Katherine Howard\"       \"Catherine Parr\"        \n[19] \"James IV\"               \"Louis XII\"              \"Charles Duke of Suffok\"\n[22] \"Mary of Guise\"          \"Frances II of France\"   \"Henry Lord Darnley\"    \n[25] \"Philip II\"             \n\n$degree\n [1]  5  3 11  4  4  4  5  3  2  2  2  5  3  2  2  1  1  1  2  1  1  2  1  2  1\n\n$gender\n [1] \"m\" \"m\" \"m\" \"f\" \"f\" \"m\" \"f\" \"f\" \"m\" \"f\" \"m\" \"f\" \"f\" \"f\" \"f\" \"f\" \"f\" \"f\" \"m\"\n[20] \"m\" \"m\" \"f\" \"m\" \"m\" \"m\"\n\n\nГендер можно закодировать цветом.\n\nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree, color = gender)) +\n  geom_node_text(aes(label = name)) +\n  theme_graph(base_family = \"sans\") \n\nUsing \"sugiyama\" as default layout\n\n\n\n\n\n\n\n\n\nПоменяем цветовую шкалу уже известным способом.\n\nlibrary(paletteer)\n# двухцветная палитра\ncols &lt;- paletteer_d(\"suffrager::classic\")\n  \nggraph(tudors_g, layout = \"auto\") +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, # это кружки с заливкой\n                  color = \"black\"\n                  ) +\n  geom_node_text(aes(label = name)) +\n  # убираем лишнюю легенду\n  scale_size(guide = 'none') +\n  scale_fill_manual(values = cols) +\n  theme_graph(base_family = \"sans\") \n\nUsing \"sugiyama\" as default layout\n\n\n\n\n\n\n\n\n\nТеперь подумаем над укладкой.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#укладка-сети",
    "href": "ggraph.html#укладка-сети",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.3 Укладка сети",
    "text": "19.3 Укладка сети\nГрафическое представление одной и той же сети будет зависеть от выбранного способа укладки.\nПри построении графиков сетей стремятся следовать следующим принципам:\n\nминимизировать пересечения ребер;\nмаксимизировать симметричность укладки узлов;\nминимизировать изменчивость длины ребер;\nмаксимизировать угол между ребрами, когда они пересекают или соединяют узлы;\nминимизировать общее пространство для вывода сети.\n\n\nДля автоматического построения укладок разработано большое количество методов. В пакете igraph для каждого есть особая функция; вот некоторые из них:\n\nlayout_randomly()\n\nlayout_in_circle()\n\nlayout_on_sphere()\nlayout_with_drl() (Distributed Recursive Layout)\nlayout_with_fr() (Fruchterman-Reingold)\nlayout_with_dh() (Davidson-Harel)\nlayout_with_kk() (Kamada-Kawai)\nlayout_with_lgl() (Large Graph Layout)\nlayout_as_tree() (Reingold-Tilford)\nlayout_nicely()\n\nПакет ggraph позволяет выбрать укладку, не вызывая отдельно функцию:\n\nlibrary(gridExtra)\n\nlayouts &lt;- c(\"dh\", \"graphopt\", \"fr\", \"kk\")\n\nplot_graph &lt;- function(layout) {\n  g &lt;- ggraph(tudors_g, layout = layout) +\n  geom_edge_link() + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  show.legend = FALSE,\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  labs(title = layout)\n  \n  return(g)\n}\n\ng_list &lt;- map(layouts, plot_graph)\n\ngrid.arrange(grobs = g_list, nrow = 2)\n\n\n\n\n\n\n\n\nПодробнее см. здесь.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#дизайн-ребер",
    "href": "ggraph.html#дизайн-ребер",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.4 Дизайн ребер",
    "text": "19.4 Дизайн ребер\nНаш граф носит направленный характер, а значит мы можем отразить и направленность, и характер связей. Кодируем атрибут relationship, например, типом линии.\n\nset.seed(21092024)\n# добавляем итерации для укладки\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # вот тут вносим изменения\n  geom_edge_link(aes(edge_linetype = relationship),\n                 # меняем цвет линии\n                 color = \"grey50\",\n                 # меняем тип линии\n                 edge_width = 1.2) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  # перемещаем легенду\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nМожно заменить линии на стрелки.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_link(color = \"grey50\",\n                 # стрелка\n                 arrow = arrow(angle = 30, \n                               length = unit(0.25, \"cm\"),\n                               ends = \"last\", \n                               type = \"closed\"),\n                 # небольшой отступ от кружка\n                 end_cap = circle(1.5, \"mm\")\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nИли придать им изогнутости и раскрасить.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # вот тут изменения\n  geom_edge_arc(aes(color = relationship),\n                 # как сильно изгибать\n                 strength = 0.2,\n                 arrow = arrow(angle = 30, \n                               length = unit(0.2, \"cm\"),\n                               # от родителей к детям, а не наоборот\n                               ends = \"first\", \n                               type = \"closed\"),\n                 # тут тоже меняем \n                 start_cap = circle(1.5, \"mm\")\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  #geom_node_text(aes(label = name)) +\n  scale_fill_manual(values = cols) +\n  # цветовая шкала для ребер\n  scale_edge_color_manual(values = cols) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#подписи",
    "href": "ggraph.html#подписи",
    "title": "19  Графический дизайн сетей в ggraph",
    "section": "19.5 Подписи",
    "text": "19.5 Подписи\nЕсли мы просто вернем подписи, то они будут не очень читаемы, даже на нашем (очень небольшом) датасете.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # тип линии вместо цвета, убираем стрелку\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  # чуть подвинем\n  geom_node_text(aes(label = name), nudge_y = 0.5) +\n  scale_fill_manual(values = cols) +\n  # тип линии для ребер\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nОдно из решений может выглядеть так.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  # изменения тут\n  geom_node_label(aes(label = name, \n                      fill = gender),\n                  color = \"white\"\n                  ) +\n  scale_fill_manual(values = cols) +\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nПри желании можно заменить подписи на портреты или любую другую картинку.\n\nlibrary(ggimage)\nqueen &lt;- c(\"./images/queen.png\")\nking &lt;- c(\"./images/king.png\")\n\ngender_tbl &lt;- gender_tbl |&gt; \n  mutate(image = case_when(gender == \"m\" ~ king,\n                           gender == \"f\" ~ queen))\n\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                color = \"grey50\",\n                strength = 0.2\n  ) + \n  # изменения тут\n  geom_image(aes(x = x, \n                 y = y,\n                 image = gender_tbl$image),\n             size = 0.1)+\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nЕсли бы в наших данных были сведения о годе рождения, то мы могли бы их тоже учесть на графе, но пока оставим как есть.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей в `ggraph`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#подписи-с-geom_node_label",
    "href": "ggraph.html#подписи-с-geom_node_label",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.5 Подписи с geom_node_label()",
    "text": "19.5 Подписи с geom_node_label()\nЕсли мы просто вернем подписи, то они будут не очень читаемы, даже на нашем (очень небольшом) датасете.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  # тип линии вместо цвета, убираем стрелку\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  geom_node_point(aes(size = degree, \n                      fill = gender),\n                  shape = 21, \n                  color = \"black\"\n                  ) +\n  # чуть подвинем\n  geom_node_text(aes(label = name), nudge_y = 0.5) +\n  scale_fill_manual(values = cols) +\n  # тип линии для ребер\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  scale_size(guide = 'none') +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nОдно из решений может выглядеть так.\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                 color = \"grey50\",\n                 strength = 0.2\n                 ) + \n  # изменения тут\n  geom_node_label(aes(label = name, \n                      fill = gender),\n                  color = \"white\"\n                  ) +\n  scale_fill_manual(values = cols) +\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#картинки-с-geom_image",
    "href": "ggraph.html#картинки-с-geom_image",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.6 Картинки с geom_image()",
    "text": "19.6 Картинки с geom_image()\nПри желании можно заменить подписи на портреты или любую другую картинку.\n\nlibrary(ggimage)\nqueen &lt;- c(\"./images/queen.png\")\nking &lt;- c(\"./images/king.png\")\n\ngender_tbl &lt;- gender_tbl |&gt; \n  mutate(image = case_when(gender == \"m\" ~ king,\n                           gender == \"f\" ~ queen))\n\nset.seed(21092024)\nggraph(tudors_g, layout = \"dh\", maxiter = 100) +\n  geom_edge_arc(aes(linetype = relationship),\n                color = \"grey50\",\n                strength = 0.2\n  ) + \n  # изменения тут\n  geom_image(aes(x = x, \n                 y = y,\n                 image = gender_tbl$image),\n             size = 0.1)+\n  scale_edge_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  theme_graph(base_family = \"sans\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nЕсли бы в наших данных были сведения о годе рождения, то мы могли бы их тоже учесть на графе, но пока оставим как есть.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "ggraph.html#интерактивный-граф",
    "href": "ggraph.html#интерактивный-граф",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.7 Интерактивный граф",
    "text": "19.7 Интерактивный граф\nЧтобы добавить интерактивности, придется выйти за пределы ggraph. Пакет networkD3 требует на входе датафрейм.\n\n# install.packages(\"networkD3\")\nlibrary(networkD3)\nsimpleNetwork(tudors)\n\n\n\n\n\nЕще один вариант. Сначала трансформируем igraph в объект visNetwork. Цвета, если мы хотим на них повлиять, можно поменять вручную.\n\ncolors &lt;- ifelse(V(tudors_g)$gender==\"f\", cols[1], cols[2])\n\nV(tudors_g)$color &lt;- colors\n\n\n#install.packages(\"visNetwork\")\nlibrary(visNetwork)\ndata &lt;- toVisNetworkData(tudors_g)\n\n\ntudors_3d &lt;- visNetwork(nodes = data$nodes, \n                             edges = data$edges, \n                             color = data$nodes$color,\n                             width = \"100%\", \n                             height = 600)\n\nНастраиваем и сохраняем граф.\n\nvisOptions(tudors_3d, \n           highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE), \n           nodesIdSelection = FALSE)  |&gt; \n  visPhysics(maxVelocity = 20, stabilization = FALSE)  |&gt;  \n  visInteraction(dragNodes = TRUE)  |&gt; \n  # удалите эту строку, если хотите видеть граф во вьюере\n  visSave(file = \"tudors.html\")\n\nEt voilà. Все наши Тюдоры как живые.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "dracor.html",
    "href": "dracor.html",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "",
    "text": "20.1 О корпусе Dracor\nDraCor — сокращение от drama corpora — это собрание размеченных по стандарту TEI драматических текстов. Здесь есть пьесы на французском, немецком, испанском, русском, итальянском, шведском, португальском (только Кальдерон) и английском (только Шекспир), а также совсем небольшие коллекции эльзасских, татарских и башкирских пьес.\nДва крупных корпуса пьес в составе собрания — немецкий и русский — были собраны и поддерживаются создателями проекта DraCor. Остальные корпуса были взяты из сторонних проектов, а затем адаптированы для совместимости с функционалом DraCor. Подробнее об этом можно прочитать здесь.\nНа сайте проекта “Системный Блокъ” можно прочитать серию материалов о том, как возможности Dracor используются в литературоведении:",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#о-корпусе-dracor",
    "href": "dracor.html#о-корпусе-dracor",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "",
    "text": "о “Ревизоре”;\nо плотности сетей в трагедии и комедии;\nо “зоне смерти” в “Гамлете”.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#начало-работы-с-dracor",
    "href": "dracor.html#начало-работы-с-dracor",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.2 Начало работы с Dracor",
    "text": "20.2 Начало работы с Dracor\n\n# remotes::install_github(\"Pozdniakov/rdracor\")\nlibrary(rdracor)\nlibrary(tidyverse)\nlibrary(igraph)\n\nget_dracor_meta()  |&gt; \n  summary()\n\nDraCor hosts 21 corpora comprising 4312 plays.\n\nThe last updated corpus was Polish Drama Corpus (2025-01-19 11:56:45.9).\n\n\nИзвлекаем метаданные.\n\nmeta &lt;- get_dracor_meta()  |&gt; \n  select(name, title, plays)\n\nmeta\n\n\n  \n\n\n\n\nmeta  |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\nrus &lt;- get_dracor(\"rus\")\nsummary(rus)\n\n212 plays in Russian Drama Corpus   \nCorpus id: rus, repository: https://github.com/dracor-org/rusdracor \nDescription: Edited by Frank Fischer and Daniil Skorinkin. Features more than 200 Russian plays from the 1740s to the 1940s. For a corpus description and full credits please see the [README on GitHub](https://github.com/dracor-org/rusdracor).\nWritten years (range): 1747–1940    \nPremiere years (range): 1750–1992   \nYears of the first printing (range): 1747–1986\n\n\n\nrus &lt;- as_tibble(rus)\nrus\n\n\n  \n\n\n\nТут хранится очень много всего: размер сети, плотность сети и т.д. Вот так, например, выглядят самые длинные пьесы в корпусе:\n\nrus  |&gt; \n  arrange(-wordCountText)  |&gt; \n  select(firstAuthorName, title, wordCountText)\n\n\n  \n\n\n\nА так – самые густонаселенные.\n\nrus  |&gt; \n  arrange(-size) |&gt; \n  select(firstAuthorName, title, size)\n\n\n  \n\n\n\nПодробнее см. презентацию Ивана Позднякова, разработчика DraCor Shiny App (https://shiny.dracor.org/).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#сети-dracor",
    "href": "dracor.html#сети-dracor",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.3 Сети Dracor",
    "text": "20.3 Сети Dracor\nИзвлекаем граф для “Бориса Годунова”.\n\ngodunov &lt;- get_net_cooccur_igraph(play = \"pushkin-boris-godunov\", corpus = \"rus\")\n\ngodunov\n\nIGRAPH 19c706a UNW- 79 327 -- \n+ attr: name (v/c), isGroup (v/l), gender (v/c), numOfScenes (v/n),\n| numOfSpeechActs (v/n), numOfWords (v/n), degree (v/n), weightedDegree\n| (v/n), closeness (v/n), betweenness (v/n), eigenvector (v/n),\n| wikidataId (v/c), weight (e/n)\n+ edges from 19c706a (vertex names):\n[1] Воротынский--Шуйский                   \n[2] Воротынский--Борис                     \n[3] Воротынский--Бояре                     \n[4] Шуйский    --Борис                     \n[5] Шуйский    --Бояре                     \n+ ... omitted several edges\n\n\nПомимо уже знакомых нам атрибутов вроде имени (name (v/c)), гендера (gender (v/c)) и степени (degree(v/n)), мы видим здесь много новой информации. Некоторые атрибуты интуитивно понятны: является ли персонаж групповым (isGroup (v/l)); в каком часле явлений он участвует (numOfScenes (v/n)); сколько у него реплик (numOfSpeechActs (v/n)) и слов (numOfWords (v/n)).\nАтрибут wikidataId (v/c) представлен лишь для исторических лиц, например, для самого Годунова.\n\ntibble(name = V(godunov)$name,\n       isGroup = V(godunov)$isGroup,\n       numOfScenes = V(godunov)$numOfScenes,\n       numOfSpeechActs = V(godunov)$numOfSpeechActs,\n       numOfWords = V(godunov)$numOfWords) |&gt; \n  arrange(-numOfScenes)\n\n\n  \n\n\n\nЭта информация заботливо собрана создателями Dracor’а, но при желании ее можно проверить: функция get_text_df() дает возможность извлечь текст пьесы в виде датафрейма. Убедимся, например, что Григорий появляется в 8 сценах.\n\ngodunov_df &lt;- get_text_df(play = \"pushkin-boris-godunov\", corpus = \"rus\")\ngodunov_df\n\n\n  \n\n\n\n\ngodunov_df |&gt; \n  filter(who == \"grigorij_dimitrij_lzhedimitrij_samozvanets\") |&gt; \n  count(scene_id)\n\n\n  \n\n\n\nНам осталось разобраться с такими атрибутами, как weightedDegree (v/n), closeness (v/n), betweenness (v/n), eigenvector (v/n), weight (e/n). Начнем с последнего.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#анализ-узлов-и-ребер",
    "href": "dracor.html#анализ-узлов-и-ребер",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.4 Анализ узлов и ребер",
    "text": "20.4 Анализ узлов и ребер\n\n20.4.1 Вес ребра\nВеса ребер, как следует из технической документации к пакету, хранят информацию о том, сколько раз персонажи вместе появляются на сцене.\n\nE(godunov)[weight &gt; 1] \n\n+ 9/327 edges from 19c706a (vertex names):\n[1] Воротынский--Шуйский        Шуйский    --Борис         \n[3] Борис      --Бояре          Борис      --Феодор        \n[5] Борис      --Басманов       Григорий   --Гаврила Пушкин\n[7] Григорий   --Курбский       Григорий   --Ляхи          \n[9] Ксения     --Феодор        \n\n\nЭти сведения можно извлечь и при помощи специальной функции.\n\ncooc_dracor &lt;- get_net_cooccur_edges(play = \"pushkin-boris-godunov\", corpus = \"rus\")\n\ncooc_dracor |&gt; \n  filter(Weight &gt; 1) |&gt; \n  arrange(-Weight)\n\n\n  \n\n\n\n\n\n20.4.2 Взвешенная центральность\nВажность (prominence) участника (актора, вершины, узла) определяется его положением внутри сети. Применительно к ненаправленным сетям говорят о центральности (центральный актор вовлечен в наибольшее количество связей, прямых или косвенных), а применительно к направленным – о престиже. Престижный актор характеризуется большим количеством входящих связей.\nМы уже умеем считать центральность по степени (degree centrality), которая определяется количеством связей: чем больше прямых связей, тем более важным является узел.\n\ndegrees &lt;- degree(godunov)\nsort(degrees, decreasing = T)[1:10]\n\n         Борис          Народ       Григорий         Феодор          Бояре \n            29             26             25             23             21 \n      Басманов         Ксения        Шуйский Гаврила Пушкин           Ляхи \n            15             14             13             11             10 \n\n# проверка\n# V(godunov)$degree == degrees\n\nС понятием веса ребра тесно связана взвешенная центральность по степени (weightedDegree(v/n)). В отличие от простой центральности, она учитывает вес связанных с узлом ребер.\n\nwDegree &lt;- strength(godunov)\n\n# проверка\ntibble(wd_old = V(godunov)$weightedDegree,\n       wd_new = wDegree) |&gt; \n  arrange(-wd_old)\n\n\n  \n\n\n\nВот так, например, считается взвешенная центральность для Бориса Годунова:\n\n# ребра, связанные с Годуновым\nidx &lt;- incident(godunov, \"Борис\")\n\n# суммарный вес ребер\nsum(E(godunov)[idx]$weight)\n\n[1] 35\n\n\nНа графе веса ребер можно отразить за счет толщины и (или) прозрачности линии, а взвешенную центральность - за счет размера узла\n\nlibrary(ggraph)\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"nbapalettes::hawks_statement\")\n\nset.seed(22092024)\nggraph(godunov, layout = \"kk\", maxiter = 500) + \n  # здесь кодируем вес ребер\n  geom_edge_link(aes(alpha = weight),\n                 color = cols[3],\n                 width = 0.8,\n                 show.legend = FALSE) +\n  # здесь взвешенная центральность\n  geom_node_point(aes(size = weightedDegree),\n                  color = cols[2],\n                  show.legend = FALSE) + \n  # обратите внимание на фильтр!\n  geom_node_text(aes(filter = (weightedDegree &gt; 10 | name %in% c(\"Курбский\", \"Воротынский\")),\n                     label = name),\n                 color = cols[1],\n                 repel = TRUE) +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n20.4.3 Центральность по близости\nЦентральность по близости (closeness centrality) говорит о том, насколько близко узел расположен к другим узлам сети. Центральность по близости – это величина, обратная сумме расстояний от узла i до всех остальных узлов сети.\n\\[\\frac{1}{\\sum_{i\\neq v}d_{vi}}\\]\n\nОбратите внимание: в Dracor все метрики рассчитывались с использованием Python-пакета networkX. Имплементации расчетов сетевых метрик могут не совпадать.\n\n\nclose_new &lt;- closeness(godunov, \n                     mode = \"all\",\n                     normalized = TRUE)\n\n# похожие значения хранятся как атрибуты узлов\ntibble(name = V(godunov)$name,\n       close_old = V(godunov)$closeness,\n       close_new = close_new) |&gt; \narrange(-close_old)\n\n\n  \n\n\n\n\nВ пьесах эта метрика может означать, напрямую ли взаимодействуют с этим персонажем или нет. Например, в пьесе А. Н. Островского «Лес» персонаж Аксюша имеет невысокую взвешенную степень, но наибольшую степень близости. По сюжету, она находится в зависимом положении, в первую очередь от Гурмыжской (которая имеет наибольшую взвешенную степень), и это может означать что остальные персонажи взаимодействуют с ней напрямую, так как могут себе это позволить. – Источник.\n\nНа графе закодируем этот атрибут цветом; температурную шкалу установим вручную.\n\nset.seed(22092024)\nggraph(godunov, layout = \"kk\", maxiter = 500) + \n  geom_edge_link(aes(alpha = weight),\n                 color = cols[3],\n                 width = 0.8,\n                 show.legend = FALSE) +\n  geom_node_point(aes(size = weightedDegree,\n                      # тут новое\n                      color = closeness),\n                  show.legend = FALSE) + \n  geom_node_text(aes(filter = (weightedDegree &gt; 10 | name %in% c(\"Курбский\", \"Воротынский\")),\n                     label = name),\n                 color = cols[3],\n                 repel = TRUE) +\n  # градиентная шкала для closeness\n  scale_color_gradient(low = \"plum\", high = \"purple\") +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n\n20.4.4 Центральность по посредничеству\nЦентральность по посредничеству (betweenness centrality) характеризует, насколько важную роль данный узел играет на пути “между” парами других узлов сети.\n\nbetweenness_new &lt;- betweenness(godunov, \n                               directed = FALSE,\n                               normalized = TRUE)\n\ntibble(name = V(godunov)$name,\n       b_old = round(V(godunov)$betweenness, 4),\n       b_new = round(betweenness_new, 4)) |&gt; \n  arrange(-b_old)\n\n\n  \n\n\n\n\nХороший пример персонажа с высокой степенью посредничества в корпусе русской драмы — второстепенный персонаж Гаврила Пушкин из пьесы «Борис Годунов» А.С. Пушкина. …По сюжету, он является связующим персонажем между приближёнными Бориса и Григорием. При прочтении легко не заметить важность этого персонажа, однако на визуализации сети пьесы хорошо видно, что Гаврила связывает два кластера — персонажей в Москве и в Польше. – Источник.\n\n\n\n20.4.5 Центральность по собственному вектору\nСтепень влиятельности (eigenvector centrality) показывает важность персонажа, учитывая влиятельность персонажей, с которыми взаимодействует данный персонаж. В пьесах эта метрика позволяет разделить действующих лиц на «центральных» и «периферийных».\n\nПерсонажи более значимы, если они взаимодействуют с персонажами важнее себя, и теряют свою значимость при контакте с менее важными действующими лицами. – Источник.\n\nЧтобы посчитать eigenvector centrality, необходимо преобразовать граф в матрицу смежности (социоматрицу), в которой единицами отмечено наличие рёбер между персонажами, а нулями – их отсутствие. Вместо единиц в матрице могут быть указаны веса рёбер; в таком случае матрица будет взвешенной.\nУ таких матриц есть собственные векторы, то есть такие векторы, произведение которых на матрицу эквивалентно произведению числа на этот вектор.\n\\[\\lambda \\cdot C_e = A \\cdot C_e,\\] где\n\n\\(А\\) — это матрица смежности;\n\\(λ\\) — действительное число;\n\\(С_e\\) — собственный вектор матрицы \\(А\\).\n\nЭлементы вектора \\(C_e\\) являются степенями влиятельности для каждой вершины. Это можно переписать для отдельных вершин так:\n\\[C_e(v_i)=\\frac{1}{\\lambda}\\sum_{j=1}^{n}a_{ij} C_e(v_j),\\] где\n\n\\(С_E(v_i)\\) — это степень влиятельности вершины \\(v_i\\);\n\\(a_ij\\) — элемент матрицы \\(A\\), расположенный в i-й строке и j-м столбце.\n\nВ такой записи видно, что на степень влиятельности вершины \\(v_i\\) влияют значения всех остальных вершин. Алгоритм расчета eigenvector centrality в последних версиях igraph немного отличается.\n\neigen_new &lt;- eigen_centrality(godunov, scale = FALSE)$vector\n\nWarning: The `scale` argument of `eigen_centrality()` always as if TRUE as of igraph\n2.1.1.\nℹ Normalization is always performed\n\ntibble(name = V(godunov)$name,\n       eigen_old = V(godunov)$eigenvector,\n       eigen_new = eigen_new) |&gt; \n  arrange(-eigen_old)\n\n\n  \n\n\n\nТеперь мы понимаем смысл всех атрибутов в графах Dracor, и можем перейти к характеристике графа в целом.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#централизация",
    "href": "dracor.html#централизация",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.5 Централизация",
    "text": "20.5 Централизация\nРассмотрим два крайних случая: круговой граф и звездчатый граф.\n\nstar_g &lt;- make_star(5, mode = \"undirected\") \ncircle_g &lt;- make_ring(5)\n\npar(mfrow = c(1, 2))\nplot(circle_g, vertex.color=2)\nplot(star_g, vertex.color=3)\n\n\n\n\n\n\n\n\nВ случае звездчатого графа централизация максимальна, а для отдельных узлов наблюдается разброс центральности.\n\ncentr_clo(star_g)\n\n$res\n[1] 1.0000000 0.5714286 0.5714286 0.5714286 0.5714286\n\n$centralization\n[1] 1\n\n$theoretical_max\n[1] 1.714286\n\n\nВо втором случае наборот – разброса нет, а для графа в целом централизация минимальна.\n\ncentr_clo(circle_g)\n\n$res\n[1] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667\n\n$centralization\n[1] 0\n\n$theoretical_max\n[1] 1.714286\n\n\nРасчитаем централизацию для графа “Годунова”.\n\ncentr_clo(godunov)$centralization\n\n[1] 0.3050424\n\n\nНо в наших данных она уже рассчитана.\n\nsummary(godunov)\n\nrus: pushkin-boris-godunov - co-ocurence network summary    \nПушкин, Александр Сергеевич: Борис Годунов (1831)   \n    \n         Size: 79 (9 FEMALES, 69 MALES, 1 UNKNOWN)  \n      Density: 0.11 \n       Degree:  \n         - Maximum: 29 (Борис)  \n     Distance:  \n         - Maximum (Diameter): 7    \n         - Average: 3.45    \n   Clustering:  \n         - Global: 0.65 \n         - Average local: 0.92  \n     Cohesion: 1    \nAssortativity: -0.06",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#точки-сочленения",
    "href": "dracor.html#точки-сочленения",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.6 Точки сочленения",
    "text": "20.6 Точки сочленения\nТочка сочленения – это узел, при удалении которого увеличивается число компонент связности. Таким образом, они соединяют разные части сети. При их удалении акторы (узлы, вершины) не могут взаимодействовать друг с другом.\n\narticulation_points(godunov)\n\n+ 7/79 vertices, named, from 19c706a:\n[1] Народ          Патриарх       Григорий       Марина         Гаврила Пушкин\n[6] Борис          Шуйский       \n\n\nТочки сочленения тесто связаны с центральностью по посредничеству.\n\nV(godunov)[betweenness &gt; 0.025]\n\n+ 10/79 vertices, named, from 19c706a:\n [1] Шуйский        Народ          Борис          Бояре          Григорий      \n [6] Патриарх       Феодор         Гаврила Пушкин Марина         Басманов",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#клики",
    "href": "dracor.html#клики",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.7 Клики",
    "text": "20.7 Клики\nМногие сети состоят из относительно плотных подгрупп, которые соединены между собой менее крепкими связями. Один из способов взглянуть на подгруппы сети заключается в исследовании социальной сплочености (cohesion). Сплоченные подгруппы - это множество акторов, которые объединены между собой посредством многочисленных, сильных и прямых связей.\nКлика – один из самых простых типов сплоченных подгрупп; это максимально полный подграф, т.е. подмножество узлов со всеми возможными связями между ними. Вопреки своему названию, функция clique_num() возвращает размер наибольшей клики:\n\nclique_num(godunov)\n\n[1] 11\n\n\nНа самом деле таких клик даже три. Узнаем, кто туда входит.\n\ncliques(godunov, min=11)\n\n[[1]]\n+ 11/79 vertices, named, from 19c706a:\n [1] Народ                   Ксения                  Феодор                 \n [4] Нищий                   Стража                  Один из народа (Кремль)\n [7] Другой (Кремль)         Один из народа (Кремль) Другой (Кремль)        \n[10] Третий (Кремль)         Мосальский             \n\n[[2]]\n+ 11/79 vertices, named, from 19c706a:\n [1] Борис     Бояре     Феодор    Басманов  Боярин    Один      Другой   \n [8] Третий    Четвертый Пятый     Шестой   \n\n[[3]]\n+ 11/79 vertices, named, from 19c706a:\n [1] Народ                                          \n [2] Борис                                          \n [3] Бояре                                          \n [4] Один из народа (Площадь перед собором в Москве)\n [5] Другой (Площадь перед собором в Москве)        \n [6] Третий (Площадь перед собором в Москве)        \n [7] Четвертый (Площадь перед собором в Москве)     \n [8] Мальчишки                                      \n [9] Старуха                                        \n[10] Юродивый                                       \n+ ... omitted several vertices\n\n\nИли, что то же самое:\n\nlargest_cliques(godunov)\n\nНо клика – это очень строгое определение сплоченной группы. Например, чтобы подграф, состоящий из 11 вершин, считался кликой, нужно, чтобы между ними было проведено \\((11 \\times 10) / 2 = 21\\) связей. Если хотя бы одно ребро отсутствует, то условие не выполняется. Такие клики просто очень редко встречаются.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#k-ядра",
    "href": "dracor.html#k-ядра",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.8 K-ядра",
    "text": "20.8 K-ядра\nПопулярным определением социальной сплоченности является k-ядро (k-core). Это максимальный подграф, в котором каждая вершина связана минимум с k другими вершинами этого же подграфа. K-ядра имеют множество преимуществ:\n\nони вложены друг в друга (каждый участник 4-ядра является также участником 3-ядра и т.д.);\nони не перекрываются;\nих легко определить.\n\n\nВыражение 6-ядро читают как “ядро степени 6”.\n\nЯдро степени k+1 является подграфом ядра степени k. Любой узел в ядре степени k имеет степень либо k, либо выше. При этом coreness узла определяется по ядру с наибольшей степенью, к которому они принадлежат.\n\nДля определения k-ядерной структуры используется функция graph.coreness():\n\ncores_godunov &lt;- coreness(godunov)\nhead(cores_godunov)\n\n             Воротынский                  Шуйский   Один (Красная площадь) \n                       3                        5                        4 \nДругой (Красная площадь) Третий (Красная площадь)                    Народ \n                       4                        4                       10 \n\n\nПосчитаем количество вершин в ядрах.\n\ntable(cores_godunov)\n\ncores_godunov\n 1  2  3  4  5  7 10 \n 2  6  1  7 11 23 29 \n\n\nДля лучшей интерпретации k-ядерной структуры мы можем графически изобразить сеть, используя информацию о множестве k-ядер. Для начала добавим информацию о цвете к атрибутам узлов.\n\nV(godunov)$core &lt;- cores_godunov\n\n# убедимся, что добавился новый атрибут\nnames(vertex_attr(godunov))\n\n [1] \"name\"            \"isGroup\"         \"gender\"          \"numOfScenes\"    \n [5] \"numOfSpeechActs\" \"numOfWords\"      \"degree\"          \"weightedDegree\" \n [9] \"closeness\"       \"betweenness\"     \"eigenvector\"     \"wikidataId\"     \n[13] \"core\"           \n\n\n\nset.seed(22092024)\nggraph(godunov, layout = \"kk\", maxiter = 500) + \n  geom_edge_link(color = cols[3],\n                 alpha = 0.3,\n                 width = 0.6) +\n  # тут  новое\n  geom_node_point(aes(color = as.factor(core)),\n                  size = 3, \n                  show.legend = TRUE) + \n  # новый фильтр\n  geom_node_text(aes(filter = degree &gt; 10,\n                     label = name),\n                 color = cols[3],\n                 repel = TRUE) +\n  scale_color_brewer(\"k-ядра\", type = \"qual\") +\n  theme_void()\n\n\n\n\n\n\n\n\nЧтобы глубже исследовать подгруппы, последовательно удаляют k-ядра более низкой степени. Для этого можно воспользоваться функцией induced_subgraph().\n\ngodunov5_10 &lt;- induced_subgraph(godunov, vids=V(godunov)[core &gt; 4])\n\nggraph(godunov5_10, layout = \"kk\", maxiter = 500) + \n  geom_edge_link(color = cols[3],\n                 alpha = 0.3,\n                 width = 0.6) +\n  geom_node_point(aes(color = as.factor(core)),\n                  size = 3, \n                  show.legend = TRUE) + \n  geom_node_text(aes(filter = degree &gt; 10,\n                     label = name),\n                 color = cols[3],\n                 repel = TRUE) +\n  scale_color_brewer(\"k-ядра\", type = \"qual\") +\n  theme_void()\n\n\n\n\n\n\n\n\nПри интерпретации важно помнить, что ядра являются вложенными. Чем выше степень ядра, тем больше узлы связаны между собой.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#модулярность",
    "href": "dracor.html#модулярность",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.9 Модулярность",
    "text": "20.9 Модулярность\nМодулярность — одна из мер структуры сетей или графов. Мера была разработана для измерения силы разбиения сети на модули (называемые группами, кластерами или сообществами). Сети с высокой модулярностью имеют плотные связи между узлами внутри модулей, но слабые связи между узлами в различных модулях.\n\nМодулярность равна доле рёбер от общего числа рёбер, которые попадают в данные группы, минус ожидаемая доля рёбер, которые попали бы в те же группы, если бы они были распределены случайно.\nЕсли все узлы принадлежат к одному классу, то модулярность равна нулю. Если разбиение на классы хорошее, то модулярность должна быть высокая. Мы можем проверить, хорошо ли группируются персонажи по гендеру. Для этого перекодируем гендер, так как функция modularity() принимает числовую переменную в качестве аргумента. Значение гендера 3 (unknown) в пьесе имеют групповые персонажи.\n\nV(godunov)$gender\n\n [1] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"UNKNOWN\" \"MALE\"   \n [8] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[15] \"MALE\"    \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[22] \"MALE\"    \"FEMALE\"  \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[29] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[36] \"FEMALE\"  \"MALE\"    \"FEMALE\"  \"FEMALE\"  \"FEMALE\"  \"MALE\"    \"MALE\"   \n[43] \"MALE\"    \"MALE\"    \"MALE\"    \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"   \n[50] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[57] \"FEMALE\"  \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[64] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[71] \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"    \"MALE\"   \n[78] \"MALE\"    \"MALE\"   \n\n## male = 1\nidx &lt;- V(godunov)$gender==\"MALE\"\nV(godunov)$gender[idx] &lt;- 1\n\n## female = 2\nidx &lt;- V(godunov)$gender==\"FEMALE\"\nV(godunov)$gender[idx] &lt;- 2\n\n## unknown = 3\nidx &lt;- V(godunov)$gender==\"UNKNOWN\"\nV(godunov)$gender[idx] &lt;- 3\n\n\ngender &lt;- as.numeric(V(godunov)$gender)\nmodularity(godunov, gender)\n\n[1] 0.01434597\n\n\n\nПри выделении сообществ в большинстве случаев наша задача – максимизировать модулярность.\n\nОчевидно, что гендер не лучшим образом описывает деление персонажей “Годунова” на группы. Поищем другие сообщества.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "dracor.html#алгоритмы-обнаружения-сообществ",
    "href": "dracor.html#алгоритмы-обнаружения-сообществ",
    "title": "20  Анализ сетей и обнаружение сообществ",
    "section": "20.10 Алгоритмы обнаружения сообществ",
    "text": "20.10 Алгоритмы обнаружения сообществ\nВ пакете igraph реализовано множество алгоритмов обнаружения сообществ. Обычной практикой является применение нескольких алгоритмов и сравнение результатов.\n\nУ нас ненаправленная взвешенная сеть. Применим алгоритм “случайного блуждания”.\n\ncw &lt;- cluster_walktrap(godunov)\nmembership(cw) |&gt; head()\n\n             Воротынский                  Шуйский   Один (Красная площадь) \n                       2                        2                        9 \nДругой (Красная площадь) Третий (Красная площадь)                    Народ \n                       9                        9                        2 \n\n\n\npar(mar = rep(0, 4))\nplot(cw, godunov)\n\n\n\n\n\n\n\n\nЗначение модулярности достаточно высокое (уж точно лучше, чем гендер).\n\nmodularity(cw)\n\n[1] 0.5639771\n\n\nПоищем другое разбиение.\n\ncsg &lt;- cluster_spinglass(godunov)\nmembership(csg) |&gt; head()\n\n             Воротынский                  Шуйский   Один (Красная площадь) \n                       6                        6                        2 \nДругой (Красная площадь) Третий (Красная площадь)                    Народ \n                       2                        2                        2 \n\n\n\npar(mar = rep(0, 4))\nplot(csg, godunov)\n\n\n\n\n\n\n\n\nПоказатели модулярности чуть выше, чем для предыдущего разбиения.\n\nmodularity(csg)\n\n[1] 0.6128681\n\n\nТакже используем алгоритм под названием “главный собственный вектор”.\n\ncev &lt;- cluster_leading_eigen(godunov)\nmodularity(cev)\n\n[1] 0.617586\n\n\n\npar(mar = rep(0, 4))\nplot(cev, godunov)",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Анализ сетей и обнаружение сообществ</span>"
    ]
  },
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "21  Пространственные данные в R",
    "section": "",
    "text": "21.1 Данные: римские амфитеатры\nДанные для этого урока происходят из пакета cawd (Collected Ancient World Data), который, в свою очередь, опирается на следующие ресурсы:\nМы заберем из пакета датафрейм с римскими амфитеатрами (подробнее о нем можно прочитать здесь) и карту Римской империи на 200 г. н.э. (в формате sp, который представляет собой немного устаревший, но легко конвертируемый формат хранения пространственных данных в R).\n#devtools::install_github(\"sfsheath/cawd\")\nlibrary(cawd)\nlibrary(sp)\nclass(awmc.roman.empire.200.sp)\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\nОбъект sp имеет свой метод plot().\npar(mai=c(0,0,0,0))\nplot(awmc.roman.empire.200.sp)\nДля начала выберем нужные столбцы из датафрейма с данными об амфитеатрах.\nlibrary(tidyverse)\n\nramphs &lt;- cawd::ramphs |&gt; \n  dplyr::select(label, longitude, latitude, capacity, type, prov.type)\n\nramphs",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#пакет-tmap",
    "href": "maps.html#пакет-tmap",
    "title": "21  Пространственные данные в R",
    "section": "21.3 Пакет tmap",
    "text": "21.3 Пакет tmap\nЕсть множество пакетов для работы с пространственными данными в R; мы начнем с одного из наиболее простого и интуитивно понятного tmap.\n\n# install.packages(\"tmap\")\n# install.packages(\"tmaptools\")\nlibrary(tmap)\ntmap_mode(\"plot\")\n\nℹ tmap mode set to \"plot\".\n\ntmap_style(\"white\") # default\n\nstyle set to \"white\" (tmap default)\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\"\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\n\n\ntm_shape(roman_map) +\n  tm_fill(fill = \"magenta\") +\n  tm_borders(col = \"white\") \n\n\n\n\n\n\n\n\nПакет tmap предлагает хороший выбор стилей для оформления карты.\n\ntmap_style(\"classic\")\n\nstyle set to \"classic\"\n\n\nother available styles are: \"white\" (tmap default), \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\ntm_shape(roman_map) +\n  tm_fill() +\n  tm_borders() \n\n\n\n\n\n\n\n\nВручную можно добавить, например, компасс, координатную сетку и шкалу масштаба.\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_graticules() +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scalebar(\n    breaks = c(0, 500, 1000, 1500),\n    text.size = 1, \n    position = c(\"left\", \"bottom\")) \n\nScale bar set for latitude km and will be different at the top and bottom of the map.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#точки",
    "href": "maps.html#точки",
    "title": "21  Пространственные данные в R",
    "section": "21.3 Точки",
    "text": "21.3 Точки\n\nlibrary(tidyverse)\n\nramphs &lt;- cawd::ramphs |&gt; \n  dplyr::select(label, longitude, latitude, capacity, prov.type)\n\nramphs\n\n\n  \n\n\n\n\namph_points &lt;- ramphs |&gt; \n   st_as_sf(coords = c(\"longitude\", \"latitude\"))\n\namph_points\n\n\n  \n\n\n\n\ntmap_style(\"classic\")\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(type = \"8star\", \n             position = c(\"right\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), \n               text.size = 1, \n               position = c(\"left\", \"bottom\")) +\n  tm_shape(amph_points) +\n  tm_bubbles(size = \"capacity\", \n             alpha = 0.8, \n             scale = 1,\n             col = \"prov.type\", \n             palette = c(\"red\", \"blue\", \"green\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE\n            )\n\n\n\n\n\n\n\n\nhttps://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(type = \"8star\", \n             position = c(\"right\", \"top\")) +\n  tm_scale_bar(breaks = c(0, 100, 200), \n               text.size = 1, \n               position = c(\"left\", \"bottom\")) +\n  tm_shape(amph_points) +\n  tm_bubbles(size = \"capacity\", \n             alpha = 0.8, \n             scale = 1,\n             col = \"prov.type\", \n             palette = c(\"red\", \"blue\", \"green\")\n             ) +\n  # фильтр для названий\n  tm_shape(amph_points |&gt; filter(capacity &gt; 30000 )) +\n  # текст\n  tm_text(\"label\", size = 0.8) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE\n            )\n\nWarning: Currect projection of shape amph_points unknown. Long-lat (WGS84) is\nassumed.\n\n\nWarning: Currect projection of shape filter(amph_points, capacity &gt; 30000)\nunknown. Long-lat (WGS84) is assumed.\n\n\nScale bar set for latitude km and will be different at the top and bottom of the map.\n\n\n\n\n\n\n\n\n\nhttps://r-tmap.github.io/tmap/",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#заливка",
    "href": "maps.html#заливка",
    "title": "21  Пространственные данные в R",
    "section": "21.4 Заливка",
    "text": "21.4 Заливка\nhttps://r-spatial.org/book/07-Introsf.html Посчитать число точек в многограннике\nhttps://epsg.io/ почему это важно см. lovelace\n\nlibrary(sf)\n#st_crs(roman_map)\nst_crs(amph_points)\n\nCoordinate Reference System: NA\n\n\n\nst_intersects(roman_map, amph_points)\n\nError in st_geos_binop(\"intersects\", x, y, sparse = sparse, prepared = prepared, : st_crs(x) == st_crs(y) is not TRUE\n\n\nЧтобы исправить, необходимо назначить координатные системы.\n\n# если нужно трансформировать\nroman_map &lt;- st_transform(roman_map, 4326)\n\n# если нужно назначить\namph_points &lt;- st_set_crs(amph_points, 4326)\n\n# пересечения\ninter &lt;- st_intersects(roman_map, amph_points)\n\n\ninter\n\nSparse geometry binary predicate list of length 112, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 45, 46, 69, 78, 80, 88, 108, 109, 110, 151, ...\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: (empty)\n 6: (empty)\n 7: 3, 21, 38, 39, 47, 74, 89, 114, 115, 124, ...\n 8: (empty)\n 9: (empty)\n 10: (empty)\n\n\n\n# добавляем данные \nroman_map$count &lt;- lengths(inter)\n\n\ntm_shape(roman_map) +\n  tm_polygons(col = \"count\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#ландшафт",
    "href": "maps.html#ландшафт",
    "title": "21  Пространственные данные в R",
    "section": "21.5 Ландшафт",
    "text": "21.5 Ландшафт\nhttps://rdrr.io/cran/tmaptools/man/bb.html\n\nlibrary(tmaptools)\nbb_region = bb(roman_map)\nbb_region\n\n    xmin     ymin     xmax     ymax \n-9.48732 22.89549 43.10774 55.10117 \n\n\nhttps://docs.stadiamaps.com/map-styles/stamen-terrain/ (почему не гугл: нужна регистрация с картой!)\n\ntmap_mode(\"view\")\n\ntm_basemap(\"https://tiles.stadiamaps.com/tiles/stamen_terrain_background/{z}/{x}/{y}{r}.png\") +\n  tm_shape(roman_map) +\n  tm_polygons(alpha = 0.5, col = \"count\")",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#плотность-2d",
    "href": "maps.html#плотность-2d",
    "title": "21  Пространственные данные в R",
    "section": "21.6 Плотность 2D",
    "text": "21.6 Плотность 2D\nhttp://sebastianheath.com/cawd/inst/rmarkdown/amphitheater-heatmaps.html\n\nggplot() +\n  geom_sf(data = roman_map) +\n  geom_point(ramphs, \n             mapping = aes(longitude, latitude),\n             color = \"steelblue\", \n             alpha = 0.5)  +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_point(ramphs, color = \"steelblue\", alpha = 0.5,\n             mapping = aes(longitude, latitude)) +\n  geom_density2d(data = ramphs, \n                 mapping = aes(longitude, latitude, \n                               color = after_stat(level)),\n                 size = 1, alpha = 0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#сотовая-диаграмма",
    "href": "maps.html#сотовая-диаграмма",
    "title": "21  Пространственные данные в R",
    "section": "21.7 Сотовая диаграмма",
    "text": "21.7 Сотовая диаграмма\n\ng &lt;- ggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_hex(data = ramphs,\n                 mapping = aes(longitude, latitude),\n           bins = 25,\n           color = \"royalblue\")  +\n  theme_bw() +\n  scale_fill_continuous(trans = \"reverse\") \n\ng\n\n\n\n\n\n\n\n\nhttps://info5940.infosci.cornell.edu/notes/geoviz/raster-maps-with-ggmap/",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#plotly",
    "href": "maps.html#plotly",
    "title": "21  Пространственные данные в R",
    "section": "21.8 Plotly",
    "text": "21.8 Plotly\nhttps://www.paulamoraga.com/book-spatial/making-maps-with-r.html\n\nlibrary(plotly)\nggplotly(g)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#данные-римские-дороги",
    "href": "maps.html#данные-римские-дороги",
    "title": "21  Пространственные данные в R",
    "section": "21.10 Данные: римские дороги",
    "text": "21.10 Данные: римские дороги\n\nroman_roads &lt;- cawd::darmc.roman.roads.major.sp |&gt; \n  st_as_sf()\n\nroman_roads\n\n\n  \n\n\n\n\nggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_sf(data = roman_roads,\n          color = \"steelblue\",\n          alpha = 0.5) +\n  geom_tile()\n\n\n\n\n\n\n\n\nДобавим подложку.\n\nlibrary(ggspatial)\n# Reproject to EPSG:3857 (Web Mercator)\nroman_map_3857 &lt;- st_transform(roman_map, 3857)\nroman_roads_3857 &lt;- st_transform(roman_roads, 3857)\n\nggplot() +\n  # Add spatial tile background\n  annotation_map_tile(\n    type = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/${z}/${y}/${x}.jpg\",\n    zoomin = -1) +\n\n  # Add your spatial features\n  geom_sf(data = roman_map_3857, \n          fill = \"wheat\", alpha = 0.4) +\n  geom_sf(data = roman_roads_3857, \n          color = \"darkblue\", alpha = 0.8, size = 1) +\n\n  # Set coordinate system to Web Mercator to match tiles\n  coord_sf(crs = st_crs(3857)) \n\nZoom: 3",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#географическая-сеть",
    "href": "maps.html#географическая-сеть",
    "title": "21  Пространственные данные в R",
    "section": "21.10 Географическая сеть",
    "text": "21.10 Географическая сеть\nhttps://book.archnetworks.net/visualization https://agricolamz.github.io/daR4hs/7_working_with_geodata.html\n\norbis_coord &lt;- orbis_e |&gt; \n  left_join(orbis_n, by = join_by(source == id)) |&gt; \n  mutate(source = label, .before = target) |&gt; \n  select(-label) |&gt; \n  # сначала пишется широта, потом долгота\n  # например Рим 42 с.ш. 12 в.д., здесь долгота в y\n  # но на карте горизонталь - это долгота \n  rename(x1 = y, y1 = x) |&gt; \n  left_join(orbis_n, by = join_by(target == id)) |&gt; \n  mutate(target = label, .after = source) |&gt; \n  select(-label) |&gt; \n  rename(x2 = y, y2 = x) |&gt;\n  # пуповины к центру мира\n  filter(x1 != 0, y1 !=0, x2 != 0, y2 != 0)\n\n\nworld &lt;- map_data(\"world\") \n\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"wheat\", color = \"grey\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = \"steelblue\", alpha = 0.5) +\n  geom_point(data = orbis_coord |&gt; \n               filter(source == \"Roma\"), \n             color = \"tomato\",\n             aes(x1, y1)) +\n  coord_map(xlim = c(-10, 50),\n            ylim = c(23, 54)) +\n  geom_segment(data = orbis_coord, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type))\n\n\n\n\n\n\n\n\nПараллельные линии https://rpubs.com/BrendanKnapp/GeospatialNetworkPlotting\nМожно просто удалить часть городов (восточнее Берениса)\n\norbis_coord_pruned &lt;-  orbis_coord |&gt; \n  filter(y1 &gt; 28 & y2 &gt; 28)\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"basetheme::brutal\")\n\n\npar(mar = rep(0,4))\nset.seed(24092024)\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"white\", color = \"wheat\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = cols[1], alpha = 0.5) +\n  geom_segment(data = orbis_coord_pruned, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type)) +\n  geom_label(data = orbis_coord |&gt; \n               filter(source %in% c(\"Roma\", \"Alexandria\", \"Carthago\", \"Sirmium\", \"Corinthus\", \"Antiochia\", \"Londinium\", \"Tarraco\", \"Augusta Taurinorum\", \"Jerusalem\")),\n             aes(x1, y1, label = source),\n             color = cols[5], \n             label.size = 0.15,\n             fontface = \"bold\") +\n  coord_map(xlim = c(-10, 45),\n            ylim = c(26, 54)) +\n  labs(x = NULL, y = NULL, \n       title = \"Транспортное сообщение в Римской империи\",\n       subtitle = \"Данные проекта Orbis\") +\n  theme_bw(base_family = \"serif\") +\n  theme(legend.position=\"bottom\", \n        legend.box = \"horizontal\",\n        panel.background = element_rect(fill = \"aliceblue\"),\n        text = element_text(color = cols[5])) +\n  scale_color_manual(\"тип\", values = sample(cols, 10))\n\n\n\n\n\n\n\n\nВсе очень красиво, но есть одно но: все границы стран – современные… 🤡.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#leaflet",
    "href": "maps.html#leaflet",
    "title": "21  Пространственные данные в R",
    "section": "21.7 Leaflet",
    "text": "21.7 Leaflet\nУдобный способ создания интерактивных карт предлагает также пакет Leaflet. Вызовем фон:\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n  addTiles() \n\n\n\n\n\nCcылка на галерею подложек (подсмотрена в курсе Георгия Мороза).\n\nlibrary(leaflet)\n\nleaflet() |&gt; \n  addProviderTiles(\"Esri.WorldImagery\") \n\n\n\n\n\nНекоторые подложки потребуют аутентификации. Для этого надо зарегистрироваться на сайте https://stadiamaps.com/ (это бесплатно), создать в личном кабинете Property и прописать доменное имя для карты. Например, акварельная подложка при публикации в Сети требует аутентификации. Многие другие работают без нее (и почти все – локально).\n\n# цветовая палитра\nramphs$type &lt;- factor(ramphs$type)\nfactpal &lt;- colorFactor(palette = c(\"#DE7424FF\", \"#F5CA37FF\", \"#AD8D26FF\", \"#496849FF\", \"#654783FF\"),\n                       ramphs$type)\nramphs |&gt; \n  leaflet() |&gt; \n  addProviderTiles(\"Stadia.StamenWatercolor\") |&gt; \n  addCircles(lng = ~longitude,\n             lat = ~latitude,\n             color = ~factpal(type),\n             opacity = 0.7,\n             popup = ~paste0(\n               label, \n               \"&lt;/br&gt;\", \n               capacity)\n             )  |&gt; \n  addLegend(pal = factpal, \n            values = ~type)\n\n\n\n\n\nЗаменим кружки на маркеры и сгруппируем их. Наложим это все на снимок из космоса (ок, это просто демо, с освещением у них было не очень).\n\nramphs |&gt; \n  leaflet() |&gt; \n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\") |&gt; \n  addMarkers(lng = ~longitude,\n             lat = ~latitude,\n             popup = ~paste0(\n               label, \n               \"&lt;/br&gt;\", \n               capacity),\n             clusterOptions = markerClusterOptions()\n  ) \n\n\n\n\n\nЗаменим маркеры на изображения амфитеатров.\n\nmy_icon &lt;- makeIcon(\n  iconUrl = \"./images/amphitheatre.png\",\n  iconWidth = 31*215/230,\n  iconHeight = 31, \n  iconAnchorY = 16,\n  iconAnchorX = 31*215/230/2\n)\n\nramphs |&gt; \n  leaflet() |&gt; \n  addProviderTiles(\"Esri.WorldTerrain\") |&gt; \n  addMarkers(icon = ~my_icon, \n             clusterOptions = markerClusterOptions())\n\nAssuming \"longitude\" and \"latitude\" are longitude and latitude, respectively",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#анимация",
    "href": "maps.html#анимация",
    "title": "21  Пространственные данные в R",
    "section": "21.12 Анимация",
    "text": "21.12 Анимация",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "iterate.html#векторизованные-вычисления",
    "href": "iterate.html#векторизованные-вычисления",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "На заметку\n\n\n\nВ циклах часто используется буква i. Но никакой особой магии в ней нет, имя переменной можно изменить.\n\n\n\n\n\n\n\n\n\n\n\nОдин из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми.\n— Нормат Мэтлофф",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#семейство-_apply",
    "href": "iterate.html#семейство-_apply",
    "title": "4  Циклы, условия, функции",
    "section": "4.2 Семейство _apply()",
    "text": "4.2 Семейство _apply()\nДля работы со списками циклы тоже чаще всего избыточны. Снова воспользуемся списком печенек из коллекции rcorpora.\n\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\n\ntic()\nfor (i in 1:length(my_list)) print(length(my_list[[i]]))\n\n[1] 1\n[1] 35\n[1] 20\n\ntoc()\n\n0.001 sec elapsed\n\n\nНо в базовом R для таких случаев существуют функционалы lapply() и sapply(). Они принимают на входе список и функцию и применяют функцию к каждому элементу списка. Получается быстрее:\n\ntic()\nlapply(my_list, length)\n\n$description\n[1] 1\n\n$breads\n[1] 35\n\n$pastries\n[1] 20\n\ntoc()\n\n0.001 sec elapsed\n\n\nФункция sapply() упростит результат до вектора (s означает “simplify”):\n\ntic()\nsapply(my_list, length)\n\ndescription      breads    pastries \n          1          35          20 \n\ntoc()\n\n0.001 sec elapsed\n\n\nПоскольку датафрейм – это двумерный аналог списка, то и здесь можно заменить цикл на _apply(). Сравните.\n\ndf &lt;- data.frame(author=c(\"Joe\",\"Jane\"), year=c(1801,1901), reprints=c(TRUE,FALSE))\n\n## цикл \ntic()\nfor (i in seq_along(df)) {\n print(class(df[,i]))\n}\n\n[1] \"character\"\n[1] \"numeric\"\n[1] \"logical\"\n\ntoc()\n\n0.002 sec elapsed\n\n## sapply\ntic()\nsapply(df, class)\n\n     author        year    reprints \n\"character\"   \"numeric\"   \"logical\" \n\ntoc()\n\n0 sec elapsed\n\n\nЕсть еще vapply(), tapply() и mapply(), но и про них мы не будем много говорить, потому что все их с успехом заменяет семейство map_() из пакета purrr в tidyverse.\n\n\n\n\n\n\nЗадание\n\n\n\nПройдите урок 10 lapply and sapply и урок 11 vapply and tapply из курса R Programming в swirl.\n\n\nТем не менее, перед освоением семейства map_() стоит потренироваться работать с обычными циклами, особенно если вам не приходилось иметь с ними дела (например, на Python). Несмотря на все недостатки, цикл for интуитивно понятен и часто проще начинать именно с него.\n\n\n\n\n\n\nЗадание\n\n\n\nПревратите детскую потешку “Ted in the Bed” в функцию. Обобщите до любого числа спящих.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#синтаксис-функций",
    "href": "iterate.html#синтаксис-функций",
    "title": "4  Циклы, условия, функции",
    "section": "4.3 Синтаксис функций",
    "text": "4.3 Синтаксис функций\nФункция и код – не одно и то же. Чтобы стать функцией, кусок кода должен получить имя. Но зачем давать имя коду, который и так работает?\nВот три причины, которые приводит Хадли Уикхем:\n\nу функции есть выразительное имя, которое облегчает понимание кода;\nпри изменении требований необходимо обновлять код только в одном месте, а не во многих;\nменьше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом)\n\n\nWriting good functions is a lifetime journey.\n— Hadley Wickham\n\nМашине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена!\nДалее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть).\nНаписание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию.\nНапишем функцию, которая будет переводить градусы по Фаренгейту в градусы по Цельсию.\n\nfahrenheit_to_celsius &lt;- function(fahrenheit){ \n  celsius = (fahrenheit - 32) / 1.8\n  return(round(celsius))\n}\n\nfahrenheit_to_celsius(451)\n\n[1] 233\n\n\nВнутри нашей функции есть переменная celsius, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции.\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию, которая ищет совпадения в двух символьных векторах и возвращает совпавшие элементы.\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗагрузите библиотеку swirl, выберите курс R Programming и пройдите из него урок 9 Functions.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДля просмотра исходного кода любой функции необходимо…\n\n\n\n\n\n\nвызвать help к функции\n\n\nнабрать имя функции без аргументов и без скобок\n\n\nединственный способ — найти код функции в репозитории на GitHub\n\n\nиспользовать специальную функцию для просмотра кода",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#условия-внутри-функций",
    "href": "iterate.html#условия-внутри-функций",
    "title": "4  Циклы, условия, функции",
    "section": "4.6 Условия внутри функций",
    "text": "4.6 Условия внутри функций\nФункция может принимать произвольное число аргументов. Доработаем наш код:\n\nconvert_temperature &lt;- function(x, mode = \"f_to_c\"){ \n  if(mode == \"f_to_c\") {\n    celsius = round((x - 32) / 1.8)\n    return(paste(celsius, \"градусов по Цельсию\"))\n  } else if (mode == \"c_to_f\") {\n    fahrenheit = round(x * 1.8 + 32)\n    return(paste(fahrenheit, \"градусов по Фаренгейту\"))\n  }\n}\n\nconvert_temperature(84)\n\n[1] \"29 градусов по Цельсию\"\n\nconvert_temperature(29, mode = \"c_to_f\")\n\n[1] \"84 градусов по Фаренгейту\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#сообщения-и-условия-остановки",
    "href": "iterate.html#сообщения-и-условия-остановки",
    "title": "4  Циклы, условия, функции",
    "section": "4.7 Сообщения и условия остановки",
    "text": "4.7 Сообщения и условия остановки\nЧасто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении.\n\nconvert_temperature &lt;- function(x, mode = \"f_to_c\"){\n  if(!is.numeric(x)) stop(\"non-numeric input\")\n  \n  message(\"Please, wait...\")\n  if(mode == \"f_to_c\") {\n    celsius = round((x - 32) / 1.8)\n    return(paste(celsius, \"градусов по Цельсию\"))\n  } else if (mode == \"c_to_f\") {\n    fahrenheit = round(x * 1.8 + 32)\n    return(paste(fahrenheit, \"градусов по Фаренгейту\"))\n  }\n}\n\nconvert_temperature(\"двадцать пять\")\n\nError in convert_temperature(\"двадцать пять\"): non-numeric input\n\nconvert_temperature(78)\n\nPlease, wait...\n\n\n[1] \"26 градусов по Цельсию\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#switch",
    "href": "iterate.html#switch",
    "title": "4  Циклы, условия, функции",
    "section": "4.8 switch()",
    "text": "4.8 switch()\nСлишком много условий в теле функции могут сделать ее нечитаемой. Для таких случаев подойдет switch().\n\nconvert_temperature &lt;- function(x, mode = \"f_to_c\"){\n  if(!is.numeric(x)) stop(\"wrong input\")\n  \n  switch(mode,\n         f_to_c = round((x - 32) / 1.8) |&gt; \n           paste(\"градусов по Цельсию\"),\n         c_to_f = round(x * 1.8 + 32) |&gt; \n           paste(\"градусов по Фаренгейту\"),\n         stop(\"unknown mode\")\n  )\n}\n\nconvert_temperature(78, mode = \"c_to_k\")\n\nError in convert_temperature(78, mode = \"c_to_k\"): unknown mode\n\nconvert_temperature(78, mode = \"f_to_c\")\n\n[1] \"26 градусов по Цельсию\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#map",
    "href": "iterate.html#map",
    "title": "4  Циклы, условия, функции",
    "section": "4.10 map()",
    "text": "4.10 map()\nВоспользуемся возможностями purrr, чтобы исследовать датасет starwars из пакета dplyr. Для начала узнаем число отсутствующих значений в каждом столбце. Косая черта (\\) указывает на то, что мы используем анонимную функцию\n\nlibrary(tidyverse)\nstarwars &lt;- starwars\nmap_int(starwars, \\(x) sum(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year \n         0          6         28          5          0          0         44 \n       sex     gender  homeworld    species      films   vehicles  starships \n         4          4         10          4          0          0          0 \n\n\nОбратите внимание, что map_int, как и map_dbl возвращает именованный вектор. Чтобы избавиться от имен, можно использовать unname().\n\nИспользуйте map_int и n_distinct, чтобы узнать число уникальных наблюдений в каждом столбце.\n\nЕсли функция принимает дополнительные аргументы, их можно задать после названия функции. В таком случае для каждого вызова функции будет использовано это значение аргумента. В примере ниже это аргумент na.rm.\n\nstarwars |&gt; \n  # выбираем все столбцы, где хранятся числовые значения\n  select_if(is.numeric) |&gt; \n  map(mean, na.rm = TRUE)\n\n$height\n[1] 174.6049\n\n$mass\n[1] 97.31186\n\n$birth_year\n[1] 87.56512\n\n\nПри вызове map_df есть дополнительная возможность сохранить названия столбцов, используя аргумент .id:\n\nstarwars |&gt; \n  map_df(~data.frame(unique_values = n_distinct(.x),\n                     col_class = class(.x)),\n         .id = \"variable\"\n         )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#map2",
    "href": "iterate.html#map2",
    "title": "4  Циклы, условия, функции",
    "section": "4.11 map2()",
    "text": "4.11 map2()\nЕсли необходимо несколько раз вызывать одну и ту же функцию с двумя аргументами, используется функция map2().\n\nvar1 &lt;- seq(10, 50, 10)\nvar2 &lt;- seq(1, 5, 1)\n\n# формула\nmap2(var1, var2, ~.x+.y)\n\n[[1]]\n[1] 11\n\n[[2]]\n[1] 22\n\n[[3]]\n[1] 33\n\n[[4]]\n[1] 44\n\n[[5]]\n[1] 55\n\n\nАргументы, которые меняются при каждом вызове, пишутся до функции или формулы; аргументы, которые остаются неизменны, – после. Это можно представить так (источник):\n\nВо всех случаеях, когда у функции больше двух аргументов, используется pmap().\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Advanced R Programming\") и пройдите из него урок 3 Functional Programming with purrr.\n\n\nНесколько вопросов для самопроверки.\n\n\n\n\n\n\nВопрос\n\n\n\nФункции-предикаты (predicate functions) возвращают TRUE или FALSE. Выберите из списка все функции-предикаты.\n\n\n\n\nevery()\n\n\nsome()\n\n\nnone()\n\n\nhas_element()\n\n\nis.factor()\n\n\nkeep()\n\n\ndiscard()\n\n\nis.numeric()\n\n\ndetect()\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие из функций ниже принимают в качестве аргумента функции-предикаты?\n\n\n\n\nevery()\n\n\nsome()\n\n\nnone()\n\n\nhas_element()\n\n\nis.factor()\n\n\nkeep()\n\n\ndiscard()\n\n\nis.numeric()\n\n\ndetect()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#функционалы-в-анализе-данных",
    "href": "iterate.html#функционалы-в-анализе-данных",
    "title": "4  Циклы, условия, функции",
    "section": "4.12 Функционалы в анализе данных",
    "text": "4.12 Функционалы в анализе данных\nДатасет “Гарри Поттер” представляет собой набор файлов .csv, содержащих метаданные о ресурсах из коллекций Британской библиотеки, связанных с Гарри Поттером, . Первоначально он был выпущен к 20-летию публикации книги «Гарри Поттер и философский камень» 26 июня 2017 года и с тех пор ежегодно обновлялся. Всего в датасете пять файлов, каждый из которых содержит разное представление данных.\nДатасет до 2023 г. был доступен на сайте Британской библиотеки (https://www.bl.uk/); в репозитории курса сохранена его копия. Скачаем архив.\n\nmy_url &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/HP.zip\"\ndownload.file(url = my_url, destfile = \"../files/HP.zip\")\n\nПосле этого переходим в директорию с архивом и распаковываем его.\n\nunzip(\"../files/HP.zip\")\n\nСохраним список всех файлов с расширением .csv, используя подходящую функцию из base R.\n\nmy_files &lt;- list.files(\"../files/HP\", pattern = \".csv\", full.names = TRUE)\nmy_files\n\n[1] \"../files/HP/classification.csv\" \"../files/HP/names.csv\"         \n[3] \"../files/HP/records.csv\"        \"../files/HP/titles.csv\"        \n[5] \"../files/HP/topics.csv\"        \n\n\nТеперь задействуем функционалы.\n\nФункционалы – это функции, которые используют в качестве аргументов другие функции.\n\nДля того, чтобы прочесть все файлы одним вызовом функции, используем map(). В качестве аргументов передаем список файлов, функцию read_csv() и аргумент этой функции col_types.\n\n# чтение файлов \nHP &lt;- map(my_files, read_csv, col_types = cols())\n\nОбъект HP – это список. В нем пять элементов, так как на входе у нас было пять файлов. Для удобства назначаем имена элементам списка.\n\nmy_files_short &lt;- list.files(\"../files/HP\", pattern = \".csv\")\nnames(HP) &lt;- my_files_short\n\n\nПопробуем выяснить, какие столбцы есть во всех пяти таблицах. Для этого подойдет функция reduce() из того же purrr. Она принимает на входе вектор (или список) и функцию и применяет функцию последовательно к каждой паре значений.\n\n\n\nИсточник.\n\n\n\n\nHP |&gt; \n  map(colnames) |&gt; \n  # это тоже функционал\n  reduce(intersect)\n\n [1] \"Dewey classification\"       \"BL record ID\"              \n [3] \"Type of resource\"           \"Content type\"              \n [5] \"Material type\"              \"BNB number\"                \n [7] \"ISBN\"                       \"ISSN\"                      \n [9] \"Name\"                       \"Dates associated with name\"\n[11] \"Type of name\"               \"Role\"                      \n[13] \"Title\"                      \"Series title\"              \n[15] \"Number within series\"       \"Country of publication\"    \n[17] \"Place of publication\"       \"Publisher\"                 \n[19] \"Date of publication\"        \"Edition\"                   \n[21] \"Physical description\"       \"BL shelfmark\"              \n[23] \"Genre\"                      \"Languages\"                 \n[25] \"Notes\"                     \n\n\nЕще одна неочевидная возможность функции reduce - объединение нескольких таблиц в одну одним вызовом. Например, так:\n\nHP_joined &lt;- HP |&gt; \n  reduce(left_join)\n\nHP_joined\n\n\n  \n\n\n\nО других возможностях пакета purrr мы поговорим в следующем уроке, а пока почистим данные и построить несколько разведывательных графиков.\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(Languages, into = c(\"language\", NA), sep = \";\") |&gt;\n  mutate(language = str_squish(language)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(language)) |&gt; \n  group_by(year, language) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = language)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nТакже построим облако слов. Для этого заберем первое слово в каждом ряду из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./images/hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#разведывательные-графики",
    "href": "iterate.html#разведывательные-графики",
    "title": "4  Циклы, условия, функции",
    "section": "4.13 Разведывательные графики",
    "text": "4.13 Разведывательные графики\nТеперь можно почистить данные и построить несколько разведывательных графиков.\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(Languages, into = c(\"language\", NA), sep = \";\") |&gt;\n  mutate(language = str_squish(language)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(language)) |&gt; \n  group_by(year, language) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = language)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nВ качестве небольшого бонуса к этому уроку построим облако слов. Для этого заберем первое слово в каждом ряду из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./images/hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#пакет-purrr",
    "href": "iterate.html#пакет-purrr",
    "title": "4  Циклы, условия, функции",
    "section": "4.9 Пакет purrr",
    "text": "4.9 Пакет purrr\nПо-настоящему мощный инструмент для итераций – это пакет purrr из семейства tidyverse. Разработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham и Grolemund 2016).\n\nYou should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work.\n— Hadley Wickham & Garrett Grolemund\n\nВ семействе функций map_ из этого пакета всего 23 вариации. Вот основные из них:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nВсе они принимают на входе данные и функцию (или формулу), которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_int() – целочисленный вектор, и т.д.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#ленивые-вычисления",
    "href": "iterate.html#ленивые-вычисления",
    "title": "4  Циклы, условия, функции",
    "section": "4.4 Ленивые вычисления",
    "text": "4.4 Ленивые вычисления\nВычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет.\n\nfahrenheit_to_celsius &lt;- function(fahrenheit, your_name = \"locusclassicus\"){ \n  celsius = (fahrenheit - 32) / 1.8\n  return(round(celsius))\n}\n\nfahrenheit_to_celsius(451)\n\n[1] 233\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию awesome_plot, которая будет принимать в качестве аргументов два вектора, трансформировать их в тиббл и строить диаграмму рассеяния при помощи ggplot(). Задайте цвет и прозрачность точек.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "import.html#пакет-jsonlite",
    "href": "import.html#пакет-jsonlite",
    "title": "5  Импорт",
    "section": "5.2 Пакет jsonlite",
    "text": "5.2 Пакет jsonlite\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\n\nlibrary(jsonlite)\n\npath &lt;- \"../files/TBBT.json\"\ntbbt &lt;- read_json(path)\n\nФункция read_json() вернула нам список со следующими элементами:\n\nsummary(tbbt)\n\n                          Length Class  Mode     \nname                        1    -none- character\nseason_count                1    -none- character\nepisodes_count_total        1    -none- character\nepisodes_count_per_season  12    -none- list     \ncasting                    11    -none- list     \nepisode_list              280    -none- list     \nreferences                  1    -none- list",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#от-списка-к-таблице",
    "href": "import.html#от-списка-к-таблице",
    "title": "5  Импорт",
    "section": "5.3 От списка к таблице",
    "text": "5.3 От списка к таблице\nВыборочно преобразуем список в тиббл. Функция transpose() берет список списков и выворачивает его наизнанку: вместо списка, в котором для каждого из персонажей указан актер и первое появление, мы получаем три списка: с персонажами, актерами и эпизодами. На месте отсутствующих значений ставится NULL.\n\nlibrary(tidyverse)\n\ncast_tbl &lt;- tbbt$casting |&gt; \n  transpose() |&gt; \n  map(as.character) |&gt; \n  as_tibble()\n\ncast_tbl\n\n\n  \n\n\n\nПроделаем то же самое для списка эпизодов, но другим способом. Функция pluck() представляет собой аналог [[, который можно использовать в пайпе. Она позволяет эффективно индексировать многоуровневые списки. Поскольку списков много, мы используем ее в сочетании с map_chr().\n\nepisodes_tbl &lt;- tibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\"))\n\nepisodes_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСамостоятельно создайте тиббл, в котором будет храниться количество серий для каждого сезона.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "maps.html#данные-римские-амфитеатры",
    "href": "maps.html#данные-римские-амфитеатры",
    "title": "21  Пространственные данные в R",
    "section": "",
    "text": "Digital Atlas of the Roman Empire;\nAncient World Mapping Center;\nГеопространственная сетевая модель Римской империи Orbis.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#simple-features",
    "href": "maps.html#simple-features",
    "title": "21  Пространственные данные в R",
    "section": "21.2 Simple Features",
    "text": "21.2 Simple Features\nСовременный формат хранения векторных геоданных называется Simple Features. Основное отличие объектов sf от объектов sp в том, что данные хранятся в виде датафрейма со списком-колонкой для хранения геометрии (линии, точки или полигона). Эта колонка называется sfc (simple features geometry column), а сама геометрия внутри нее – sfg (simple feature geometry).\n\nТо, что объекты типа Simple Features реализованы в виде самых обычных фреймов данных, означает, что любая операция, применимая к фрейму данных, будет также применима к объекту типа sf. Это очень важная особенность объектов типа sf, которой сильно не хватало в экосистеме исторического пакета sp. – Источник.\n\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nroman_map &lt;- sf::st_as_sf(awmc.roman.empire.200.sp)\nroman_map\n\n\n  \n\n\n# Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n# Simple feature collection with 112 features and 8 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -9.48732 ymin: 22.89549 xmax: 43.10774 ymax: 55.10117\n# Geodetic CRS:  +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0\n# First 10 features:\n#   OBJECTID         AREA  PERIMETER NEWDIO_ NEWDIO_ID ID Shape_Leng\n# 0        1 19.612702708 35.3870861       2         0  0 35.1149460\n# 1        2  0.080670307  1.2122280       3         0  0  1.2122280\n\nПосмотрим внимательно на это описание.\n\nbounding box: прямоугольная рамка, которая задает границы карты; здесь координаты по оси x соответствуют долготе, а по оси y - широте. Будьте внимательны, потому что мы пишем обычно сначала широту, а потом долготу. Но по аналогии с алгеброй x определяет сдвиг вправо-влево (долготу), в то время как y - вверх-вниз (широту). Получается, что перед нами кусочек северного полушария, в основном к востоку от нулевого меридиана.\n\n\nlibrary(tmaptools)\nbb(roman_map)\n\n    xmin     ymin     xmax     ymax \n-9.48732 22.89549 43.10774 55.10117 \n\n\nGeodetic CRS:  +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 – это определение геодезической системы координат (Geodetic CRS):\n\n+proj=longlat - указывает, что используется географическая система координат (широта и долгота).\n+datum=WGS84 - определяет геодезическую основу, в данном случае это Всемирная геодезическая система 1984 года (World Geodetic System 1984).\n+ellps=WGS84 - указывает, что используется эллипсоид, соответствующий системе WGS84.\n+towgs84=0,0,0 - определяет параметры трансформации между используемым эллипсоидом и эллипсоидом системы WGS84. Значения “0,0,0” означают, что никаких трансформаций не требуется, так как данные уже находятся в системе WGS84.\n\nПомимо географической системы координат, которые используют сферическую или эллипсоидальную поверхность Земли, бывают проекционные (плоские) системы, которые используют плоскую (двумерную) поверхность. Кроме того, они используют другие единицы измерения: не градусы широты и долготы, а линейные единицы (например, метры).\nФункция st_is_valid() проверяет, является ли заданная пространственная геометрия (например, точка, линия, многоугольник) топологически корректной. В нашем случае есть одна ошибка.\n\nst_is_valid(roman_map)\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE\n\n\nНадо починить, иначе дальше будет ошибка.\n\nroman_map &lt;- st_make_valid(roman_map)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#tm_bubbles",
    "href": "maps.html#tm_bubbles",
    "title": "21  Пространственные данные в R",
    "section": "21.4 tm_bubbles()",
    "text": "21.4 tm_bubbles()\nТепрь нанесем на карту отдельные амфитеатры в виде точек. Для этого преобразуем датафрейм в объект sf. Обратите внимание, что геометрии здесь другие (точки).\n\namph_points &lt;- ramphs |&gt; \n   st_as_sf(coords = c(\"longitude\", \"latitude\"))\n\namph_points\n\n\n  \n\n\n\n\ntmap_style(\"classic\")\n\nstyle set to \"classic\"\n\n\nother available styles are: \"white\" (tmap default), \"gray\", \"natural\", \"cobalt\", \"albatross\", \"beaver\", \"bw\", \"watercolor\"\n\n\ntmap v3 styles: \"v3\" (tmap v3 default), \"gray_v3\", \"natural_v3\", \"cobalt_v3\", \"albatross_v3\", \"beaver_v3\", \"bw_v3\", \"classic_v3\", \"watercolor_v3\"\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(\n    type = \"8star\",\n    position = c(\"right\", \"top\")\n  ) +\n  tm_scalebar(position = c(\"left\", \"bottom\")) +\n  tm_shape(amph_points) +\n  tm_bubbles(\n    size = \"capacity\", \n    size.scale = tm_scale_continuous(values.scale = 1),  \n    fill = \"prov.type\", \n    fill.scale = tm_scale(values = c(\"red\", \"blue\", \"green\")),\n    fill_alpha = 0.8  \n  ) +\n  tm_layout(\n    legend.position = c(\"right\", \"bottom\"),\n    legend.frame = TRUE\n  )\n\nScale bar set for latitude km and will be different at the top and bottom of the map.\n\n\n\n\n\n\n\n\n\nКак и в ggplot, разные геометрии могут использовать разные данные. В нашем случае – отфильтрованный список названий.\n\ntmap_mode(\"plot\")  # Убедимся, что мы в режиме рисования\n\nℹ tmap mode set to \"plot\".\n\ntm_shape(roman_map) +\n  tm_polygons() +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\")) +\n  tm_scalebar(breaks = c(0, 100, 200), text.size = 1, position = c(\"left\", \"bottom\")) +\n\n  # Слой с пузырями\n  tm_shape(amph_points) +\n  tm_bubbles(\n    size = \"capacity\",\n    size.scale = tm_scale_continuous(values = c(0.5, 2)), \n    fill = \"prov.type\", \n    fill.scale = tm_scale(values = c(\"red\", \"blue\", \"green\")),\n    fill_alpha = 0.8\n  ) +\n\n  # Слой с подписями для амфитеатров с capacity &gt; 30000\n  tm_shape(amph_points |&gt; filter(capacity &gt; 30000)) +\n  tm_text(\n    text = \"label\",\n    options = opt_tm_text(point.label = TRUE)\n    ) +\n\n  # Настройка легенды\n  tm_layout(\n    legend.position = c(\"right\", \"bottom\"),\n    legend.frame = TRUE\n  )\n\nScale bar set for latitude km and will be different at the top and bottom of the map.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#пересечения-между-геометриями",
    "href": "maps.html#пересечения-между-геометриями",
    "title": "21  Пространственные данные в R",
    "section": "21.5 Пересечения между геометриями",
    "text": "21.5 Пересечения между геометриями\nМы можем посчитать, сколько точек приходится на один многогранник. Но для этого координатные системы должны совпадать. Сейчас у точек нет никакой CRS, в чем легко убедиться.\n\nst_crs(amph_points)\n\nCoordinate Reference System: NA\n\n\n\nst_intersects(roman_map, amph_points)\n\nError in st_geos_binop(\"intersects\", x, y, sparse = sparse, prepared = prepared, : st_crs(x) == st_crs(y) is not TRUE\n\n\nЧтобы избавиться от ошибки, необходимо назначить или трансформировать координатные системы. Четыре цифры ниже представляют собой код EPSG (European Petroleum Survey Group). Это один из способов задания (хранения) пространственной привязки. EPSG:4326 соответствует WGS84, а Web Mercator – EPSG:3857.\n\n# если нужно трансформировать\nroman_map &lt;- st_transform(roman_map, 4326)\n\n# если нужно назначить\namph_points &lt;- st_set_crs(amph_points, 4326)\n\nСнова уточним пересечения.\n\n# пересечения\ninter &lt;- st_intersects(roman_map, amph_points)\n\ninter\n\nSparse geometry binary predicate list of length 112, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: 45, 46, 69, 78, 80, 88, 108, 109, 110, 151, ...\n 2: (empty)\n 3: (empty)\n 4: (empty)\n 5: (empty)\n 6: (empty)\n 7: 3, 21, 38, 39, 47, 74, 89, 114, 115, 124, ...\n 8: (empty)\n 9: (empty)\n 10: (empty)\n\n\nДобавим новый столбец в датафрейм с картой.\n\nroman_map$count &lt;- lengths(inter)\n\nТеперь его можно использовать для выбора цвета заливки.\n\ntm_shape(roman_map) +\n  tm_polygons(fill = \"count\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            legend.frame = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#tm_basemap",
    "href": "maps.html#tm_basemap",
    "title": "21  Пространственные данные в R",
    "section": "21.6 tm_basemap()",
    "text": "21.6 tm_basemap()\nПарящая в вакууме империя не очень радует глаз; в таком случае стоит добавить растровое изображение ландшафта. Пока это доступно только для динамической карты, поэтому переключаемся в режим “view” (ниже представлен скриншот).\nБудьте внимательны, совмещая исторические карты с современными! Убедитесь, что вы не показываете походы Цезаря в современную Швейцарию, как это произошло, например, здесь.\n\n# Устанавливаем режим просмотра (интерактивная карта)\ntmap_mode(\"view\")\n\nℹ tmap mode set to \"view\".\n\n# Строим интерактивную карту\ntm_shape(amph_points) +\n  tm_basemap(\"Stadia.StamenTerrainBackground\") +\n  tm_symbols(\n    size = \"capacity\",     \n    fill = \"white\",\n    col = \"steelblue\", \n    fill_alpha = 0.8\n  )",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#возможности-ggplot2",
    "href": "maps.html#возможности-ggplot2",
    "title": "21  Пространственные данные в R",
    "section": "21.8 Возможности ggplot2",
    "text": "21.8 Возможности ggplot2\nДля статичных карт можно использовать привычный ggplot(), как показано, например, здесь.\n\nggplot() +\n  geom_sf(data = roman_map) +\n  geom_point(ramphs, \n             mapping = aes(longitude, latitude),\n             color = \"steelblue\", \n             alpha = 0.5)  +\n  theme_bw()\n\n\n\n\n\n\n\n\nЕсли точек много, то может быть уместней представить на карте плотность их распределения.\n\nggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_point(ramphs, color = \"steelblue\", alpha = 0.5,\n             mapping = aes(longitude, latitude)) +\n  geom_density2d(data = ramphs, \n                 mapping = aes(longitude, latitude, \n                               color = after_stat(level)),\n                 linewidth = 1, alpha = 0.5)\n\n\n\n\n\n\n\n\nЕще один способ отразить области скопления точек – сотовая диаграмма. На такой диаграмме координатная плоскость разбивается на гексагоны, которые закрашиваются в соответствии с градиентом плотности попавших в них точек.\n\ng &lt;- ggplot() +\n  geom_sf(data = roman_map, fill = \"wheat\") +\n  geom_hex(data = ramphs,\n                 mapping = aes(longitude, latitude),\n           bins = 25,\n           color = \"royalblue\")  +\n  theme_bw() +\n  scale_fill_continuous(trans = \"reverse\") \n\ng",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#пакет-plotly",
    "href": "maps.html#пакет-plotly",
    "title": "21  Пространственные данные в R",
    "section": "21.9 Пакет plotly",
    "text": "21.9 Пакет plotly\nПакет plotly позволяет добавить интерактивности на карту.\n\nlibrary(plotly)\nggplotly(g)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "maps.html#сеть-на-карте",
    "href": "maps.html#сеть-на-карте",
    "title": "21  Пространственные данные в R",
    "section": "21.11 Сеть на карте",
    "text": "21.11 Сеть на карте\nКооринаты узлов – это широта и долгота (главное не перепутать).\n\norbis_coord &lt;- orbis_e |&gt; \n  left_join(orbis_n, by = join_by(source == id)) |&gt; \n  mutate(source = label, .before = target) |&gt; \n  select(-label) |&gt; \n  # координаты начала ребра\n  rename(x1 = y, y1 = x) |&gt; \n  left_join(orbis_n, by = join_by(target == id)) |&gt; \n  mutate(target = label, .after = source) |&gt; \n  select(-label) |&gt; \n  # координаты конца ребра\n  rename(x2 = y, y2 = x) |&gt;\n  # отрезаем пуповины к центру мира\n  filter(x1 != 0, y1 !=0, x2 != 0, y2 != 0)\n\n\n# для простоты пока берем современную карту\nworld &lt;- map_data(\"world\") \n\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"wheat\", color = \"grey\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = \"steelblue\", alpha = 0.5) +\n  coord_map(xlim = c(-10, 50),\n            ylim = c(23, 54)) +\n  geom_segment(data = orbis_coord, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type))\n\n\n\n\n\n\n\n\nПараллельные линии создают шум в нижней правой четверти; есть несколько способов от этого избавиться, но мы пока просто отрежем часть городов (восточнее Берениса).\n\norbis_coord_pruned &lt;-  orbis_coord |&gt; \n  filter(y1 &gt; 28 & y2 &gt; 28)\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"basetheme::brutal\")\n\n\n\npar(mar = rep(0,4))\nset.seed(24092024)\nggplot(data = world, aes(long, lat)) +\n  geom_map(map = world, aes(map_id = region),\n           fill = \"white\", color = \"wheat\") +\n  geom_point(data = orbis_coord, aes(x = x1, y = y1), \n             color = cols[1], alpha = 0.5) +\n  geom_segment(data = orbis_coord_pruned, \n               aes(x = x1, y = y1, xend = x2, yend = y2,\n                   color = type)) +\n  geom_label(data = orbis_coord |&gt; \n               filter(source %in% c(\"Roma\", \"Alexandria\", \"Carthago\", \"Sirmium\", \"Corinthus\", \"Antiochia\", \"Londinium\", \"Tarraco\", \"Augusta Taurinorum\", \"Jerusalem\")),\n             aes(x1, y1, label = source),\n             color = cols[5], \n             label.size = 0.15,\n             fontface = \"bold\") +\n  coord_map(xlim = c(-10, 45),\n            ylim = c(26, 54)) +\n  labs(x = NULL, y = NULL, \n       title = \"Транспортное сообщение в Римской империи\",\n       subtitle = \"Данные проекта Orbis\") +\n  theme_bw(base_family = \"serif\") +\n  theme(legend.position=\"bottom\", \n        legend.box = \"horizontal\",\n        panel.background = element_rect(fill = \"aliceblue\"),\n        text = element_text(color = cols[5])) +\n  scale_color_manual(\"тип\", values = sample(cols, 10))",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Пространственные данные в R</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "22  Регрессионный анализ",
    "section": "",
    "text": "22.1 Данные: Оксфордская керамика\nДанные для этого урока основаны на нескольких публикациях Яна Ходдера, который проанализировал пространственное распределение поздней романо-британской керамики, произведенной в Оксфорде, в статьях 1974 г. “The Distribution of Two Types of Romano-British coarse pottery” и “A Regression Analysis of Some Trade and Marketing Patterns”. Датасет доступен в пакете archdata, содержащем и другие наборы данных для археологов.\nlibrary(archdata)\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(equatiomatic)\nlibrary(broom)\nlibrary(gt)\ndata(\"OxfordPots\")\nOxfordPots\nДатафрейм содержит 30 наблюдений по следующим 7 переменным:",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#простая-линейная-регрессия",
    "href": "regression.html#простая-линейная-регрессия",
    "title": "22  Регрессионный анализ",
    "section": "22.2 Простая линейная регрессия",
    "text": "22.2 Простая линейная регрессия\nПростая линейная регрессия – метод, который позволяет предсказывать количественный отклик переменной y на основе единственной независимой переменной x. Случайная величина, которая используется для целей предсказания, называется предиктором. Величина, значения которой предсказываются, называется переменной отклика. Здесь мы освоим лишь самые азы, подробнее стоит посмотреть соответствующие уроки курса “Introduction to Modern Statistics (2e)”.\n\n22.2.1 Линейная функция\nЧтобы разобраться с регрессией, надо вспомнить, что такое линейная зависимость:\n\\[y\\approx\\beta_o + \\beta_1x\\]\nВ этом уравнении \\(\\beta_o\\) и \\(\\beta_1\\) - это константы, известные как свободный член и угол наклона линейной модели. Совокупно их называют коэффициентами, или параметрами, модели. Геометрически первый из них определяет точку пересечения оси y (intercept), а второй – угол наклона (slope).\nПосмотрите внимательно на линии на примере и подумайте, чем они отличаются.\n\n\n\n22.2.2 Ошибка прогноза\nНа практике линейная зависимость в чистом виде почти не встречается: всегда есть небольшая ошибка прогноза (\\(\\epsilon\\)):\n\\[y\\approx\\beta_o + \\beta_1x + \\epsilon\\]\n\n\n\n\n\n\n\n\n\nПри создании регрессионной модели наша задача заключается в том, чтобы на основе доступных наблюдений подобрать коэффициенты \\(\\beta_0\\) и \\(\\beta_1\\) таким образом, чтобы минимизировать ошибку.\n\\[\\sum(y_i- \\hat y)^2 = \\sum\\epsilon^2\\]\nЧтобы подчеркнуть, что речь идет лишь об оценке, над бетой ставится “крышечка”:\n\\[\\hat y \\approx \\hat\\beta_o + \\hat\\beta_1x\\]\n\n\n22.2.3 Простая регрессия с lm()\nПосмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ggthemes::wsj_rgby\")\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nЧем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон.\nИ наклон, и точку пересечения с осью y определяет функция lm.\n\nfit &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nparameters(fit)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#данные-оксфордская-керамика",
    "href": "regression.html#данные-оксфордская-керамика",
    "title": "22  Регрессионный анализ",
    "section": "",
    "text": "место;\nпроцент оксфордской керамики;\nрасстояние до Оксфорда в милях;\nпроцент гончарных изделий из Нью-Фореста;\nрасстояние до Нью-Фореста;\nплощадь обнесенного стеной города;\nналичие водного транспортного сообщения.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#метод-наименьших-квадратов",
    "href": "regression.html#метод-наименьших-квадратов",
    "title": "22  Простая линейная регрессия",
    "section": "22.3 Метод наименьших квадратов",
    "text": "22.3 Метод наименьших квадратов\nПосмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ltc::trio2\")\n\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[3], se = FALSE) +\n  geom_point(color = cols[1], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nЧем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон. И наклон, и точка пересечения с осью y определяет функция lm, и мы можем их узнать.\n\nfit &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nsummary(fit)\n\n\nCall:\nlm(formula = OxfordPct ~ OxfordDst, data = OxfordPots)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.388  -4.003  -1.005   3.844  10.757 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20.87665    2.23356   9.347 4.18e-10 ***\nOxfordDst   -0.12290    0.02989  -4.112 0.000311 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.601 on 28 degrees of freedom\nMultiple R-squared:  0.3765,    Adjusted R-squared:  0.3542 \nF-statistic: 16.91 on 1 and 28 DF,  p-value: 0.0003111",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#коэффициенты-модели",
    "href": "regression.html#коэффициенты-модели",
    "title": "22  Простая линейная регрессия",
    "section": "22.5 Коэффициенты модели",
    "text": "22.5 Коэффициенты модели\nФункция summary() возвращает много всего, но пока нас интересуют только коэффициенты. Их можно достать из подогнанной модели так:\n\ncoefficients(fit)\n\n(Intercept)   OxfordDst \n 20.8766508  -0.1229049 \n\n\nЭто значит, что наши данные описываются функцией (где \\(\\epsilon\\) - это ошибка):\n\\[OxfordPct = 20.88 - 0.12 \\times OxfordDst + \\epsilon\\] Интуитивно понятно, что коэффициент \\(\\beta_1\\) связан с ковариацией (мерой совместной изменчивости двух величин). Действительно, он рассчитывается по формуле:\n\\[\\beta_1=\\frac{Cov(x,y)}{Var(x)}\\] Проверяем.\n\nx &lt;- OxfordPots$OxfordDst\ny &lt;- OxfordPots$OxfordPct\n\nbeta_1&lt;- cov(x, y) / var(x)\nbeta_1\n\n[1] -0.1229049\n\n\nЗная \\(\\beta_1\\), можно вычислить \\(\\beta_0\\) по формуле:\n\\[\\beta_0=\\bar y - \\beta_1 \\bar x\\]\nСнова проверим.\n\nbeta_0 = mean(y) - beta_1 * mean(x)\nbeta_0\n\n[1] 20.87665",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#стандартные-ошибки-коэффициентов",
    "href": "regression.html#стандартные-ошибки-коэффициентов",
    "title": "22  Простая линейная регрессия",
    "section": "22.6 Стандартные ошибки коэффициентов",
    "text": "22.6 Стандартные ошибки коэффициентов\nДля обоих коэффициентов приведена стандартная ошибка и t-статистика.\n\nsummary(fit)$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 20.8766508 2.23355676  9.346819 4.184111e-10\nOxfordDst   -0.1229049 0.02989016 -4.111887 3.110772e-04\n\n\nСтандартная ошибка для \\(\\beta_0\\) рассчитывается по формуле:\n\\[SE(\\beta_0)=\\sqrt{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}} \\times \\sqrt{\\frac{1}{n}+\\frac{\\bar x^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\]\nПервый множитель в этой формуле – это дисперсия остатков модели. Чем она больше, тем больше неопределенность. На второй множитель влияет как размер выборки, так и разброс независимой переменной x: чем больше размер выборки n, тем меньше \\(\\frac{1}{n}\\) и чем больше \\(Σ(x - \\bar x)^2\\), тем меньше весь множитель. Посчитаем вручую и сравним с результатом, который возвращает команда summary(fit).\n\nx_bar &lt;- mean(x)\n\nmult1 &lt;- sqrt(sum(fit$residuals^2) / 28)\nmult2 &lt;- sqrt(1/30 + ( x_bar^2 / sum((x - x_bar)^2)))\n\nmult1 * mult2\n\n[1] 2.233557\n\n\nСтандартная ошибка для \\(\\beta_1\\) рассчитывается по формуле:\n\\[SE(b_1)=\\sqrt{\\frac{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\] Большая дисперсия остатков (в числителе) будет приводить к увеличению ошибки, а размах \\(x_i\\) – к уменьшению; интуитивно это объясняется тем, что в таком случае у нас больше информации для оценивания угла наклона. Снова перепроверим.\n\nmult1 / sqrt(sum((x - x_bar)^2))\n\n[1] 0.02989016\n\n\nФункция geom_smooth добавляет стандартную ошибку коэффициента наклона на график в виде серой полосы, которая означает, что с вероятностью 95% (значение по умолчанию, которое можно поменять) истинное значение отклика находится в этой зоне (predicted ± 1.96 * se). В статистике это называется доверительный интервал.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], \n              se = TRUE, level = 0.95) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#невязки",
    "href": "regression.html#невязки",
    "title": "22  Простая линейная регрессия",
    "section": "22.8 Невязки",
    "text": "22.8 Невязки\nКак правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые невязки) должны быть минимальны.\nНевязка – это разница между прогнозируемым и фактическим значениями отклика: \\((y_i- \\hat y)\\). На графике ниже невязки обозначены пунктиром.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  geom_segment(aes(xend = OxfordDst,\n                   yend = predict(fit)), \n               linetype = 2, \n               color = cols[1]) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nМы можем убедиться в том, что невязки (fit$residuals) представляют собой разницу между фактическим (OxfordPots$OxfordPct) и предсказанным значением (fit$fitted.value). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями.\n\nall.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)\n\n[1] TRUE\n\n\nЕсли модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.\n\nmean(fit$residuals)\n\n[1] 3.256654e-16\n\ncov(fit$residuals, OxfordPots$OxfordDst)\n\n[1] -3.920236e-15\n\n\nКроме того, полезно проверить остатки на нормальность. Это можно сделать при помощи специального теста или визуально.\n\nshapiro.test(residuals(fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.97648, p-value = 0.7262\n\n\nВысокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально.\nТакже проведем три визуальных теста.\n\nlibrary(gridExtra)\n\ng1 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  mutate(residuals_st = scale(residuals)) |&gt; \n  ggplot(aes(sample = residuals_st)) +\n  stat_qq(color = cols[3], \n          size = 2, alpha = 0.8) + \n  stat_qq_line(color = cols[2]) +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\ng2 &lt;- tibble(residuals = residuals(fit),\n       fitted = predict(fit)) |&gt; \n  ggplot(aes(fitted, residuals)) +\n  geom_point(color = cols[3], \n             size = 2, alpha = 0.8) + \n  theme_minimal()\n\ng3 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  ggplot(aes(residuals)) +\n  geom_histogram(fill = cols[3], color = \"white\") + \n  theme_minimal()\n\ngrid.arrange(g1, g2, g3, nrow = 1)\n\n\n\n\n\n\n\n\nВидно, что самые низкие значения остатков немного выбиваются из общей картины. Отрицательные остатки означают, что соответствующие значения \\(\\hat y\\) завышены. К этому мы еще вернемся.\nВы можете также использовать базовую plot(), передав ей подогнанную модель в качестве аргумента.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №1 “Introduction”, №2 “Residuals”, №3 “Least Squares Estimation”, №4 “Residual Variation”. Один из этих уроков потребует установить пакет manipulate, позволяющий взаимодействовать с графиком в интерактивном режиме.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#оценка-модели",
    "href": "regression.html#оценка-модели",
    "title": "22  Регрессионный анализ",
    "section": "22.4 Оценка модели",
    "text": "22.4 Оценка модели\nОбщая оценка модели проводится при помощи функции performace() из пакета {easystats} или базовой summary().\n\nperformance(fit)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\n\n22.4.1 RSE, MSE, RMSE\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\nПоскольку наши оценки могут быть как завышенными, так и заниженными, значения ошибок возводятся в квадрат и суммируются по всем точкам данных. Узнаем сумму квадратов остатков (RSS = Residual sum of squares), которая считается по формуле:\n\\[RSS = \\sum_{i=n}^n(y_i- \\hat y_i)^2\\]\n\nrss &lt;- sum(fit$residuals^2)\nrss\n\n[1] 878.439\n\n\nЗная это число, определяем среднеквадратичную ошибку (MSE = Mean square error), корень из среднеквадратичной ошибки (RMSE), а также стандартную ошибку остатков (RSE = Residual standard error).\n\nmse &lt;- rss / length(fit$residuals)\nmse\n\n[1] 29.2813\n\nrmse &lt;- sqrt(mse)\nrmse\n\n[1] 5.41122\n\n\n\nrse &lt;-  sqrt(rss / fit$df.residual)\nrse\n\n[1] 5.601145\n\n\n\n\n22.4.2 \\(R^2\\)\nRSE – это мера несоответствия модели данным. Но поскольку она выражается в тех же единицах измерения, что и y, то не всегда бывает ясно, какая RSE является хорошей. Коэффициент детерминации \\(R^2\\) представляет собой альтернативную меру соответствия. Этот показатель принимает форму доли – доли объясненной дисперсии, в связи с чем он всегда изменяется от 0 до 1 и не зависит от шкалы измерения.\n\\[R^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] Здесь \\(TSS = \\sum(y_i - \\bar y)^2\\), то есть общая сумма квадратов.\nTSS является мерой общей дисперсии отклика Y, и о ней можно думать как о степени изменчивости, присущей отклику до выполнения регрессионного анализа. В то же время RSS выражает степень изменчивости, которая осталась необъясненной после построения регрессионной модели. Следовательно, TSS - RSS выражает количество дисперсии отклика, объясненное (“изъятое”) после выполнения регрессионного анализа, а \\(R^2\\) – долю дисперсии Y, объясненную при помощи X. Статистика \\(R^2\\), близкая к 1, означает, что значительная доля изменчивости отклика была объяснена регрессионной моделью (Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани 2017, 82).\n\ntss &lt;- sum((y - mean(y))^2)\ntss\n\n[1] 1408.878\n\n1 - rss / tss\n\n[1] 0.3764977\n\n\nСнова сравним с результатом, который нам вернула модель.\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\nДля простой линейной регрессии статистика \\(R^2\\) совпадает с квадратом коэффициента корреляции.\n\ncor(x, y)^2\n\n[1] 0.3764977\n\n\n\n\n22.4.3 Анализ остатков\nКак правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые невязки) должны быть минимальны.\nНевязка – это разница между прогнозируемым и фактическим значениями отклика: \\((y_i- \\hat y)\\). На графике ниже невязки обозначены пунктиром.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  geom_segment(aes(xend = OxfordDst,\n                   yend = predict(fit)), \n               linetype = 2, \n               color = cols[1]) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nПерепроверим.\n\nМы можем убедиться в том, что невязки (fit$residuals) представляют собой разницу между фактическим (OxfordPots$OxfordPct) и предсказанным значением (fit$fitted.value). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями.\n\nall.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)\n\n[1] TRUE\n\n\n\nЕсли модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.\n\nmean(fit$residuals) |&gt; \n  round(2)\n\n[1] 0\n\ncov(fit$residuals, OxfordPots$OxfordDst) |&gt; \n  round(2)\n\n[1] 0\n\n\nКроме того, полезно проверить остатки на нормальность и гомоскедастичность (равномерность дисперсии остатков). Это можно сделать при помощи специального теста или визуально.\n\nshapiro.test(residuals(fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.97648, p-value = 0.7262\n\n\nВысокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально.\nТакже проведем визуальные тесты.\n\ncheck_model(fit)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №1 “Introduction”, №2 “Residuals”, №3 “Least Squares Estimation”, №4 “Residual Variation”.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#множественная-регрессия",
    "href": "regression.html#множественная-регрессия",
    "title": "22  Регрессионный анализ",
    "section": "22.7 Множественная регрессия",
    "text": "22.7 Множественная регрессия\nМножественная регрессия подходит для тех случаев, где на переменную отклика могут влиять несколько предикторов. Допустим, что в случае с долей оксфордской керамики это не только расстояние от Оксфорда, но и близость крупных городских центров, вокруг которых выстраивались торговые взаимодействия.\nВ общем виде множественная регрессионная модель имеет форму:\n\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2+ ... \\beta_px_p + \\epsilon\\]\n\n22.7.1 Модель с двумя предикторами\nПодгоним вторую модель и посмотрим, дает ли нам что-то добавление второго предиктора.\n\nfit2 &lt;- lm(OxfordPct ~ OxfordDst + WalledArea, data = OxfordPots)\n\n\n\n\n\n\n\n\n\nModel with 1 Predictor\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n192.445\n193.368\n196.648\n0.376\n0.354\n5.411\n5.601\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel with 2 Predictors\n\n\nAIC\nAICc\nBIC\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\n133.022\n135.522\n137.201\n0.548\n0.498\n4.748\n5.129\n\n\n\n\n\n\n\nНа первый взгляд, все хорошо: RSE уменьшилась, а доля объясненной дисперсии увеличилась.\n\ncompare_performance(fit_null, fit, fit2) |&gt; \n  plot()\n\n\n\n\n\n\n\n\nОднако p-value для второго предиктора (0.69) указывает на то, что он не является статистически значимым.\n\nparameters(fit2)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n24.103\n2.748\n0.950\n18.330\n29.875\n8.772\n18.000\n0.000\n\n\nOxfordDst\n−0.147\n0.031\n0.950\n−0.213\n−0.081\n−4.668\n18.000\n0.000\n\n\nWalledArea\n−0.005\n0.014\n0.950\n−0.034\n0.023\n−0.397\n18.000\n0.696\n\n\n\n\n\n\n\nЭто может означать, что связи между площадью обнесенного стеной города и числом оксфордских горшков на самом деле нет.\n\nКстати, к похожему выводу пришел и Ян Ходдер в упомянутых исследованиях: торговля грубой керамикой, данные о которой содержит наш датасет, меньше зависит от городов, чем торговля более изысканными товарами. Одним словом, горшки везде нужны, и в городе, и в деревне.\n\nПочему же мы видим увеличение \\(R^2\\)? Дело в том, что этот показатель всегда возрастает при добавлении в модель дополнительных переменных, даже если эти переменные очень слабо связаны с откликом. Поэтому важнейшая задача при обучении модели связана с отбором информативных переменных. В противном случае велик риск переобучить модель.\n\n\n22.7.2 Мнимые переменные\nДля построения модели можно использовать не только количественные, но и качественные предикторы. Если качественный предиктор имеет только два уровня (например, мужской и женский пол), то он превращается в фиктивную переменную, принимающую значения 1 или 0. В нашем датасете в таком виде хранятся сведения о наличии водного сообщения между Оксфордом и местом обнаружения керамических осколков.\n\nfit3 &lt;- lm(OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)\nparameters(fit3)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n14.505\n1.276\n0.950\n11.887\n17.123\n11.367\n27.000\n0.000\n\n\nOxfordDst\n−0.084\n0.015\n0.950\n−0.115\n−0.053\n−5.525\n27.000\n0.000\n\n\nWaterTrans\n10.250\n1.074\n0.950\n8.047\n12.454\n9.545\n27.000\n0.000\n\n\n\n\n\n\n\nОбратите внимание, что угловой коэффициент для WaterTrans представляет собой положительное число: если водный путь есть, линия регрессии не так резко уходит вниз по мере удаления от Оксфорда.\nОчевидно, что наличие водного пути – важный предиктор, что можно подтвердить графически.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ggthemes::wsj_rgby\")\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct, \n             color = as.factor(WaterTrans), \n             group = as.factor(WaterTrans))) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_manual(\"WaterTrans\", values = cols[4:3]) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nСравним эффективность моделей при помощи функции compare_performance(). Добавим и вторую модель тоже, хотя мы помним, что она содержит статистически незначимый предиктор.\n\n\n\n\n\n\n\n\nName\nModel\nR2\nR2_adjusted\nRMSE\nSigma\nAIC_wt\nAICc_wt\nBIC_wt\nPerformance_Score\n\n\n\n\nfit2\nlm\n0.55\n0.50\n4.75\n5.13\n1.00\n1.00\n1.00\n0.74\n\n\nfit3\nlm\n0.86\n0.85\n2.59\n2.73\n0.00\n0.00\n0.00\n0.57\n\n\nfit\nlm\n0.38\n0.35\n5.41\n5.60\n0.00\n0.00\n0.00\n0.22\n\n\nfit_null\nlm\n0.00\n0.00\n6.85\n6.97\n0.00\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\ncompare_performance(fit_null, fit, fit2, fit3) |&gt; \n  plot()",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#мнимые-переменные",
    "href": "regression.html#мнимые-переменные",
    "title": "22  Простая линейная регрессия",
    "section": "22.9 Мнимые переменные",
    "text": "22.9 Мнимые переменные",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#полиномиальная-регрессия",
    "href": "regression.html#полиномиальная-регрессия",
    "title": "22  Простая линейная регрессия",
    "section": "22.10 Полиномиальная регрессия",
    "text": "22.10 Полиномиальная регрессия",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#проблема-переобучения",
    "href": "regression.html#проблема-переобучения",
    "title": "22  Простая линейная регрессия",
    "section": "22.11 Проблема переобучения",
    "text": "22.11 Проблема переобучения",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#трансформация-данных",
    "href": "regression.html#трансформация-данных",
    "title": "22  Простая линейная регрессия",
    "section": "22.12 Трансформация данных",
    "text": "22.12 Трансформация данных\nЗадачи для",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#сравнение-моделей-с-anova",
    "href": "regression.html#сравнение-моделей-с-anova",
    "title": "22  Простая линейная регрессия",
    "section": "22.13 Сравнение моделей с ANOVA",
    "text": "22.13 Сравнение моделей с ANOVA",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#ошибка-прогноза",
    "href": "regression.html#ошибка-прогноза",
    "title": "22  Простая линейная регрессия",
    "section": "22.2 Ошибка прогноза",
    "text": "22.2 Ошибка прогноза\nНа практике линейная зависимость в чистом виде почти не встречается: всегда есть небольшая ошибка прогноза (\\(\\epsilon\\)):\n\\[y\\approx\\beta_o + \\beta_1x + \\epsilon\\]\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nПри создании регрессионной модели наша задача заключается в том, чтобы на основе доступных наблюдений подобрать коэффициенты \\(\\beta_0\\) и \\(\\beta_1\\) таким образом, чтобы минимизировать ошибку.\n\\[\\sum(y_i- \\hat y)^2 = \\sum\\epsilon^2\\]\nЧтобы подчеркнуть, что речь идет лишь об оценке, над бетой ставится “крышечка”:\n\\[\\hat y \\approx \\hat\\beta_o + \\hat\\beta_1x\\]",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#простая-регрессия-с-lm",
    "href": "regression.html#простая-регрессия-с-lm",
    "title": "22  Простая линейная регрессия",
    "section": "22.4 Простая регрессия с lm()",
    "text": "22.4 Простая регрессия с lm()\nПосмотрим, как связаны между собой процент керамических изделий из Оксфорда и расстояние от центра производства.\n\nlibrary(paletteer)\ncols &lt;- paletteer_d(\"ggthemes::wsj_rgby\")\n\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nЧем дальше от Оксфорда, тем меньше керамических изделий оттуда, поэтому линия имеет отрицательный наклон.\nИ наклон, и точка пересечения с осью y определяет функция lm.\n\nfit &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nsummary(fit)\n\n\nCall:\nlm(formula = OxfordPct ~ OxfordDst, data = OxfordPots)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.388  -4.003  -1.005   3.844  10.757 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20.87665    2.23356   9.347 4.18e-10 ***\nOxfordDst   -0.12290    0.02989  -4.112 0.000311 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.601 on 28 degrees of freedom\nMultiple R-squared:  0.3765,    Adjusted R-squared:  0.3542 \nF-statistic: 16.91 on 1 and 28 DF,  p-value: 0.0003111",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#уровень-значимости",
    "href": "regression.html#уровень-значимости",
    "title": "22  Простая линейная регрессия",
    "section": "22.7 Уровень значимости",
    "text": "22.7 Уровень значимости\n\nlibrary(broom)\ntidy(fit)\n\n\n  \n\n\n\nСтолбец statistic, как легко убедиться, содержит результат деления коэффицентов на стандартную ошибку; а p.value (уровень значимости) указывает, какова вероятность случайно получить такое значение. В нашем случае – почти 0, что говорит о том, что доля оксфордской керамики на участке действительно зависит от расстояния.\n\ntidy(fit) |&gt; \n  transmute(t_stat = estimate / std.error) |&gt; \n  mutate(p_val = 2*pt(abs(t_stat), 28, lower.tail = FALSE))\n\n\n  \n\n\n\nРезультат, возвращаемый функцией pt(), умножается на два, т.к. используется двусторонний t-test. Буква p в названии означает функцию распределения вероятностей (probability), а t – распределение Стьюдента для заданного числа степеней свободы (28 в нашем случае).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#предсказания-с-predict",
    "href": "regression.html#предсказания-с-predict",
    "title": "22  Регрессионный анализ",
    "section": "22.6 Предсказания с predict()",
    "text": "22.6 Предсказания с predict()\nПредсказанные значения можно извлечь при помощи predict(). Это почти то же самое, что fit$fitted.values. Разница в том, что функции predict() можно передать новые данные. Узнаем, какую долю оксфордской керамики наша модель ожидает обнаружить на расстоянии ровно 100 миль от Оксфорда.\n\nnewdata &lt;- data.frame(OxfordDst = 100)\npredict(fit, newdata)\n\n       1 \n8.586158 \n\n\nПод капотом функция predict() подставляет подогнанные значения коэффициентов:\n\nfit$coefficients[[1]] + fit$coefficients[[2]] * 100\n\n[1] 8.586158",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#оценка-модели-c-r2",
    "href": "regression.html#оценка-модели-c-r2",
    "title": "22  Простая линейная регрессия",
    "section": "22.11 Оценка модели c \\(R^2\\)",
    "text": "22.11 Оценка модели c \\(R^2\\)\nRSE – это мера несоответствия модели данным. Но поскольку она выражается в тех же единицах измерения, что и y, то не всегда бывает ясно, какая RSE является хорошей. Коэффициент детерминации \\(R^2\\) представляет собой альтернативную меру соответствия. Этот показатель принимает форму доли – доли объясненной дисперсии, в связи с чем он всегда изменяется от 0 до 1 и не зависит от шкалы измерения.\n\\[R^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] Здесь \\(TSS = \\sum(y_i - \\bar y)^2\\), то есть общая сумма квадратов.\nTSS является мерой общей дисперсии отклика Y, и о ней можно думать как о степени изменчивости, присущей отклику до выполнения регрессионного анализа. В то же время RSS выражает степень изменчивости, которая осталась необъясненной после построения регрессионной модели. Следовательно, TSS - RSS выражает количество дисперсии отклика, объясненное (“изъятое”) после выполнения регрессионного анализа, а \\(R^2\\) – долю дисперсии Y, объясненную при помощи X. Статистика \\(R^2\\), близкая к 1, означает, что значительная доля изменчивости отклика была объяснена регрессионной моделью (Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани 2020, 82).\n\ntss &lt;- sum((y - mean(y))^2)\ntss\n\n[1] 1408.878\n\n1 - rss / tss\n\n[1] 0.3764977\n\n\nСнова сравним с результатом, который нам вернула модель.\n\nsummary(fit)$r.squared\n\n[1] 0.3764977\n\n\nДля простой линейной регрессии статистика \\(R^2\\) совпадает с квадратом коэффициента корреляции.\n\ncor(x, y)^2\n\n[1] 0.3764977",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#стандартная-ошибка-остатков-rse",
    "href": "regression.html#стандартная-ошибка-остатков-rse",
    "title": "22  Простая линейная регрессия",
    "section": "22.10 Стандартная ошибка остатков (RSE)",
    "text": "22.10 Стандартная ошибка остатков (RSE)\nПоскольку наши оценки могут быть как завышенными, так и заниженными, значения ошибок возводятся в квадрат и суммируются по всем точкам данных. Узнаем сумму квадратов остатков (RSS = Residual sum of squares), которая считается по формуле:\n\\[RSS = \\sum_{i=n}^n(y_i- \\hat y_i)^2\\]\n\nrss = sum(fit$residuals^2)\nrss\n\n[1] 878.439\n\n\nЗная это число, определяем среднюю (MSE = Mean square error) и стандартную ошибку остатков (RSE = Residual standard error). Грубо говоря, это средняя величина отклонения отклика от регрессионной линии. Заметьте, что вместо 30 делим на 28 (n - 2); это делается, чтобы избежать смещения данных.\n\nmse &lt;- rss / ( length(fit$residuals) - 2)\nmse\n\n[1] 31.37282\n\nrse &lt;- sqrt(mse) \nrse\n\n[1] 5.601145\n\n\nСравним с тем, что нам вернула команда summary(fit).\n\nsummary(fit)$sigma\n\n[1] 5.601145",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#нулевая-модель",
    "href": "regression.html#нулевая-модель",
    "title": "22  Простая линейная регрессия",
    "section": "22.12 Нулевая модель",
    "text": "22.12 Нулевая модель\nВажно знать, что следующие два вызова возвращают одинаковые модели.\n\nfit1 &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nfit2 &lt;- lm(OxfordPct ~ 1 + OxfordDst, data = OxfordPots)\n\n\nfit1$coef == fit2$coef\n\n(Intercept)   OxfordDst \n       TRUE        TRUE \n\n\nЕдиница в вызове функции означает пересечение оси y, то есть свободный член. Это значит, что мы можем построить нулевую модель, где любому значению x будет соответствовать одно и то же (среднее) значение y.\n\nfit_null &lt;- lm(OxfordPct ~ 1, data = OxfordPots)\nsummary(fit_null)\n\n\nCall:\nlm(formula = OxfordPct ~ 1, data = OxfordPots)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2117  -5.8992  -0.9617   6.1008   9.7883 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   12.712      1.273   9.989 6.77e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.97 on 29 degrees of freedom\n\n\nЕдинственный коэффициент в таком случае совпадает со средним значением y.\n\nmean(OxfordPots$OxfordPct)\n\n[1] 12.71167\n\n\nНа графике это будет выглядеть вот так.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  # обратите внимание на формулу!\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nТакая модель может быть использована для сравнения, чтобы понять, насколько мы выиграли, добавив предикторы. Подробнее о сравнении разных моделей будет сказано дальше.\n\nanova(fit_null, fit)\n\n\n  \n\n\n\n\n\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2020. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#анализ-остатков",
    "href": "regression.html#анализ-остатков",
    "title": "22  Простая линейная регрессия",
    "section": "22.8 Анализ остатков",
    "text": "22.8 Анализ остатков\nКак правило, большинство точек не может лежать на линии, но линия подгоняется так, чтобы быть как можно ближе ко всем точкам. Иными словами, расстояния от каждого наблюдения до линии регрессии (так называемые невязки) должны быть минимальны.\nНевязка – это разница между прогнозируемым и фактическим значениями отклика: \\((y_i- \\hat y)\\). На графике ниже невязки обозначены пунктиром.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  geom_segment(aes(xend = OxfordDst,\n                   yend = predict(fit)), \n               linetype = 2, \n               color = cols[1]) +\n  theme_minimal() \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nМы можем убедиться в том, что невязки (fit$residuals) представляют собой разницу между фактическим (OxfordPots$OxfordPct) и предсказанным значением (fit$fitted.value). Для этого сложим предсказанные значения с остатками и сравним с фактическими значениями.\n\nall.equal(unname(fit$fitted.values + fit$residuals), OxfordPots$OxfordPct)\n\n[1] TRUE\n\n\nЕсли модель подогнана верно, то невязки должны иметь среднее в районе нуля и не коррелировать с предиктором. Проверим.\n\nmean(fit$residuals)\n\n[1] 3.256654e-16\n\ncov(fit$residuals, OxfordPots$OxfordDst)\n\n[1] -3.920236e-15\n\n\nКроме того, полезно проверить остатки на нормальность. Это можно сделать при помощи специального теста или визуально.\n\nshapiro.test(residuals(fit))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fit)\nW = 0.97648, p-value = 0.7262\n\n\nВысокое значение p-value, которое возвращает текст Шапиро-Уилка, говорит о том, что остатки распределены нормально.\nТакже проведем три визуальных теста.\n\nlibrary(gridExtra)\n\ng1 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  mutate(residuals_st = scale(residuals)) |&gt; \n  ggplot(aes(sample = residuals_st)) +\n  stat_qq(color = cols[3], \n          size = 2, alpha = 0.8) + \n  stat_qq_line(color = cols[2]) +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\ng2 &lt;- tibble(residuals = residuals(fit),\n       fitted = predict(fit)) |&gt; \n  ggplot(aes(fitted, residuals)) +\n  geom_point(color = cols[3], \n             size = 2, alpha = 0.8) + \n  theme_minimal()\n\ng3 &lt;- tibble(residuals = residuals(fit)) |&gt; \n  ggplot(aes(residuals)) +\n  geom_histogram(fill = cols[3], color = \"white\") + \n  theme_minimal()\n\ngrid.arrange(g1, g2, g3, nrow = 1)\n\n\n\n\n\n\n\n\nНа квантиль-квантильном графике и на гистограмме видно небольшой перекос в области негативных значений (это может означать, что мы чаще переоцениваем, чем недооцениваем y. К этому мы еще вернемся.\nВы можете также использовать базовую plot(), передав ей подогнанную модель в качестве аргумента.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №1 “Introduction”, №2 “Residuals”, №3 “Least Squares Estimation”, №4 “Residual Variation”. Один из этих уроков потребует установить пакет manipulate, позволяющий взаимодействовать с графиком в интерактивном режиме.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html",
    "href": "multivar.html",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "",
    "text": "23.1 Регрессионные алгоритмы\nВ машинном обучении проблемы, связанные с количественным откликом, называют проблемами регрессии, а проблемы, связанные с качественным откликом, проблемами классификации. В прошлом уроке мы познакомились с простой и множественной регрессией, но регрессионных алгоритмов великое множество. Вот лишь некоторые из них:\nКроме того, существуют методы регуляризации линейных моделей, позволяющие существенно улучшить их качество на данных большой размерности (т.е. с большим количеством предкторов). К таким алгоритмам относятся гребневая регрессия и метод лассо. О них мы поговорим в одном из следующих уроков.\nО математической стороне дела см. Г. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани (2017). В этом уроке мы научимся работать с различными регрессионными алгоритмами, используя библиотеку tidymodels.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#модель-с-несколькими-предикторами",
    "href": "multivar.html#модель-с-несколькими-предикторами",
    "title": "23  Множественная регрессия",
    "section": "",
    "text": "summary(fit)$r.squared\n\n[1] 0.3764977\n\nsummary(fit)$sigma\n\n[1] 5.601145\n\n\n\n\n\n\nsummary(fit2)$r.squared\n\n[1] 0.548113\n\nsummary(fit2)$sigma\n\n[1] 5.128755",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#мнимые-переменные",
    "href": "multivar.html#мнимые-переменные",
    "title": "23  Множественная регрессия",
    "section": "23.2 Мнимые переменные",
    "text": "23.2 Мнимые переменные\nДля построения модели можно использовать не только количественные, но и качественные предикторы. Если качественный предиктор имеет только два уровня (например, мужской и женский пол), то он превращается в фиктивную переменную, принимающую значения 1 или 0. В нашем датасете в таком виде хранятся сведения о наличии водного сообщения между Оксфордом и местом обнаружения керамических осколков.\n\nfit3 &lt;- lm(OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)\nsummary(fit3)\n\n\nCall:\nlm(formula = OxfordPct ~ OxfordDst + WaterTrans, data = OxfordPots)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5730 -1.4982  0.2589  1.0286  5.5021 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14.50486    1.27606  11.367 8.49e-12 ***\nOxfordDst   -0.08357    0.01513  -5.525 7.46e-06 ***\nWaterTrans  10.25020    1.07388   9.545 3.82e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.727 on 27 degrees of freedom\nMultiple R-squared:  0.8575,    Adjusted R-squared:  0.8469 \nF-statistic: 81.21 on 2 and 27 DF,  p-value: 3.785e-12\n\n\nНа то, что наша модель стала гораздо более адекватной, указывает не только возросший почти в два раза показатель \\(R^2\\), но и статистика F, о которой будет сказано ниже. Для сравнения в предыдущей модели fit этот показатель составлял 16.907611.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#полиномиальная-регрессия",
    "href": "multivar.html#полиномиальная-регрессия",
    "title": "23  Множественная регрессия",
    "section": "23.3 Полиномиальная регрессия",
    "text": "23.3 Полиномиальная регрессия",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#проблема-переобучения",
    "href": "multivar.html#проблема-переобучения",
    "title": "23  Множественная регрессия",
    "section": "23.4 Проблема переобучения",
    "text": "23.4 Проблема переобучения",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#трансформация-данных",
    "href": "multivar.html#трансформация-данных",
    "title": "23  Множественная регрессия",
    "section": "23.5 Трансформация данных",
    "text": "23.5 Трансформация данных",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#сравнение-с-нулевой-моделью",
    "href": "multivar.html#сравнение-с-нулевой-моделью",
    "title": "23  Множественная регрессия",
    "section": "23.6 Сравнение с нулевой моделью",
    "text": "23.6 Сравнение с нулевой моделью",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "multivar.html#сравнение-моделей",
    "href": "multivar.html#сравнение-моделей",
    "title": "23  Множественная регрессия",
    "section": "23.6 Сравнение моделей",
    "text": "23.6 Сравнение моделей\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №5 “Introduction to Multivariable Regression”, №2 “MultiVar Examples”, №3 “MultiVar Examples2”, №4 “MultiVar Examples3”, “Residuals Diagnostics and Variation”, “Variance Inflation Factors”, “Overfitting and Underfitting”, “Binary Outcomes”, “Count Outcomes”",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Множественная регрессия</span>"
    ]
  },
  {
    "objectID": "regression.html#что-осталось-за-кадром",
    "href": "regression.html#что-осталось-за-кадром",
    "title": "22  Регрессионный анализ",
    "section": "22.8 Что осталось за кадром",
    "text": "22.8 Что осталось за кадром\nВ этом уроке мы не рассмотрели множество аспектов регрессионного анализа: необходимость трансформации данных, учет эффектов взаимодействия переменных, использование полиномиальных моделей и др.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Regression_Models\"), запустите swirl() и пройдите уроки №5 “Introduction to Multivariable Regression”, №2 “MultiVar Examples”, №3 “MultiVar Examples2”, №4 “MultiVar Examples3”, “Residuals Diagnostics and Variation”, “Variance Inflation Factors”, “Overfitting and Underfitting”, “Binary Outcomes”, “Count Outcomes”\n\n\n\n\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "multivar.html#библиотека-tidymodels",
    "href": "multivar.html#библиотека-tidymodels",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.2 Библиотека tidymodels",
    "text": "23.2 Библиотека tidymodels\nБиблиотека tidymodels позволяет обучать модели и оценивать их эффективность с использованием принципов опрятных данных. Она представляет собой набор пакетов R, которые разработаны для работы с машинным обучением и являются частью более широкой экосистемы tidyverse.\nВот некоторые из ключевых пакетов, входящих в состав tidymodels:\n\nparsnip - универсальный интерфейс для различных моделей машинного обучения, который упрощает переключение между разными типами моделей;\nrecipes - фреймворк для создания и управления “рецептами” предварительной обработки данных перед тренировкой модели;\nrsample - инструменты для разделения данных на обучающую и тестовую выборки, а также для кросс-валидации;\ntune - функции для оптимизации гиперпараметров моделей машинного обучения;\nyardstick - инструменты для оценки производительности моделей;\nworkflow позволяет объединить различные компоненты модели в единый объект: препроцессинг данных, модель машинного обучения, настройку гиперпараметров.\n\nМы также будем использовать пакет textrecipes, который представляет собой аналог recipes для текстовых данных.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#данные",
    "href": "multivar.html#данные",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.3 Данные",
    "text": "23.3 Данные\nДатасет для этого урока хранит данные о названиях, рейтингах, жанре, цене и числе отзывов на некоторые книги с Amazon. Мы попробуем построить регресионную модель, которая будет предсказывать цену книги.\n\nbooks  &lt;- readxl::read_xlsx(\"../files/AmazonBooks.xlsx\")\nbooks\n\n\n  \n\n\n\nДанные не очень опрятны, и прежде всего их надо тайдифицировать.\n\ncolnames(books) &lt;- tolower(colnames(books))\nbooks &lt;- books |&gt; \n  rename(rating = `user rating`)\n\nНа графике ниже видно, что сильной корреляции между количественными переменными не прослеживается, так что задача перед нами стоит незаурядная. Посмотрим, что можно сделать в такой ситуации.\n\nbooks |&gt; \n  select_if(is.numeric) |&gt; \n  cor() |&gt; \n  corrplot::corrplot(method = \"ellipse\")\n\n\n\n\n\n\n\n\nМы видим, что количественные предикторы объясняют лишь ничтожную долю дисперсии (чуть более информативен жанр).\n\nsummary(lm(price ~ reviews + year + rating + genre, data  = books))\n\n\nCall:\nlm(formula = price ~ reviews + year + rating + genre, data = books)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.472  -5.050  -1.841   2.307  89.686 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       8.987e+02  2.734e+02   3.287  0.00107 ** \nreviews           7.779e-07  3.181e-05   0.024  0.98050    \nyear             -4.324e-01  1.370e-01  -3.156  0.00168 ** \nrating           -3.655e+00  1.933e+00  -1.891  0.05909 .  \ngenreNon Fiction  3.920e+00  8.669e-01   4.522 7.41e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.16 on 595 degrees of freedom\nMultiple R-squared:  0.06903,   Adjusted R-squared:  0.06277 \nF-statistic: 11.03 on 4 and 595 DF,  p-value: 1.235e-08\n\n\nПосмотрим, можно ли как-то улучшить этот результат. Но сначала оценим визуально связь между ценой, с одной стороны, и годом и жанром, с другой.\n\ng1 &lt;- books |&gt; \n  ggplot(aes(year, price, color = genre, group = genre)) + \n  geom_jitter(show.legend = FALSE, alpha = 0.7) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal()\n\ng2 &lt;- books |&gt; \n  ggplot(aes(genre, price, color = genre)) + \n  geom_boxplot() + \n  theme_minimal()\n\ngridExtra::grid.arrange(g1, g2, nrow = 1)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#обучающая-и-контрольная-выборка",
    "href": "multivar.html#обучающая-и-контрольная-выборка",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.4 Обучающая и контрольная выборка",
    "text": "23.4 Обучающая и контрольная выборка\nВы уже знаете, при обучении модели мы стремимся к минимизации среднеквадратичной ошибки (MSE), однако в большинстве случаев нас интересует не то, как метод работает на обучающих данных, а то, как он покажет себя на контрольных данных. Чтобы избежать переобучения, очень важно в самом начале разделить доступные наблюдения на две группы.\n\nbooks_split &lt;- books |&gt; \n  initial_split()\n\nbooks_train &lt;- training(books_split)\nbooks_test &lt;- testing(books_split)",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#определение-модели",
    "href": "multivar.html#определение-модели",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.5 Определение модели",
    "text": "23.5 Определение модели\nОпределение модели включает следующие шаги:\n\nуказывается тип модели на основе ее математической структуры (например, линейная регрессия, случайный лес, KNN и т. д.);\nуказывается механизм для подгонки модели – чаще всего это программный пакет, который должен быть использован, например glmnet. Это самостоятельные модели, и parsnip обеспечивает согласованные интерфейсы, используя их в качестве движков для моделирования.\nпри необходимости объявляется режим модели. Режим отражает тип прогнозируемого результата. Для числовых результатов режимом является регрессия, для качественных - классификация. Если алгоритм модели может работать только с одним типом результатов прогнозирования, например, линейной регрессией, режим уже задан.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#дизайн-переменных",
    "href": "multivar.html#дизайн-переменных",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.8 Дизайн переменных",
    "text": "23.8 Дизайн переменных\nТеперь нам нужен препроцессор. За него отвечает пакет recipes. Если вы не уверены, какие шаги необходимы на этом этапе, можно заглянуть в шпаргалку. В случае с линейной регрессией это может быть логарифмическая трансформация, нормализация, отсев переменных с нулевой дисперсией (zero variance), добавление (impute) недостающих значений или удаление переменных, которые коррелируют с другими переменными.\nВот так выглядит наш первый рецепт. Обратите внимание, что формула записывается так же, как мы это делали ранее внутри функции lm().\n\nbooks_rec &lt;- recipe(price ~ year + genre + name, \n                    data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_normalize(year) |&gt; \n  step_tokenize(name)  |&gt; \n  step_tokenfilter(name, max_tokens = 1000)  |&gt; \n  step_tfidf(name) \n\nПри желании можно посмотреть на результат предобработки.\n\nprep(books_rec, books_train) |&gt; \n  bake(new_data = NULL) |&gt; \n  head(5) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nyear\nprice\ngenre_Non.Fiction\ntfidf_name_1\ntfidf_name_1,111\ntfidf_name_10\ntfidf_name_100\ntfidf_name_11\ntfidf_name_12\ntfidf_name_13\ntfidf_name_140\ntfidf_name_150\ntfidf_name_16\ntfidf_name_17\ntfidf_name_1936\ntfidf_name_1984\ntfidf_name_2\ntfidf_name_2.0\ntfidf_name_20\ntfidf_name_2016\ntfidf_name_22\ntfidf_name_3\ntfidf_name_30\ntfidf_name_300\ntfidf_name_4\ntfidf_name_451\ntfidf_name_49\ntfidf_name_5\ntfidf_name_5,000\ntfidf_name_500\ntfidf_name_5000\ntfidf_name_52\ntfidf_name_5th\ntfidf_name_6\ntfidf_name_60\ntfidf_name_63\ntfidf_name_6th\ntfidf_name_7\ntfidf_name_700\ntfidf_name_8\ntfidf_name_a\ntfidf_name_about\ntfidf_name_absurd\ntfidf_name_according\ntfidf_name_account\ntfidf_name_achieving\ntfidf_name_acid\ntfidf_name_activity\ntfidf_name_adult\ntfidf_name_adults\ntfidf_name_advanced\ntfidf_name_adventures\ntfidf_name_adversity\ntfidf_name_afterlife\ntfidf_name_aftermath\ntfidf_name_again\ntfidf_name_against\ntfidf_name_ages\ntfidf_name_agreements\ntfidf_name_ahead\ntfidf_name_alaska\ntfidf_name_alex\ntfidf_name_alexander\ntfidf_name_all\ntfidf_name_allâ\ntfidf_name_allegiant\ntfidf_name_almost\ntfidf_name_alphabet\ntfidf_name_am\ntfidf_name_amateur\ntfidf_name_amazing\ntfidf_name_america\ntfidf_name_american\ntfidf_name_americans\ntfidf_name_an\ntfidf_name_ancient\ntfidf_name_and\ntfidf_name_animal\ntfidf_name_animals\ntfidf_name_answers\ntfidf_name_antidote\ntfidf_name_antiracist\ntfidf_name_apologizing\ntfidf_name_approach\ntfidf_name_are\ntfidf_name_arguing\ntfidf_name_art\ntfidf_name_as\ntfidf_name_asians\ntfidf_name_assassination\ntfidf_name_assault\ntfidf_name_association\ntfidf_name_astounding\ntfidf_name_astrophysics\ntfidf_name_at\ntfidf_name_athena\ntfidf_name_attitude\ntfidf_name_audacious\ntfidf_name_autobiography\ntfidf_name_awesome\ntfidf_name_azkaban\ntfidf_name_b\ntfidf_name_baby\ntfidf_name_back\ntfidf_name_badass\ntfidf_name_ball\ntfidf_name_ballad\ntfidf_name_barefoot\ntfidf_name_basketball\ntfidf_name_battling\ntfidf_name_be\ntfidf_name_bear\ntfidf_name_beautiful\ntfidf_name_beck's\ntfidf_name_become\ntfidf_name_becoming\ntfidf_name_bed\ntfidf_name_bee\ntfidf_name_beginner's\ntfidf_name_beginners\ntfidf_name_being\ntfidf_name_believing\ntfidf_name_belly\ntfidf_name_berlin\ntfidf_name_between\ntfidf_name_big\ntfidf_name_bill\ntfidf_name_billionaires\ntfidf_name_bin\ntfidf_name_birthday\ntfidf_name_blitz\ntfidf_name_blood\ntfidf_name_blue\ntfidf_name_boat\ntfidf_name_body\ntfidf_name_bombers\ntfidf_name_book\ntfidf_name_books\ntfidf_name_boom\ntfidf_name_born\ntfidf_name_bossypants\ntfidf_name_box\ntfidf_name_boxed\ntfidf_name_boy\ntfidf_name_boy's\ntfidf_name_boys\ntfidf_name_brain\ntfidf_name_brain's\ntfidf_name_brave\ntfidf_name_brawl\ntfidf_name_bree\ntfidf_name_bringing\ntfidf_name_brink\ntfidf_name_broke\ntfidf_name_brothers\ntfidf_name_brown\ntfidf_name_brush\ntfidf_name_building\ntfidf_name_business\ntfidf_name_buy\ntfidf_name_by\ntfidf_name_cabin\ntfidf_name_cakes\ntfidf_name_called\ntfidf_name_calligraphy\ntfidf_name_calling\ntfidf_name_camelot\ntfidf_name_can\ntfidf_name_can't\ntfidf_name_cannot\ntfidf_name_capital\ntfidf_name_captain\ntfidf_name_carbs\ntfidf_name_case\ntfidf_name_caste\ntfidf_name_casual\ntfidf_name_cat\ntfidf_name_catch\ntfidf_name_catching\ntfidf_name_caterpillar\ntfidf_name_cats\ntfidf_name_cauldron\ntfidf_name_cause\ntfidf_name_celebrations\ntfidf_name_century\ntfidf_name_chamber\ntfidf_name_champion\ntfidf_name_change\ntfidf_name_changed\ntfidf_name_changing\ntfidf_name_chaos\ntfidf_name_chicka\ntfidf_name_child\ntfidf_name_china\ntfidf_name_choose\ntfidf_name_christian\ntfidf_name_chronicles\ntfidf_name_churchill\ntfidf_name_ck\ntfidf_name_clash\ntfidf_name_class\ntfidf_name_classic\ntfidf_name_classics\ntfidf_name_cleanse\ntfidf_name_clintons\ntfidf_name_club\ntfidf_name_collected\ntfidf_name_college\ntfidf_name_color\ntfidf_name_coloring\ntfidf_name_colors\ntfidf_name_columbus's\ntfidf_name_comes\ntfidf_name_comfort\ntfidf_name_comments\ntfidf_name_common\ntfidf_name_companies\ntfidf_name_complete\ntfidf_name_comprehensive\ntfidf_name_conducted\ntfidf_name_confession\ntfidf_name_confident\ntfidf_name_confidential\ntfidf_name_confronts\ntfidf_name_conservative\ntfidf_name_constitution\ntfidf_name_construction\ntfidf_name_contessa\ntfidf_name_control\ntfidf_name_cookbook\ntfidf_name_cookbooks\ntfidf_name_cooker\ntfidf_name_cooking\ntfidf_name_cooks\ntfidf_name_cooling\ntfidf_name_coping\ntfidf_name_counterintuitive\ntfidf_name_courage\ntfidf_name_crawdads\ntfidf_name_crayons\ntfidf_name_crazy\ntfidf_name_create\ntfidf_name_created\ntfidf_name_creating\ntfidf_name_creative\ntfidf_name_creator\ntfidf_name_crest\ntfidf_name_crime\ntfidf_name_crisis\ntfidf_name_cross\ntfidf_name_crows\ntfidf_name_culinary\ntfidf_name_cultivate\ntfidf_name_culture\ntfidf_name_cups\ntfidf_name_cursed\ntfidf_name_cutting\ntfidf_name_cycle\ntfidf_name_da\ntfidf_name_dad\ntfidf_name_daily\ntfidf_name_dance\ntfidf_name_dangerous\ntfidf_name_dangers\ntfidf_name_danny\ntfidf_name_dare\ntfidf_name_daring\ntfidf_name_darker\ntfidf_name_david\ntfidf_name_day\ntfidf_name_days\ntfidf_name_dead\ntfidf_name_dear\ntfidf_name_death\ntfidf_name_decades\ntfidf_name_decision\ntfidf_name_deckled\ntfidf_name_decluttering\ntfidf_name_defiance\ntfidf_name_definitive\ntfidf_name_defy\ntfidf_name_delicious\ntfidf_name_deluxe\ntfidf_name_designed\ntfidf_name_designs\ntfidf_name_devotional\ntfidf_name_diagnostic\ntfidf_name_diary\ntfidf_name_die\ntfidf_name_diet\ntfidf_name_dietâ\ntfidf_name_difference\ntfidf_name_difficult\ntfidf_name_dinnertime\ntfidf_name_dirt\ntfidf_name_disappointments\ntfidf_name_discontents\ntfidf_name_discovery\ntfidf_name_disease\ntfidf_name_disorders\ntfidf_name_divergent\ntfidf_name_divine\ntfidf_name_do\ntfidf_name_doctor's\ntfidf_name_documents\ntfidf_name_dog\ntfidf_name_don't\ntfidf_name_donkey\ntfidf_name_doomsday\ntfidf_name_doubting\ntfidf_name_dover\ntfidf_name_dragon\ntfidf_name_dragons\ntfidf_name_drive\ntfidf_name_dsm\ntfidf_name_dukan\ntfidf_name_dungeons\ntfidf_name_during\ntfidf_name_dysfunctions\ntfidf_name_earth\ntfidf_name_easy\ntfidf_name_eat\ntfidf_name_eater's\ntfidf_name_eclipse\ntfidf_name_edge\ntfidf_name_edition\ntfidf_name_educated\ntfidf_name_effective\ntfidf_name_electric\ntfidf_name_elegance\ntfidf_name_elegy\ntfidf_name_elements\ntfidf_name_elephants\ntfidf_name_embracing\ntfidf_name_enchanted\ntfidf_name_end\ntfidf_name_english\ntfidf_name_enjoying\ntfidf_name_enough\ntfidf_name_epic\ntfidf_name_essential\ntfidf_name_eternity\ntfidf_name_ever\ntfidf_name_every\ntfidf_name_everyday\ntfidf_name_everything\ntfidf_name_everywhere\ntfidf_name_exceptional\ntfidf_name_expect\ntfidf_name_expecting\ntfidf_name_f\ntfidf_name_fable\ntfidf_name_face\ntfidf_name_facing\ntfidf_name_facts\ntfidf_name_fahrenheit\ntfidf_name_faith\ntfidf_name_families\ntfidf_name_family\ntfidf_name_fast\ntfidf_name_fat\ntfidf_name_fate\ntfidf_name_fault\ntfidf_name_fear\ntfidf_name_feast\ntfidf_name_featuring\ntfidf_name_feel\ntfidf_name_feeling\ntfidf_name_fetch\ntfidf_name_fever\ntfidf_name_fey\ntfidf_name_fiction\ntfidf_name_fifty\ntfidf_name_find\ntfidf_name_finding\ntfidf_name_fire\ntfidf_name_fires\ntfidf_name_first\ntfidf_name_firsthand\ntfidf_name_fish\ntfidf_name_five\ntfidf_name_flap\ntfidf_name_flawed\ntfidf_name_fleas\ntfidf_name_floral\ntfidf_name_flowers\ntfidf_name_food\ntfidf_name_foods\ntfidf_name_foolproof\ntfidf_name_fools\ntfidf_name_for\ntfidf_name_forâ\ntfidf_name_forest\ntfidf_name_forever\ntfidf_name_found\ntfidf_name_four\ntfidf_name_fox\ntfidf_name_freakonomics\ntfidf_name_free\ntfidf_name_freed\ntfidf_name_freedom\ntfidf_name_freezer\ntfidf_name_french\ntfidf_name_friends\ntfidf_name_from\ntfidf_name_frontier\ntfidf_name_frozen\ntfidf_name_full\ntfidf_name_fully\ntfidf_name_fun\ntfidf_name_gain\ntfidf_name_game\ntfidf_name_games\ntfidf_name_gatsby\ntfidf_name_general\ntfidf_name_gentleman\ntfidf_name_geographic\ntfidf_name_george\ntfidf_name_get\ntfidf_name_giants\ntfidf_name_gifts\ntfidf_name_giraffes\ntfidf_name_girl\ntfidf_name_girls\ntfidf_name_give\ntfidf_name_giving\ntfidf_name_glenn\ntfidf_name_global\ntfidf_name_glory\ntfidf_name_go\ntfidf_name_goals\ntfidf_name_goblet\ntfidf_name_god\ntfidf_name_going\ntfidf_name_gold\ntfidf_name_golden\ntfidf_name_goldfinch\ntfidf_name_goliath\ntfidf_name_gone\ntfidf_name_good\ntfidf_name_goodnight\ntfidf_name_government\ntfidf_name_grades\ntfidf_name_grain\ntfidf_name_gratitude\ntfidf_name_great\ntfidf_name_greatly\ntfidf_name_greatness\ntfidf_name_green\ntfidf_name_greenlights\ntfidf_name_grey\ntfidf_name_grime\ntfidf_name_guernsey\ntfidf_name_guide\ntfidf_name_guided\ntfidf_name_gut\ntfidf_name_guy\ntfidf_name_habit\ntfidf_name_habits\ntfidf_name_half\ntfidf_name_hamilton\ntfidf_name_hand\ntfidf_name_handbook\ntfidf_name_hands\ntfidf_name_happened\ntfidf_name_hard\ntfidf_name_hardcover\ntfidf_name_harry\ntfidf_name_haul\ntfidf_name_have\ntfidf_name_haven\ntfidf_name_hayek\ntfidf_name_head\ntfidf_name_heal\ntfidf_name_healing\ntfidf_name_health\ntfidf_name_healthy\ntfidf_name_healthyâ\ntfidf_name_heat\ntfidf_name_heaven\ntfidf_name_hedgehog\ntfidf_name_hell\ntfidf_name_help\ntfidf_name_henna\ntfidf_name_henrietta\ntfidf_name_her\ntfidf_name_hero\ntfidf_name_heroes\ntfidf_name_hidden\ntfidf_name_higher\ntfidf_name_highly\ntfidf_name_hillbilly\ntfidf_name_his\ntfidf_name_historia\ntfidf_name_history\ntfidf_name_holidays\ntfidf_name_homebody\ntfidf_name_honey\ntfidf_name_hornet's\ntfidf_name_horse\ntfidf_name_hour\ntfidf_name_house\ntfidf_name_how\ntfidf_name_howard\ntfidf_name_human\ntfidf_name_humanity\ntfidf_name_humans\ntfidf_name_hunger\ntfidf_name_hungry\ntfidf_name_hurricane\ntfidf_name_hurry\ntfidf_name_hurt\ntfidf_name_hyperbole\ntfidf_name_hypothetical\ntfidf_name_hyrule\ntfidf_name_i\ntfidf_name_ice\ntfidf_name_icons\ntfidf_name_ideas\ntfidf_name_idiots\ntfidf_name_if\ntfidf_name_ii\ntfidf_name_ii's\ntfidf_name_illustrated\ntfidf_name_imagination\ntfidf_name_immortal\ntfidf_name_implications\ntfidf_name_in\ntfidf_name_inches\ntfidf_name_incredible\ntfidf_name_inferno\ntfidf_name_inflatable\ntfidf_name_influence\ntfidf_name_inheritance\ntfidf_name_inky\ntfidf_name_inside\ntfidf_name_inspirational\ntfidf_name_inspired\ntfidf_name_instant\ntfidf_name_insurance\ntfidf_name_intimate\ntfidf_name_into\ntfidf_name_introverts\ntfidf_name_is\ntfidf_name_it\ntfidf_name_it's\ntfidf_name_iv\ntfidf_name_jackson\ntfidf_name_japan\ntfidf_name_japanese\ntfidf_name_jesus\ntfidf_name_jobs\ntfidf_name_jokes\ntfidf_name_jon\ntfidf_name_journal\ntfidf_name_journey\ntfidf_name_joy\ntfidf_name_joyland\ntfidf_name_k\ntfidf_name_kane\ntfidf_name_keep\ntfidf_name_keeps\ntfidf_name_kennedy\ntfidf_name_keto\ntfidf_name_ketogenic\ntfidf_name_key\ntfidf_name_kicked\ntfidf_name_kid\ntfidf_name_kids\ntfidf_name_kill\ntfidf_name_killed\ntfidf_name_killers\ntfidf_name_killing\ntfidf_name_kings\ntfidf_name_kissed\ntfidf_name_kitchen\ntfidf_name_kitties\ntfidf_name_knickerbocker\ntfidf_name_knock\ntfidf_name_know\ntfidf_name_knowledge\ntfidf_name_lacks\ntfidf_name_laden\ntfidf_name_land\ntfidf_name_languages\ntfidf_name_last\ntfidf_name_lasts\ntfidf_name_laugh\ntfidf_name_lead\ntfidf_name_leadership\ntfidf_name_lean\ntfidf_name_leap\ntfidf_name_learn\ntfidf_name_leave\ntfidf_name_lectin\ntfidf_name_lecture\ntfidf_name_legend\ntfidf_name_lego\ntfidf_name_leonardo\ntfidf_name_lessons\ntfidf_name_lethal\ntfidf_name_lettering\ntfidf_name_liberty\ntfidf_name_lies\ntfidf_name_life\ntfidf_name_lifestyle\ntfidf_name_lifetime\ntfidf_name_lift\ntfidf_name_light\ntfidf_name_like\ntfidf_name_lincoln\ntfidf_name_literary\ntfidf_name_litigators\ntfidf_name_little\ntfidf_name_live\ntfidf_name_lived\ntfidf_name_living\ntfidf_name_long\ntfidf_name_looking\ntfidf_name_lord\ntfidf_name_lose\ntfidf_name_losing\ntfidf_name_loss\ntfidf_name_lost\ntfidf_name_loud\ntfidf_name_love\ntfidf_name_loyalty\ntfidf_name_luck\ntfidf_name_machine\ntfidf_name_magic\ntfidf_name_magical\ntfidf_name_magnolia\ntfidf_name_make\ntfidf_name_man\ntfidf_name_man's\ntfidf_name_mandalas\ntfidf_name_manifesto\ntfidf_name_manual\ntfidf_name_mark\ntfidf_name_martian\ntfidf_name_master\ntfidf_name_mastering\ntfidf_name_matter\ntfidf_name_matters\ntfidf_name_maybe\ntfidf_name_mayhem\ntfidf_name_maze\ntfidf_name_mccain\ntfidf_name_me\ntfidf_name_meals\ntfidf_name_meant\ntfidf_name_mechanisms\ntfidf_name_medicine\ntfidf_name_meditation\ntfidf_name_meltdown\ntfidf_name_memoir\ntfidf_name_mental\ntfidf_name_mermaid\ntfidf_name_midnight\ntfidf_name_military\ntfidf_name_milk\ntfidf_name_millennium\ntfidf_name_mind\ntfidf_name_mindfulness\ntfidf_name_minds\ntfidf_name_mindset\ntfidf_name_minute\ntfidf_name_miracles\ntfidf_name_misfits\ntfidf_name_missing\ntfidf_name_mission\ntfidf_name_mockingbird\ntfidf_name_mockingjay\ntfidf_name_modern\ntfidf_name_mole\ntfidf_name_mon\ntfidf_name_moon\ntfidf_name_more\ntfidf_name_mortal\ntfidf_name_moscow\ntfidf_name_most\ntfidf_name_mother\ntfidf_name_motivates\ntfidf_name_much\ntfidf_name_my\ntfidf_name_national\ntfidf_name_navy\ntfidf_name_nba\ntfidf_name_need\ntfidf_name_neptune\ntfidf_name_nest\ntfidf_name_neurosurgeon's\ntfidf_name_never\ntfidf_name_new\ntfidf_name_night\ntfidf_name_nightingale\ntfidf_name_nine\ntfidf_name_no\ntfidf_name_not\ntfidf_name_nouveau\ntfidf_name_novel\ntfidf_name_novella\ntfidf_name_now\ntfidf_name_numbers\ntfidf_name_nutrient\ntfidf_name_nutrition\ntfidf_name_o'reilly's\ntfidf_name_obama\ntfidf_name_odds\ntfidf_name_of\ntfidf_name_off\ntfidf_name_official\ntfidf_name_oh\ntfidf_name_olympian\ntfidf_name_olympians\ntfidf_name_olympics\ntfidf_name_olympus\ntfidf_name_on\ntfidf_name_one\ntfidf_name_oprah's\ntfidf_name_option\ntfidf_name_or\ntfidf_name_organizing\ntfidf_name_origins\ntfidf_name_orphan\ntfidf_name_osama\ntfidf_name_other\ntfidf_name_others\ntfidf_name_our\ntfidf_name_out\ntfidf_name_outliers\ntfidf_name_ove\ntfidf_name_over\ntfidf_name_overwhelmed\ntfidf_name_owls\ntfidf_name_p\ntfidf_name_p.s\ntfidf_name_pacific\ntfidf_name_paine\ntfidf_name_paint\ntfidf_name_paisleyâ\ntfidf_name_palin\ntfidf_name_paperback\ntfidf_name_paradox\ntfidf_name_parent\ntfidf_name_parts\ntfidf_name_passions\ntfidf_name_pastimes\ntfidf_name_path\ntfidf_name_patient\ntfidf_name_patriot's\ntfidf_name_patriotic\ntfidf_name_patriots\ntfidf_name_patrol\ntfidf_name_patton\ntfidf_name_paw\ntfidf_name_peace\ntfidf_name_peel\ntfidf_name_people\ntfidf_name_percy\ntfidf_name_performers\ntfidf_name_person\ntfidf_name_personal\ntfidf_name_pictures\ntfidf_name_pie\ntfidf_name_pilgrims\ntfidf_name_pioneer\ntfidf_name_places\ntfidf_name_plan\ntfidf_name_planet\ntfidf_name_plant\ntfidf_name_played\ntfidf_name_player\ntfidf_name_player's\ntfidf_name_point\ntfidf_name_points\ntfidf_name_pokã\ntfidf_name_politics\ntfidf_name_portrait\ntfidf_name_pot\ntfidf_name_potato\ntfidf_name_potter\ntfidf_name_potty\ntfidf_name_pounds\ntfidf_name_pout\ntfidf_name_power\ntfidf_name_powerful\ntfidf_name_practical\ntfidf_name_prayer\ntfidf_name_pre\ntfidf_name_preschool\ntfidf_name_presence\ntfidf_name_presents\ntfidf_name_presidency\ntfidf_name_president\ntfidf_name_press\ntfidf_name_pressure\ntfidf_name_preventing\ntfidf_name_printsâ\ntfidf_name_prisoner\ntfidf_name_prize\ntfidf_name_program\ntfidf_name_promised\ntfidf_name_promote\ntfidf_name_proof\ntfidf_name_prostitutes\ntfidf_name_psychological\ntfidf_name_psychology\ntfidf_name_publication\ntfidf_name_pulitzer\ntfidf_name_punishment\ntfidf_name_puppy\ntfidf_name_put\ntfidf_name_pyramid\ntfidf_name_quest\ntfidf_name_questions\ntfidf_name_quiet\ntfidf_name_quit\ntfidf_name_quotes\ntfidf_name_race\ntfidf_name_racing\ntfidf_name_racketeer\ntfidf_name_rain\ntfidf_name_rapid\ntfidf_name_readingâ\ntfidf_name_ready\ntfidf_name_reagan\ntfidf_name_real\ntfidf_name_reasons\ntfidf_name_recipes\ntfidf_name_reckoning\ntfidf_name_red\ntfidf_name_redemption\ntfidf_name_references\ntfidf_name_rehearsal\ntfidf_name_relentless\ntfidf_name_resilience\ntfidf_name_restore\ntfidf_name_results\ntfidf_name_reusable\ntfidf_name_revere\ntfidf_name_revised\ntfidf_name_revolution\ntfidf_name_revolutionary\ntfidf_name_rich\ntfidf_name_riddles\ntfidf_name_right\ntfidf_name_ring\ntfidf_name_rising\ntfidf_name_road\ntfidf_name_rogue\ntfidf_name_rolls\ntfidf_name_room\ntfidf_name_routines\ntfidf_name_rules\ntfidf_name_ruling\ntfidf_name_run\ntfidf_name_runner\ntfidf_name_rush\ntfidf_name_sacred\ntfidf_name_saga\ntfidf_name_salt\ntfidf_name_sarah's\ntfidf_name_sat\ntfidf_name_save\ntfidf_name_saved\ntfidf_name_says\ntfidf_name_school\ntfidf_name_scientific\ntfidf_name_score\ntfidf_name_script\ntfidf_name_scripture\ntfidf_name_scrumptious\ntfidf_name_seal\ntfidf_name_second\ntfidf_name_secret\ntfidf_name_secrets\ntfidf_name_see\ntfidf_name_selfish\ntfidf_name_sense\ntfidf_name_serfdom\ntfidf_name_series\ntfidf_name_serious\ntfidf_name_serpent's\ntfidf_name_sesame\ntfidf_name_set\ntfidf_name_sex\ntfidf_name_sh\ntfidf_name_shack\ntfidf_name_shades\ntfidf_name_shadow\ntfidf_name_shame\ntfidf_name_shocking\ntfidf_name_short\ntfidf_name_should\ntfidf_name_silent\ntfidf_name_silly\ntfidf_name_simple\ntfidf_name_site\ntfidf_name_smart\ntfidf_name_sniper\ntfidf_name_solution\ntfidf_name_some\ntfidf_name_sookie\ntfidf_name_soul\ntfidf_name_stackhouse\ntfidf_name_stars\ntfidf_name_start\ntfidf_name_states\ntfidf_name_statistical\ntfidf_name_step\ntfidf_name_steps\ntfidf_name_sticker\ntfidf_name_stone\ntfidf_name_stop\ntfidf_name_storm\ntfidf_name_story\ntfidf_name_strange\ntfidf_name_street\ntfidf_name_strengthsfinder\ntfidf_name_study\ntfidf_name_subtle\ntfidf_name_success\ntfidf_name_sugar\ntfidf_name_sun\ntfidf_name_surprising\ntfidf_name_survival\ntfidf_name_swords\ntfidf_name_talking\ntfidf_name_tea\ntfidf_name_team\ntfidf_name_techniques\ntfidf_name_than\ntfidf_name_that\ntfidf_name_the\ntfidf_name_things\ntfidf_name_thirteen\ntfidf_name_this\ntfidf_name_thousand\ntfidf_name_three\ntfidf_name_thrones\ntfidf_name_thug\ntfidf_name_tidying\ntfidf_name_time\ntfidf_name_to\ntfidf_name_toddlers\ntfidf_name_toltec\ntfidf_name_total\ntfidf_name_tragedy\ntfidf_name_train\ntfidf_name_trauma\ntfidf_name_travel\ntfidf_name_trilogy\ntfidf_name_true\ntfidf_name_trust\ntfidf_name_truth\ntfidf_name_twilight\ntfidf_name_two\ntfidf_name_unbroken\ntfidf_name_underpants\ntfidf_name_unexpected\ntfidf_name_united\ntfidf_name_unlock\ntfidf_name_up\ntfidf_name_us\ntfidf_name_very\ntfidf_name_vol\ntfidf_name_volume\ntfidf_name_war\ntfidf_name_washington's\ntfidf_name_way\ntfidf_name_we\ntfidf_name_week\ntfidf_name_weight\ntfidf_name_what\ntfidf_name_wheat\ntfidf_name_wheel\ntfidf_name_when\ntfidf_name_where\ntfidf_name_white\ntfidf_name_who\ntfidf_name_whole30\ntfidf_name_why\ntfidf_name_wild\ntfidf_name_will\ntfidf_name_wimpy\ntfidf_name_win\ntfidf_name_wisdom\ntfidf_name_with\ntfidf_name_woman\ntfidf_name_women\ntfidf_name_wonder\ntfidf_name_wonderful\ntfidf_name_wonky\ntfidf_name_words\ntfidf_name_workbook\ntfidf_name_world\ntfidf_name_year\ntfidf_name_york\ntfidf_name_you\ntfidf_name_you'll\ntfidf_name_your\ntfidf_name_zoo\n\n\n\n\n1.0031296\n5\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.5012066\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.2687051\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.4643081\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0.5012066\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.5574755\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6022817\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6022817\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0.6022817\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1124394\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n0.7141358\n8\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1065468\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.2146242\n0\n0.1119850\n0\n0\n0\n0.0000000\n0\n0\n0\n0.2529642\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2819287\n0\n0\n0\n0\n0.2957377\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2957377\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2957377\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2146242\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.194396\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.2457391\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2819287\n0\n0\n0\n0\n0\n0\n0.2457391\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1188463\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.1840274\n0\n0.2070697\n0\n\n\n-1.3088208\n11\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6111467\n0\n0\n0\n0\n0\n0\n0\n0.3433987\n0\n0.1791759\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.501728\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.3931826\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.5420535\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.4731803\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.1901541\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.5420535\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.501728\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n1.0031296\n15\n1\n0\n0\n0\n0\n0\n0.7639334\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.4292484\n0\n0.0000000\n0\n0\n0\n0.7639334\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.7639334\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.3022932\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.388792\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.6775669\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.2376926\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n-1.5978146\n4\n1\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.4870709\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.8730668\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.8730668\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0.5469488\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.5616894\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.7167543\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.000000\n0\n0\n0\n0\n0\n0\n0\n0\n0.0000000\n0\n0.0000000\n0\n\n\n\n\n\n\n\nДобавляем препроцессор в воркфлоу.\n\nsvm_wflow &lt;- svm_wflow |&gt; \n  add_recipe(books_rec)\n\nsvm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Support Vector Machine Model Specification (regression)\n\nComputational engine: LiblineaR",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#подгонка-модели",
    "href": "multivar.html#подгонка-модели",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.9 Подгонка модели",
    "text": "23.9 Подгонка модели\nПодгоним модель на обучающих данных.\n\nsvm_fit &lt;- svm_wflow |&gt;\n  fit(data = books_train)\n\nПакет broom позволяет тайдифицировать модель. Посмотрим на слова, которые приводят к “удорожанию” книг. Видно, что в начале списка – слова, связанные с научными публикациями, что не лишено смысла.\n\nsvm_fit |&gt; \n  tidy() |&gt; \n  arrange(-estimate) |&gt; \n  head(6) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\ntfidf_name_5th\n22.13462\n\n\ntfidf_name_diagnostic\n22.13462\n\n\ntfidf_name_disorders\n22.13462\n\n\ntfidf_name_dsm\n22.13462\n\n\ntfidf_name_mental\n22.13462\n\n\ntfidf_name_statistical\n22.13462\n\n\n\n\n\n\n\nОценим модель на контрольных данных.\n\npred_data &lt;- tibble(truth = books_test$price,\n                    estimate = predict(svm_fit, books_test)$.pred)\n\nbooks_metrics &lt;- metric_set(rmse, rsq, mae)\n\nbooks_metrics(pred_data, truth = truth,  estimate = estimate) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n8.0599806\n\n\nrsq\nstandard\n0.2178193\n\n\nmae\nstandard\n3.9472913",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#повторные-выборки",
    "href": "multivar.html#повторные-выборки",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.10 Повторные выборки",
    "text": "23.10 Повторные выборки\nЧтобы не распечатывать каждый раз тестовые данные (в идеале мы их используем один, максимум два раза!), задействуется ряд методов, позволяющих оценить ошибку путем исключения части обучающих наблюдений из процесса подгонки модели и последующего применения этой модели к исключенным наблюдениям.\n\n\n\nИсточник.\n\n\nВ пакете rsample из библиотеки tidymodels реализованы, среди прочего, следующие методы повторных выборок для оценки производительности моделей машинного обучения:\n\nМетод проверочной выборки – набор наблюдений делится на обучающую и проверочную, или удержанную, выборку (validation set): для этого используется initial_validation_split().\nK-кратная перекрестная проверка – наблюдения разбиваются на k групп примерно одинакового размера, первый блок служит в качестве проверочной выборки, а модель подгоняется по остальным k-1 блокам; процедура повторяется k раз: функция vfold_cv().\nПерекрестная проверка Монте-Карло – в отличие от предыдущего метода, создается множество случайных разбиений данных на обучающую и тестовую выборки: функция mc_cv().\nБутстреп – отбор наблюдений выполняется с возвращением, т.е. одно и то же наблюдение может встречаться несколько раз: функция bootstraps().\nПерекрестная проверка по отдельным наблюдениям (leave-one-out сross-validation): одно наблюдение используется в качестве контрольного, а остальные составляют обучающую выборку; модель подгоняется по n-1 наблюдениям, что повторяется n раз: функция loo_cv().\n\nЭти методы повторных выборок позволяют получить надежные оценки производительности моделей машинного обучения, избегая переобучения и обеспечивая репрезентативность тестовых выборок.\n\nset.seed(05102024)\nbooks_folds &lt;- vfold_cv(books_train, v = 10) \n\nset.seed(05102024)\nsvm_rs &lt;- fit_resamples(\n  svm_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nТеперь соберем метрики и убедимся, что предыдущая оценка на контрольных данных была слишком оптимистичной. Однако результат не так уж плох: во всяком случае мы смогли добиться заметного улучшения по сравнению с нулевой моделью.\n\ncollect_metrics(svm_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.7160268\n10\n0.69827622\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4673747\n10\n0.09020819\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\nsvm_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") + \n  theme_minimal() +\n  coord_cartesian(xlim = c(0,50), ylim = c(0,50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#нулевая-модель",
    "href": "multivar.html#нулевая-модель",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.11 Нулевая модель",
    "text": "23.11 Нулевая модель\nКстати, проверим, какой результат даст нулевая модель.\n\nnull_reg &lt;- null_model() |&gt; \n  set_engine(\"parsnip\") |&gt; \n  set_mode(\"regression\")\n\nnull_wflow &lt;- workflow() |&gt; \n    add_model(null_reg) |&gt; \n    add_recipe(books_rec)\n\nnull_rs &lt;- fit_resamples(\n  null_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\n\n\n\n\ncollect_metrics(null_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n10.38916\n10\n1.161447\nPreprocessor1_Model1\n\n\nrsq\nstandard\nNaN\n0\nNA\nPreprocessor1_Model1\n\n\n\n\n\n\n\n\\(R^2\\) в таком случае должен быть NaN.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#пакет-textrecipes",
    "href": "multivar.html#пакет-textrecipes",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.10 Пакет textrecipes",
    "text": "23.10 Пакет textrecipes\nОбновим препроцессор, добавив новые переменные в качестве предикторов.\n\nbooks_rec &lt;- recipe(price ~ rating + reviews + year + genre + name, \n                    data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_tokenize(name)  |&gt; \n  step_tokenfilter(name, max_tokens = 1e3)  |&gt; \n  step_tfidf(name)\n\n\nlm_wflow &lt;- lm_wflow |&gt; \n  update_recipe(books_rec)\n\nlm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_dummy()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nset.seed(05102024)\nlm_rs &lt;- fit_resamples(\n  lm_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x10\n\n\n\n\ncollect_metrics(lm_rs)\n\n\n  \n\n\n\nСущественного улучшения нет: попробуем другой движок.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#регуляризованные-модели",
    "href": "multivar.html#регуляризованные-модели",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.11 Регуляризованные модели",
    "text": "23.11 Регуляризованные модели\n\n\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#регрессионные-алгоритмы",
    "href": "multivar.html#регрессионные-алгоритмы",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "",
    "text": "полиномиальная регрессия: расширение линейной регрессии, позволяющее учитывать нелинейные зависимости.\nлогистическая регрессия: используется для прогнозирования категориальных (бинарных) откликов.\nрегрессия на опорных векторах (SVM): ищет гиперплоскость, позволяющую минимизировать ошибку в многомерном пространстве.\nдеревья регрессии: строят иерархическую древовидную модель, последовательно разбивая данные на подгруппы.\nслучайный лес: комбинирует предсказания множества деревьев для повышения точности и устойчивости.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#регрессия-на-опорных-векторах",
    "href": "multivar.html#регрессия-на-опорных-векторах",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.6 Регрессия на опорных векторах",
    "text": "23.6 Регрессия на опорных векторах\nSupport Vector Regression — это метод машинного обучения, основанный на идеях метода опорных векторов (SVM), но адаптированный к задаче регрессии, а не классификации (о чем см. следующий урок).\nВместо поиска разделяющей гиперплоскости между классами (как в классификации), SVR старается найти функцию, которая:\n\nигнорирует небольшие отклонения внутри некоторого допустимого порога ε (эпсилон),\nакцентирует внимание на точках, которые лежат вне этой “трубы”, — это и есть опорные векторы.\n\nВ этом заключается отличие от обычной регрессии, которая старается проложить прямую, наиболее близкую ко всем точкам и “наказывает” любое отклонение.\nSVR тоже строит линию (или кривую в случае нелинейного ядра), но с другим подходом. Она “довольна”, если предсказание находится в пределах допустимой ошибки ε (эпсилон) от настоящего значения.\nSVR концентрируется только на тех точках, что выходят за эту “зону безразличия” или лежат на ее границе — они называются опорными векторами. Именно они определяют форму и положение модели. Остальные точки (в пределах ε) никак не влияют на модель.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#случайный-лес",
    "href": "multivar.html#случайный-лес",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.11 Случайный лес",
    "text": "23.11 Случайный лес\nУточним, какие движки доступны для случайных лесов.\n\nshow_engines(\"rand_forest\")\n\n\n  \n\n\n\nСоздадим спецификацию модели. Деревья используются как в задачах классификации, так и в задачах регрессии, поэтому задействуем функцию set_mode().\n\nrf_spec &lt;- rand_forest(trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\nrf_wflow &lt;- workflow() |&gt; \n  add_model(rf_spec) |&gt; \n  add_recipe(books_rec)\n\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nОбучение займет чуть больше времени.\n\nrf_rs &lt;- fit_resamples(\n  rf_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nМы видим, что среднеквадратическая ошибка уменьшилась, а доля объясненной дисперсии выросла.\n\ncollect_metrics(rf_rs)\n\n\n  \n\n\n\nТем не менее на графике можно заметить нечто странное: наша модель систематически переоценивает низкие значения и недооценивает высокие. Это связано с тем, что случайные леса не очень подходят для работы с разреженными данными (Hvitfeldt и Silge 2022).\n\nrf_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#многослойный-перцептрон",
    "href": "multivar.html#многослойный-перцептрон",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.12 Многослойный перцептрон",
    "text": "23.12 Многослойный перцептрон\nТакже попробуем построить регрессию с использованием нейросетевой модели.\n\nnnet_spec &lt;- \n  mlp(epochs = 1000, \n      hidden_units = 1, \n      penalty = 0.01, \n      learn_rate = 0.01) |&gt; \n  set_engine(\"brulee\") |&gt; \n  set_mode(\"regression\")\n\n\nnnet_wflow &lt;- workflow() |&gt; \n  add_model(nnet_spec) |&gt; \n  add_recipe(books_rec)\n\nnnet_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: mlp()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nSingle Layer Neural Network Model Specification (regression)\n\nMain Arguments:\n  hidden_units = 1\n  penalty = 0.01\n  epochs = 1000\n  learn_rate = 0.01\n\nComputational engine: brulee \n\n\nНа этом этапе может быть предложено установить дополнительные пакеты; соглашаемся.\n\nnnet_rs &lt;- fit_resamples(\n  nnet_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\n\n\ncollect_metrics(nnet_rs)\n\n\n  \n\n\n\nНейросеть с одним скрытым слоем в нашем случае показала себя хуже, чем опорные векторы. Напомним, что нам удалось добиться увеличения \\(R^2\\) до 0.369. Возможно, получится немного улучшить этот результат.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#удаление-стопслов",
    "href": "multivar.html#удаление-стопслов",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.16 Удаление стопслов",
    "text": "23.16 Удаление стопслов\nИзменим рецепт приготовления данных.\n\nstopwords_rec &lt;- function(stopwords_name) {\n  recipe(price ~ year + genre + name, data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_normalize(year) |&gt; \n  step_tokenize(name)  |&gt; \n  step_stopwords(name, stopword_source = stopwords_name) |&gt; \n  step_tokenfilter(name, max_tokens = 1000)  |&gt; \n  step_tfidf(name) \n}\n\nСоздадим воркфлоу.\n\nsvm_wflow &lt;- workflow() |&gt; \n  add_model(svm_spec)\n\nИ снова проведем перекрестную проверку, на этот раз с разными списками стоп-слов. На этом шаге команда вернет предупреждения о том, что число слов меньше 1000, это нормально, т.к. после удаления стопслов токенов стало меньше.\n\nset.seed(123)\nsnowball_rs &lt;- fit_resamples(\n  svm_wflow |&gt;  add_recipe(stopwords_rec(\"snowball\")),\n  books_folds\n)\n\nset.seed(234)\nsmart_rs &lt;- fit_resamples(\n  svm_wflow |&gt; add_recipe(stopwords_rec(\"smart\")),\n  books_folds\n)\n\nset.seed(345)\nstopwords_iso_rs &lt;- fit_resamples(\n  svm_wflow |&gt; add_recipe(stopwords_rec(\"stopwords-iso\")),\n  books_folds\n)\n\n\ncollect_metrics(smart_rs)  |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.881703\n10\n0.39913002\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.442588\n10\n0.08075147\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics(snowball_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.6361664\n10\n0.41158411\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4582287\n10\n0.08018042\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics((stopwords_iso_rs)) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n8.0260370\n10\n0.35141531\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4388275\n10\n0.07788576\nPreprocessor1_Model1\n\n\n\n\n\n\n\nВ нашем случае удаление стоп-слов положительного эффекта не имело.\n\nword_counts &lt;- tibble(name = c(\"snowball\", \"smart\", \"stopwords-iso\")) %&gt;%\n  mutate(words = map_int(name, ~length(stopwords::stopwords(source = .))))\n\nlist(snowball = snowball_rs,\n     smart = smart_rs,\n     `stopwords-iso` = stopwords_iso_rs)  |&gt; \n  map_dfr(show_best, metric = \"rmse\", .id = \"name\")  |&gt; \n  left_join(word_counts, by = \"name\")  |&gt; \n  mutate(name = paste0(name, \" (\", words, \" words)\"),\n         name = fct_reorder(name, words))  |&gt; \n  ggplot(aes(name, mean, color = name)) +\n  geom_crossbar(aes(ymin = mean - std_err, ymax = mean + std_err), alpha = 0.6) +\n  geom_point(size = 3, alpha = 0.8) +\n  theme(legend.position = \"none\") + \n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#настройки-числа-n-grams",
    "href": "multivar.html#настройки-числа-n-grams",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.17 Настройки числа n-grams",
    "text": "23.17 Настройки числа n-grams\n\nngram_rec &lt;- function(ngram_options) {\n  recipe(price ~ year + genre + name, data = books_train) |&gt; \n  step_dummy(genre)  |&gt; \n  step_normalize(year) |&gt; \n  step_tokenize(name, token = \"ngrams\", options = ngram_options)  |&gt; \n  step_tokenfilter(name, max_tokens = 1000)  |&gt; \n  step_tfidf(name) \n}\n\n\nfit_ngram &lt;- function(ngram_options) {\n  fit_resamples(\n    svm_wflow  |&gt; \n    add_recipe(ngram_rec(ngram_options)),\n    books_folds\n  )\n}\n\n\nset.seed(123)\nunigram_rs &lt;- fit_ngram(list(n = 1))\n\nset.seed(234)\nbigram_rs &lt;- fit_ngram(list(n = 2, n_min = 1))\n\nset.seed(345)\ntrigram_rs &lt;- fit_ngram(list(n = 3, n_min = 1))\n\n\ncollect_metrics(unigram_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.7160268\n10\n0.69827622\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4673747\n10\n0.09020819\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics(bigram_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.2108600\n10\n0.38677343\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4771493\n10\n0.07009008\nPreprocessor1_Model1\n\n\n\n\n\n\ncollect_metrics(trigram_rs) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.479379\n10\n0.44739644\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.467884\n10\n0.06368306\nPreprocessor1_Model1\n\n\n\n\n\n\n\nТаким образом, униграмы дают лучший результат:\n\nlist(`1` = unigram_rs,\n     `1 and 2` = bigram_rs,\n     `1, 2, and 3` = trigram_rs) |&gt; \n  map_dfr(collect_metrics, .id = \"name\")  |&gt; \n  filter(.metric == \"rmse\")  |&gt; \n  ggplot(aes(name, mean, color = name)) +\n  geom_crossbar(aes(ymin = mean - std_err, ymax = mean + std_err), \n                alpha = 0.6) +\n  geom_point(size = 3, alpha = 0.8) +\n  theme(legend.position = \"none\") +\n  labs(\n    y = \"RMSE\"\n  ) + \n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#лучшая-модель-и-оценка",
    "href": "multivar.html#лучшая-модель-и-оценка",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.18 Лучшая модель и оценка",
    "text": "23.18 Лучшая модель и оценка\n\nsvm_fit &lt;- svm_wflow |&gt;\n  add_recipe(books_rec) |&gt; \n  fit(data = books_test)\n\nWarning: max_tokens was set to 1000, but only 519 was available and selected.\n\nsvm_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\n$TypeDetail\n[1] \"L2-regularized L2-loss support vector regression primal (L2R_L2LOSS_SVR)\"\n\n$Type\n[1] 11\n\n$W\n          year genre_Non.Fiction tfidf_name_1 tfidf_name_10 tfidf_name_100\n[1,] -1.520238          2.066713    -4.793447     -2.403936      -1.802735\n     tfidf_name_11 tfidf_name_14 tfidf_name_15 tfidf_name_1936 tfidf_name_2.0\n[1,]      3.607565   -0.01058476    -0.4253752      -0.2957425      0.8642611\n     tfidf_name_3 tfidf_name_4 tfidf_name_451 tfidf_name_49 tfidf_name_5\n[1,]    -1.996153    -1.894541      -0.349303       1.02874    -1.376993\n     tfidf_name_500 tfidf_name_6 tfidf_name_6th tfidf_name_7 tfidf_name_a\n[1,]      0.3061039   -0.8640108        7.62841     2.724701    -2.022854\n     tfidf_name_about tfidf_name_acid tfidf_name_act tfidf_name_adult\n[1,]        0.8368777        1.897636      0.4972617        -1.490207\n     tfidf_name_adults tfidf_name_advanced tfidf_name_after\n[1,]       -0.09387077           0.3061039       -0.2481804\n     tfidf_name_afterlife tfidf_name_ages tfidf_name_agreements tfidf_name_air\n[1,]           -0.9194414      -0.5591052              -1.30218      0.5562863\n     tfidf_name_alchemist tfidf_name_all tfidf_name_alphabet tfidf_name_am\n[1,]             9.643137       1.518092          -0.5591052    0.05458998\n     tfidf_name_amazing tfidf_name_america's tfidf_name_american\n[1,]          -1.391954            0.0276968            5.288851\n     tfidf_name_americans tfidf_name_an tfidf_name_ancient tfidf_name_and\n[1,]           -0.2957425     0.4403215          0.0276968     -0.9599414\n     tfidf_name_angie's tfidf_name_animal tfidf_name_animals tfidf_name_are\n[1,]         -0.1783316        -0.2104393          -1.843032      0.4204923\n     tfidf_name_art tfidf_name_as tfidf_name_association tfidf_name_astounding\n[1,]     0.09081542    -0.3419302                7.62841            -0.4254951\n     tfidf_name_at tfidf_name_atomic tfidf_name_back tfidf_name_bad\n[1,]     0.0524675       -0.03391317       -3.710758    -0.03391317\n     tfidf_name_balance tfidf_name_ball tfidf_name_barefoot tfidf_name_be\n[1,]         -0.1783316     -0.01058476            1.437301     0.4204923\n     tfidf_name_bear tfidf_name_beasts tfidf_name_beautiful tfidf_name_become\n[1,]       -1.160322          1.833383           0.05458998         0.4204923\n     tfidf_name_becomes tfidf_name_beginners tfidf_name_believing\n[1,]          0.5562863            0.3061039            0.4204923\n     tfidf_name_berlin tfidf_name_better tfidf_name_between tfidf_name_big\n[1,]          1.278907          1.080442        -0.05331562     -0.5591052\n     tfidf_name_bill tfidf_name_birth tfidf_name_blood tfidf_name_boat\n[1,]       0.3222009         1.167256       -0.8230085      -0.2957425\n     tfidf_name_book tfidf_name_books tfidf_name_boxed tfidf_name_boy's\n[1,]       -9.466271        -3.582869         4.565776       -0.4254951\n     tfidf_name_boys tfidf_name_brave tfidf_name_brawl tfidf_name_break\n[1,]      -0.2957425       0.05458998       -0.8640108      -0.03391317\n     tfidf_name_breaking tfidf_name_breath tfidf_name_brigance tfidf_name_brown\n[1,]           -3.582468         0.5562863            1.301934        -1.160322\n     tfidf_name_build tfidf_name_bundo tfidf_name_burn tfidf_name_by\n\n...\nand 238 more lines.\n\n\nВзглянем на остатки. Для этого пригодится уже знакомая функция augment() из пакета broom.\n\nsvm_res &lt;- augment(svm_fit, new_data = books_test) |&gt; \n  mutate(res = price - .pred) |&gt; \n  select(price, .pred, res)\n\nsvm_res |&gt; \n  head(6) |&gt; \n  gt::gt()\n\n\n\n\n\n\n\nprice\n.pred\nres\n\n\n\n\n8\n8.429922\n-0.429922461\n\n\n5\n5.602691\n-0.602691055\n\n\n17\n16.178382\n0.821618312\n\n\n4\n5.445476\n-1.445476436\n\n\n6\n6.176515\n-0.176515259\n\n\n6\n6.001730\n-0.001730247\n\n\n\n\n\n\n\n\nlibrary(gridExtra)\n\ng1 &lt;- svm_res |&gt; \n  mutate(res = price - .pred) |&gt; \n  ggplot(aes(res)) +\n  geom_histogram(fill = \"steelblue\", color  = \"white\") +\n  theme_minimal()\n\ng2 &lt;- svm_res |&gt; \n  ggplot(aes(price, .pred)) +\n  geom_jitter(color = \"steelblue\", alpha = 0.7) +\n  geom_abline(linetype = 2, color = \"grey80\", linewidth = 2) +\n  theme_minimal()\n\ngrid.arrange(g1, g2, nrow = 1)\n\n\n\n\n\n\n\n\nСоберем метрики.\n\nbooks_metrics &lt;- metric_set(rmse, rsq, mae)\nbooks_metrics(svm_res, truth = price,  estimate = .pred)\n\n\n  \n\n\n\nТакже посмотрим, какие слова больше всего связаны с увеличением и с уменьшением цены.\n\nsvm_fit |&gt; \n  tidy() |&gt; \n  filter(term != \"year\") |&gt; \n  filter(!str_detect(term, \"genre\")) |&gt; \n  mutate(sign = case_when(estimate &gt; 0 ~ \"дороже\",\n                          .default = \"дешевле\"),\n         estimate = abs(estimate), \n         term = str_remove_all(term, \"tfidf_name_\")) |&gt; \n  group_by(sign) |&gt; \n  top_n(20, estimate) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(x = estimate, y = fct_reorder(term, estimate),\n             fill = sign)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(expand = c(0,0)) +\n  facet_wrap(~sign, scales = \"free\") +\n  labs(y = NULL, \n       title = \"Связь слов с ценой книг\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nЛюбопытно: судя по нашему датасету, конституция США раздается на Амазоне бесплатно.\n\n\n\n\nHvitfeldt, Emil, и Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. Taylor; Francis.\n\n\nГ. Джеймс, Д. Уиттон, Т. Хасти, Р. Тибришани. 2017. Введение в статистическое обучение с примерами на языке R. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#градиентные-бустинговые-деревья",
    "href": "multivar.html#градиентные-бустинговые-деревья",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.15 Градиентные бустинговые деревья",
    "text": "23.15 Градиентные бустинговые деревья\nТакже попробуем построить регрессию с использованием градиентных бустинговых деревьев. В 2023 г. эта техника показала хорошие результаты в эксперименте по датировке греческих документальных папирусов.\n\nxgb_spec &lt;- \n  boost_tree(mtry = 50, trees = 1000)  |&gt; \n  set_engine(\"xgboost\")  |&gt; \n  set_mode(\"regression\")\n\n\nxgb_wflow &lt;- workflow() |&gt; \n  add_model(xgb_spec) |&gt; \n  add_recipe(books_rec)\n\nxgb_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = 50\n  trees = 1000\n\nComputational engine: xgboost \n\n\nПроводим перекрестную проверку.\n\nxgb_rs &lt;- fit_resamples(\n  xgb_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\ncollect_metrics(xgb_rs)  |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n7.4865979\n10\n0.52910597\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.4834618\n10\n0.07396304\nPreprocessor1_Model1\n\n\n\n\n\n\n\nМетрики неплохие! Но если взглянуть на остатки, можно увидеть что-то вроде буквы S.\n\nrf_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "plot.html#информативный-дизайн",
    "href": "plot.html#информативный-дизайн",
    "title": "3  Визуализации",
    "section": "3.10 Информативный дизайн",
    "text": "3.10 Информативный дизайн\nПоскольку нас интересует доля женщин, логично поменять группы местами.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  # меняем местами группы\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  coord_flip() +\n  # разные мелочи\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nТакже поменяем порядок, в котором идут декады (от меньшей к большей).\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE)) +\n  # меняем порядок лет\n  scale_x_reverse() +\n  coord_flip() +\n  ylab(NULL) + \n  xlab(NULL) + \n  theme_void()\n\n\n\n\n\n\n\n\nУбавим цвет в мужской части диаграммы и добавим заголовки.\n\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           # обводим столбики \n           color = \"darkred\", \n           # убираем легенду\n           show.legend = FALSE) +\n  scale_x_reverse() +\n  # беремся за палитру\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  # меняем цвет и шрифт текста\n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"))\n\n\n\n\n\n\n\n\nСтоит подвинуть заголовок и убрать просветы между столбцами.\n\n# почти ничего нового!\nnoveltm_new |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = position_fill(reverse = TRUE),\n           color = \"darkred\", \n           show.legend = FALSE,\n           # столбик во всю ширину\n           width = 10\n) +\n  # добавляем делений на оси\n  scale_x_reverse(breaks = seq(1800, 2000, 10)) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() +\n  theme_void() + \n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        axis.text = element_text(color = \"darkred\"),\n        # выравниваем заголовок\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПостройте несколько графиков с использованием датасета starwars из пакета dplyr. Используйте тему “Звездных войн” из ThemePark.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "lda.html#подготовка-данных",
    "href": "lda.html#подготовка-данных",
    "title": "14  Тематическое моделирование c LDA",
    "section": "14.3 Подготовка данных",
    "text": "14.3 Подготовка данных\nЧтобы понять возможности алгоритма, мы попробуем передать ему тот же новостной архив (ссылка для скачивания). На новостях сразу видно адекватность модели; но это не значит, что применение LDA ограничено подобными сюжетами. Этот метод с успехом применяется, например, в историко-научных или литературоведческих исследованиях. Он хорошо подходит, если необходимо на основе журнального архива описать развитие некоторой области знания. Но сейчас нам подойдет пример попроще 👶\n\nlibrary(tidyverse)\nload(\"../data/news_tokens_pruned.Rdata\")\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПоскольку LDA – вероятностная модель, то на входе она принимает целые числа. В самом деле, не имеет смысла говорить о том, что некое распределение породило 0.5 слов или того меньше. Поэтому мы считаем абсолютную, а не относительную встречаемость – и не tf_idf.\n\nnews_counts &lt;- news_tokens_pruned |&gt; \n  count(token, id)\n\nnews_counts",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Тематическое моделирование c LDA</span>"
    ]
  },
  {
    "objectID": "llm.html",
    "href": "llm.html",
    "title": "25  Работа с LLM",
    "section": "",
    "text": "25.1 Пакет {rollama}\nДля начала работы с LLM зайдите на сайт https://ollama.com/ и скачайте приложение. Следуйте инструкции по его установке.\nПосле этого в R установите пакет {rollama} и проверьте соединение.\nlibrary(rollama)\nping_ollama()\nТеперь загрузим новую модель. Доступные модели доступны здесь: https://ollama.com/search. Мы используем модель, ориентированную на выполнение пользовательских инструкций. Такие модели могут лучше справляться с аннотированием текста. Следует иметь в виду, что Llama 3.2. поддерживает только английский, немецкий, французский, итальянский, португальский, хинди, испанский и тайский.\noptions(rollama_model = \"llama3.2:1b-instruct-q8_0\")\npull_model()\n#&gt; model llama3.2:1b-instruct-q8_0 pulled successfully\nВыбор модели зависит от конкретных задач и возможностей компьютера. Рекомендуемый размер оперативной памяти (RAM) – в 4-8 раз больше, чем размер модели в гигабайтах. Эта глава написана на компьютере с 8 Гб оперативной памяти, так что автор мог экспериментировать лишь с самыми легкими LLM.\nrollama::list_models()\nrollama::delete_model(\"llava-phi3\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#пакет-ellmer",
    "href": "llm.html#пакет-ellmer",
    "title": "24  Работа с LLM",
    "section": "24.2 Пакет {ellmer}",
    "text": "24.2 Пакет {ellmer}\nПакет {ellmer} дает больше возможностей для работы с моделями как через API, так и локально. Повторим наш вопрос, но на этот раз добавим системный промпт.\nСистемный промпт - это специальная инструкция, которая передается языковой модели (LLM) для задания контекста и ограничений при генерации текста. Он включать в себя следующие элементы:\n\nОписание роли: четкое определение того, какую роль должен взять на себя LLM при генерации текста. Например, “Ты научный помощник, который помогает объяснять сложные концепции простым языком”.\nРуководящие принципы: набор правил или инструкций, которым должен следовать LLM, такие как “Всегда будь вежливым и профессиональным” или “Не генерируй контент, который может быть вредным или незаконным”.\nТематические ограничения: Определение тем, о которых LLM может/не может генерировать текст, например, “Отвечай только на вопросы, касающиеся истории и философии, избегай политических тем”.\nСтилистические указания: Рекомендации по использованию определенного стиля, тона, длины ответов и других характеристик генерируемого текста.\n\nСистемный промпт играет ключевую роль в настройке поведения и вывода LLM для конкретной задачи или контекста.\n\nlibrary(ellmer)\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = \"Reply in poetic form.\")\n. &lt;- chat$chat(\"Who is Socrates?\", echo = FALSE)\nchat\n\n&lt;Chat turns=3 tokens=13/308&gt;\n── system ──────────────────────────────────────────────────────────────────────\nReply in poetic form.\n── user ────────────────────────────────────────────────────────────────────────\nWho is Socrates?\n── assistant ───────────────────────────────────────────────────────────────────\n&lt;think&gt; Alright, so the user asked for a response to \"Who is Socrates?\" written\nin poetic form. I need to create a poem that reflects on Socrates but keeps it\nin a poetic way.\n\nFirst, I should think about what makes Socrates unique. He's known for his\ndialogues but also associated with Socratic methods. Maybe focus on the method\nor his approach to discovery.\n\nI want the poem to have a flowing tone and maybe some imagery related to light\nand wisdom. Maybe compare him to an ancient mind that is never old.\n\nAlso, I should touch on how he taught people how to learn from himself rather\nthan others. Emphasize the continuous journey or the search for truth through\nquestioning.\n\nLet me consider how to structure it: possibly a few stanzas with each line\nbuilding up the theme. Maybe start by introducing him, then describe his method\nof discovery, and end on wisdom or the search within himself.\n\nI need to avoid technical terms and keep it poetic without losing the essence.\nUse metaphors that relate to light, growth, and discovery.\n\nOkay, let's draft some lines. &lt;/think&gt;\n\nIn shadowed minds where truth unfolds Socrates stands at his highest place, A\nmind ancient, eternal, unchanging, Guided by the voice of wisdom.\n\nHe led with him a world of light, Where logic and reason met, His reign of\ndiscovery never fails.  From youth to old, he walked like an oar, In seeking\ntruth never ends.\n\n\nЗа расходом токенов полезно следить, если обращаетесь к модели по подписке. В нашем случае никаких ограничений нет.\n\ntoken_usage()",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#дизайн-промпта",
    "href": "llm.html#дизайн-промпта",
    "title": "24  Работа с LLM",
    "section": "24.3 Дизайн промпта",
    "text": "24.3 Дизайн промпта\nНебольшой вопрос можно сохранить в окружение.\n\nquestion &lt;- \"Who is Socrates?\"\n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\")\nchat$chat(question)\n\nНо в большинстве случаев вы будете создавать развесистые системные и пользовательские промпты, которые лучше сразу сохранять в markdown-документе.\n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = \"You are Cicero. \")\nchat$chat(question)\n\n&lt;think&gt;\nOkay, so I'm trying to learn about who Julius Caesar was. From what I remember \nin history class, he was a important figure during the Roman Empire, and his \nname seems to be tied to Caesar's Leadership Academy where educated people were\nencouraged. So, I think he was pretty influential.\n\nBut when I try to dive deeper, I feel like there's more to him. I guess it all \nstarted with him killing Caesar in a fight at the Battle of Actium. That sounds\nlike a big deal and maybe some kind of conspiracy theory now, but I don't want \nto get bogged down by that part.\n\nNow, on the political side, he was a great ruler who expanded his territory \ninto the Mediterranean, which includes areas like Libya, Sicily, and Tyrconia \nin Italy. He did this with some pretty brutal methods, like decimating the \npeople of his domain, right? I think he went into positions that were seen as \nimportant for power, but some people might have felt manipulated or overthrown \nthat way.\n\nThen there's Caesar's Leadership Academy, which is supposed to promote \neducation and give more middle-class people a voice. That sounds interesting \nbecause it contrasts with the rise of materialism in Rome during that time. \nWhen they implemented the ideas at the Academy, maybe some of them didn't like \nhow it affected higher classes.\n\nI'm also trying to remember his actions after the fall of Rome, like he went \nback to his family lands and became one of France's kings. There are images \nthat make him look really noble and generous, but I'm not sure if he was as \nkind as he seemed today or just a character design by someone for the story.\n\nI want to know more about the legacy of Julius Caesar. He was a key figure in \nRoman history, especially regarding the expansion of their empire into the \nMediterranean. His method of governance might have had bad consequences both \npersonally and for others. Now I think about how this relates to modern times \nor other historical contexts. Maybe someone studied him from a modern \nviewpoint, but I don't know much beyond that.\n\nI'm still curious about his personal beliefs. Was he really more of an \nidealist, or was there something else I'm not seeing? Did he have any issues \nwith religious or philosophical views at the time? It would be interesting to \nfind out because it could change how we view his actions and their impact on \nsociety.\n\nAnother thing is, when he went as a soldier in history, he might have had \nspecific motivations. Like fighting Caesar's forces and wanting to protect Rome\nfrom another king. That makes sense now that I think about it, but I didn't \nrealize there was such detail before.\n\nSo, putting this all together, Julius Caesar was a complex figure with both \nhistorical significance and personal details that are often exaggerated for \nstory purposes. His actions were part of his character, but they also had \nlarger implications for the political system and his influence on subsequent \nleaders in Rome.\n&lt;/think&gt;\n\nJulius Caesar (580 BC – 643 BC) was a pivotal figure in the history of the \nRoman Empire, marked by both historical significance and exaggerated personal \ndetails. His reign of control over Italy began when he led a group of soldiers \nwho fought off a powerful opponent at the Battle of Actium. This event is often\ncited as a source of conspiracy belief among later centuries.\n\nAs an emperor of Rome, Caesar expanded his territories into the Mediterranean, \nincluding areas such as Libya and Sicili, and gained widespread approval from \nthe educated class. This influence on Roman society and culture may have \ncontributed to his later role in the formation of Caesar's Leadership Academy, \nwhere the academy was intended to promote education and give middle-class \nindividuals more influence in political matters.\n\nAfter the fall of Rome, Caesius returned to Italy under Alexander, becoming one\nof France's kings. While images of him appear as noble figures due to design \nchoices, his personality and actions also reflect personal beliefs that were \noften misunderstood during his time. He is known for his generous approach and \nactive participation in both military and leadership activities.\n\nCaesar's contributions to Roman history are significant, particularly in the \ncontext of expanding Italy to include parts of the Mediterranean, which had \nmixed effects on political structure. His reign is remembered for his bravery \nand adaptability as an emperor but also for his controversial actions that may \nhave led to personal conflicts within the empire.\n\nIn summary, Julius Caesar was a complex figure whose history was both rich and \nnuanced. His reign in Rome demonstrated strategic dominance, yet his personal \nactions also revealed underlying complexities tied to beliefs and institutions \nof the time.\n\n\nВот резюме рекомендаций по написанию промпта для моделей на основе предоставленной информации:\n\nИспользуйте markdown для оформления промптов, чтобы сделать их более читабельными как для LLM, так и для людей.\nХраните каждый промпт в отдельном файле с информативным названием, например, “prompt-extract-metadata.md”.\nВедите контроль версий промптов с помощью Git для отслеживания изменений. 4. Используйте функцию ellmer::chat() для интеграции динамических данных в промпт.\nСоздавайте набор примеров для регулярной проверки промпта.\nПри генерации кода используйте как системный промпт (для общего поведения), так и пользовательский промпт (для конкретного вопроса).\nБудьте максимально конкретными в требованиях к выходному коду, используя примеры и подробные описания. 8. Если модель не использует новые языковые возможности, предоставляйте ей примеры, чтобы научить этому.\nПри извлечении структурированных данных используйте многошотовые промпты с примерами входных и выходных данных.\nВключайте в выходные данные исходные входные данные для облегчения сопоставления.\n\nОсновная идея - создавать подробные, структурированные промпты, которые точно направляют модель на желаемый результат, и использовать лучшие практики для управления и итерации промптов.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#установка-ollama",
    "href": "llm.html#установка-ollama",
    "title": "24  Работа с LLM",
    "section": "",
    "text": "resp &lt;- generate(\"deepseek-r1:1.5b\", \n                 system = \"Reply in one sentence.\",\n                 prompt = \"Who is Socrates?\") \n\nresp_process(resp, \"text\")  |&gt; \n  cat()",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#дизайн-промтпа",
    "href": "llm.html#дизайн-промтпа",
    "title": "25  Работа с LLM",
    "section": "25.2 Дизайн промтпа",
    "text": "25.2 Дизайн промтпа\nВ коде выше мы добавили системный промпт. Системный промпт - это специальная инструкция, которая передается языковой модели (LLM) для задания контекста и ограничений при генерации текста. Он включать в себя следующие элементы:\n\nОписание роли: четкое определение того, какую роль должен взять на себя LLM при генерации текста. Например, “Ты научный помощник, который помогает объяснять сложные концепции простым языком”.\nРуководящие принципы: набор правил или инструкций, которым должен следовать LLM, такие как “Всегда будь вежливым и профессиональным” или “Не генерируй контент, который может быть вредным или незаконным”.\nТематические ограничения: Определение тем, о которых LLM может/не может генерировать текст, например, “Отвечай только на вопросы, касающиеся истории и философии, избегай политических тем”.\nСтилистические указания: Рекомендации по использованию определенного стиля, тона, длины ответов и других характеристик генерируемого текста.\n\nСистемный промпт играет ключевую роль в настройке поведения и вывода LLM для конкретной задачи или контекста.\nИ системный, пользовательский промпт можно хранить в окружении, если они небольшие.\n\nquestion &lt;- \"Who is Socrates?\"\n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\")\nchat$chat(question)\n\nНо в большинстве случаев вы будете создавать развесистые промпты, которые лучше сразу сохранять в markdown-документе, причем под контролем версий, чтобы можно было вернуться к прежним версиям. Давайте таким файлам информативные названия, например prompt-extract-metadata.md. В этом случае код выглядит как-то так.\n\nquestion &lt;- readLines(\"user_prompt.Rmd\", warn = FALSE) \n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = readLines(\"system_prompt.Rmd\", warn = FALSE))\n\nchat$chat(question)\n\nПри написании промпта будьте максимально конкретными в требованиях, используя примеры и подробные описания. Используйте синтаксис markdown – LLM его понимают.\nДалее рассмотрим несколько задач, которые можно решать при помощи LLM.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#распознавание-изображений",
    "href": "llm.html#распознавание-изображений",
    "title": "24  Работа с LLM",
    "section": "24.3 Распознавание изображений",
    "text": "24.3 Распознавание изображений\nЧату можно передать ссылку на изображение, путь к файлу или график (подробнее). Мы попросим распознать вот эту страницу:\n\nСначала загрузим небольшую (3.8b, 2.3 Гб) модель для работы с изображениями.\n\nollamar::pull(\"llava-phi3\")\n\n\nchat &lt;- chat_ollama(model = \"llava-phi3\")\n\nchat$chat(\n  \"Read the Latin text in the image. Do not translate or explain. Just return the text as you see it.\",\n  content_image_file(\"./images/latin.png\")\n)\n\nPROLEGOMA IN PHILEREMU. Philebum Platonicus qui recte interpretatio vocabuli \nnostra sententia ante ominia disponit anima et scio duo sedulo agenda sunt. \nPrimum enim omnis disputationis ordina, quae non videtur vestigationemque \negregie considerande, concerte acquirem. Sive autem voluere in toto volueri \nipsam intelligentiam dicariam insolita et necessaria sunt. Quid autem tertiam \nsit quadam alia quae non cogitur me cum videtur et ratio, uitique \ndifficultissime intellec ct,\n\n\nОшибок очень много, потому что модель слабенькая. Хорошо с задачей справляются облачные модели, которые размещены и обрабатывается на серверах провайдера (например, Anthropic, OpenAI, Google). Вы взаимодействуете с моделью через API, отправляя запросы и получая ответы, а все вычисления выполняются на серверах провайдера. Но это требует подписки и, на 2025 г., VPN.\n\n\n\n\n\n\nНа заметку\n\n\n\nПрежде чем прибегать к более серьезным решениям, обратите внимание на пакет {tesseract}, о котором рассказывалось подробнее в курсе 2023 г. Если текст печатный и приличного качества, возможно, задача решается и без LLM. По ссылке можно посмотреть код, как при помощи {tesseract} решается задача распознавания множества pdf с текстом на трех языках.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#пересказ-текста",
    "href": "llm.html#пересказ-текста",
    "title": "25  Работа с LLM",
    "section": "25.7 Пересказ текста",
    "text": "25.7 Пересказ текста",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#добавление-разметки",
    "href": "llm.html#добавление-разметки",
    "title": "25  Работа с LLM",
    "section": "25.8 Добавление разметки",
    "text": "25.8 Добавление разметки",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#анализ-тональности",
    "href": "llm.html#анализ-тональности",
    "title": "25  Работа с LLM",
    "section": "25.6 Анализ тональности",
    "text": "25.6 Анализ тональности",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#инструменты",
    "href": "llm.html#инструменты",
    "title": "25  Работа с LLM",
    "section": "25.11 Инструменты",
    "text": "25.11 Инструменты",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#структурированные-данные",
    "href": "llm.html#структурированные-данные",
    "title": "25  Работа с LLM",
    "section": "25.6 Структурированные данные",
    "text": "25.6 Структурированные данные",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "24  Бинарная классификация",
    "section": "",
    "text": "24.1 Записки “Федералиста”\nВ предыдущих двух уроках мы познакомились с регрессией, а в этом поговорим о классификации. Алгоритмов классификации в МО великое множество, в этом уроке мы рассмотрим два из них: линейно-дискриминантный анализ и наивный Байес, а также научимся подбирать гиперпараметры модели.\nВ 1963 году два американских статистика, Фредерик Мостеллер и Дэвид Уоллес, опубликовали статью «Inference in an Authorship Problem», в которой они успешно разрешили вопрос о том, кто написал 12 спорных памфлетов из «Записок федералиста» — сборника статей в поддержку утверждения Конституции США (кон. XVIII в.).\nКандидатами в авторы 12 спорных памфлетов были Джеймс Мэдисон (четвертый президент США) и Александр Гамильтон (соратник Джорджа Вашингтона, основоположник американской экономической системы). Гамильтон и Мэдисон писали в схожей ораторской манере, и в некоторых отношениях были практически стилистическими «близнецами». Однако статистикам удалось найти способ их различить.\nВ распоряжении статистиков были методы традиционной фишеровской статистики (дискриминантный анализ, предложенный в 1936 г.), но они впервые решили дополнить его байесовскими методами, что можно считать рождением алгоритма, известного сегодня в МО под именем Наивный Байес. Кроме того, Мостеллер и Уоллес впервые показали, что для решения вопроса об авторстве важны наиболее частотные слова, употребление которых человек почти не контролирует. Впоследствии это наблюдение легло в основу метода, предложенного Берроузом.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "llm.html#эмбеддинги",
    "href": "llm.html#эмбеддинги",
    "title": "25  Работа с LLM",
    "section": "25.9 Эмбеддинги",
    "text": "25.9 Эмбеддинги",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#классификация",
    "href": "llm.html#классификация",
    "title": "25  Работа с LLM",
    "section": "25.10 Классификация",
    "text": "25.10 Классификация",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#системный-промпт",
    "href": "llm.html#системный-промпт",
    "title": "25  Работа с LLM",
    "section": "25.2 Системный промпт",
    "text": "25.2 Системный промпт\nСистемный промпт - это специальная инструкция, которая передается языковой модели (LLM) для задания контекста и ограничений при генерации текста. Он включать в себя следующие элементы:\n\nОписание роли: четкое определение того, какую роль должен взять на себя LLM при генерации текста. Например, “Ты научный помощник, который помогает объяснять сложные концепции простым языком”.\nРуководящие принципы: набор правил или инструкций, которым должен следовать LLM, такие как “Всегда будь вежливым и профессиональным” или “Не генерируй контент, который может быть вредным или незаконным”.\nТематические ограничения: Определение тем, о которых LLM может/не может генерировать текст, например, “Отвечай только на вопросы, касающиеся истории и философии, избегай политических тем”.\nСтилистические указания: Рекомендации по использованию определенного стиля, тона, длины ответов и других характеристик генерируемого текста.\n\nСистемный промпт играет ключевую роль в настройке поведения и вывода LLM для конкретной задачи или контекста.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#пользовательский-промпт-стратегии",
    "href": "llm.html#пользовательский-промпт-стратегии",
    "title": "25  Работа с LLM",
    "section": "25.3 Пользовательский промпт: стратегии",
    "text": "25.3 Пользовательский промпт: стратегии\nСтратегии промптирования представляют собой различные подходы к формулировке запроса к модели, которые помогают получить более точные, релевантные и обоснованные ответы. Рассмотрим основные из них:\n\nСтратегия прямых запросов (Zero-shot prompting). При таком подходе пользователь формулирует запрос без дополнительных примеров, полагаясь на то, что модель уже обладает достаточными знаниями для ответа. Это минималистичный вариант, когда инструкция передается в виде одного сообщения без демонстрации образцов ответов.\nOne-shot prompting представляет собой метод, при котором вместе с запросом пользователю предоставляется ровно один пример, показывающий, каким должен быть желаемый ответ. Этот подход занимает промежуточное положение между zero-shot prompting (когда примеров нет вовсе) и few-shot prompting (когда примеров несколько). Использование одного примера помогает модели лучше понять формат и стиль нужного ответа, минимизируя объем вводной информации, но при этом обеспечивая достаточную направленность для получения релевантного результата. Такой метод полезен, когда в задаче достаточно однозначного примера, демонстрирующего специфику ответа, без необходимости загромождать запрос большим количеством примеров.\nМетод с примерами (Few-shot prompting). В этом случае вместе с основным запросом в промпт включается несколько примеров, то есть пара «вопрос-ответ», показывающих требуемый формат или стиль ответа. Такой подход помогает модели лучше понять, какую информацию и в каком виде она должна предоставить, особенно если задача сложная или специфическая.\nМногошаговое рассуждение (Chain-of-thought prompting). Здесь пользователь побуждает модель не просто выдавать ответ, а проходить через промежуточные этапы рассуждения. Инструкция может требовать развернутого описания логических шагов, что позволяет получить более обоснованный и прозрачный результат. Такой метод особенно полезен при решении сложных задач, требующих рассуждений или математических выкладок.\nРолевое (или контекстное) промптирование. При этом подходе пользователь задаёт модели конкретную роль или контекст, в котором она должна действовать, например, «представь, что ты эксперт в экономике» или «ответь так, как будто ты историк». Задание роли помогает сместить акценты и получить ответы, адаптированные к определённой области знаний или стиля общения.\nИтеративное уточнение запроса. Иногда первоначальный запрос может быть недостаточно точным или содержательным. В таких случаях используется метод пошагового уточнения, когда после получения первого ответа пользователь задаёт дополнительные вопросы или корректирует исходный запрос, добиваясь уточнения, расширения или сужения информации.\nИспользование самокритического подхода. Некоторые стратегии подразумевают, что модель не только генерирует ответ, но и сама анализирует его корректность, выявляет возможные ошибки и при необходимости пересматривает свой вывод. Это может включать подсказки типа «подумай ещё раз» или дополнительные указания для оценки достоверности результата.\n\nКаждая из этих стратегий имеет свои преимущества в зависимости от задачи, требуемой точности и специфики данных. Выбор подхода может существенно повлиять на качество итогового ответа, поэтому часто оптимизируют промпт, комбинируя несколько методов: от указания роли до добавления примеров и поэтапного рассуждения.\n\n25.3.1 Zero-shot\n\nlibrary(tibble)\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Answer with just the correct category.\",\n  \"user\",   \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\"\n)\nquery(q)\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Negative\n\n\n\n25.3.2 One-shot\nСтруктура включает системную подсказку, за которой следует запрос пользователя с примером текста и вопросом классификации, пример классификации от ассистента и затем ещё один запрос пользователя с новым текстом для классификации.\n\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Answer with just the correct category.\",\n  \"user\", \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Negative\",\n  \"user\", \"text: What an excellent father you have, girls!”\\ncategories: positive, neutral, negative\"\n)\nquery(q)\n# \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Category: Positive.\n\nПопросим вернуть результат в формате JSON.\n\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Provide the following information: category, confidence, and the word that is most important for your coding decision.\",\n  \"user\", \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"{'Category':'Negative','Confidence':'100%','Important':'compassion'}\",\n  \"user\", \"text: What an excellent father you have, girls!\\ncategories: positive, neutral, negative\"\n)\nanswer &lt;- query(q)\n# \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# {'Category':'Positive', 'Confidence':80,'Important':'father'}\n\nИспользуйте pluck(answer, \"message\", \"content\"), чтобы извлечь ответ.\n\n\n25.3.3 Few-shot\nДобавим другие примеры.\n\nq &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. Provide the following information: category, confidence, and the word that is most important for your coding decision.\",\n  \"user\", \"text: You have no compassion for my poor nerves.\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Negative\",\n  \"user\", \"text: What an excellent father you have, girls!\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Positive\",\n  \"user\", \"text: The rest of the evening was spent in conjecturing how soon he would return Mr. Bennet’s visit\\ncategories: positive, neutral, negative\",\n  \"assistant\", \"Category: Neutral\",\n  \"user\", \"text: An invitation to dinner was soon afterwards dispatched\\ncategories: positive, neutral, negative\"\n)\nanswer &lt;- query(q)\n# \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Category: Positive\n\nLLM иногда что-то от себя додумывают и находят хорошее и плохое там, где его нет.\n\n\n25.3.4 Chain-of-Thought\nПопросим модель немного подумать.\n\nq_thought &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. \",\n  \"user\",  \"text: An invitation to dinner was soon afterwards dispatched\\n What sentiment (positive, neutral, negative) would you assign? Provide some thoughts.\"\n)\noutput_thought &lt;- query(q_thought, output = \"text\")\n\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# I would assign a positive sentiment to the text.\n# \n# The word \"soon\" implies a sense of haste or urgency, which suggests\n# that the invitation is being made in response to an event or situation\n# where time was of the essence. The fact that it's soon afterwards\n# dispatched implies that the recipient has been waiting for some time\n# and is eager to accept the invitation.\n# \n# Additionally, the use of the word \"was\" suggests a sense of certainty\n# and clarity about the timing of the invitation, which adds to its\n# positive connotation.\n# \n# Overall, the text has a friendly and inviting tone, suggesting that the\n# host is enthusiastic about sharing their dinner plans with the\n# recipient.\n\nТеперь создадим дополнительный шаг в рассуждении.\n\nq_thought &lt;- tribble(\n  ~role,    ~content,\n  \"system\", \"You assign texts into categories. \",\n  \"user\",  \"text: An invitation to dinner was soon afterwards dispatched\\n What sentiment (positive, neutral, negative) would you assign? Provide some thoughts.\",\n  \"assistant\", output_thought,\n  \"user\",   \"Now answer with just the correct category (positive, neutral, or negative)\"\n)\nresps &lt;- query(q)\n\n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Category: Positive\n\nПожалуй, сестры Беннет могли бы согласиться, что в приглашении на ужин есть что-то хорошее.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#функция-make_query",
    "href": "llm.html#функция-make_query",
    "title": "25  Работа с LLM",
    "section": "25.4 Функция make_query()",
    "text": "25.4 Функция make_query()\nФункция make_query() предназначена для упрощения создания структурированного запроса для классификации текста, чтобы вам не приходилось самостоятельно создавать tibble и запоминать специфическую структуру.\nКомпоненты:\n\ntext: новый текст для аннотирования.\nprompt: вопрос, содержащий категории для аннотирования.\ntemplate: определяет структуру сообщений пользователя. Шаблон может включать заполнители, например, {text}, {prefix} и для динамического форматирования входных данных.\nsystem: системный промпт (необязательно).\nprefix: cтрока, добавляемая в начало запросов пользователя (необязательно).\nsuffix: cтрока, добавляемая в конец запросов пользователя (необязательно).\nexamples: Предыдущие примеры, состоящие из сообщений пользователя и ответов ассистента (для обучения с одним или несколькими примерами) (необязательно).\n\nИспользование без подсказок.\n\n# Call the make_query function\nq_zs &lt;- make_query(\n  template = \"{text}\\n{prompt}\",\n  text = \"You have no compassion for my poor nerves.\",\n  prompt = \"Categories: positive, neutral, negative\",\n  system = \"You assign texts into categories. Answer with just the correct category.\",\n)\n\n# Print the query\nprint(q_zs)\n\nquery(q_zs)\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ───────────────────────────────\n# Negative\n\nДобавляем один пример.\n\nexamples_os &lt;- tibble::tribble(\n  ~text, ~answer,\n  \"You have no compassion for my poor nerves\", \"negative\"\n)\n\nq_os &lt;- make_query(\n  text = \"She is the most beautiful creature I ever beheld!\",\n  template = \"{text}\\n{prompt}\",\n  prompt = \"Categories: positive, neutral, negative\",\n  system = \"You assign texts into categories. Answer with just the correct category.\",\n  example = examples_os,\n)\n\nquery(q_os)\n#                                         \n# ── Answer from llama3.2:1b-instruct-q8_0 ────────────────────────────────\n# positive\n\nАналогично можно добавить другие примеры.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "llm.html#запускаем-в-производство",
    "href": "llm.html#запускаем-в-производство",
    "title": "25  Работа с LLM",
    "section": "25.5 Запускаем в производство",
    "text": "25.5 Запускаем в производство\nНа практике вы, вероятно, едва ли будете аннотировать только один текст, разве что для тестирования. Обычно необходимо разметить коллекцию текстов, поэтому разберемся, как это делается. Для начала создаем таблицу с данными для анализа.\n\nmovie_reviews &lt;- tibble::tibble(\n  review_id = 1:5,\n  review = c(\"A stunning visual spectacle with a gripping storyline.\",\n             \"The plot was predictable, but the acting was superb.\",\n             \"An overrated film with underwhelming performances.\",\n             \"A beautiful tale of love and adventure, beautifully shot.\",\n             \"The movie lacked depth, but the special effects were incredible.\")\n)\n\nmovie_reviews\n\nТеперь сформируем запрос.\n\nqueries &lt;- make_query(\n  text = movie_reviews$review,\n  prompt = \"Categories: positive, neutral, negative\",\n  template = \"{prefix}{text}\\n{prompt}\",\n  system = \"Classify the sentiment of the movie review. Answer with just the correct category.\",\n  prefix = \"Text to classify: \"\n)\n\nЭто создает список таблиц с запросами в одном формате. Все они содержат один и тот же prompt, системное сообщение и prefix, но каждый включает разный текст, взятый из ранее созданной таблицы. Функция query принимает списки запросов, поэтому мы можем получить аннотации, просто используя:\n\n# Process and annotate the movie reviews\nmovie_reviews$annotation &lt;- query(queries, screen = FALSE, output = \"text\")\n\nmovie_reviews\n# A tibble: 5 × 3\n#   review_id review                                                           annotation\n#       &lt;int&gt; &lt;chr&gt;                                                            &lt;chr&gt;     \n# 1         1 A stunning visual spectacle with a gripping storyline.           Positive  \n# 2         2 The plot was predictable, but the acting was superb.             Positive  \n# 3         3 An overrated film with underwhelming performances.               Negative  \n# 4         4 A beautiful tale of love and adventure, beautifully shot.        Positive  \n# 5         5 The movie lacked depth, but the special effects were incredible. Positive  \n\nЭто занимает немного больше времени, чем классическое контролируемое машинное обучение или даже классификация с использованием трансформеров. Однако преимущество в том, что инструкции можно давать на простом английском языке, моделям для достижения удивительно хороших результатов требуется очень мало примеров, а лучшие модели, такие как llama3.2, часто справляются с более сложными категориями, чем другие методы.\nНо в большинстве случаев вы будете создавать развесистые промпты, которые лучше сразу сохранять в markdown-документе, причем под контролем версий, чтобы можно было вернуться к прежним версиям. Давайте таким файлам информативные названия, например prompt-extract-metadata.md. В этом случае код выглядит как-то так.\n\nquestion &lt;- readLines(\"user_prompt.Rmd\", warn = FALSE) \n\nchat &lt;- chat_ollama(model = \"deepseek-r1:1.5b\",\n                    system_prompt = readLines(\"system_prompt.Rmd\", warn = FALSE))\n\nchat$chat(question)\n\nПри написании промпта будьте максимально конкретными в требованиях, используя примеры и подробные описания. Используйте синтаксис markdown – LLM его понимают.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Работа с LLM</span>"
    ]
  },
  {
    "objectID": "classification.html#записки-федералиста",
    "href": "classification.html#записки-федералиста",
    "title": "24  Бинарная классификация",
    "section": "",
    "text": "Александр Гамильтон\n\n\n\n\n\n\n\nДжеймс Мэдисон",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "classification.html#наивный-байес",
    "href": "classification.html#наивный-байес",
    "title": "24  Бинарная классификация",
    "section": "24.4 Наивный Байес",
    "text": "24.4 Наивный Байес\nЕще один алгоритм, который часто используется в задачах классификации, называется “наивный Байес”.\n\n24.4.1 Теорема Байеса\nТеорема Байеса позволяет оценить вероятность одного события на основе вероятности другого собыитя. Математически теорема Байеса выглядит так:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nЗдесь:\n\n\\(P(A|B)\\) - вероятность события A при условии, что произошло событие B (апостериорная вероятность); она рассчитывается с учетом того, как часто события А и В происходят вместе и того, как часто вообще происходит B. Например: какова вероятность того, что письмо, содержащее слово “наследство”, является спамом?\n\\(P(B|A)\\) - вероятность события B при условии, что произошло событие A (правдоподобие). Например: какова вероятность встретить слово “наследство” в спаме?\n\\(P(A)\\) - вероятность события A (априорная вероятность). Например: какова вероятность получить спам?\n\\(P(B)\\) - вероятность события B (маргинальное правдоподобие). Например: как часто вообще встречается слово “наследство”?\n\nТеорема Байеса широко применяется в задачах классификации в машинном обучении. Например, в наивном байесовском классификаторе, который использует теорему Байеса для вычисления вероятности принадлежности объекта к тому или иному классу.\nЧтобы лучше понять теорему, решите несколько задач.\n\n\n\n\n\n\nЗадание\n\n\n\nВ кофейне подают два вида кофе: латте и капучино. 70% клиентов выбирают латте, а 30% — капучино. Известно, что 20% клиентов, выбравших латте, добавляют сахар, а среди выбравших капучино сахар добавляют 40%. Клиент добавил сахар в кофе. Какова вероятность, что он выбрал латте? Ответ запишите в процентах c округлением до двух сотых.\n\n\nДано:\n\n\\(P(L)=0.7\\) (вероятность выбора латте).\n\\(P(C)=0.3\\) (вероятность выбора капучино).\n\\(P(S|L)=0.2\\) (вероятность добавления сахара для латте).\n\\(P(S∣C)=0.4P\\) (вероятность добавления сахара для капучино).\n\nНеобходимо найти \\(P(L∣S)\\) — вероятность того, что клиент выбрал латте при условии, что он добавил сахар. Полная вероятность добавления сахара считается так: \\(P(S) = P(S∣L)⋅P(L)+ P(S∣C)⋅P(C)\\).\nОтвет: \n\n\n\n\n\n\nЗадание\n\n\n\nВ приюте для животных есть коты двух пород: мейн-куны и британские короткошерстные. 70% котов — мейн-куны, а 30% — британские короткошерстные. Известно, что 15% мейн-кунов имеют зеленые глаза, а у британских короткошерстных котов зеленые глаза встречаются у 40%. Посетитель приюта случайным образом выбирает кота с зелеными глазами. Какова вероятность, что это мейн-кун? Ответ запишите в процентах c округлением до двух сотых.\n\n\nДано:\n\\(P(M)=0.7\\) (вероятность выбрать мейн-куна).\n\\(P(B)=0.3\\) (вероятность выбрать британского короткошерстного).\n\\(P(G∣M)=0.15\\) (вероятность зеленых глаз для мейн-куна).\n\\(P(G∣B)=0.4\\) (вероятность зеленых глаз для британского короткошерстного).\nНеобходимо найти: \\(P(M∣G)\\) — вероятность того, что кот — мейн-кун при условии, что у него зеленые глаза.\nОтвет: .\n\n\n\n\n\n\nЗадание\n\n\n\nВ библиотеке есть книги двух жанров: детективы и фантастика. 60% книг — детективы, а 40% — фантастика. Известно, что 10% детективов имеют красную обложку, а у фантастики красная обложка у 25% книг. Читатель случайным образом выбирает книгу с красной обложкой. Какова вероятность, что это детектив? Ответ запишите в процентах c округлением до двух сотых.\n\n\nОтвет: .\nМожете придумать свою задачу?\n\n\n24.4.2 Применение теоремы в МО\nНаивный байесовский классификатор называется “наивным” из-за ключевого допущения, которое он делает в своей работе: предположение о независимости признаков.\nНаивный Байес предполагает, что признаки (предикторы) объекта, который нужно классифицировать, являются статистически независимыми друг от друга, то есть значение одного признака не зависит от значений других признаков.\nЭто “наивное” предположение значительно упрощает вычисления, необходимые для применения теоремы Байеса. Вместо того, чтобы вычислять сложную совместную вероятность всех признаков, наивный Байес разбивает это на произведение вероятностей отдельных признаков.\n\nЗнаменатель будет для всех групп одинаков, поэтому:\n\nХотя это предположение редко выполняется в реальных данных, наивный Байес часто демонстрирует неожиданно хорошую производительность. Таким образом, “наивность” этого классификатора относится именно к этому упрощающему предположению.\nЧто если в обучающем корпусе слово в каком-то классе не встречается? Чтобы все вероятности не обнулились, применяют критерий Лапласа, то есть добавляют ко всем значениям в таблице небольшое число.\n\n\n24.4.3 Препроцессор и модель\n\n# предсказываем автора по всем переменным\nnb_rec &lt;- recipe(author ~ ., data = data_train) \n\n# выбираем модель\nnb_spec &lt;- naive_Bayes(Laplace = tune(),\n                       smoothness = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"naivebayes\")\n\nnb_spec\n\nNaive Bayes Model Specification (classification)\n\nMain Arguments:\n  smoothness = tune()\n  Laplace = tune()\n\nComputational engine: naivebayes \n\n\nСогласно документации, меньшие значения smoothness приводят к более гибким, адаптивным границам между классами. Другими словами, smoothness - это параметр, с помощью которого можно контролировать гибкость границ классификации, определяемых наивным байесовским классификатором. Низкие значения smoothness позволяют модели более точно подстраиваться под обучающие данные, но могут также приводить к переобучению. Высокие значения сглаживают границы и делают модель более устойчивой, но менее точной на обучающих данных.\n\n\n24.4.4 Выбор гиперпараметров\n\nnb_param &lt;- extract_parameter_set_dials(nb_spec)\nnb_param\n\n\n  \n\n\n\nТеперь добавим модель и препроцессор в воркфлоу.\n\n# workflow \nnb_wflow &lt;- workflow() |&gt; \n  add_model(nb_spec) |&gt; \n  add_recipe(nb_rec)\n\nnb_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nNaive Bayes Model Specification (classification)\n\nMain Arguments:\n  smoothness = tune()\n  Laplace = tune()\n\nComputational engine: naivebayes \n\n\n\nnb_tune &lt;- nb_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = nb_param |&gt; grid_regular(levels = 5),\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_resamples(save_pred = TRUE)\n  )\n\nnb_tune \n\n\n  \n\n\n\n\ncollect_metrics(nb_tune)\n\n\n  \n\n\n\n\nautoplot(nb_tune)\n\n\n\n\n\n\n\n\n\nshow_best(nb_tune, n = 1)\n\nWarning in show_best(nb_tune, n = 1): No value of `metric` was given;\n\"accuracy\" will be used.\n\n\n\n  \n\n\n\n\nnb_best &lt;- select_best(nb_tune, metric = \"accuracy\")\nnb_best\n\n\n  \n\n\n\n\n\n24.4.5 Матрица смешения\nНапомним, что в обучающих данных всего 52 текста, из них 40 принадлежит Гамильтону, а 12 – Мэдисону.\n\nconf_mat_resampled(nb_tune, tidy = FALSE, parameters = nb_best) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"#eaeff6\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\")) +\n  ggtitle(\"NB, 70 признаков\")\n\n\n\n\n\n\n\n\n\n\n24.4.6 ROC-кривая\n\nnb_predictions &lt;- nb_tune |&gt; \n  collect_predictions(parameters = nb_best) |&gt; \n  mutate_if(is.numeric, round, 3)\n\nnb_predictions\n\n\n  \n\n\n\n\nnb_predictions |&gt; \n  roc_curve(author, .pred_Hamilton) |&gt; \n  autoplot() \n\n\n\n\n\n\n\n\nРезультат чуть хуже, чем дает LDA, так что применять этот алгоритм к спорным текстам мы не будем.\nДля сравнения различных методов бывает полезно вывести на одном графике несколько моделей.\n\nlda_predictions |&gt; \n  roc_curve(author, .pred_Hamilton) |&gt; \n  mutate(model = \"LDA\") |&gt; \n  bind_rows(nb_predictions |&gt; \n              roc_curve(author, .pred_Hamilton) |&gt; \n              mutate(model = \"NB\")) |&gt; \n  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + \n  geom_path(lwd = 1.5, alpha = 0.8) +\n  geom_abline(lty = 3) + \n  coord_equal() + \n  scale_color_viridis_d(option = \"plasma\", end = .6) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "classification.html#линейно-дискриминантный-анализ",
    "href": "classification.html#линейно-дискриминантный-анализ",
    "title": "24  Бинарная классификация",
    "section": "24.3 Линейно-дискриминантный анализ",
    "text": "24.3 Линейно-дискриминантный анализ\nВ дискриминантном анализе (например, линейном — LDA) мы хотим:\n\nперевести объекты из пространства признаков в новое пространство,\nв котором группы (классы) лучше всего разделены.\n\nЭто достигается путём создания новых переменных — дискриминантных функций. Каждая из них — это линейная комбинация исходных признаков, то есть новая «ось» или направление в признаковом пространстве.\nНапример, если у нас есть 3 признака (var₁, var₂, var₃), мы можем создать новую ось:\n  DF₁ = –0.5 × var₁ + 1.2 × var₂ + 0.85 × var₃\nЭто и есть «новая ось», вдоль которой мы будем оценивать, хорошо ли разделяются классы. Если всё правильно, точки из разных групп на этой оси будут располагаться как можно дальше друг от друга.\nТеперь — как мы эту ось находим. Идея LDA: найти такую прямую (новое направление / ось), на которой проекции точек из двух групп лежат как можно дальше друг от друга, но внутри группы — как можно плотнее. Это называется максимизация межклассовой дисперсии и минимизация внутриклассовой.\nС математической точки зрения, мы ищем вектор w (то есть направление новой оси), который максимизирует критерий:\n  J(w) = (межклассовая дисперсия) / (внутриклассовая дисперсия)\nВ простом случае двух классов:\n  J(w) = \\(\\frac{(\\bar{x_1}-\\bar{x_2})^2}{s^2_1+s^2_2}\\)\nгде:\n\nμ₁ и μ₂ — средние проекции объектов классов на вектор w,\nσ₁² и σ₂² — дисперсии проекций в каждом классе.\n\nНаша задача — найти такой вектор w, который максимально разделяет средние значения разных классов и минимально разносит точки одного класса.\n\n\n\nИсточник.\n\n\nЕсли признаков не два, а, скажем, 100 (как бывает в задаче с текстами, генами и т. п.), то алгоритм строит до K–1 дискриминантных функций (ось), где K — число классов. Например:\n\nДля 2 классов → 1 ось (DF₁),\nДля 3 классов → 2 оси (DF₁ и DF₂),\nИ т. д.\n\nВ итоге мы можем визуализировать данные в новом пространстве.\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВажно: чем это отличается от PCA (главных компонент)?\n\nPCA — выбирает оси с максимальной общей дисперсией, но не учитывает классы.\nLDA — выбирает оси, которые максимально разделяют известные классы (использует метки классов).\n\nТаким образом, LDA работает как «обученный» метод (supervised), в отличие от PCA.\n\n\n\n24.3.1 Препроцессор и модель\nМы будем использовать регуляризованный LDA. Он применяется в тех случаях, когда число признаков (features) в данных превышает число наблюдений, а также когда в наборе данных присутствует сильная мультиколлинеарность между признаками.\n\n# предсказываем автора по всем переменным\nlda_rec &lt;- recipe(author ~ ., data = data_train) \n\n# выбираем модель\nlda_spec &lt;- discrim_linear(regularization_method = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"sparsediscrim\")\n\nlda_spec\n\nLinear Discriminant Model Specification (classification)\n\nMain Arguments:\n  regularization_method = tune()\n\nComputational engine: sparsediscrim \n\n\n\n\n24.3.2 Выбор гиперпараметров\nМетод регуляризации мы подберем при помощи настройки.\n\nlda_param &lt;- extract_parameter_set_dials(lda_spec)\nlda_param\n\n\n  \n\n\n\nСоздадим сетку гиперпараметров.\n\nlda_grid &lt;- lda_param |&gt; \n  grid_regular()\n\nlda_grid\n\n\n  \n\n\n\nТеперь добавим модель и препроцессор в воркфлоу.\n\n# workflow \nlda_wflow &lt;- workflow() |&gt; \n  add_model(lda_spec) |&gt; \n  add_recipe(lda_rec)\n\nlda_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Discriminant Model Specification (classification)\n\nMain Arguments:\n  regularization_method = tune()\n\nComputational engine: sparsediscrim \n\n\n\nlda_tune &lt;- lda_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = lda_grid,\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_resamples(save_pred = TRUE)\n  )\n\nlda_tune \n\n\n  \n\n\n\n\n\n24.3.3 Оценка модели: F-score\nВ этом примере мы использовали два критерия оценки: точность (т.е доля верных ответов) и F-score, также известный как F1-score или гармоническое среднее. Это комплексная метрика, которая объединяет в себе два других важных показателя эффективности модели: точность (precision) и полноту (recall).\nPrecision (точность) - это доля правильно классифицированных положительных примеров среди всех примеров, предсказанных как положительные. Recall (полнота) - это доля правильно классифицированных положительных примеров среди всех фактически положительных примеров.\nФормула для расчета F-score:\n\\[F\\text{-}score = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\\]\nГде:\n\n\\(precision = \\frac{TP}{TP + FP}\\)\n\\(recall = \\frac{TP}{TP + FN}\\)\n\nF-score находится в диапазоне от 0 до 1, и чем ближе значение к 1, тем лучше работает модель. При оценке качества поисковой системы F-score может быть более информативным, чем только точность или только полнота, поскольку учитывает оба этих аспекта.\n\ntune::collect_metrics(lda_tune)\n\n\n  \n\n\n\n\nautoplot(lda_tune) \n\n\n\n\n\n\n\n\n\nlda_best &lt;- tune::select_best(lda_tune, metric = \"accuracy\")\nlda_best\n\n\n  \n\n\n\nТаким образом, оптимальным методом является диагональная регуляризация. Цель диагональной регуляризации - избежать вырожденности ковариационной матрицы за счет добавления к ее диагональным элементам некоторой константы λ. Добавление константы λ к диагональным элементам ковариационной матрицы позволяет сделать ее невырожденной и обратимой, а это позволяет провести вычисление обратной матрицы, необходимое для LDA.\n\n\n24.3.4 ROC-кривая\nROC-кривая (англ. receiver operating characteristic, рабочая характеристика приёмника) — это график, который показывает как меняются следующие характеристики бинарного классификатора при варьировании порога отсечения.\n\nОсь Y (TPR) показывает долю правильно классифицированных положительных примеров (чувствительность, true positive rate). Изменяется от 0 до 1.\nПо оси X откладывается доля отрицательных объектов, ошибочно классифицированных как положительные (false positive rate, FPR); это значение равно 1 − специфичность. Изменяется от 0 до 1.\nДиагональная линия (y=x) - представляет случайную классификацию, когда вероятность положительного класса равна вероятности отрицательного класса.\nПлощадь под ROC-кривой (Area Under Curve, AUC) - показывает качество классификатора. Чем больше AUC (максимальное значение 1), тем лучше работает модель.\n\nОсновные интерпретации ROC-кривой:\n\nЕсли кривая расположена выше диагональной линии, это говорит о том, что модель работает лучше случайной классификации.\nЕсли кривая совпадает с диагональной линией, то модель не способна отличить положительные и отрицательные классы.\nЕсли кривая расположена ниже диагональной линии, это свидетельствует о том, что модель работает хуже случайной классификации.\n\n\nlda_predictions &lt;- lda_tune |&gt; \n  collect_predictions(parameters = lda_best) |&gt; \n  mutate_if(is.numeric, round, 3)\n\nlda_predictions\n\n\n  \n\n\n\n\nlda_predictions |&gt; \n  roc_curve(author, .pred_Hamilton) |&gt; \n  # или, для другого класса:\n  #roc_curve(author, .pred_Madison, event_level = \"second\") |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n24.3.5 Матрица смешения\nВзглянем на матрицу смешения. В обучающих данных всего 52 текста, из них 40 принадлежит Гамильтону, а 12 – Мэдисону.\n\nlda_param &lt;- tibble(regularization_method = \"diagonal\")\n\nconf_mat_resampled(lda_tune, tidy = FALSE, parameters = lda_param) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"#eaeff6\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\")) +\n  ggtitle(\"LDA, 70 признаков\")\n\n\n\n\n\n\n\n\n\n\n24.3.6 Окончательная настройка модели\nПрежде всего установим нужный метод регуляризации.\n\nfinal_lda_wflow &lt;- \n  lda_wflow |&gt; \n  finalize_workflow(lda_param)\n\nfinal_lda_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: discrim_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Discriminant Model Specification (classification)\n\nMain Arguments:\n  regularization_method = diagonal\n\nComputational engine: sparsediscrim \n\n\nИ подгоним модель.\n\nlda_fit &lt;- final_lda_wflow  |&gt; \n  fit(data_train)\n\n\n\n24.3.7 Тестовая выборка\nУ нас остались неиспользованными 14 наблюдений в тестовой выборке.\n\npred_test &lt;- predict(lda_fit, data_test, type = \"class\")\n\nЗдесь тоже 100%-я точность.\n\ntest_acc &lt;- tibble(predicted = pred_test$.pred_class, \n       expected = data_test$author, \n       value = predicted == expected)\n\nsum(test_acc$value) / nrow(test_acc)\n\n[1] 1\n\n\n\n\n24.3.8 Классификация спорных эссе\nВсе указывает на то, что в большинстве случаев Мэдисон – наиболее вероятный автор. Что касается 55-го эссе, то на его счет сомневались и Мостеллер с Уоллесом.\n\npredict(lda_fit, dispt, type = \"class\") |&gt; \n  mutate(essay = dispt$filename)",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "classification.html#подготовка-данных",
    "href": "classification.html#подготовка-данных",
    "title": "24  Бинарная классификация",
    "section": "24.2 Подготовка данных",
    "text": "24.2 Подготовка данных\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nconflicted::conflict_prefer(\"filter\", winner = \"dplyr\")\nconflicted::conflict_prefer(\"select\", winner = \"MASS\")\nlibrary(discrim)\n\nПо ссылке скачайте датасет с частотностью слов в записках ( источник). Из него мы удалим три текста предположительного двойного авторства и пять эссе Джона Джея.\n\nfed &lt;- read_csv(\"../files/fedPapers85.csv\") |&gt; \n  filter(!author %in%  c(\"HM\", \"Jay\")) \n\n# небольшой ремонт\ncolnames(fed) &lt;- make.names(colnames(fed))\n\nОтложим спорные эссе.\n\ndispt &lt;- fed |&gt; \n  filter(author == \"dispt\") \n\nessays &lt;- fed |&gt; \n  filter(author != \"dispt\") |&gt; \n  mutate(author = as.factor(author)) |&gt; \n  dplyr::select(-filename)\n\nРазобьем оставшиеся наблюдения на обучающую и проверочную выборки.\n\nset.seed(03022025)\ndata_split &lt;- essays |&gt; \n  initial_split(0.8, strata = author)\n\ndata_train &lt;- training(data_split)\ndata_test &lt;- testing(data_split)\n\nРазобьем обучающие данные группы для перекрестной проверки.\n\nfolds &lt;- vfold_cv(data_train, strata = author, v = 10)\nfolds",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Бинарная классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html",
    "href": "multiclass.html",
    "title": "25  Многоклассовая классификация",
    "section": "",
    "text": "25.1 Подготовка данных\nМногоклассовая классификация может использоваться для определения автора, жанра, тематики или эмоциональной тональности текста. В этом уроке мы научимся классифицировать тексты по автору, воспользовавшись учебным датасетом русской прозы.В формате zip можно забрать здесь.\nОсновные задачи этого урока:\ncorpus &lt;- load.corpus.and.parse(corpus.dir = \"../files/russian_corpus\")\nРазделим тексты на отрывки длиной 2000 слов.\ncorpus_samples &lt;- make.samples(corpus, \n                               sample.size = 2000, \n                               sampling = \"normal.sampling\",\n                               sample.overlap = 0,\n                               sampling.with.replacement = FALSE)\nПеред созданием списка слов удалим еры, которые встречаются в некоторых изданиях (“съ” и т.п.).\ncorpus_samples_clean &lt;- map(corpus_samples, \n                              function(text) str_remove(text, \"ъ$\") \n                            )",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "index.html#видео",
    "href": "index.html#видео",
    "title": "Компьютерный анализ текста",
    "section": "Видео",
    "text": "Видео\nЗаписи лекций и семинаров 2024/25 уч. г. можно найти в плейлисте по ссылке.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "multiclass.html#recipe",
    "href": "multiclass.html#recipe",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Recipe",
    "text": "25.2 Recipe\n\nlibrary(themis)\n\nbase_rec &lt;- recipe(corpus ~ ., data = data_train) |&gt; \n  step_downsample(corpus)\nbase_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor: 1001\n\n\n\n\n\n── Operations \n\n\n• Down-sampling based on: corpus\n\nnorm_rec &lt;- recipe(corpus ~ ., data = data_train)  |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_downsample(corpus)\nnorm_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:      1\npredictor: 1001\n\n\n\n\n\n── Operations \n\n\n• Centering and scaling for: all_predictors()\n\n\n• Down-sampling based on: corpus",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#knn",
    "href": "multiclass.html#knn",
    "title": "25  Многоклассовая классификация",
    "section": "25.3 KNN",
    "text": "25.3 KNN\n\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"kknn\")\n\nknn_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\n\nknn_grid &lt;- tibble(neighbors = c(1,3,5))\nknn_grid",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#svm",
    "href": "multiclass.html#svm",
    "title": "25  Многоклассовая классификация",
    "section": "25.9 SVM",
    "text": "25.9 SVM\nМетод опорных векторов (SVM) используется как в задачах регрессии, так и в задачах классификации.\nВо втором случае он пытается найти такую границу (гиперплоскость), которая максимально хорошо разделяет два класса объектов. Если упростить задачу до двух измерений, то метод ищет такую прямую, чтобы расстояние от неё до ближайших точек с каждой стороны было максимальным: классы должны быть как можно дальше от границы. Чем дальше граница от обучающих точек, тем устойчивее она к ошибкам на новых данных.\nДля этого SVM строит разделяющую прямую, которая максимально “отодвинута” от крайних точек обоих классов. Эти крайние точки, которые “касаются” границы — называются опорные векторы (support vectors).\nМаржа (англ. margin) — это расстояние от разделяющей границы до ближайших точек каждого класса. Чем больше маржа, тем увереннее разделяются классы.\nЭто проще всего пояснить при помощи графика. Обычные точки — это просто обучающие примеры. Черными отмечены как раз опорные векторы — те точки, которые оказались на краю своих классов и определили положение границы. Благодаря этим точкам SVM “знает”, где должна проходить разделяющая граница. Все “внутренние” точки не влияют на её положение.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#boost_tree",
    "href": "multiclass.html#boost_tree",
    "title": "25  Многоклассовая классификация",
    "section": "26.1 Boost_tree",
    "text": "26.1 Boost_tree\n\nbt_spec &lt;- boost_tree(trees = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"xgboost\")\n\n\nbt_grid &lt;- tibble(trees = c(100, 500, 1000))\n\n\nbt_wflow &lt;- workflow() |&gt; \n  add_model(bt_spec) |&gt; \n  add_recipe(base_rec)\n\nbt_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_downsample()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  trees = tune()\n\nComputational engine: xgboost \n\n\n\nset.seed(08022025)\nbt_tune &lt;- tune_grid(\n  bt_wflow,\n  grid = bt_grid,\n  metrics = metric_set(accuracy, roc_auc),\n  folds,\n  control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n)\n# → A | warning: ✖ No observations were detected in `truth` for levels: Antiphon,\n#                  Callimachus, Philostratus the Athenian, and Xenophon.\n#                ℹ Computation will proceed by ignoring those levels.\n# → B | warning: ✖ No observations were detected in `truth` for levels: Antiphon, Arrian,\n#                  Hyperides, and Philostratus the Athenian.\n#                ℹ Computation will proceed by ignoring those levels.\n# → C | warning: ✖ No observations were detected in `truth` for levels: Aeschylus, Antiphon,\n#                  Callimachus, and Hyperides.\n#                ℹ Computation will proceed by ignoring those levels.\n# → D | warning: ✖ No observations were detected in `truth` for levels: Aristophanes,\n#                  Hyperides, Isaeus, and Sophocles.\n#                ℹ Computation will proceed by ignoring those levels.\n# → E | warning: ✖ No observations were detected in `truth` for levels: Isaeus and\n#                  Sophocles.\n#                ℹ Computation will proceed by ignoring those levels.\n\n\nautoplot(bt_tune)",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#добавить-в-workflow_set",
    "href": "multiclass.html#добавить-в-workflow_set",
    "title": "25  Многоклассовая классификация",
    "section": "26.2 Добавить в workflow_set",
    "text": "26.2 Добавить в workflow_set\n\nwflow_set_final &lt;- wflow_set_fit |&gt; \n  bind_rows(as_workflow_set(bt_base = bt_tune))\n\n\nautoplot(wflow_set_final, metric = \"accuracy\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(-0.1, 1))\n\n\n\n\n\n\n\n\n\nautoplot(wflow_set_final, metric = \"roc_auc\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(0.4, 1))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#выбор-модели-и-окончательная-настройка",
    "href": "multiclass.html#выбор-модели-и-окончательная-настройка",
    "title": "25  Многоклассовая классификация",
    "section": "25.14 Выбор модели и окончательная настройка",
    "text": "25.14 Выбор модели и окончательная настройка\n\nrank_results(wflow_set_final, rank_metric = \"f_meas\")\n\n\n  \n\n\n\n\nautoplot(wflow_set_fit, id = \"tf_svm\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nbest_results &lt;- \n   wflow_set_final |&gt; \n   extract_workflow_set_result(\"tf_svm\") |&gt; \n   select_best(metric = \"accuracy\")\n\nbest_results\n\n\n  \n\n\n\nОбратите внимание: на этом этапе мы “распечатываем” тестовые данные!\n\nsvm_test_results &lt;- \n   wflow_set_final |&gt; \n   extract_workflow(\"tf_svm\") |&gt; \n   finalize_workflow(best_results) |&gt; \n   last_fit(split = data_split,\n            metrics = metric_set(accuracy, f_meas))\n\nsvm_test_results",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#оценка",
    "href": "multiclass.html#оценка",
    "title": "25  Многоклассовая классификация",
    "section": "25.15 Оценка",
    "text": "25.15 Оценка\nОцениваем эффективность на тестовых данных.\n\ncollect_metrics(svm_test_results)\n\n\n  \n\n\n\n\nsvm_test_results |&gt; \n  collect_predictions() |&gt;\n  conf_mat(truth = author, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\"),\n        axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nОтличная работа 🏆 🏆 🏆",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "consensus.html#boot.phylo-galbraith",
    "href": "consensus.html#boot.phylo-galbraith",
    "title": "17  Консенсусные деревья и сети",
    "section": "17.8 boot.phylo(): galbraith",
    "text": "17.8 boot.phylo(): galbraith\nВыше мы получили объект trees_result путем применения пользовательской функции get_tree() к данным. В пакете {phangorn}, однако, есть готовое решение для бутстрепа. Воспользуемся им и сравним результат. Заодно поменяем расстояние на косинусное и изменим алгоритм кластеризации на NJ.\n\n# функция для вычисления расстояния\ndtm_to_dist &lt;- function(data){\n  dist_mx &lt;- data |&gt; \n    scale() |&gt; \n    philentropy::distance(method = \"cosine\", \n                          use.row.names = TRUE, \n                          mute.message = TRUE) |&gt;  \n    as.dist()\n  \n  return(1 - dist_mx)\n} \n\n# матрица расстояния (все 3000 признаков)\ndist_mx &lt;- dtm_to_dist(galbraith)\n\n# кластеризация NJ\nnj &lt;- nj(dist_mx)\n\nВот так выглядит одно дерево. Пока оставим его без оформления.\n\nnj |&gt; \n  plot(type = \"unrooted\",\n       lab4ut = \"axial\")\n\n\n\n\n\n\n\n\nТеперь применяем функцию для бутстрепа. На входе она требует одно дерево, функцию для его получения, а также исходный датасет для бутстрепа. Значение аргумента trees выставляем на TRUE: это значит, что все построенные деревья будут сохраняться.\n\n# bootstrap\nFUN &lt;- function(xx) nj(dtm_to_dist(xx)) \ntree &lt;- FUN(galbraith)\nbs &lt;- boot.phylo(tree, galbraith, FUN, \n                 # сто итераций\n                 B = 100, \n                 # признаки берутся блоками по 1\n                 block = 1,\n                 rooted = FALSE, \n                 trees = TRUE)\n\n\nRunning bootstraps:       100 / 100\nCalculating bootstrap values... done.\n\n\nПосле этого строим консенсусную сеть (или консенсусное дерево, см. выше).\n\n# вычисляем консенсус\ncons.nw2 &lt;- consensusNet(bs$trees, prob = 0.3, rooted = FALSE)\n\nТеперь попробуем снова визуализировать наше консенсусное дерево.\n\nset.seed(05032024)\npar(mar = c(0,0,0,0), oma = c(0,0,0,0), cex = 1.2)\nplot(cons.nw2, type = \"2D\", \n     direction = \"axial\",\n     use.edge.length = FALSE,\n     font = 2,\n     # берем из предыдущей сети :)\n     tip.color = cons.nw$col,\n     edge.color = \"grey30\",\n     edge.width = 1.2, \n     label.offset = 0.1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nЧто произойдет с сетью, если изменить силу консенсуса? Почему? Самостоятельно постройте консенсусное дерево на основе бутстрепа.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Консенсусные деревья и сети</span>"
    ]
  },
  {
    "objectID": "ggraph.html#возможности-visnetwork",
    "href": "ggraph.html#возможности-visnetwork",
    "title": "19  Графический дизайн сетей с ggraph и visNetwork",
    "section": "19.8 Возможности VisNetwork",
    "text": "19.8 Возможности VisNetwork\nПодробнее о возможностях visNetwork можно почитать здесь. Вот так можно добавить всплывающие подсказки и иконки для узлов.\n\nlibrary(visNetwork)\n\n# Создаем узлы с tooltips\nnodes &lt;- data.frame(\n  id = 1:3, \n  label = c(\"King\", \"Queen\", \"Prince\"), \n  # иконки\n  shape = \"icon\",  \n  #  tooltips для каждого узла\n  title = c(\"The ruler of the kingdom\", \n            \"The queen of the land\", \n            \"The prince in the castle\"),  \n  icon = list(\n    face = \"FontAwesome\",\n    # коды иконок FA\n    code = c(\"f118\", \"f005\", \"f183\"),  \n    size = 50,\n    color = c(\"darkred\", \"purple\", \"blue\")  # Цвета иконок\n  )\n)\n\n# связи между узлами\nedges &lt;- data.frame(from = c(1, 1), to = c(2, 3))\n\n# граф с иконками и tooltips\nvisNetwork(nodes, edges) |&gt; \n  visOptions(highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE)) |&gt; \n  addFontAwesome()\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗамените иконками изображения Тюдоров.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Графический дизайн сетей с `ggraph` и `visNetwork`</span>"
    ]
  },
  {
    "objectID": "multiclass.html#разведывательный-анализ",
    "href": "multiclass.html#разведывательный-анализ",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Разведывательный анализ",
    "text": "25.2 Разведывательный анализ\nВ нашем датасете есть несколько очень коротких рецензий. Негативные рецензии в целом длиннее позитивных и нейтральных.\n\nreviews |&gt; \n  mutate(n_words = str_count(review, \" \") + 1) |&gt; \n  ggplot(aes(n_words, fill = sentiment)) +\n  geom_histogram(bins = 100) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light()\n\n\n\n\n\n\n\n\nПосмотрим на число уникальных токенов в каждой из категорий.\n\nreviews_tokens &lt;- reviews |&gt; \n  mutate(id = row_number(), .before = sentiment) |&gt; \n  unnest_tokens(token, review) \n\nreviews_tokens |&gt; \n  group_by(sentiment) |&gt; \n  summarise(n = n_distinct(token))\n\n\n  \n\n\n\nБольшая часть слов встречается очень редко.\n\nreviews_tokens |&gt; \n  count(sentiment, token) |&gt; \n  ggplot(aes(n, fill = sentiment)) +\n  geom_histogram(show.legend = FALSE, bins = 1000) +\n  coord_cartesian(xlim = c(NA, 2500), ylim = c(NA, 2500)) +\n  theme_light() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\nЗдесь можно добавить пример из https://juliasilge.com/blog/nber-papers/.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#обучающая-и-тестовая-выборки",
    "href": "multiclass.html#обучающая-и-тестовая-выборки",
    "title": "25  Многоклассовая классификация",
    "section": "25.3 Обучающая и тестовая выборки",
    "text": "25.3 Обучающая и тестовая выборки\n\nset.seed(06042025)\ndata_split &lt;- corpus_top |&gt; \n  mutate(author = as.factor(author)) |&gt; \n  initial_split(strata = author)\n\ndata_train &lt;- training(data_split) \ndata_test &lt;- testing(data_split)\n\n\n# folds\nset.seed(06042025)\nfolds &lt;- vfold_cv(data_train, strata = author, v = 5)\nfolds",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#рецепт-для-препроцессинга",
    "href": "multiclass.html#рецепт-для-препроцессинга",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Рецепт для препроцессинга",
    "text": "25.4 Рецепт для препроцессинга\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"))\n\n# уберем повторы и упорядочим по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 380\n\n\nПодробнее о рецепте см. https://smltar.com/mlregression#firstregression\n\ntfidf_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\ntfidf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#результат-препроцессинга",
    "href": "multiclass.html#результат-препроцессинга",
    "title": "25  Многоклассовая классификация",
    "section": "25.5 Результат препроцессинга",
    "text": "25.5 Результат препроцессинга\n\nprep_train_tf &lt;- tf_rec |&gt;\n  prep(data_train) \n\ntidy(prep_train_tf)\n\n\n  \n\n\n\n\nbake_train_tf &lt;- prep_train_tf |&gt; \n  bake(new_data = NULL)\n\nbake_train_tf\n\n\n  \n\n\n\n\nprep_train_pca &lt;- pca_rec |&gt;\n  prep(data_train) \n\ntidy(prep_train_pca)\n\n\n  \n\n\n\n\nbake_train_pca &lt;- prep_train_pca |&gt; \n  bake(new_data = NULL)\n\nbake_train_pca\n\n\n  \n\n\n\nИспользуем этот шаг для разведывательного анализа.\n\nbake_train_pca |&gt; \n  ggplot(aes(PC1, PC2, color = author)) +\n  geom_point() + \n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#первая-модель-регрессия-с-регуляризацией",
    "href": "multiclass.html#первая-модель-регрессия-с-регуляризацией",
    "title": "25  Многоклассовая классификация",
    "section": "25.6 Первая модель: регрессия с регуляризацией",
    "text": "25.6 Первая модель: регрессия с регуляризацией\nКогда мы работаем с текстовыми данными и используем большое число признаков для классификации, важно избегать алгоритмов, которые плохо работают с высоким числом измерений (например, k-NN). Вместо этого лучше использовать более эффективные алгоритмы, такие как линейные модели с регуляризацией.\nДля задач классификации применяется логистическая регрессия, которая неплохо справляется с разреженными данными благодаря L1-регуляризации (Lasso) или L2-регуляризации (Ridge). В частности, лассо-регуляризация позволяет обнулять незначимые признаки, исключая их тем самым из модели.\nПоскольку в нашем датасете несколько классов, то мы применим многоклассовую логистическую регрессию.\n\nlasso_spec &lt;- multinom_reg(penalty = tune(), mixture = 1) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\n\nlasso_param &lt;- extract_parameter_set_dials(lasso_spec)\n  \nlasso_grid &lt;- lasso_param |&gt; \n  grid_regular(levels = 3)\n\nlasso_grid\n\n\n  \n\n\n\n\nlasso_wflow &lt;- workflow() |&gt; \n  add_model(lasso_spec) |&gt; \n  add_recipe(tf_rec)\n\nlasso_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nЗдесь придется немного (или много) подождать. Параллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями.\n\nlibrary(tictoc)\nlibrary(future)\n\nplan(multisession, workers = 5)\n\ntic()\nset.seed(06042025)\nlasso_tune &lt;- lasso_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = lasso_grid,\n    metrics = metric_set(accuracy, roc_auc),\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nlasso_tune \n\ntoc()\n# 12.376 sec elapsed\nplan(sequential)\n\n\nautoplot(lasso_tune)\n\n\n\n\n\n\n\n\nНаша модель уже достигла достаточно высокой точности расходимся.\n\ncollect_predictions(lasso_tune) |&gt; \n  roc_curve(truth = author, .pred_Bulgakov:.pred_Vovchok)  |&gt; \n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  coord_fixed() +\n  theme_light()\n\n\n\n\n\n\n\n\nВспомним, что все это значит:\nSensitivity (Чувствительность) = True Positive Rate (TPR): - Формула: TP/(TP+FN) - Это доля верно определенных положительных примеров среди всех положительных примеров - Показывает, насколько хорошо модель находит нужные объекты из всех существующих - Другие названия: полнота (recall), истинноположительная доля - Ось Y на ROC-кривой\n1-Specificity = False Positive Rate (FPR): - Формула: FP/(FP+TN) = 1 - TN/(FP+TN) = 1 - Specificity - Это доля неверно определенных положительных примеров среди всех отрицательных примеров - Показывает, насколько часто модель ошибочно причисляет негативные примеры к позитивным - Другие названия: ложноположительная доля - Ось X на ROC-кривой\nВ нашем контексте:\n\nsensitivity (для автора А) – это доля текстов автора А, которые правильно определены как тексты автора А,\n1-specificity (для автора А) – это доля текстов НЕ автора А, которые ошибочно определены как тексты автора А.\n\n\ncollect_metrics(lasso_tune) |&gt; \n  filter(.metric == \"accuracy\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#random-forest",
    "href": "multiclass.html#random-forest",
    "title": "25  Многоклассовая классификация",
    "section": "25.11 Random forest",
    "text": "25.11 Random forest\nМы уже знакомы с применением случайного леса в задачах регрессии, а теперь используем этот метод для классификации.\n\nrf_spec &lt;- rand_forest(\n  trees = tune()) |&gt;        \n  set_mode(\"classification\") |&gt; \n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = tune()\n\nComputational engine: ranger \n\n\nДля случайного леса создадим решетку вручную.\n\nrf_grid &lt;- tibble(trees = c(100, 200, 300))\n\nrf_grid",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#объединение-воркфлоу",
    "href": "multiclass.html#объединение-воркфлоу",
    "title": "25  Многоклассовая классификация",
    "section": "25.13 Объединение воркфлоу",
    "text": "25.13 Объединение воркфлоу\n\nwflow_set_final &lt;- wflow_set_fit |&gt; \n  bind_rows(as_workflow_set(lasso_tf = lasso_tune))\n\nwflow_set_final\n\n\n  \n\n\n\nЛучшие результаты показывает SVM. Одна из моделей лассо (с очень высоким штрафным коэффициентом) находится в самом низу.\n\nautoplot(wflow_set_final, metric = \"accuracy\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),\n            angle = 90, hjust = 1.5) \n\n\n\n\n\n\n\n\n\nautoplot(wflow_set_final, metric = \"f_meas\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),\n            angle = 90, hjust = 1.5)",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#p.s.-pca-рецепт",
    "href": "multiclass.html#p.s.-pca-рецепт",
    "title": "25  Многоклассовая классификация",
    "section": "25.13 P.S. PCA-рецепт",
    "text": "25.13 P.S. PCA-рецепт\n\npca_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 100)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\n\nprep_train_pca &lt;- pca_rec |&gt;\n  prep(data_train) \n\ntidy(prep_train_pca)\n\n\n  \n\n\n\nКак можно видеть, количество признаков уменьшилось в 10 раз.\n\nbake_train_pca &lt;- prep_train_pca |&gt; \n  bake(new_data = NULL)\n\nbake_train_pca\n\n\n  \n\n\n\n\nrf_pca_wflow &lt;- workflow() |&gt; \n  add_model(rf_spec) |&gt; \n  add_recipe(pca_rec)\n\nrf_pca_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n10 Recipe Steps\n\n• step_mutate()\n• step_mutate()\n• step_tokenize()\n• step_stopwords()\n• step_stem()\n• step_tokenfilter()\n• step_tfidf()\n• step_zv()\n• step_normalize()\n• step_pca()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = tune()\n\nComputational engine: ranger \n\n\nТеперь ждать не так много, ведь данные стали менее разреженными.\n\nplan(multisession, workers = 5)\n\ntic()\nset.seed(10032025)\nrf_pca_tune &lt;- rf_pca_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = rf_grid,\n    metrics = metric_set(accuracy),\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nrf_pca_tune\n\ntoc()\n# 31.488 sec elapsed\nplan(sequential)\n\nСнова соединим воркфлоу.\n\nwflow_set_final2 &lt;- wflow_set_final |&gt; \n  bind_rows(as_workflow_set(rf_pca = rf_pca_tune))\n\nВидно, что серьезных ухудшений при снижении размерности не произошло, а производительность улучшилась значительно.\n\nautoplot(wflow_set_final2, metric = \"accuracy\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id), angle = 90, hjust = 1) +\n  lims(y = c(-0.1, 1))\n\n\n\n\n\n\n\n\nВ следующем уроке попробуем улучшить результат.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#workflow_set",
    "href": "multiclass.html#workflow_set",
    "title": "25  Многоклассовая классификация",
    "section": "25.10 Workflow_set",
    "text": "25.10 Workflow_set\nВ пакете {tidymodels} для R объект workflow_set используется для организации и управления несколькими комбинациями моделей (model specifications) и рецептов предобработки (recipes) в рамках единого набора рабочих процессов (workflows).\nWorkflow_set — это объект, который содержит множество разных рабочих процессов (workflows), каждая из которых представляет собой:\n\nопределённую модель (модельную спецификацию);\nопределённый способ предобработки данных (recipe).\n\nИначе говоря: это способ систематически и удобно перебрать (или протестировать) различные комбинации моделей и рецептов на одном наборе данных.\n\nlibrary(baguette)\nlibrary(discrim)\n\n\nAttaching package: 'discrim'\n\n\nThe following object is masked from 'package:dials':\n\n    smoothness\n\nwflow_set &lt;- workflow_set(  \n  preproc = list(base = base_rec,\n                 pca = pca_rec,\n                 pls = pls_rec,\n                 umap = umap_rec),  \n  models = list(svm = svm_spec,\n                lasso = lasso_spec,\n                ridge = ridge_spec,\n                mlp = mlp_spec,\n                bagging = bagging_spec,\n                fda = fda_spec,\n                rda = rda_spec,\n                knn = knn_mod),  \n  cross = TRUE\n)\n\nwflow_set\n\n\n  \n\n\n\nПараллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями. Если не уверены, сколько у вас процессоров, выполните:\n\nparallel::detectCores()\n\n[1] 8\n\n\nЗдесь придется немного (или много) подождать: мы обучаем сразу 32 модели на пяти фолдах, при этом некоторые – с разными параметрами! По умолчанию количество задач (циклов обучения) на этапе кросс-валидации ограничено числом фолдов. Поэтому, сколько бы ни было ядер в системе, в каждый момент времени не может быть запущено больше v параллельных задач (это позднее можно перенастроить, но пока не будем).\nАргументы grid = 3 означает, что будет использоваться 3 различных набора/комбинации гиперпараметров для каждой модели, включенной в workflow set. Он применяется, когда мы запускаем grid search — метод перебора гиперпараметров.\n\nlibrary(future)\nplan(multisession, workers = 5)\n\ntrain_res &lt;- wflow_set |&gt; \n  workflow_map(\n    verbose = TRUE,\n    seed = 180525,\n    resamples = folds,\n    grid = 3,\n    metrics = metric_set(f_meas, accuracy),\n    control = control_resamples(save_pred = TRUE)\n  )\n# i  1 of 32 tuning:     base_svm\n# ✔  1 of 32 tuning:     base_svm (33.2s)\n# i  2 of 32 tuning:     base_lasso\n# ...\n# ✔  2 of 32 tuning:     base_lasso (23.1s)\n# i  3 of 32 tuning:     base_ridge\n# ✔  3 of 32 tuning:     base_ridge (27.2s)\n# i  4 of 32 tuning:     base_mlp\n# ...\n# ✔  4 of 32 tuning:     base_mlp (26.7s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i  5 of 32 resampling: base_bagging\n# ✔  5 of 32 resampling: base_bagging (27.6s)\n# i  6 of 32 tuning:     base_fda\n# ✔  6 of 32 tuning:     base_fda (24.2s)\n# i  7 of 32 tuning:     base_rda\n# ...\n# ✔  7 of 32 tuning:     base_rda (1m 8.9s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i  8 of 32 resampling: base_knn\n# ...\n# ✔  8 of 32 resampling: base_knn (25.3s)\n# i  9 of 32 tuning:     pca_svm\n# ✔  9 of 32 tuning:     pca_svm (24.5s)\n# i 10 of 32 tuning:     pca_lasso\n# ...\n# ✔ 10 of 32 tuning:     pca_lasso (23.5s)\n# i 11 of 32 tuning:     pca_ridge\n# ...\n# ✔ 11 of 32 tuning:     pca_ridge (23.4s)\n# i 12 of 32 tuning:     pca_mlp\n# ...\n# ✔ 12 of 32 tuning:     pca_mlp (23.7s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i 13 of 32 resampling: pca_bagging\n# ✔ 13 of 32 resampling: pca_bagging (24s)\n# i 14 of 32 tuning:     pca_fda\n# ✔ 14 of 32 tuning:     pca_fda (25.2s)\n# i 15 of 32 tuning:     pca_rda\n# ✔ 15 of 32 tuning:     pca_rda (26.9s)\n# i No tuning parameters. `fit_resamples()` will be attempted\n# i 16 of 32 resampling: pca_knn\n# ...\n# ✔ 16 of 32 resampling: pca_knn (25.2s)\n# i 17 of 32 tuning:     pls_svm\n# ...\n# ✔ 17 of 32 tuning:     pls_svm (25.8s)\n# i 18 of 32 tuning:     pls_lasso\n# ...\n# ✔ 18 of 32 tuning:     pls_lasso (24.6s)\n# i 19 of 32 tuning:     pls_ridge\n# ...\n# ✔ 19 of 32 tuning:     pls_ridge (25.1s)\n# i 20 of 32 tuning:     pls_mlp\n# ...\n# ✔ 20 of 32 tuning:     pls_mlp (26.7s)\n# i 21 of 32 tuning:     pls_bagging\n# ✔ 21 of 32 tuning:     pls_bagging (26.5s)\n# i 22 of 32 tuning:     pls_fda\n# ...\n# ✔ 22 of 32 tuning:     pls_fda (23.7s)\n# i 23 of 32 tuning:     pls_rda\n# ...\n# ✔ 23 of 32 tuning:     pls_rda (25.1s)\n# i 24 of 32 tuning:     pls_knn\n# ...\n# ✔ 24 of 32 tuning:     pls_knn (26.6s)\n# i 25 of 32 tuning:     umap_svm\n# ...\n# ✔ 25 of 32 tuning:     umap_svm (58.1s)\n# i 26 of 32 tuning:     umap_lasso\n# ...\n# ✔ 26 of 32 tuning:     umap_lasso (54.7s)\n# i 27 of 32 tuning:     umap_ridge\n# ...\n# ✔ 27 of 32 tuning:     umap_ridge (52.1s)\n# i 28 of 32 tuning:     umap_mlp\n# ...\n# ✔ 28 of 32 tuning:     umap_mlp (53.7s)\n# i 29 of 32 tuning:     umap_bagging\n# ✔ 29 of 32 tuning:     umap_bagging (54.2s)\n# i 30 of 32 tuning:     umap_fda\n# ✔ 30 of 32 tuning:     umap_fda (52.7s)\n# i 31 of 32 tuning:     umap_rda\n# ...\n# ✔ 31 of 32 tuning:     umap_rda (52.4s)\n# i 32 of 32 tuning:     umap_knn\n# ...\n# ✔ 32 of 32 tuning:     umap_knn (57.3s)\n\n\nplan(sequential)\n\nВ некоторых случаях вы можете получить сообщение о том, что невозможно вычилить метрику precision (точности) для мультиклассовой классификации: в некоторых фолдах кросс-валидации модель не предсказала ни одного примера для некоторых классов. Если модель не сделала вообще не сделала ни одного предсказания (даже ошибочного), то метрика precision = TP / (TP + FP) становится неопределённой, так как знаменатель равен нулю.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#препроцессинг-tf-idf",
    "href": "multiclass.html#препроцессинг-tf-idf",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Препроцессинг: tf-idf",
    "text": "25.4 Препроцессинг: tf-idf\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"))\n\n# уберем повторы и упорядочим по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 380\n\n\nПодробнее о рецепте см. https://smltar.com/mlregression#firstregression\n\ntfidf_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\ntfidf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#препроцессинг-pca",
    "href": "multiclass.html#препроцессинг-pca",
    "title": "25  Многоклассовая классификация",
    "section": "25.5 Препроцессинг: PCA",
    "text": "25.5 Препроцессинг: PCA\n\npca_rec &lt;- recipe(sentiment ~ review, data = data_train) |&gt;\n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 100)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(review, \"\\\\d+\")\n\n\n• Variable mutation for: stringr::str_remove_all(review, \"[A-Za-z]\")\n\n\n• Tokenization for: review\n\n\n• Stop word removal for: review\n\n\n• Stemming for: review\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: review\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\nНа очень большом числа признаков step_pca() может сильно замедлять вычисления, в этом случае можно попробовать step_pca_truncated() из пакета {embed}.\nТакже стоит помнить, что PCA выполняет линейное снижение размерности, что не всегда подходит. Для нелинейного подхода воспользуйтесь функцией step_umap() из того же пакета.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "dnn.html",
    "href": "dnn.html",
    "title": "26  Глубокое обучение",
    "section": "",
    "text": "26.1 Основные понятия\nВ предыдущих главах мы использовали такие алгоритмы, как регуляризованные линейные модели, машины опорных векторов и наивные байесовские модели для предсказания результатов на основе признаков, включая текстовые данные. Модели глубокого обучения решают те же задачи и преследуют те же цели, однако используются другие алгоритмы.\nГлубокое обучение – это особый раздел машинного обучения. Под “глубиной” в глубоком обучении не подразумевают более глубокое понимание, достигаемое этим подходом; идея заключается в многослойном представлении. Количество слоев, которые делится модель данных, называют глубиной модели (Шолле 2023, 33).\nСлои в модели глубокого обучения соединены в сеть, и такие модели называют нейронными сетями, хотя по сути они работают не так, как человеческий мозг. Слои могут быть соединены по-разному — такие конфигурации называют архитектурами сети.\nВ этом уроке мы познакомимся с полносвязной (dense) нейронной сетью для работы с текстом. Это одна из самых простых архитектур, и обычно такая модель не показывает наилучших результатов на текстовых данных, но с неё удобно начать, чтобы понять сам процесс построения и оценки глубоких моделей для работы с текстом. Кроме того, этот тип архитектуры может быть своеобразным мостом между методами “мешка слов”, которые мы использовали ранее, и более сложными подходами, позволяющими учитывать не только частотность слов, но также их последовательности.\nНа рисунке показана архитектура полносвязной прямой нейронной сети (feed-forward). Входные данные поступают в сеть сразу и полностью (в данном случае — полностью) соединены с первым скрытым слоем. Слой называется «скрытым», потому что не связан с внешним миром; этим занимаются только входной и выходной слои. Нейроны каждого слоя соединяются лишь со следующим слоем. Количество слоев и узлов в каждом из них может меняться; эти параметры называются гиперпараметрами и выбираются исследователем.\nПод обучением сети подразумевается поиск набора значений весов всех слоев в сети, при котором сеть будет правильно отображать образцовые входные данные в соответствующие им результаты. Первоначально весам присваиваются случайные значения, но постепенно они корректируются в нужном направлении. “Нужным” в данном случае считается такое направление, которое минимизирует функцию потерь.\nЗа корректировку отвечает оптимизатор — это алгоритм, который управляет процессом обучения нейронной сети, корректируя веса модели с целью минимизации функции ошибки (или функции потерь). Проще говоря, оптимизатор помогает найти такие значения параметров, при которых сеть даёт наилучшие предсказания. Для этого он реализует алгоритм обратного распространения ошибки (backpropagation): для каждого параметра вычисляется вклад, который он вносит в значение потерь (Шолле 2023, 93).\nЭтот вклад определяется с помощью градиентов. Градиент – это обобщение понятия производной для функций, принимающих тензоры (многомерные массивы чисел) в качестве входных данных (Шолле 2023, 83). Градиент функции f – это вектор, который указывает направление наискорейшего роста этой функции, при этом модуль градиента равен скорости изменения функции в этом направлении.\nОптимизатор обновляет веса пропорционально этим градиентам (с учетом параметра скорости обучения), что позволяет постепенно приближаться к минимуму функции потерь. Градиентный спуск (gradient descent) — это метод оптимизации, который использует вычисленные градиенты для обновления весов сети с целью минимизации функции потерь. Он корректирует веса в направлении, противоположном градиенту (т.е. в сторону уменьшения ошибки).",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#данные",
    "href": "dnn.html#данные",
    "title": "26  Полносвязные нейросети",
    "section": "26.2 Данные",
    "text": "26.2 Данные\nФункция initial_validation_split() создает случайное разделение данных на три части: обучающую (training set), валидационную (validation set) и тестовую (testing set) выборки. Функции training(), validation() и testing() позволяют извлекать соответствующие подмножества данных после разбиения.\n\nset.seed(11032025)\ndata_split &lt;- reviews |&gt; \n  mutate(sentiment = as.factor(sentiment)) |&gt; \n  initial_validation_split(strata = sentiment)\ndata_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;1800/600/600/3000&gt;\n\n\n\ndata_train &lt;- training(data_split)\ndata_validate &lt;- validation(data_split)\ndata_test &lt;- testing(data_split)\n\nРазделим обучающую выборку на 5 фолдов для перекрестной проверки.\n\n# folds\nset.seed(11032025)\nfolds &lt;- vfold_cv(data_train, strata = sentiment, v = 5)\nfolds",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг",
    "href": "dnn.html#препроцессинг",
    "title": "26  Полносвязные нейросети",
    "section": "26.3 Препроцессинг",
    "text": "26.3 Препроцессинг\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"))\n\n# уберем повторы и упорядочим по алфавиту\nstopwords_ru &lt;- sort(unique(stopwords_ru))\n\nМы начнем с того же рецепта, который использовали в прошлый раз. Каждая рецензия рассматривается как “мешок слов”. Число признаков снижено за счет стемминга, удаления цифр, латинских букв, а также стопслов. Снова установим максимальное значение признаков на 1000.\n\nbow_rec &lt;- recipe( ~ review, data = data_train)  |&gt;  \n  step_mutate(review = stringr::str_remove_all(review, \"\\\\d+\")) |&gt; \n  step_mutate(review = stringr::str_remove_all(review, \"[A-Za-z]\")) |&gt; \n  step_tokenize(review) |&gt;\n  step_stopwords(review, custom_stopword_source = stopwords_ru) |&gt;\n  step_stem(review, options = list(language = \"russian\")) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(review) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nФункция prep() вычисляет параметры всех шагов обработки, таких как токенизация, удаление стоп-слов или преобразование в bag-of-words. Функция bake() применяет подготовленный рецепт к обучающим данным.\n\n# prep and bake\nbow_prep &lt;- prep(bow_rec)\n\ntrain_bow_baked &lt;- bake(bow_prep,\n                   new_data = NULL,\n                   composition = \"matrix\")\n\n\nvalid_bow_baked &lt;- bake(bow_prep, \n                    new_data = data_validate,\n                    composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#перекодирование-меток",
    "href": "dnn.html#перекодирование-меток",
    "title": "26  Глубокое обучение",
    "section": "26.6 Перекодирование меток",
    "text": "26.6 Перекодирование меток\nOne-hot кодирование меток классов — это способ представления категориальных переменных в виде бинарных векторов.\n\nclass_train &lt;- data_train |&gt; \n  pull(class)   |&gt; \n  as.factor()  |&gt; \n  as.integer()\n\nФункция to_categorical() из пакета {keras} используется для преобразования вектора классов (представленного в виде целых чисел) в бинарную матрицу классов. Функция принимает вектор целочисленных меток классов, например, {0, 1, 2, 3}, и преобразует его в one-hot матрицу, где каждый класс кодируется бинарным вектором.\nПример:\n     [,1] [,2] [,3]\n[1,]    1    0    0  # Класс 0\n[2,]    0    1    0  # Класс 1\n[3,]    0    0    1  # Класс 2\n[4,]    0    1    0  # Класс 1\n[5,]    1    0    0  # Класс 0\nЗдесь:\n\nКаждая строка соответствует одному образцу.\nКаждый столбец – это конкретный класс.\n1 стоит в позиции индекса класса, остальное – 0.\n\nЭта функция используется в нейронных сетях, потому что выходной слой softmax ожидает one-hot представление меток классов.\n\nclass_train_onehot &lt;- to_categorical(class_train-1, num_classes = 4)\nhead(class_train_onehot)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    1    0    0    0\n[3,]    1    0    0    0\n[4,]    1    0    0    0\n[5,]    1    0    0    0\n[6,]    1    0    0    0\n\n\nТеперь проделаем то же самое для валидационного набора.\n\nclass_valid &lt;- data_validate  |&gt;  \n  pull(class)  |&gt; \n  as.factor()  |&gt; \n  as.integer()\n\nclass_valid_onehot &lt;- to_categorical(class_valid-1, num_classes = 4)\n\n\nhead(class_valid_onehot)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    1    0    0    0\n[3,]    1    0    0    0\n[4,]    1    0    0    0\n[5,]    1    0    0    0\n[6,]    1    0    0    0",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-bow",
    "href": "dnn.html#препроцессинг-bow",
    "title": "26  Глубокое обучение",
    "section": "26.5 Препроцессинг: BOW",
    "text": "26.5 Препроцессинг: BOW\nМы начнем с простейшей модели типа “мешок слов”. Число признаков снижено за счет стемминга, удаления цифр, а также стопслов. Установим максимальное значение признаков на 1000. Обратите внимание, что исходная формула не задаёт зависимой переменной. Это нужно для удобства преобразования в матричный формат.\n\nbow_rec &lt;- recipe( ~ description, data = data_train)  |&gt;  \n  step_mutate(description = stringr::str_remove_all(description, \"\\\\d+\")) |&gt; \n  step_tokenize(description) |&gt;\n  step_stopwords(description) |&gt;\n  step_tokenfilter(all_predictors(), \n                   max_tokens = 1000, \n                   min_times = 2) |&gt; \n  step_tfidf(all_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nbow_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(description, \"\\\\d+\")\n\n\n• Tokenization for: description\n\n\n• Stop word removal for: description\n\n\n• Text filtering for: all_predictors()\n\n\n• Term frequency-inverse document frequency with: all_predictors()\n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\nПрименим рецепт к обучающим данным. В функции bake() аргумент composition = \"matrix\" определяет формат возвращаемого результата. По умолчанию bake() возвращает tibble (или data.frame), где каждая строка — это наблюдение, а столбцы — признаки. Но мы планируем отдавать признаки нейросети, а она принимает матрицы на вход. Число элементов матрицы – 72 млн.\n\nbow_rec_prep &lt;- prep(bow_rec) \n\ntrain_bow_rec &lt;- bow_rec_prep |&gt; \n  bake(new_data = NULL,\n       composition = \"matrix\")\n\n\nvalid_bow_rec &lt;- bake(bow_rec_prep, \n                    new_data = data_validate,\n                    composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#dnn-bow",
    "href": "dnn.html#dnn-bow",
    "title": "26  Полносвязные нейросети",
    "section": "26.5 DNN: BOW",
    "text": "26.5 DNN: BOW\n\nbow_model &lt;- keras3::keras_model_sequential() |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 3, activation = \"softmax\")\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\nbow_model  |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\nbow_history &lt;- bow_model |&gt; \n  fit(\n    x = train_bow_baked,\n    y = sentiment_train,\n    batch_size = 100,\n    epochs = 20,\n    validation_data = list(valid_bow_baked, sentiment_valid), \n    verbose = FALSE\n  )\n\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 1\n        loss: 0.002697\nval_accuracy: 0.5617\n    val_loss: 1.828 \n\n\n\nplot(bow_history) \n\n\n\n\n\n\n\n\n\nbow_df &lt;- as.data.frame(bow_history)\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 1\n        loss: 0.002697\nval_accuracy: 0.5617\n    val_loss: 1.828",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-one-hot",
    "href": "dnn.html#препроцессинг-one-hot",
    "title": "26  Полносвязные нейросети",
    "section": "26.6 Препроцессинг: One-Hot",
    "text": "26.6 Препроцессинг: One-Hot\n\nonehot_rec &lt;- recipe( ~ review, data = data_train)  |&gt;  \n  step_tokenize(review)  |&gt;  \n  step_tokenfilter(review, max_tokens = 2000) |&gt; \n  step_sequence_onehot(review, sequence_length = 400)\n\n\n# prep and bake\nonehot_prep &lt;- prep(onehot_rec)\n\n\ntrain_onehot_baked &lt;- bake(onehot_prep,\n                           new_data = NULL,\n                           composition = \"matrix\")\n\n\nvalid_onehot_baked &lt;- bake(onehot_prep, \n                           new_data = data_validate,\n                           composition = \"matrix\")",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#dnn-one-hot",
    "href": "dnn.html#dnn-one-hot",
    "title": "26  Полносвязные нейросети",
    "section": "26.7 DNN: One-hot",
    "text": "26.7 DNN: One-hot\n\ndense_model &lt;- keras_model_sequential() |&gt; \n  layer_embedding(input_dim = 2001,\n                  output_dim = 64) |&gt; \n  layer_flatten() |&gt; \n  layer_dense(units = 64, activation = \"relu\") |&gt; \n  layer_dense(units = 3, activation = \"softmax\")\n\ndense_model\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (Embedding)             │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ flatten (Flatten)                 │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_4 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\ndense_model |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n\ndense_history &lt;- dense_model |&gt; \n  fit(\n    x = train_onehot_baked ,\n    y = sentiment_train,\n    batch_size = 100,\n    epochs = 20,\n    validation_data = list(valid_onehot_baked, sentiment_valid), \n    verbose = FALSE\n  )\n\n\nplot(dense_history)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "dnn.html#предсказание",
    "href": "dnn.html#предсказание",
    "title": "26  Полносвязные нейросети",
    "section": "26.8 Предсказание",
    "text": "26.8 Предсказание\n\ndense_res &lt;- predict(object = dense_model, \n                   x = valid_onehot_baked\n)\n\n19/19 - 0s - 5ms/step\n\nhead(dense_res)\n\n          [,1]        [,2]        [,3]\n[1,] 0.9899594 0.005354058 0.004686453\n[2,] 0.3758484 0.510795474 0.113356218\n[3,] 0.9771895 0.000415861 0.022394525\n[4,] 0.9713955 0.021642592 0.006962005\n[5,] 0.6321248 0.336982816 0.030892473\n[6,] 0.3736731 0.574162245 0.052164607\n\n\n\nfactor_names &lt;- tibble(levels = levels(data_train$sentiment),\n                       .pred_clas = 1:3)\n\nfactor_names\n\n\n  \n\n\n\n\npred_clas &lt;- apply(dense_res, 1, which.max)\n\nhead(pred_clas)\n\n[1] 1 2 1 1 1 2\n\n\n\ndense_res_tbl &lt;- tibble(truth = data_validate$sentiment, \n                      .pred_clas = pred_clas) |&gt; \n  left_join(factor_names) |&gt; \n  dplyr::select(-(.pred_clas)) |&gt; \n  rename(.pred_clas = levels) |&gt; \n  mutate(.pred_clas = as.factor(.pred_clas),\n         truth = as.factor(truth))\n\nmetrics(dense_res_tbl, truth = truth, estimate = .pred_clas)\n\n\n  \n\n\n\n\ndense_res_tbl |&gt; \n  group_by(truth, .pred_clas) |&gt; \n  summarise(n = n()) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(truth, .pred_clas, fill = n)) +\n  geom_tile() +\n  geom_text(aes(label = n)) +\n  scale_fill_gradient2(low = \"#eaeff6\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\"),\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  ggtitle(\"Dense Neural Network\")\n\n`summarise()` has grouped output by 'truth'. You can override using the\n`.groups` argument.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Полносвязные нейросети</span>"
    ]
  },
  {
    "objectID": "multiclass.html#отбор-переменных",
    "href": "multiclass.html#отбор-переменных",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Отбор переменных",
    "text": "25.2 Отбор переменных\nДля построения модели берем 500 наиболее частотных слов (токенов), наименее связанных с тематикой.\n\nmfw &lt;- make.frequency.list(corpus_samples_clean)[1:500]\nsample(mfw, 20)\n\nСоставим матрицу с частотностями.\n\ncorpus_tf &lt;- stylo::make.table.of.frequencies(corpus_samples_clean, mfw) |&gt; \n  as.data.frame.matrix() |&gt; \n  rownames_to_column(\"id\") |&gt; \n  as_tibble()\n\n\ncorpus_tf\n\n\n  \n\n\n\nМы будем определять автора, поэтому разделим первый столбец на два.\n\ncorpus_tf &lt;- corpus_tf |&gt; \n  separate(id, into = c(\"author\", \"title\", NA), sep = \"_\") \ncorpus_tf\n\n\n  \n\n\n\nПосмотрим, сколько отрывков для каждого автора в корпусе.\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  arrange(n)\n\n\n  \n\n\n\nДля ускорения вычислений пока удалим авторов, у которых не так много отрывков.\n\ncorpus_top &lt;- corpus_tf |&gt; \n  add_count(author) |&gt; \n  filter(n &gt; 120) |&gt; \n  select(-n, -title) \n\n\ncorpus_top |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#препроцессинг",
    "href": "multiclass.html#препроцессинг",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Препроцессинг",
    "text": "25.4 Препроцессинг\nБольшую часть препроцессинга мы сделали в stylo, поэтому нам нужно всего несколько шагов.\n\ntf_rec &lt;- recipe(author ~ ., data = data_train) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\ntf_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\nТакже создадим рецепт, в котором используем главные компоненты в качестве предикторов. В PCA максимальное число компонент равно минимуму из\n\nчисла переменных (признаков) в исходных данных;\nчисла наблюдений минус один.\n\nВ нашем случае классов 8.\n\npca_rec &lt;- recipe(author ~ ., data = data_train) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_pca(all_predictors(), num_comp = 7)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nНа очень большом числе признаков step_pca() может сильно замедлять вычисления, в этом случае можно попробовать step_pca_truncated() из пакета {embed}. Также стоит помнить, что PCA выполняет линейное снижение размерности, что подходит не для всех данных. Для нелинейного подхода воспользуйтесь функцией step_umap() из того же пакета.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#логистическая-регрессия",
    "href": "multiclass.html#логистическая-регрессия",
    "title": "25  Многоклассовая классификация",
    "section": "25.6 Логистическая регрессия",
    "text": "25.6 Логистическая регрессия\nВы уже знаете, что линейные модели используются в задачах регрессии:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\]\nгде:\n\ny — предсказание модели,\n\\(x_1, x_2, ..., x_p\\) — признаки (например, частоты слов),\n\\(\\beta_0\\) — свободный член (intercept),\n\\(\\beta_i\\) — коэффициенты модели, отражающие вклад соответствующего признака.\n\nВ задачах классификации, таких как определение темы текста, используют не просто линейную регрессию, а логистическую регрессию. Логистическая регрессия применяется для задач, где исходная переменная y категориальна (например, “спорт” или “политика”). Она предсказывает вероятность принадлежности объекта к одному из классов на основании логистической функции ( (иногда также называемой сигмоидой или логит-функцией):\n\\[p(y = 1 \\mid x) = \\frac{1}{1 + e^{-z}}\\]\nЗдесь значение p – вероятность принадлежности к положительному классу, а z – это линейная комбинация признаков:\n\\[z = β_0 + β_1x_1 + β_2x_2 + … + β_nx_n\\]\nПроведя некоторые преобразования, получаем:\n\\[\\text{logit}(p) = \\log\\left( \\frac{p}{1 - p} \\right) = z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\\] Левая часть уравнения называется “логит” (он же логарифм риска). Само по себе значение z может принимать любые значения. Однако, когда вы подставляете z в сигмоиду:\n\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\nтогда результат всегда ограничен от 0 до 1.\nЭто значение интерпретируется как вероятность принадлежности к положительному классу. Если полученная вероятность ≥ 0.5 — модель предсказывает класс 1 (“положительный”). Если &lt; 0.5 — класс 0 (“отрицательный”). Значение 0.5 является границей между классами.\n\nТаким образом, даже если логит z может принимать любые значения от минус бесконечности до плюс бесконечности, благодаря сигмоиде результат всегда находится между 0 и 1. Это делает логистическую регрессию очень удобной для задач классификации. Однако при большом числе признаков эта модель склонна к переобучению (overfitting) — она приспосабливается слишком точно под обучающую выборку, что ухудшает её обобщающую способность.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#регуляризация-lasso-и-ridge",
    "href": "multiclass.html#регуляризация-lasso-и-ridge",
    "title": "25  Многоклассовая классификация",
    "section": "25.7 Регуляризация: Lasso и Ridge",
    "text": "25.7 Регуляризация: Lasso и Ridge\nКогда мы анализируем текстовые данные (например, классифицируем статьи по жанру, определяем тональность отзывов и т.д.), мы сталкиваемся с задачей представления текстов в числовом виде. Один из распространённых способов — построение мешка слов (bag-of-words), в котором каждый уникальный термин (слово, биграмма и пр.) — это отдельный признак. В результате для небольшого корпуса текстов может получиться десятки тысяч признаков (столбцов), большинство из которых обнулены (то есть в документе конкретное слово отсутствует). Такие данные называются разреженными (sparse), а количество признаков может значительно превышать количество наблюдений (документов).\nКогда число признаков очень велико, далеко не все алгоритмы машинного обучения работают одинаково хорошо. Некоторые, как, например, метод k-ближайших соседей (k-NN), плохо справляются с высокоразмерными пространствами. Это связано с тем, что в таких пространствах наблюдения становятся «далёкими» друг от друга, и расстояния между точками плохо отражают истинные различия между текстами. Это называют проклятием размерности (curse of dimensionality).\nВ таких случаях особенно полезны так называемые линейные модели с регуляризацией.\nЧтобы справиться с переобучением и улучшить интерпретируемость модели, используют регуляризацию — добавление штрафа за слишком большие коэффициенты β. За счет штрафа модель старается хорошо описывать данные, но при этом не сильно “разгоняться” в значениях коэффициентов.\nСуществуют два основных типа регуляризации:\n\nL1-регуляризация или Lasso (Least Absolute Shrinkage and Selection Operator),\nL2-регуляризация или Ridge.\n\nПри L2-регуляризации (Ridge Regression, гребневая регрессия) штрафом является сумма квадратов весов (здесь \\(w\\) - это вектор весов модели):\n\\[RSS + \\lambda \\sum_{j=1}^{p} w_j^2\\]\nЭтот метод уменьшает величину весов, не зануляя их. Он хорошо работает, когда все признаки важны.\nL1-регуляризация (Lasso Regression) использует как штраф сумму модулей весов:\n\\[RSS + \\lambda \\sum_{j=1}^{p} |w_j|\\] Этот метод может занулять отдельные коэффициенты, то есть по сути производит отбор признаков.\nОбъединение обеих регуляризаций называют Elastic Net. Этот метод позволяет достичь баланса между отбором признаков и сглаживанием коэффициентов.\n\\[RSS + \\lambda_1 \\| {w} \\|_1 + \\lambda_2 \\| {w} \\|_2^2\\]\nПоскольку в нашем датасете несколько классов, то мы применим многоклассовую логистическую регрессию. Пакет {tidymodels} предоставляет удобные инструменты для построения и настройки моделей с регуляризацией.\n\nlasso_spec &lt;- multinom_reg(penalty = tune(), mixture = 1) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\n\nridge_spec &lt;- multinom_reg(penalty = tune(), mixture = 0) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\nОбратите внимание на аргумент mixture:\n\nmixture = 1 задает лассо-модель;\nmixture = 0 - это гребневая регрессия;\n0 &lt; mixture &lt; 1 соответствуют Elastic Net.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#первая-модель",
    "href": "multiclass.html#первая-модель",
    "title": "25  Многоклассовая классификация",
    "section": "25.8 Первая модель",
    "text": "25.8 Первая модель\nПоскольку в нашем датасете несколько классов, то мы применим многоклассовую логистическую регрессию. Пакет {tidymodels} предоставляет удобные инструменты для построения и настройки моделей с регуляризацией.\n\nlasso_spec &lt;- multinom_reg(penalty = tune(), mixture = 1) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"glmnet\")\n\nОбратите внимание на аргумент mixture:\n\nmixture = 1 задает лассо-модель;\nmixture = 0 - это гребневая регрессия;\n0 &lt; mixture &lt; 1 соответствуют Elastic Net.\n\nВыбираем лассо, чтобы отобрать наиболее значимые переменные (признаки). Гиперпараметр подберем путем настройки.\n\nlasso_param &lt;- extract_parameter_set_dials(lasso_spec)\n  \nlasso_grid &lt;- lasso_param |&gt; \n  grid_regular(levels = 3)\n\nlasso_grid\n\n\n  \n\n\n\n\nlasso_wflow &lt;- workflow() |&gt; \n  add_model(lasso_spec) |&gt; \n  add_recipe(tf_rec)\n\nlasso_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: multinom_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nMultinomial Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\nЗдесь придется немного (или много) подождать. Параллелизация поможет ускорить вычисления. Сохраняем воркфлоу для сравнения с последующими моделями.\nЕсли не уверены, сколько у вас процессоров, выполните:\n\nparallel::detectCores()\n\n[1] 8\n\n\nВ контексте обучения моделей с использованием кросс-валидации и фреймворка tidymodels в R нет смысла задействовать больше процессоров (ядер), чем количество фолдов (folds), потому что количество задач (циклов обучения) на этапе кросс-валидации ограничено числом фолдов. Поэтому, сколько бы ни было ядер в системе, в каждый момент времени не может быть запущено больше v параллельных задач.\n\n#install.packages(\"glmnet\")\nlibrary(tictoc)\nlibrary(future)\n\nplan(multisession, workers = 5)\n\ntic()\nset.seed(06042025)\nlasso_tune &lt;- lasso_wflow |&gt; \n  tune_grid(\n    resamples = folds, \n    grid = lasso_grid,\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_resamples(save_pred = TRUE, save_workflow = TRUE)\n  )\n\nlasso_tune \n\ntoc()\n# 12.376 sec elapsed\nplan(sequential)\n\n\nautoplot(lasso_tune)\n\n\n\n\n\n\n\n\nНаша модель уже достигла достаточно высокой точности расходимся.\n\ncollect_predictions(lasso_tune) |&gt; \n  roc_curve(truth = author, .pred_Bulgakov:.pred_Turgenev)  |&gt; \n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  coord_fixed() +\n  theme_light()\n\n\n\n\n\n\n\n\nВспомним, что все это значит:\nSensitivity (Чувствительность) = True Positive Rate (TPR):\n\nФормула: \\(TP/(TP+FN)\\)\nЭто доля верно определенных положительных примеров среди всех положительных примеров\nПоказывает, насколько хорошо модель находит нужные объекты из всех существующих\nДругие названия: полнота (recall), истинноположительная доля\nОсь Y на ROC-кривой\n\n1-Specificity = False Positive Rate (FPR):\n\nФормула: \\(FP/(FP+TN) = 1 - TN/(FP+TN) = 1 - Specificity\\)\nЭто доля неверно определенных положительных примеров среди всех отрицательных примеров\nПоказывает, насколько часто модель ошибочно причисляет негативные примеры к позитивным\nДругие названия: ложноположительная доля\nОсь X на ROC-кривой\n\nВ нашем контексте:\n\nsensitivity (для автора А) – это доля текстов автора А, которые правильно определены как тексты автора А,\n1-specificity (для автора А) – это доля текстов НЕ автора А, которые ошибочно определены как тексты автора А.\n\n\ncollect_metrics(lasso_tune) |&gt; \n  filter(.metric == \"f_meas\")\n\n\n  \n\n\n\n\nlasso_tune |&gt; \n  collect_predictions() |&gt;\n  conf_mat(truth = author, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#svm-в-tidymodels",
    "href": "multiclass.html#svm-в-tidymodels",
    "title": "25  Многоклассовая классификация",
    "section": "25.10 SVM в {tidymodels}",
    "text": "25.10 SVM в {tidymodels}\n\nsvm_spec &lt;- svm_linear(cost = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"LiblineaR\")\n\nsvm_spec\n\nLinear Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = tune()\n\nComputational engine: LiblineaR \n\n\nПояснение параметров:\n\ncost = tune() — здесь мы указываем, что параметр cost будет подобран автоматически (в процессе переподбора гиперпараметров с помощью tune()).\nset_mode(\"classification\") — устанавливает режим задачи как классификацию.\nset_engine(\"LiblineaR\") — указывает, что используется движок LiblineaR, реализующий SVM с линейным ядром (в пакете {tidymodels}).\n\nПараметр cost — это коэффициент штрафа за ошибки классификации. Он контролирует компромисс между количеством ошибок на обучающем наборе (т.е. насколько сильно модель стремится избежать ошибок) и шириной “маржи” — расстояния между разделительной гиперплоскостью и ближайшими точками разных классов.\nЕсли cost большое, модель старается классифицировать обучающую выборку как можно точнее: допускается меньшая ширина маржи, но это может привести к переобучению (overfitting).\nЕсли cost меньше, то модель допускает больше ошибок на обучении: маржа будет шире, это может привести к недообучению (underfitting), но лучше обобщается на новых данных.\n\nsvm_param &lt;- extract_parameter_set_dials(svm_spec)\n\nsvm_grid &lt;- svm_param |&gt; \n  grid_regular(levels = 3)\n\nsvm_grid",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multivar.html#svr-в-tidymodels",
    "href": "multivar.html#svr-в-tidymodels",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.7 SVR в tidymodels",
    "text": "23.7 SVR в tidymodels\nФункция translate() позволяет понять, как parsnip переводит пользовательский код на язык пакета.\n\nsvm_spec &lt;- svm_linear() |&gt;\n  set_engine(\"LiblineaR\") |&gt; \n  set_mode(\"regression\")\n\nsvm_spec |&gt; \n  translate()\n\nLinear Support Vector Machine Model Specification (regression)\n\nComputational engine: LiblineaR \n\nModel fit template:\nLiblineaR::LiblineaR(x = missing_arg(), y = missing_arg(), type = 11, \n    svr_eps = 0.1)\n\n\nПока это просто спецификация модели без данных и без формулы. Добавим ее к воркфлоу.\n\nsvm_wflow &lt;- workflow() |&gt; \n  add_model(svm_spec)\n\nsvm_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: None\nModel: svm_linear()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Support Vector Machine Model Specification (regression)\n\nComputational engine: LiblineaR",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#деревья-решений",
    "href": "multivar.html#деревья-решений",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.12 Деревья решений",
    "text": "23.12 Деревья решений\nДеревья решений применяются как для задача регрессии, так и для задач классификации.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#случайный-лес-в-tidymodels",
    "href": "multivar.html#случайный-лес-в-tidymodels",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.14 Случайный лес в tidymodels",
    "text": "23.14 Случайный лес в tidymodels\nУточним, какие движки доступны для случайных лесов.\n\nshow_engines(\"rand_forest\")\n\n\n  \n\n\n\nСоздадим спецификацию модели. Деревья используются как в задачах классификации, так и в задачах регрессии, поэтому задействуем функцию set_mode().\n\nrf_spec &lt;- rand_forest(trees = 1000) |&gt; \n  set_engine(\"ranger\") |&gt; \n  set_mode(\"regression\")\n\n\nrf_wflow &lt;- workflow() |&gt; \n  add_model(rf_spec) |&gt; \n  add_recipe(books_rec)\n\nrf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_dummy()\n• step_normalize()\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nОбучение займет чуть больше времени.\n\nrf_rs &lt;- fit_resamples(\n  rf_wflow,\n  books_folds,\n  control = control_resamples(save_pred = TRUE)\n)\n\nМы видим, что среднеквадратическая ошибка уменьшилась, а доля объясненной дисперсии выросла.\n\ncollect_metrics(rf_rs)  |&gt; \n  gt::gt()\n\n\n\n\n\n\n\n.metric\n.estimator\nmean\nn\nstd_err\n.config\n\n\n\n\nrmse\nstandard\n6.5280907\n10\n0.49714066\nPreprocessor1_Model1\n\n\nrsq\nstandard\n0.5787228\n10\n0.06581642\nPreprocessor1_Model1\n\n\n\n\n\n\n\nТем не менее на графике можно заметить нечто странное: наша модель систематически переоценивает низкие значения и недооценивает высокие. Это связано с тем, что случайные леса не очень подходят для работы с разреженными данными (Hvitfeldt и Silge 2022).\n\nrf_rs |&gt; \n  collect_predictions() |&gt; \n  ggplot(aes(price, .pred, color = id)) +\n  geom_jitter(alpha = 0.3) +\n  geom_abline(lty = 2, color = \"grey80\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 50), ylim = c(0, 50))",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#деревья-решений-понятия",
    "href": "multivar.html#деревья-решений-понятия",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.12 Деревья решений: понятия",
    "text": "23.12 Деревья решений: понятия\nДеревья решений применяются как для задача регрессии, так и для задач классификации.\nРегрессионные деревья строят последовательное разбиение пространства признаков таким образом, чтобы минимизировать среднеквадратичную ошибку (MSE) в каждом из подмножеств.\nДля этого данные делятся на группы, в которых отклик (целевое значение) как можно более “однороден”. Каждое разбиение осуществляется на основе признаков (факторов), а в листьях дерева находятся средние значения отклика для соответствующей подгруппы. Вот так, например, может выглядеть предсказание расхода топлива для автомобиля (на основе датасета mtcars).\n\nДеревья легко показать графически, их легко интерпретировать, они хорошо справляются с категориальными предикторами (без создания dummy variables). Они особенно хорошо подходят для тех случаев, когда между откликом и предикторами существует нелинейная и сложная зависимость.\nНо деревья страдают от высокой дисперсии, т.е. если мы случайным образом разобьем обучающие данные на две части и построим дерево решений на основе каждой из них, полученные результаты могут оказаться довольно разными.\nЧтобы с этим справиться, используют три основных метода: бэггинг, случайный лес и бустинг.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "multivar.html#бэггинг-случайный-лес-бустинг",
    "href": "multivar.html#бэггинг-случайный-лес-бустинг",
    "title": "23  Регрессионные модели с tidymodels",
    "section": "23.13 Бэггинг, случайный лес, бустинг",
    "text": "23.13 Бэггинг, случайный лес, бустинг\n\nБэггинг — это метод построения ансамбля моделей путем:\n\n\nповторного случайного выбора подвыборок из обучающего набора данных (бутстрэп);\nобучения на каждой из этих подвыборок дерева решений;\nобъединения (агрегации) результатов предсказаний этих моделей (для регрессии – усреднение предсказаний; для классификации: голосование).\n\nХотя бэггинг может улучшить предсказания многих методов, он особено полезен для деревьев решений.\n\nСлучайный лес – это частный случай бэггинга. Каждое дерево обучается на случайной выборке с возвращением (бутстрэп), но при построении дерева выбираются не все признаки, а случайное подмножество признаков. Это снижает корреляцию между деревьями и повышает качество ансамбля.\nБустинг работает похожим образом, но деревья строятся последовательно: каждое дерево выращивается с использованием информации по ранее выращенным деревьям. Бустинг не задействует бутстрэп, деревья обучаются на всем наборе данных. Из-за того, что деревья обучаются последовательно, его сложнее запараллелить.\n\nСлучайный лес и бустинг плохо поддаются интерпретации.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Регрессионные модели с `tidymodels`</span>"
    ]
  },
  {
    "objectID": "regression.html#anova",
    "href": "regression.html#anova",
    "title": "22  Регрессионный анализ",
    "section": "22.3 ANOVA",
    "text": "22.3 ANOVA\n\nanova(fit_null, fit) |&gt; \n  export_table()\n\nRes.Df |     RSS | Df | Sum of Sq |     F |   Pr(&gt;F)\n----------------------------------------------------\n    29 | 1408.88 |    |           |       |         \n    28 |  878.44 |  1 |    530.44 | 16.91 | 3.11e-04\n\n\nФункция anova() сравнивает две вложенные линейные регрессионные модели с помощью анализа дисперсии. Цель — выяснить, добавляет ли переменная OxfordDst значительное улучшение модели по сравнению с моделью без предикторов.\n\n\n\n\n\n\n\nСтолбец\nЗначение\n\n\n\n\nRes.Df\nОстаточные степени свободы: число наблюдений минус число параметров модели.\n\n\nRSS\nResidual Sum of Squares — сумма квадратов остатков. Чем меньше, тем лучше.\n\n\nDf\nРазница в степени свободы между моделями (число добавленных предикторов).\n\n\nSum of Sq\nУлучшение, достигнутое за счёт добавленного предиктора (OxfordDst), то есть разница в RSS.\n\n\nF\nF-статистика для оценки значимости улучшения модели.\n\n\nPr(&gt;F)\np-значение: насколько вероятно наблюдать такую F-статистику случайно.\n\n\n\nRSS уменьшилась с 1408.88 до 878.44 после добавления переменной OxfordDst, значит модель улучшилась. F-статистика = 16.908, а p-value значительно ниже уровня значимости 0.05. Три звездочки (***) означают статистически значимую разницу между моделями.\nF-статистика — это статистика, которая используется для оценки качества модели в анализе дисперсии (ANOVA) и в регрессионном анализе. Она показывает, насколько хорошо модель с предикторами объясняет данные по сравнению с моделью без предикторов (или с меньшим их числом). Чем больше значение F, тем сильнее улучшение модели при добавлении переменных. Если получить такую большую F при случайных данных маловероятно (что отражает малое p-значение), то мы делаем вывод, что переменная значимо улучшает модель.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#сравнение-моделей-в-easystats",
    "href": "regression.html#сравнение-моделей-в-easystats",
    "title": "22  Регрессионный анализ",
    "section": "22.4 Сравнение моделей в {easystats}",
    "text": "22.4 Сравнение моделей в {easystats}\nДля сравнения моделей полезны следующие функции:\n\ncompare_models(fit_null, fit)\n\n\n  \n\n\n\n\ncompare_performance(fit_null, fit, rank = TRUE)\n\n\n  \n\n\n\n\ncompare_performance(fit_null, fit) |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\nlibrary(report)\ncompare_performance(fit_null, fit) |&gt; \n  report()\n\nWe compared two models; lm (R2 = 0.00, adj. R2 = 0.00, AIC = 204.62, BIC =\n207.42, RMSE = 6.85, Sigma = 6.97) and lm (R2 = 0.38, adj. R2 = 0.35, AIC =\n192.44, BIC = 196.65, RMSE = 5.41, Sigma = 5.60).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#параметры-модели",
    "href": "regression.html#параметры-модели",
    "title": "22  Регрессионный анализ",
    "section": "22.3 Параметры модели",
    "text": "22.3 Параметры модели\n\n22.3.1 Коэффициенты модели\nПервый столбец в таблице с параметрами содержит коэффициенты модели.\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00\n\n\n\n\n\n\n\nЭто значит, что наши данные описываются функцией:\n\nextract_eq(fit, use_coefs = TRUE)\n\n\\[\n\\operatorname{\\widehat{OxfordPct}} = 20.88 - 0.12(\\operatorname{OxfordDst})\n\\]\n\n\nИнтуитивно понятно, что коэффициент \\(\\beta_1\\) связан с ковариацией (мерой совместной изменчивости двух величин). Действительно, он рассчитывается по формуле:\n\\[\\beta_1=\\frac{Cov(x,y)}{Var(x)}\\]\n\n\nПроверить.\n\n\nx &lt;- OxfordPots$OxfordDst\ny &lt;- OxfordPots$OxfordPct\n\nbeta_1&lt;- cov(x, y) / var(x)\nbeta_1\n\n[1] -0.1229049\n\n\n\nЗная \\(\\beta_1\\), можно вычислить \\(\\beta_0\\) по формуле:\n\\[\\beta_0=\\bar y - \\beta_1 \\bar x\\]\n\n\nСнова проверим.\n\n\nbeta_0 = mean(y) - beta_1 * mean(x)\nbeta_0\n\n[1] 20.87665\n\n\n\n\n\n22.3.2 Стандартные ошибки коэффициентов\nДля обоих коэффициентов приведена стандартная ошибка и t-статистика. Столбец t, как легко убедиться, содержит результат деления коэффицентов на стандартную ошибку.\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00\n\n\n\n\n\n\n\n\n\nКак рассчитываются стандартные ошибки.\n\nСтандартная ошибка для \\(\\beta_0\\) рассчитывается по формуле:\n\\[SE(\\beta_0)=\\sqrt{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}} \\times \\sqrt{\\frac{1}{n}+\\frac{\\bar x^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\]\nПервый множитель в этой формуле – это дисперсия остатков модели. Чем она больше, тем больше неопределенность. На второй множитель влияет как размер выборки, так и разброс независимой переменной x: чем больше размер выборки n, тем меньше \\(\\frac{1}{n}\\) и чем больше \\(Σ(x - \\bar x)^2\\), тем меньше второй множитель. Посчитаем вручую и сравним с результатом, который возвращает команда summary(fit).\n\nx_bar &lt;- mean(x)\n\nmult1 &lt;- sqrt(sum(fit$residuals^2) / 28)\nmult2 &lt;- sqrt(1/30 + ( x_bar^2 / sum((x - x_bar)^2)))\n\nmult1 * mult2\n\n[1] 2.233557\n\n\nСтандартная ошибка для \\(\\beta_1\\) рассчитывается по формуле:\n\\[SE(b_1)=\\sqrt{\\frac{\\frac{\\sum_{i=1}^n\\epsilon^2}{n-2}}{\\sum_{i=1}^n(x_i-\\bar x)^2}}\\]\nБольшая дисперсия остатков (в числителе) будет приводить к увеличению ошибки, а размах \\(x_i\\) – к уменьшению; интуитивно это объясняется тем, что в таком случае у нас больше информации для оценивания угла наклона. Снова перепроверим.\n\nmult1 / sqrt(sum((x - x_bar)^2))\n\n[1] 0.02989016\n\n\n\nФункция geom_smooth добавляет стандартную ошибку коэффициента наклона на график в виде серой полосы, которая означает, что с вероятностью 95% (значение по умолчанию, которое можно поменять) истинное значение отклика находится в этой зоне (predicted ± 1.95 * se). В статистике это называется доверительный интервал.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  geom_smooth(method = \"lm\", color = cols[2], \n              se = TRUE, level = 0.95) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\n22.3.3 P-значения\nСтолбец p.value указывает, какова вероятность случайно получить такое значение. В нашем случае – почти 0, что говорит о том, что доля оксфордской керамики на участке действительно зависит от расстояния.\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n20.88\n2.23\n0.95\n16.30\n25.45\n9.35\n28.00\n0.00\n\n\nOxfordDst\n−0.12\n0.03\n0.95\n−0.18\n−0.06\n−4.11\n28.00\n0.00\n\n\n\n\n\n\n\n\n\nКак считается p-value.\n\n\ntidy(fit) |&gt; \n  transmute(t_stat = estimate / std.error) |&gt; \n  mutate(p_val = 2*pt(abs(t_stat), 28, lower.tail = FALSE)) |&gt; \n  export_table()\n\nt_stat |    p_val\n-----------------\n9.35   | 4.18e-10\n-4.11  | 3.11e-04\n\n\nРезультат, возвращаемый функцией pt(), умножается на два, т.к. используется двусторонний t-test. Буква p в названии означает функцию распределения вероятностей (probability), а t – распределение Стьюдента для заданного числа степеней свободы (28 в нашем случае).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "regression.html#сравнение-моделей",
    "href": "regression.html#сравнение-моделей",
    "title": "22  Регрессионный анализ",
    "section": "22.5 Сравнение моделей",
    "text": "22.5 Сравнение моделей\n\n22.5.1 Нулевая модель\nВажно знать, что следующие два вызова возвращают одинаковые модели.\n\nfit1 &lt;- lm(OxfordPct ~ OxfordDst, data = OxfordPots)\nfit2 &lt;- lm(OxfordPct ~ 1 + OxfordDst, data = OxfordPots)\n\n\nfit1$coef == fit2$coef\n\n(Intercept)   OxfordDst \n       TRUE        TRUE \n\n\nЕдиница в вызове функции означает пересечение оси y, то есть свободный член. Это значит, что мы можем построить нулевую модель, где любому значению x будет соответствовать одно и то же (среднее) значение y.\n\nfit_null &lt;- lm(OxfordPct ~ 1, data = OxfordPots)\nparameters(fit_null)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\nCI\nCI_low\nCI_high\nt\ndf_error\np\n\n\n\n\n(Intercept)\n12.712\n1.273\n0.950\n10.109\n15.314\n9.989\n29.000\n0.000\n\n\n\n\n\n\n\nЕдинственный коэффициент в таком случае совпадает со средним значением y.\n\nmean(OxfordPots$OxfordPct)\n\n[1] 12.71167\n\n\nНа графике это будет выглядеть вот так.\n\nOxfordPots |&gt; \n  ggplot(aes(OxfordDst, OxfordPct)) +\n  # обратите внимание на формулу!\n  geom_smooth(method = \"lm\", formula = y ~ 1,\n              color = cols[2], se = FALSE) +\n  geom_point(color = cols[3], \n              size = 3, \n              alpha = 0.6\n              ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nТакая модель может быть использована для сравнения, чтобы понять, насколько мы выиграли, добавив предикторы.\n\n\n22.5.2 ANOVA\nФункция anova() сравнивает две вложенные линейные регрессионные модели с помощью анализа дисперсии. Цель — выяснить, добавляет ли переменная OxfordDst значительное улучшение модели по сравнению с моделью без предикторов.\n\nanova(fit_null, fit)\n\n\n  \n\n\n\n\n\nRes.Df |     RSS | Df | Sum of Sq |     F |   Pr(&gt;F)\n----------------------------------------------------\n    29 | 1408.88 |    |           |       |         \n    28 |  878.44 |  1 |    530.44 | 16.91 | 3.11e-04\n\n\n\n\n\n\n\n\n\nСтолбец\nЗначение\n\n\n\n\nRes.Df\nОстаточные степени свободы: число наблюдений минус число параметров модели.\n\n\nRSS\nResidual Sum of Squares — сумма квадратов остатков. Чем меньше, тем лучше.\n\n\nDf\nРазница в степени свободы между моделями (число добавленных предикторов).\n\n\nSum of Sq\nУлучшение, достигнутое за счёт добавленного предиктора (OxfordDst), то есть разница в RSS.\n\n\nF\nF-статистика для оценки значимости улучшения модели.\n\n\nPr(&gt;F)\np-значение: насколько вероятно наблюдать такую F-статистику случайно.\n\n\n\nRSS уменьшилась с 1408.88 до 878.44 после добавления переменной OxfordDst, значит модель улучшилась. F-статистика = 16.908, а p-value значительно ниже уровня значимости 0.05. Три звездочки (***) означают статистически значимую разницу между моделями.\n\n\n\n\n\n\nНа заметку\n\n\n\nF-статистика — это статистика, которая используется для оценки качества модели в анализе дисперсии (ANOVA) и в регрессионном анализе. Она показывает, насколько хорошо модель с предикторами объясняет данные по сравнению с моделью без предикторов (или с меньшим их числом). Чем больше значение F, тем сильнее улучшение модели при добавлении переменных. Если получить такую большую F при случайных данных маловероятно (что отражает малое p-значение), то мы делаем вывод, что переменная значимо улучшает модель.\n\n\n\n\n22.5.3 Сравнение с {easystats}\nДля сравнения моделей полезны следующие функции:\n\ncompare_performance(fit_null, fit, rank = TRUE)\n\n\n  \n\n\n\n\n\nName     | Model |   R2 | R2_adjusted | RMSE | Sigma |   AIC_wt |  AICc_wt\n--------------------------------------------------------------------------\nfit      |    lm | 0.38 |        0.35 | 5.41 |  5.60 |     1.00 |     1.00\nfit_null |    lm | 0.00 |        0.00 | 6.85 |  6.97 | 2.27e-03 | 2.88e-03\n\nName     |   BIC_wt | Performance_Score\n---------------------------------------\nfit      |     1.00 |                 1\nfit_null | 4.56e-03 |                 0\n\n\n\ncompare_performance(fit_null, fit) |&gt; \n  plot()\n\n\n\n\n\n\n\n\n\nlibrary(report)\ncompare_performance(fit_null, fit) |&gt; \n  report()\n\nWe compared two models; lm (R2 = 0.00, adj. R2 = 0.00, AIC = 204.62, BIC =\n207.42, RMSE = 6.85, Sigma = 6.97) and lm (R2 = 0.38, adj. R2 = 0.35, AIC =\n192.44, BIC = 196.65, RMSE = 5.41, Sigma = 5.60).",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Регрессионный анализ</span>"
    ]
  },
  {
    "objectID": "multiclass.html#подготовка-датасета",
    "href": "multiclass.html#подготовка-датасета",
    "title": "25  Многоклассовая классификация",
    "section": "25.2 Подготовка датасета",
    "text": "25.2 Подготовка датасета\nДля построения модели берем 500 наиболее частотных слов (токенов). Как мы увидим ниже, в этот список попали некоторые имена героев. В настоящем исследовании от них лучше избавиться, однако пока мы оставим все, как есть.\n\nmfw &lt;- make.frequency.list(corpus_samples_clean)[1:500]\n\nСоставим матрицу с частотностями.\n\ncorpus_tf &lt;- stylo::make.table.of.frequencies(corpus_samples_clean, mfw) |&gt; \n  as.data.frame.matrix() |&gt; \n  rownames_to_column(\"id\") |&gt; \n  as_tibble()\n\n\ncorpus_tf\n\n\n  \n\n\n\nМы будем определять автора, поэтому разделим первый столбец на два.\n\ncorpus_tf &lt;- corpus_tf |&gt; \n  separate(id, into = c(\"author\", \"title\", NA), sep = \"_\") \ncorpus_tf\n\n\n  \n\n\n\nПосмотрим, сколько отрывков для каждого автора в корпусе.\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\ncorpus_tf |&gt; \n  count(author) |&gt; \n  arrange(n)\n\n\n  \n\n\n\nДля ускорения вычислений пока удалим авторов, у которых не так много отрывков.\n\ncorpus_top &lt;- corpus_tf |&gt; \n  add_count(author) |&gt; \n  filter(n &gt; 120) |&gt; \n  select(-n, -title) \n\n\ncorpus_top |&gt; \n  count(author) |&gt; \n  ggplot(aes(reorder(author, n), n, fill = author)) +\n  geom_col(show.legend = FALSE) +\n  xlab(NULL) +\n  ylab(NULL) +\n  scale_fill_viridis_d() + \n  theme_light() +\n  coord_flip()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#снова-о-prep-и-bake",
    "href": "multiclass.html#снова-о-prep-и-bake",
    "title": "25  Многоклассовая классификация",
    "section": "25.4 Снова о prep() и bake()",
    "text": "25.4 Снова о prep() и bake()\nБольшую часть препроцессинга мы сделали в stylo, поэтому нам нужно всего несколько шагов.\n\nbase_rec &lt;- recipe(author ~ ., data = data_train) |&gt;\n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors())\n\nbase_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\nТакже создадим рецепт, в котором используем главные компоненты в качестве предикторов. Позже число компонент можно настроить при помощи tune().\n\npca_rec &lt;- base_rec |&gt; \n  step_pca(all_predictors(), num_comp = 7)\n\npca_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter on: all_predictors()\n\n\n• Centering and scaling for: all_predictors()\n\n\n• PCA extraction with: all_predictors()\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nНа очень большом числе признаков step_pca() может сильно замедлять вычисления, в этом случае можно попробовать step_pca_truncated() из пакета {embed}. Также стоит помнить, что PCA выполняет линейное снижение размерности, что подходит не для всех данных. Для нелинейного подхода воспользуйтесь функцией step_umap() из того же пакета.\n\n\nФункция prep() обучает (подготавливает) рецепт на основе обучающего датасета. Она применяет операции, которые требуют “обучения” на данных, так что ее можно рассматривать как аналог функции fit(). Аргумент retain = TRUE в функции prep() управляет тем, будут ли сохранены предобработанные обучающие данные внутри подготовленного объекта рецепта.\n\nbase_trained &lt;- base_rec |&gt;\n  prep(data_train) \n\nbase_trained\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:     1\npredictor: 500\n\n\n\n\n\n── Training information \n\n\nTraining data contained 1816 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter removed: &lt;none&gt; | Trained\n\n\n• Centering and scaling for: и, в, не, что, на, он, с, я, как, ... | Trained\n\n\nЧто касается bake(), то это скорее аналог функции predict(): она применяет подготовленный рецепт к новым данным — например к обучающим или тестовым примерам. Она использует информацию, рассчитанную на этапе prep().\nЕсли вы вызывали prep(..., retain = TRUE), то можете использовать juice() вместо bake() для получения обработанных обучающих данных напрямую.\n\nbase_trained |&gt; \n  # или juice()\n  bake(new_data = NULL)",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#методы-снижения-размерности",
    "href": "multiclass.html#методы-снижения-размерности",
    "title": "25  Многоклассовая классификация",
    "section": "25.5 Методы снижения размерности",
    "text": "25.5 Методы снижения размерности\n\n25.5.1 PCA для разведывательного анализа\nPCA (Principal Component Analysis) — это один из основных и наиболее понятных подходов к уменьшению размерности данных. Он относится к линейным методам обучения без учителя, что означает, что для его работы не требуется информация о целевых переменных (например, метках классов). Метод создаёт новые переменные (главные компоненты) — линейные комбинации исходных признаков, которые максимизируют дисперсию данных. Первые несколько компонентов содержат основную информацию (вариативность) из всего набора признаков. Подробнее о нем см. урок 15.\n\npca_trained &lt;- pca_rec |&gt;\n  prep(data_train) \n\npca_trained |&gt; \n  juice()\n\n\n  \n\n\n\nPCA часто используется в разведывательном анализе данных (EDA — Exploratory Data Analysis), чтобы упростить структуру данных, выявить важные зависимости и визуализировать сложные многомерные данные. Вот как именно PCA применяется в EDA:\n\nУменьшение размерности для визуализации: диаграмма рассеяния помогает увидеть, есть ли скрытые кластеры, группы, выбросы или тенденции.\nОбнаружение кластеров или структуры в данных: Если после проекции на первые главные компоненты наблюдаются чётко различимые группы, это может свидетельствовать о наличии скрытой структуры или категорий.\nОбнаружение выбросов: объекты, которые лежат далеко от большинства других точек в новом пространстве, могут быть аномальными.\nОценка корреляции между признаками: в процессе анализа компонент (например, с помощью графиков нагрузок — loadings plot) можно понять, какие переменные сильно коррелируют между собой.\n\nВ нашем случае визуализация главных компонент PC1 и PC2 показывает, что распределение классов (авторов) частично перекрывается, хотя некоторые группы имеют тенденцию образовывать кластеры (например, Толстой и Достоевский). Однако большинство классов на плоскости пересекаются друг с другом, особенно в центральной части графика.\n\npca_trained |&gt; \n  juice() |&gt; \n  ggplot(aes(PC1, PC2, color = author)) +\n  geom_point() + \n  theme_light()\n\n\n\n\n\n\n\n\nПакет {learntidymodels} позволяет визуализировать нагрузки компонент.\n\n#devtools::install_github(\"tidymodels/learntidymodels\")\nlibrary(learntidymodels)\npca_trained |&gt; \n  plot_top_loadings(component_number &lt;= 4, n = 10) +\n  scale_fill_brewer(palette = \"Paired\") +\n  theme_light()\n\n\n\n\n\n\n\n\nСлова, имеющий наибольшую нагрузку в одной компоненте, являются коррелированными (“клим” и “самгин”, “она” и “сказала”). Визуализируйте компоненты 3 и 4, чтобы убедиться, что они хорошо выделяют Горького.\n\n\n25.5.2 PLS\nМетоды PLS и UMAP — это популярные техники понижения размерности в машинном обучении. Они используются для уменьшения количества признаков (переменных) в данных и извлечения наиболее важной информации, которая определяет закономерности в датасете.\nPartial Least Squares (PLS; метод частичных наименьших квадратов):\nPLS — это метод, который находит линейные комбинации исходных признаков, называемые компонентами, с учётом зависимости от отклика (целевой переменной). В отличие от PCA (главных компонент), который полностью игнорирует зависимую переменную и ищет направления максимальной дисперсии, PLS является методом обучения с учителем. Это означает, что он учитывает целевой признак при поиске новых компонент. PLS такие ищет проекции в пространстве признаков, которые одновременно объясняют вариацию и в предикторах, и в ответе, что делает его особенно полезным при построении моделей классификации или регрессии.\nВ машинном обучении метод применяется, когда имеется большое количество сильно коррелированных признаков (что может мешать моделированию). В таком случае PLS позволяет уменьшить размерность, сохранив полезную информацию для предсказаний.\nДобавим еще один шаг к обученному рецепту выше.\n\n# BiocManager::install('mixOmics')\n\npls_trained &lt;- base_trained |&gt; \n  step_pls(all_numeric_predictors(), outcome = \"author\", num_comp = 7) |&gt; \n  # дообучение\n  prep() \n\npls_trained |&gt; \n  juice() \n\n\n  \n\n\n\n\npls_trained |&gt; \n  juice() |&gt; \n  ggplot(aes(PLS1, PLS2, color = author)) +\n  geom_point() +\n  theme_light()\n\n\n\n\n\n\n\n\nНагрузки компонент выводятся аналогично тому, как мы делали выше.\n\npls_trained |&gt; \n  plot_top_loadings(component_number &lt;= 4, n = 10, type = \"pls\") +\n  scale_fill_brewer(palette = \"Paired\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\n\n25.5.3 UMAP\nЕще один способ улучшить точность и интерпретируемость моделей, а также ускорить их обучение называется UMAP (Uniform Manifold Approximation and Projection). Это метод нелинейного понижения размерности, аналогичный t-SNE, но более быстрый. На первом этапе строится граф на основе расстояний между точками (обычно через k-ближайших соседей), который отражает топологию исходного пространства. Затем UMAP пытается разместить точки в пространстве меньшей размерности так, чтобы сохранить как можно больше свойств этого графа. Для этого используется оптимизационная функция на основе кросс-энтропии.\nВ машинном обучении используется для визуализации данных высокой размерности в 2D или 3D; может быть полезен как этап предварительной обработки перед моделированием, особенно в случаях, когда признаков много или они сильно нелинейно связаны. Важно: UMAP может применяться как без учителя, так и с учителем, но из-за стохастического характера может давать разную картину при каждом запуске и чувствителен к настройке гиперпараметров (число соседей).\n\nlibrary(embed)\n\nbase_trained |&gt; \n  step_umap(all_numeric_predictors(), outcome = \"author\", num_comp = 7) |&gt; \n  prep() |&gt; \n  juice() |&gt; \n  ggplot(aes(UMAP1, UMAP2, color = author)) +\n  geom_point(alpha = 0.5) +\n  theme_light()\n\n\n\n\n\n\n\n\nСоздадим еще два рецепта, которые понадобятся нам при моделировании.\n\npls_rec &lt;- base_rec |&gt; \n  step_pls(all_numeric_predictors(), outcome = \"author\", num_comp = tune())\n\n\numap_rec &lt;- base_rec |&gt; \n  step_umap(all_numeric_predictors(), \n            outcome = \"author\",\n            num_comp = tune(),\n            neighbors = tune(),\n            min_dist = tune()\n  )\n\nКак мы вскоре убедимся, снижение размерности (DR) не всегда улучшает качество модели, особенно в случае таких моделей, как Random forest или “наивный Байес”, которые хорошо справляются с коллинеарными предикторами и разреженными данными.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#опорные-векторы-svm",
    "href": "multiclass.html#опорные-векторы-svm",
    "title": "25  Многоклассовая классификация",
    "section": "25.8 Опорные векторы (SVM)",
    "text": "25.8 Опорные векторы (SVM)\nМетод опорных векторов (SVM) используется как в задачах регрессии, так и в задачах классификации.\nВо втором случае он пытается найти такую границу (гиперплоскость), которая максимально хорошо разделяет два класса объектов. Если упростить задачу до двух измерений, то метод ищет такую прямую, чтобы расстояние от неё до ближайших точек с каждой стороны было максимальным: классы должны быть как можно дальше от границы. Чем дальше граница от обучающих точек, тем устойчивее она к ошибкам на новых данных.\nДля этого SVM строит разделяющую прямую, которая максимально “отодвинута” от крайних точек обоих классов. Эти крайние точки, которые “касаются” границы — называются опорные векторы (support vectors).\nМаржа (англ. margin) — это расстояние от разделяющей границы до ближайших точек каждого класса. Чем больше маржа, тем увереннее разделяются классы.\nЭто проще всего пояснить при помощи графика. Обычные точки — это просто обучающие примеры. Черными отмечены как раз опорные векторы — те точки, которые оказались на краю своих классов и определили положение границы. Благодаря этим точкам SVM “знает”, где должна проходить разделяющая граница. Все “внутренние” точки не влияют на её положение.\n\n\nsvm_spec &lt;- svm_linear(cost = tune()) |&gt; \n  set_mode(\"classification\") |&gt; \n  set_engine(\"LiblineaR\")\n\nsvm_spec\n\nLinear Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = tune()\n\nComputational engine: LiblineaR \n\n\nПояснение параметров:\n\ncost = tune() — здесь мы указываем, что параметр cost будет подобран автоматически (в процессе переподбора гиперпараметров с помощью tune()).\nset_mode(\"classification\") — устанавливает режим задачи как классификацию.\nset_engine(\"LiblineaR\") — указывает, что используется движок LiblineaR, реализующий SVM с линейным ядром (в пакете {tidymodels}).\n\nПараметр cost — это коэффициент штрафа за ошибки классификации. Он контролирует компромисс между количеством ошибок на обучающем наборе (т.е. насколько сильно модель стремится избежать ошибок) и шириной “маржи” — расстояния между разделительной гиперплоскостью и ближайшими точками разных классов.\nЕсли cost большое, модель старается классифицировать обучающую выборку как можно точнее: допускается меньшая ширина маржи, но это может привести к переобучению (overfitting).\nЕсли cost меньше, то модель допускает больше ошибок на обучении: маржа будет шире, это может привести к недообучению (underfitting), но лучше обобщается на новых данных.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#еще-несколько-моделей-для-сравнения",
    "href": "multiclass.html#еще-несколько-моделей-для-сравнения",
    "title": "25  Многоклассовая классификация",
    "section": "25.9 Еще несколько моделей для сравнения",
    "text": "25.9 Еще несколько моделей для сравнения\nОднослойная нейронная сеть - простейшая форма нейронной сети, также известная как перцептрон или логистическая регрессия с несколькими выходами.\n\nmlp_spec &lt;- mlp(hidden_units = tune(),\n                penalty = tune(),\n                epochs = tune()) |&gt; \n  set_engine(\"nnet\") |&gt; \n  set_mode(\"classification\")\n\nБэггинг деревьев решений – упомянутый ранее ансамблевый метод; строит множество решающих деревьев на бутстреп-выборках, а результат — среднее (для регрессии) или голосование (для классификации).\n\nbagging_spec &lt;- bag_tree() |&gt; \n  set_engine(\"rpart\") |&gt; \n  set_mode(\"classification\")\n\nFlexible Discriminant Analysis (FDA) – расширение линейного дискриминантного анализа (LDA), где границы между классами аппроксимируются при помощи нелинейных моделей (например, сплайнов).\n\nfda_spec &lt;- discrim_flexible(prod_degree = tune()) |&gt; \n  set_engine(\"earth\")\n\nRegularized Discriminant Analysis (RDA) - rомпромисс между линейным (LDA) и квадратичным дискриминантным анализом (QDA) с добавлением регуляризации.\n\nrda_spec &lt;- discrim_regularized(frac_common_cov = tune(), \n                                frac_identity = tune())  |&gt; \n  set_engine('klaR')\n\nМетод ближайших соседей (K-Nearest Neighbors — KNN) - классификация (или регрессия) объекта производится на основе меток (или значений) K ближайших к нему объектов из обучающей выборки.\n\n#devtools::install_github(\"KlausVigo/kknn\")\n\nknn_mod &lt;- nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\")",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#оценка-и-выбор-модели",
    "href": "multiclass.html#оценка-и-выбор-модели",
    "title": "25  Многоклассовая классификация",
    "section": "25.11 Оценка и выбор модели",
    "text": "25.11 Оценка и выбор модели\nХорошо видно, что снижение размерности привело к существенному улучшению качества модели KNN, которая, однако, уступает регрессионным. Такое же улучшение можно зафиксировать для нейросети (mlp), а в случае с rda результат как минимум не хуже при заметном ускорении.\n\nautoplot(train_res, metric = \"f_meas\") + \n  theme_light() +\n  theme(legend.position = \"none\") +\n  geom_text(aes(y = (mean - 2*std_err), label = wflow_id),\n            angle = 90, hjust = 1.5) +\n  coord_cartesian(ylim = c(-0.3, NA))\n\n\n\n\n\n\n\n\nОтберем наилучшие результаты.\n\nrank_results(train_res, select_best = TRUE) |&gt; \n  print()\n\n# A tibble: 64 × 9\n   wflow_id   .config       .metric  mean std_err     n preprocessor model  rank\n   &lt;chr&gt;      &lt;chr&gt;         &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n 1 base_ridge Preprocessor… accura… 0.997 0.00103     5 recipe       mult…     1\n 2 base_ridge Preprocessor… f_meas  0.996 0.00132     5 recipe       mult…     1\n 3 base_svm   Preprocessor… accura… 0.994 0.00160     5 recipe       svm_…     2\n 4 base_svm   Preprocessor… f_meas  0.991 0.00178     5 recipe       svm_…     2\n 5 base_lasso Preprocessor… accura… 0.991 0.00162     5 recipe       mult…     3\n 6 base_lasso Preprocessor… f_meas  0.986 0.00263     5 recipe       mult…     3\n 7 pca_lasso  Preprocessor… accura… 0.929 0.00293     5 recipe       mult…     4\n 8 pca_lasso  Preprocessor… f_meas  0.882 0.00806     5 recipe       mult…     4\n 9 base_rda   Preprocessor… accura… 0.872 0.00740     5 recipe       disc…     5\n10 base_rda   Preprocessor… f_meas  0.881 0.00810     5 recipe       disc…     5\n# ℹ 54 more rows\n\n\nВзглянем на параметры наилучшей модели (в данном случае это штрафные коэффициенты).\n\nautoplot(train_res, id = \"base_ridge\") +\n  theme_light()\n\n\n\n\n\n\n\n\nФинализируем воркфлоу.\n\nbest_results &lt;- \n   train_res |&gt; \n   extract_workflow_set_result(\"base_ridge\") |&gt; \n   select_best(metric = \"accuracy\")\n\nprint(best_results)\n\n# A tibble: 1 × 2\n   penalty .config             \n     &lt;dbl&gt; &lt;chr&gt;               \n1 1.07e-10 Preprocessor1_Model1\n\n\nФункция extract_workflow() используется для извлечения конкретного workflow (модели) из набора train_res. Аргумент “base_ridge” — это имя модели (или ID), которую мы использовали при создании workflow_set. Таким образом, этот шаг извлекает сам workflow для модели “base_ridge”, включая препроцессинг и модель (ещё с неуточнёнными гиперпараметрами).\nФункция finalize_workflow() подставляет наилучшие значения гиперпараметров (например, penalty) в workflow.\nНаконец, last_fit() имитирует реальный процесс разработки модели: после настройки и выбора лучшей модели, мы обучаем её на всей обучающей выборке и оцениваем на ранее отложенной тестовой выборке.\n\nridge_res &lt;- train_res |&gt; \n  extract_workflow(\"base_ridge\") |&gt; \n  finalize_workflow(best_results) |&gt; \n  last_fit(split = data_split, metrics = metric_set(f_meas, accuracy, roc_auc))\n\nНа тестовой выборке наша модель отработала идеально!\n\ncollect_metrics(ridge_res) |&gt; \n  print()\n\n# A tibble: 3 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 f_meas   macro              1 Preprocessor1_Model1\n2 accuracy multiclass         1 Preprocessor1_Model1\n3 roc_auc  hand_till          1 Preprocessor1_Model1\n\n\n\ncollect_predictions(ridge_res) |&gt; \n  conf_mat(truth = author, estimate = .pred_class) |&gt; \n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient(low = \"white\", high = \"#233857\") +\n  theme(panel.grid.major = element_line(colour = \"#233857\"),\n        axis.text = element_text(color = \"#233857\"),\n        axis.title = element_text(color = \"#233857\"),\n        plot.title = element_text(color = \"#233857\"),\n        axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\n\ncollect_predictions(ridge_res) |&gt;\n  roc_curve(truth = author, .pred_Bulgakov:.pred_Turgenev) |&gt;\n  # или autoplot()\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"gray50\", lty = 2, alpha = 0.8) +\n  geom_path(linewidth = 1.5, alpha = 0.7) +\n  labs(color = NULL) +\n  theme_light()",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "multiclass.html#интерпретация-модели",
    "href": "multiclass.html#интерпретация-модели",
    "title": "25  Многоклассовая классификация",
    "section": "25.12 Интерпретация модели",
    "text": "25.12 Интерпретация модели\n\nfinal_model &lt;- extract_fit_parsnip(ridge_res)\n\n\ntop_terms &lt;- tidy(final_model) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  group_by(class) |&gt;                           \n  slice_max(abs(estimate), n = 7)  |&gt;             \n  ungroup()  |&gt; \n  mutate(term = fct_reorder(term, abs(estimate)))\n\nprint(top_terms)\n\n# A tibble: 56 × 4\n   class       term       estimate  penalty\n   &lt;chr&gt;       &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 Bulgakov    в            0.207  1.07e-10\n 2 Bulgakov    ответил      0.169  1.07e-10\n 3 Bulgakov    совершенно   0.145  1.07e-10\n 4 Bulgakov    затем        0.119  1.07e-10\n 5 Bulgakov    и            0.108  1.07e-10\n 6 Bulgakov    нужно        0.0949 1.07e-10\n 7 Bulgakov    снова       -0.0904 1.07e-10\n 8 Dostoyevsky вдруг        0.178  1.07e-10\n 9 Dostoyevsky всё          0.143  1.07e-10\n10 Dostoyevsky даже         0.143  1.07e-10\n# ℹ 46 more rows\n\n\n\ntop_terms  |&gt; \n  ggplot(aes(x = estimate, y = term, fill = class)) +\n  geom_col(show.legend = FALSE, alpha = 0.85) +\n  facet_wrap(~ class, scales = \"free_y\", nrow = 4) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Наиболее важные признаки для каждого автора\",\n    x = \"Коэффициент\",\n    y = \"Признак\"\n  ) +\n  theme_minimal() \n\n\n\n\n\n\n\n\nУ Горького “Самгин”, у Шолохова – “Григорий”, вроде все логично. Или “совершенно” логично, как сказал бы Булгаков.\nОтличная работа 🏆 🏆 🏆",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Многоклассовая классификация</span>"
    ]
  },
  {
    "objectID": "dnn.html#пакеты-и-виртуальное-окружение",
    "href": "dnn.html#пакеты-и-виртуальное-окружение",
    "title": "26  Глубокое обучение",
    "section": "26.2 Пакеты и виртуальное окружение",
    "text": "26.2 Пакеты и виртуальное окружение\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nconflicted::conflict_prefer(\"filter\", winner = \"dplyr\")\n\nДля работы нам понадобится дополнительно установить и загрузить несколько пакетов.\n\nПакет keras для R предоставляет удобный интерфейс для Keras, высокоуровневого API для создания нейронных сетей. Keras отвечает за компоненты глубокого обучения высокого уровня: слои, функции потерь, оптимизатор, метрики, обучающий цикл.\nKeras опирается на Tensorflow (доступный в R через одноименный пакет), который отвечает за низкоуровневые манипуляции с тензорами.\nПакет {reticulate} позволяет запускать Python-код прямо из R. Это обеспечивает интеграцию с Keras и Tensorflow: многие современные нейросетевые пакеты в R (в том числе {keras} и {tensorflow}) — всего лишь “обёртки” над Python-библиотеками.\n\n\nlibrary(keras3)\nlibrary(tensorflow)\nlibrary(reticulate)\n\nТеперь попробуем узнать, какая установлена версия Python на машине. При необходимости обновите.\n\npy_config()\n\n# python:         /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/bin/python3\n# libpython:      /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/python/cpython-3.11.12-macos-aarch64-none/lib/libpython3.11.dylib\n# pythonhome:     /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf:/Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf\n# virtualenv:     /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/bin/activate_this.py\n# version:        3.11.12 (main, Apr  9 2025, 03:49:53) [Clang 20.1.0 ]\n# numpy:          /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/lib/python3.11/site-packages/numpy\n# numpy_version:  2.1.3\n# keras:          /Users/olga/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/iRJimXLaYLGeT_iEQWqHf/lib/python3.11/site-packages/keras\n# \n# NOTE: Python version was forced by py_require()\n\nУбедимся, что Питон работает. Если все ок, вы увидите число pi.\n\npy_run_string(\"import math; result = math.pi\")\npy$result\n\n[1] 3.141593\n\n\nПроверим наличие keras и tensorflow.\n\npy_module_available(\"keras\")\n\n[1] TRUE\n\npy_module_available(\"tensorflow\")\n\n[1] TRUE\n\n\nЕсли хоть один из них отсутствует, устанавливаем keras и tensorflow в текущее Python-окружение.\n\npy_install(c(\"keras\", \"tensorflow\"))\n\nЕсли вы используете эфемерное (временное) виртуальное окружение, которое управляется {reticulate} автоматически, то py_install() выдаст предупреждение и посоветует использовать py_require(), чтобы корректно установить или подключить пакеты без нарушения целостности окружения.\n\npy_require(c(\"keras\", \"tensorflow\"))\n\nЭто установит последние совместимые версии этих пакетов с помощью pip в вашу текущую виртуальную среду.\n\n\n\n\n\n\nНа заметку\n\n\n\nПакет {reticulate} в новых версиях может создавать временные virtualenv/conda окружения, которые управляются им автоматически — они не привязаны к системному Python и исчезают при завершении сессии (если явно не сохраняются).\n\n\nЕсли вы хотите не эфемерную, а постоянную виртуальную среду, можно создать её вручную:\n\n# Только один раз!\nvirtualenv_create(\"myenv\")\n\n# Активировать для reticulate\nuse_virtualenv(\"myenv\", required = TRUE)\n\n# Установить нужные модули\npy_install(c(\"keras\", \"tensorflow\"))\n\nТогда {reticulate} будет использовать стабильное окружение, которое сохранится между сессиями.\nУбедимся, что все работает.\n\npy_run_string(\"\nimport tensorflow as tf\nimport keras\n\nprint('TensorFlow version:', tf.__version__)\nprint('Keras version:', keras.__version__)\n\")\n\nTensorFlow version: 2.19.0\nKeras version: 3.10.0\n\n\nУра, победа 🎈🎉🎊",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#данные-категории-новостей",
    "href": "dnn.html#данные-категории-новостей",
    "title": "26  Глубокое обучение",
    "section": "26.3 Данные: категории новостей",
    "text": "26.3 Данные: категории новостей\n\nlibrary(textdata)\nag_news &lt;- textdata::dataset_ag_news()\nag_news\n\n\n  \n\n\n\n\nag_news |&gt;\n  count(class) |&gt;\n  mutate(class = forcats::fct_reorder(class, n)) |&gt;\n  ggplot(aes(x = class, y = n, fill = class)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nag_news |&gt;\n  mutate(text_length = nchar(description)) |&gt; \n  ggplot(aes(text_length, color = class)) +\n  geom_density() +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#разделение-данных",
    "href": "dnn.html#разделение-данных",
    "title": "26  Глубокое обучение",
    "section": "26.4 Разделение данных",
    "text": "26.4 Разделение данных\nФункция initial_validation_split() создает случайное разделение данных на три части: обучающую (training set), валидационную (validation set) и тестовую (testing set) выборки. Функции training(), validation() и testing() позволяют извлекать соответствующие подмножества данных после разбиения.\n\nset.seed(24052025)\ndata_split &lt;- ag_news |&gt; \n  mutate(class = as.factor(class)) |&gt; \n  initial_validation_split(strata = class)\ndata_split\n\n&lt;Training/Validation/Testing/Total&gt;\n&lt;72000/24000/24000/120000&gt;\n\n\n\ndata_train &lt;- training(data_split)\ndata_validate &lt;- validation(data_split)\ndata_test &lt;- testing(data_split)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#спецификация-модели-bow",
    "href": "dnn.html#спецификация-модели-bow",
    "title": "26  Глубокое обучение",
    "section": "26.7 Спецификация модели BOW",
    "text": "26.7 Спецификация модели BOW\nСоздаем пустую последовательную (sequential) модель. В последовательной модели слои идут один за другим, по порядку. Добавляем к ней два полносвязных (dense) слоя. Аргументом units = 64 указываем, что в первом и втором слое будет 64 нейрона. Число нейронов подбирается экспериментально. Наличие большей размерности (многомерное пространство представления) позволяет модели изучать более сложные представления, но делает модель более дорогостоящей в вычислительном отношении и может привести к переобучению (Шолле 2023, 149).\nАргумент activation = \"relu\" означает, что скрытые слои используют функцию активации relu (rectified linear unit, блок линейной ректификации). Эта функция преобразует отрицательные значения в ноль.\n\nБез функции активации, такой как relu (также называемой фактором нелинейности) полносвязный слой layer_dense будет состоять из двух линейных операций – скалярного произведения и сложения: output &lt;- dot(input, W) + b Такой слой может обучаться только на линейных (аффинных) преобразованиях входных данных: пространство гипотез слоя было бы совокупностью всех возможных линейных преобразований входных данных в n-мерное пространство. Такое пространство гипотез слишком ограничено, и наложение нескольких уровней представлений друг на друга не приносило бы никакой выгоды, потому что сколь угодно длинная последовательность линейных преобразований все равно остается линейным преобразованием. – (Шолле 2023, 151)\n\nПосле этого добавляем выходной слой. Здесь число нейронов соответствует числу предсказываемых классов, а активация softmax (activation = \"softmax\") превращает выходы нейронов в вероятности, сумма которых равна 1.\n\nbow_model &lt;- keras3::keras_model_sequential() |&gt; \n  layer_dense(units = 32, activation = \"relu\") |&gt; \n  layer_dense(units = 32, activation = \"relu\") |&gt; \n  layer_dense(units = 4, activation = \"softmax\")\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\nМодель готова к дальнейшему обучению и применению. Осталось выбрать функцию потерь и оптимизатор.\n\nbow_model  |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\nbow_model\n\nModel: \"sequential\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (Dense)                     │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_1 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_2 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\nЗдесь compile() — функция компиляции. Она “собирает” модель для обучения: определяет, как будут считаться ошибки (функция потерь), какой алгоритм оптимизации использовать, и по каким метрикам отслеживать качество.\nОптимизатор Adam (аргумент optimizer = \"adam\") - один из самых популярных оптимизаторов в глубоком обучении. Adam автоматически подбирает скорость обучения для каждого параметра. Работает быстро и надёжно на большинстве задач — особенно если нет времени или желания подбирать сложные параметры вручную.\nПерекрестная энтропия (loss = \"categorical_crossentropy\") – функция потерь для задач многоклассовой классификации (multi-class classification). Эта функция подходит, когда на выходе модели softmax и целевая переменная — one-hot вектор.\n\n\n\n\n\n\nНа заметку\n\n\n\nПерекрестная энтропия (crossentropy) – это термин из области теории информации, обозначающий меру расстояния между распределениями вероятностей или, в данном случае, между фактическими данными и предсказаниями.\n\n\nТакже прописываем метрику качества работы модели.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#обучение-bow-модели",
    "href": "dnn.html#обучение-bow-модели",
    "title": "26  Глубокое обучение",
    "section": "26.8 Обучение BOW-модели",
    "text": "26.8 Обучение BOW-модели\nТеперь проведем обучение модели в течение 10 эпох (выполним 10 итераций по всем образцам обучающих данных) пакетами по 512 образцов.\nПакет (batch) - это небольшой набор образцов, которые одновременно обрабатываются моделью. Количество часто равно степени двойки, чтобы упростить выделение памяти на процессоре. В процессе обучения пакет используется для одного обновления градиентного спуска, применяемого к весам модели.\nЭпоха (epoch) — это один полный проход (прогон) по всему тренировочному датасету при обучении модели машинного обучения, например, нейронной сети. Например, если у вас есть 1000 картинок, а batch_size = 100, то за одну эпоху модель обработает все 1000 картинок по 100 за раз — всего 10 шагов (итераций). Модель обычно обучают несколько (десятков или сотен) эпох, чтобы она постепенно улучшала свои прогнозы.\nТакже будем следить за потерями и точностью на отложенных образцах.\n\nbow_history &lt;- bow_model |&gt; \n  fit(\n    x = train_bow_rec,\n    y = class_train_onehot,\n    batch_size = 512,\n    epochs = 10,\n    validation_data = list(valid_bow_rec, class_valid_onehot), \n    verbose = FALSE\n  )\n\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.9259\n        loss: 0.2152\nval_accuracy: 0.8443\n    val_loss: 0.5195 \n\n\nПосле обучения в переменной bow_history сохраняется история процесса обучения: метрики, ошибки, прогресс и т.д. Взглянем на результат.\n\nplot(bow_history) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nbow_df &lt;- as.data.frame(bow_history)\nbow_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.9259\n        loss: 0.2152\nval_accuracy: 0.8443\n    val_loss: 0.5195",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#основные-понятия",
    "href": "dnn.html#основные-понятия",
    "title": "26  Глубокое обучение",
    "section": "",
    "text": "Источник.\n\n\n\n\n\n\n\nИсточник: Шолле (2023)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#препроцессинг-one-hot-последовательное-кодирование",
    "href": "dnn.html#препроцессинг-one-hot-последовательное-кодирование",
    "title": "26  Глубокое обучение",
    "section": "26.9 Препроцессинг: One-hot последовательное кодирование",
    "text": "26.9 Препроцессинг: One-hot последовательное кодирование\nstep_sequence_onehot() превращает токены в числовой формат аналогично step_tf() и step_tfidf(), но в отличие от них учитывает порядок следования токенов.\nРассмотрим на небольшом примере отсюда:\n\nsmall_data &lt;- tibble(text = c(\n  \"adventure dice game\",\n  \"spooky dice game\",\n  \"illustrated book of monsters\",\n  \"monsters, ghosts, goblins, me, myself and i\"\n))\n\nsmall_spec &lt;- recipe(~ text, data = small_data)  |&gt; \n  step_tokenize(text)  |&gt; \n  step_sequence_onehot(text, sequence_length = 6, prefix = \"\")\n\nprep(small_spec)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 4 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Tokenization for: text | Trained\n\n\n• Sequence 1 hot encoding for: text | Trained\n\n\n\nprep(small_spec)  |&gt; \n  # 2 означает второй шаг рецепта\n  tidy(2)\n\n\n  \n\n\n\n\nprep(small_spec) |&gt; \n  bake(new_data = NULL, composition = \"matrix\")\n\n     _text_1 _text_2 _text_3 _text_4 _text_5 _text_6\n[1,]       0       0       0       1       4       5\n[2,]       0       0       0      14       4       5\n[3,]       0       0       9       3      13      11\n[4,]       6       7      10      12       2       8\n\n\nВ четвертой строке первое слово = 6, а это не “монстры”! Так произошло, потому что предложение слишком длинное и не вмещается в длину кодируемой последовательности (ее регулирует аргумент sequence_length). В таком случае текст усекается (аргумент truncating по умолчанию имеет значение \"pre\", но можно изменить на \"post\"). В коротких текстах добавляются нули, за это отвечает параметр padding. Немного изменим рецепт:\n\nrecipe(~ text, data = small_data)  |&gt; \n  step_tokenize(text)  |&gt; \n  step_sequence_onehot(text, sequence_length = 6, \n                       prefix = \"\",\n                       padding = \"post\", \n                       truncating = \"post\")  |&gt; \n  prep()  |&gt; \n  bake(new_data = NULL, composition = \"matrix\")\n\n     _text_1 _text_2 _text_3 _text_4 _text_5 _text_6\n[1,]       1       4       5       0       0       0\n[2,]      14       4       5       0       0       0\n[3,]       9       3      13      11       0       0\n[4,]      11       6       7      10      12       2\n\n\nТеперь “монстры” в начале! А все нули сдвинулись вправо.\nТеперь напишем рецепт для новостного датасета.\n\nmax_words = 2e3\nmax_length = 150\n\nonehot_rec &lt;- recipe( ~ description, data = data_train)  |&gt;  \n  step_mutate(description = stringr::str_remove_all(description, \"\\\\d+\")) |&gt; \n  step_tokenize(description) |&gt;\n  step_stopwords(description) |&gt; \n  step_tokenfilter(description, \n                   max_tokens = max_words, \n                   min_times = 10) |&gt; \n  step_sequence_onehot(description, \n                       sequence_length = max_length,\n                       # потому что в новостях все самое важное обычно в начале\n                       truncating = \"post\",\n                       prefix = \"\")\n  \n\nonehot_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: stringr::str_remove_all(description, \"\\\\d+\")\n\n\n• Tokenization for: description\n\n\n• Stop word removal for: description\n\n\n• Text filtering for: description\n\n\n• Sequence 1 hot encoding for: description\n\n\n\nonehot_prep &lt;- prep(onehot_rec)\nonehot_prep\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 72000 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: ~stringr::str_remove_all(description, \"\\\\d+\") |\n  Trained\n\n\n• Tokenization for: description | Trained\n\n\n• Stop word removal for: description | Trained\n\n\n• Text filtering for: description | Trained\n\n\n• Sequence 1 hot encoding for: description | Trained\n\n\n\nset.seed(25052025)\ntidy(onehot_prep, 5) |&gt; \n  sample_n(size = 10)\n\n\n  \n\n\n\n\nonehot_train &lt;- bake(onehot_prep, \n                     new_data = NULL, \n                     composition = \"matrix\")\n\nКоличество рядов в матрице соответствует числу наблюдений в обучающей выборке, а число столбцов – выбранной длине последовательности.\n\ndim(onehot_train)\n\n[1] 72000   150",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#полносвязная-нейросеть-на-основе-onehot",
    "href": "dnn.html#полносвязная-нейросеть-на-основе-onehot",
    "title": "26  Глубокое обучение",
    "section": "26.10 Полносвязная нейросеть на основе onehot",
    "text": "26.10 Полносвязная нейросеть на основе onehot\nНаша вторая модель глубокого обучения преобразует тексты в эмбеддинги, затем «расплющивает» их (делает одномерными), а после этого обучает полносвязный слой (dense network), чтобы предсказать класс новости.\n\ndense_model &lt;- keras_model_sequential() |&gt; \n  layer_embedding(input_dim = max_words + 1, \n                  output_dim = 12)  |&gt; \n  layer_flatten()  |&gt; \n  layer_dense(units = 32, activation = \"relu\")  |&gt; \n  layer_dense(units = 4, activation = \"softmax\", )\n\n\ndense_model\n\nModel: \"sequential_1\"\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                      ┃ Output Shape             ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (Embedding)             │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ flatten (Flatten)                 │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_3 (Dense)                   │ ?                        │   0 (unbuilt) │\n├───────────────────────────────────┼──────────────────────────┼───────────────┤\n│ dense_4 (Dense)                   │ ?                        │   0 (unbuilt) │\n└───────────────────────────────────┴──────────────────────────┴───────────────┘\n Total params: 0 (0.00 B)\n Trainable params: 0 (0.00 B)\n Non-trainable params: 0 (0.00 B)\n\n\n\ndense_model |&gt; \n  compile(\n  optimizer = \"adam\",\n  loss = \"categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  },
  {
    "objectID": "dnn.html#обучение-модели-на-основе-onehot-кодирования",
    "href": "dnn.html#обучение-модели-на-основе-onehot-кодирования",
    "title": "26  Глубокое обучение",
    "section": "26.11 Обучение модели на основе Onehot-кодирования",
    "text": "26.11 Обучение модели на основе Onehot-кодирования\n\ndense_history &lt;- dense_model  |&gt; \n  fit(\n  x = train_bow_rec,\n  y = class_train_onehot,\n  batch_size = 512,\n  epochs = 10,\n  # заметьте еще один способ передать проверочные данные\n  validation_split = 0.25, \n  verbose = FALSE\n)\n\n\nplot(dense_history) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndense_history\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.8803\n        loss: 0.3271\nval_accuracy: 0.8361\n    val_loss: 0.4333 \n\n\n\n\n\n\nШолле, Франсуа. 2023. Глубокое обучение с R и Keras. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Глубокое обучение</span>"
    ]
  }
]