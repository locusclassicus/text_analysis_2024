[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Компьютерный анализ текста",
    "section": "",
    "text": "Введение",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#об-этом-курсе",
    "href": "index.html#об-этом-курсе",
    "title": "Компьютерный анализ текста",
    "section": "Об этом курсе",
    "text": "Об этом курсе\nЭтот сайт содержит материалы к курсу “Компьютерный анализ текста в R” для магистерской программы НИУ ВШЭ “Цифровые методы в гуманитарных науках”. Предыдущую версию курса можно найти здесь.\nИ тексты, и инструменты для работы с ними подобраны таким образом, чтобы помочь студентам гуманитарных специальностей (филологам, философам, историкам и др.) как можно быстрее, но с полным пониманием дела перейти к применению количественных методов в собственной работе.\nЧтобы лучше понимать, какие из этих методов более всего востребованы в научной работе, преподаватели магистратуры “Цифровые методы в гуманитарных науках” – Б.В. Орехов, А.А. Осмоловская и О.В. Алиева – организовали в 2024 г. серию встреч с ведущими представителями отрасли. Видео этих встреч и литературу к семинарам можно найти на сайте http://criticaldh.ru/.\nТам мы собрали именно теоретические обсуждения и литературу к ним, а в этом курсе предлагаем приступить к практике DH (на языке R). Оба этих аспекта, в нашем представлении и в программе магистратуры тесно связаны: одного программирования не хватит, чтобы стать “цифровым гуманистом”, а теории недостаточно, чтобы судить об успешности тех или иных цифровых проектов. Поэтому этот курс старается стоять на двух ногах и соединять кодинг с теоретической рефлексией. Это почти невыполнимая задача но когда нам это мешало.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#ресурсы",
    "href": "index.html#ресурсы",
    "title": "Компьютерный анализ текста",
    "section": "Ресурсы",
    "text": "Ресурсы\nИ в теоретическом, и в практическом плане курс опирается на огромную работу, уже проделанную преподавателями магистратуры ЦМГН. Важнейшие наши достижения собрал Б.В. Орехов: https://github.com/nevmenandr/awesome-dh-hse. Здесь вы найдете ссылки на видео, научно-популярные и научные публикации и датасеты.\nЕсли вдруг вам не хватит практических заданий по R, то в качестве дополнения к оффлайн-курсу можно рекомендовать онлайн-курс Георгия Мороза “Введение в анализ данных на R для гуманитарных и социальных наук”. К этому курсу прилагается онлайн-ноутбук (https://agricolamz.github.io/daR4hs/) с комментариями и всем кодом, и он полностью открыт. Надо иметь в виду, однако, что онлайн-курс рассчитан всего на 9 недель, в то время как наш – на два семестра, так что его можно использовать лишь как вспомогательный ресурс, но не замену.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#программа",
    "href": "index.html#программа",
    "title": "Компьютерный анализ текста",
    "section": "Программа",
    "text": "Программа\nКурс 2024/2025 г. включает в себя три основных блока и 24 темы. Программа носит предварительный характер и может быть чуть изменена.\nЧасть 1. Основы работы в R\n\nЗнакомство с R и RStudio. Начало работы. Объекты, функции, операторы.\nВизуализация данных: базовый R, lattice, ggplot2.\nТрансформация данных. «Опрятные» данные с dplyr и tidyverse.\nУсловия и циклы. Написание собственных функций. Итерации с purrr.\nИмпорт данных. Импорт данных из XML.\nВоспроизводимые исследования. RMarkdown. Quarto.\nРегулярные выражения: базовый R и stringr.\nHTML. Веб-скрапинг.\n\nЧасть 2. Методы анализа текста\n\nТокенизация. Морфологический и синтаксический анализ.\nРаспределения слов и анализ частотностей.\nАнализ эмоциональной тональности (метод словарей).\nВекторные представления слов: LSA.\nВекторные модели на основе PMI.\nВероятностные модели (LDA) и Word2Vec.\nАнализ понятий с помощью сетей (графов).\nСетевой анализ системы персонажей.\n\nЧасть 3. Статистика и машинное обучение\n\nОбучение без учителя: кластеризация.\nПростая и множественная линейная регрессия.\nАлгоритмы для бинарной и многоклассовой классификации.\n\nДеревья решений и правил. Бэггинг, случайные леса, бустинг.\nПротоколы проверки моделей. Проблема переобучения.\nКонструирование признаков. Методы снижения размерности.\nМетоды “черного ящика”: опорные векторы.\nГлубокое обучение.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#оценивание",
    "href": "index.html#оценивание",
    "title": "Компьютерный анализ текста",
    "section": "Оценивание",
    "text": "Оценивание\nДомашние задания выполняются в GitHub Classroom. Еженедельно выполняются небольшие задания, которые оцениваются по бинарной шкале (1/0), раз в месяц – консолидирующие задания на весь пройденный материал (оценка 0-10). Все необходимые ссылки вы найдете в чате курса в Telegram.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#благодарности",
    "href": "index.html#благодарности",
    "title": "Компьютерный анализ текста",
    "section": "Благодарности",
    "text": "Благодарности\nЗа помощь в разработке курса и подготовке датасетов к нему автор благодарит Георгия Мороза и Бориса Орехова. Идеей количественного сравнения британских эмпириков в десятой главе я обязана своей коллеге по Школе философии и культурологии НИУ ВШЭ Дарье Дроздовой.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "index.html#обратная-связь",
    "href": "index.html#обратная-связь",
    "title": "Компьютерный анализ текста",
    "section": "Обратная связь",
    "text": "Обратная связь\nЕсли вы заметили ошибку или опечатку, можно сообщить по почте oalieva@hse.ru или оставить issue в репозитории курса на GitHub.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "1  Начало работы",
    "section": "",
    "text": "1.1 Установка R и RStudio\nМы будем использовать R, так что для занятий понадобятся:\nМы будем использовать следующую версию R:\nR version 4.3.3 (2024-02-29)\nНекоторые люди не любят устанавливать лишние программы себе на компьютер, несколько вариантов есть и для них:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#установка-r-и-rstudio",
    "href": "start.html#установка-r-и-rstudio",
    "title": "1  Начало работы",
    "section": "",
    "text": "R\n\nна Windows\nна Mac\nна Linux.\n\nRStudio — IDE для R (можно скачать здесь)\n\n\n\n\n\nRStudio cloud — полная функциональность RStudio с некоторыми ограничениями;\nwebR REPL — ограниченная версия компилятора R, которая работает в вашем браузере и не требует никаких установок на компьютер\nJupyter ноутбуки;\nGoogle Colab (нужно в настройках переключить ядро);\nVS Code — другое IDE, которое также позволяет работать с R;\nв принципе, в IDE нет нужды, можно работать из терминала, после установки, нужно всего лишь набрать R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#знакомство-с-rstudio",
    "href": "start.html#знакомство-с-rstudio",
    "title": "1  Начало работы",
    "section": "1.2 Знакомство с RStudio",
    "text": "1.2 Знакомство с RStudio\nRStudio — основная среда разработки (IDE) для R. После установки R и RStudio можно открыть RStudio и перед вами предстанет что-то похожее на изображение ниже:\n\n\n\nRStudio при первом открытии\n\n\nПосле нажатия на двойное окошко чуть левее надписи Environment откроется окно скрипта.\n\n\n\nПодокна RStudio\n\n\nВсе следующие команды можно:\n\nвводить в окне консоли, и тогда для исполнения следует нажимать клавишу Enter.\nвводить в окне скрипта, и тогда для исполнения следует нажимать клавиши Ctrl/Cmd + Enter или на команду Run на панели окна скрипта. Все, что введено в окне скрипта можно редактировать как в любом текстовом редакторе, в том числе сохранять Ctrl/Cmd + S.\n\nДля начала попробуйте получить информацию о сессии, введя в консоли такую команду:\n\nsessionInfo()\n\nsessionInfo() – это функция. О функциях можно думать как о глаголах (“сделай то-то!”). За названием функции всегда следуют круглые скобки, внутри которых могут находиться аргументы функции. Аргументы – это что-то вроде дополнений и обстоятельств. Аргументы могут быть обязательные и необязательные. Чтобы узнать, каких аргументов требует функция, надо вызывать help: ?mean(). В правой нижней панели появится техническая документация. Но также можно воспользоваться функцией args(). Попробуйте набрать в консоли args(round).\n\n\n\n\n\n\nВопрос\n\n\n\nСколько аргументов функции round() имеют значения по умолчанию?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#пакеты",
    "href": "start.html#пакеты",
    "title": "1  Начало работы",
    "section": "1.3 Пакеты",
    "text": "1.3 Пакеты\nПосле установки R вы получите доступ к уже готовым методам статистического анализа и инструментам для визуализации. Если в базовой инсталляции R нет нужного решения – надо поискать в библиотеке пакетов. Пакет – это набор функций и иногда датасетов, созданный пользователями. На 1 июля 2023 г. в репозитории CRAN доступно 19789 пакетов. И это далеко не все: многие пакеты доступны только на GitHub.\n\n\n\n\n\n\nНа заметку\n\n\n\nНекоторые функции, которые вы найдете в пакетах, частично дублируют друг друга – это нормально, как и в естественном языке, “сказать” что-то можно разными способами.\n\n\nПо технической документации и так называемым “виньеткам” можно понять, какой пакет вам нужен. Например, вот так выглядит виньетка пакета RPerseus, при помощи которого можно получить доступ к корпусу греческой и латинской литературы.\nБывают еще “пакеты пакетов”, то есть очень большие семейства функций, своего рода “диалекты” R. Таково семейство tidyverse, объединяемое идеологией “опрятных” данных. Про него мы еще будем говорить.\nПакеты для работы устанавливаются один раз, однако подключать их надо во время каждой сессии. Чтобы установить новый пакет, можно воспользоваться меню Tools &gt; Install Packages. Также можно устанавливать пакеты из консоли. Установим пакет с интерактивными уроками программирования на языке R:\n\ninstall.packages(\"swirl\")\n\nДля подключения используем функцию library(), которой передаем в качестве аргумента название пакета без кавычек:\n\nlibrary(swirl)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#рабочая-директория",
    "href": "start.html#рабочая-директория",
    "title": "1  Начало работы",
    "section": "1.4 Рабочая директория",
    "text": "1.4 Рабочая директория\nПеред началом работы проверьте свою рабочую директорию при помощи getwd(). Для смены можно использовать как абсолютный, так и относительный путь:\n\nsetwd(\"/Users/name/folder\")\n\n# искать в текущей директории\nsetwd(\"./folder\")\n\n# перейти на уровень вверх\nsetwd(\"../\")\n\nТакже для выбора рабочей директории можно использовать меню R Session &gt; Set Working Directory. А теперь – первое задание.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс программирования на R: install_course(\"R Programming\"). После этого привяжите пакет командой library(swirl) и наберите: swirl(). Укажите ваше имя. Пройдите урок 2 Workspace and Files.\n\n\n После выполнения ответьте на несколько вопросов на закрепление материала.\n\n\n\n\n\n\nВопрос\n\n\n\nКакие действия в рабочей директории можно совершать из консоли?\n\n\n\n\nсоздать директорию\n\n\nудалить директорию\n\n\nсоздать файл\n\n\nпереименовать файл\n\n\nкопировать файл\n\n\nудалить файл\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nЧтобы создать вложенную директорию при помощи функции dir.create(), аргументу recursive следует задать значение…\n\n\n\n\nTRUE\nFALSE\n\n\n\n\n\nЕсли все получилось, двигаемся дальше.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#r-как-калькулятор",
    "href": "start.html#r-как-калькулятор",
    "title": "1  Начало работы",
    "section": "1.5 R как калькулятор",
    "text": "1.5 R как калькулятор\nМожно использовать R как калькулятор. Для этого вводим данные рядом с символом приглашения &gt;, который называется prompt.\n\nsqrt(4) # квадратный корень\n\n[1] 2\n\n2^3 # степень\n\n[1] 8\n\nlog10(100) #логарифм\n\n[1] 2\n\n\nЕсли в начале консольной строки стоит +, значит предыдущий код не завершен. Например, вы забыли закрыть скобку функции. Ее можно дописать на следующей строке. Попробуйте набрать sqrt(2 в консоли.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#операторы-присваивания",
    "href": "start.html#операторы-присваивания",
    "title": "1  Начало работы",
    "section": "1.6 Операторы присваивания",
    "text": "1.6 Операторы присваивания\nЧтобы в окружении появился новый объект, надо присвоить результат вычислений какой-нибудь переменной при помощи оператора присваивания &lt;- (Alt + - (Windows) или Option + - (Mac)). Знак = также работает как оператор присваивания, но не во всех контекстах, поэтому им лучше не пользоваться.\n\nx &lt;- 2 + 2 # создаем переменную\ny &lt;- 0.1 # создаем еще одну переменную\nx &lt;- y # переназначаем  \nx + y\n\n[1] 0.2\n\n\nСочетание клавиш для оператора присваивания: Option/Alt + -. Имя переменной, как и имя функции, может содержать прописные и строчные буквы, точку и знак подчеркивания.\nТеперь небольшое упражнение.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(). Укажите ваше имя. Пройдите урок 1 Basic Building Blocks.\n\n\nЕсли все получилось, можно двигаться дальше! Но сначала зафиксируем несколько новых функций из этих первого урока.\n\n\n\n\n\n\nВопрос\n\n\n\nЧто вычисляет функция abs()?\n\n\n\n\nсреднее\n\n\nмодуль\n\n\nквадратный корень\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько значений вернет функция, если разделить c(2, 4, 6) на 2?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nБуква “c” в названии функции c() означает…\n\n\n\n\ncover\n\n\ncollapse\n\n\nconcatenate",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#пайпы-конвееры",
    "href": "start.html#пайпы-конвееры",
    "title": "1  Начало работы",
    "section": "1.7 Пайпы (конвееры)",
    "text": "1.7 Пайпы (конвееры)\nВ нашем коде мы часто будем использовать знаки конвеера (или пайпы): |&gt; (в вашей версии он может выглядить иначе: %&gt;%; переключить оператор можно в Global Options). Они призваны показывать последовательность действий. Сочетание клавиш: Ctrl/Cmd + M.\n\nmean(sqrt(abs(sin(1:100))))\n\n[1] 0.7654264\n\n1:100 |&gt; \n  sin() |&gt; \n  abs() |&gt; \n  sqrt() |&gt; \n  mean()\n\n[1] 0.7654264",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#векторы",
    "href": "start.html#векторы",
    "title": "1  Начало работы",
    "section": "1.8 Векторы",
    "text": "1.8 Векторы\nВектор – это объект, предназначенный для хранения данных. К таким же объектам относятся также матрицы, списки, таблицы данных и др. Заметим, что в языке R нет скаляров (отдельных чисел). Числа считаются векторами из одного элемента.\n\nx &lt;- 2\nclass(x) # числовой вектор\n\n[1] \"numeric\"\n\nlength(x) # длина вектора\n\n[1] 1\n\n\nКак вы уже поняли, функция c() позволяет собрать несколько элементов в единый вектор:\n\nx &lt;- c(3, 5, 7)\nx_mean &lt;- mean(x) \nx_mean\n\n[1] 5\n\n\n Над векторами можно совершать арифметические операции, но будьте внимательны, применяя операции к векторам разной длины: в этом случае более короткий вектор будет переработан, то есть повторен до тех пор, пока его длина не сравняется с длиной вектора большей длины.\n\nx &lt;- 2\ny &lt;- c(10, 20, 30)\ny / x \n\n[1]  5 10 15\n\nx + y \n\n[1] 12 22 32\n\n\nВекторы можно индексировать, то есть забирать из них какие-то элементы:\n\nx &lt;- seq(1, 5, 0.5)\nx[4:5] # индексы начинаются с 1 (в отличие от Python)\n\n[1] 2.5 3.0\n\n\nВектор может хранить данные разных типов:\n\nцелое число (integer);\nчисло с плавающей точкой (numeric, также называются double, то есть число двойной точности);\nстроку (character);\nлогическую переменную (logical);\nкатегориальную переменную, или фактор (factor).\n\n\n# проверить тип данных \nx &lt;- sqrt(2)\nclass(x)\n\n[1] \"numeric\"\n\nis.integer(x)\n\n[1] FALSE\n\nis.numeric(x)\n\n[1] TRUE\n\n\nСоздавать векторы можно не только при помощи c(). Вот еще два способа.\n\nseq(1, 5, 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nrep(\"foo\", 5)\n\n[1] \"foo\" \"foo\" \"foo\" \"foo\" \"foo\"\n\n\nНаучиться генерировать векторы поможет небольшое упражнение.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 3 Sequences of Numbers.\n\n\nПроверьте свои знания, прежде чем двигаться дальше.\n\n\n\n\n\n\nВопрос\n\n\n\nКакие числа вернет команда pi:10?\n\n\n\n\nнатуральные\n\n\nцелые\n\n\nрациональные\n\n\nвещественные\n\n\nкомплексные\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие функции могут использоваться для создания символьных векторов?\n\n\n\n\nseq()\n\n\nrep()\n\n\nc()\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько значений вернет команда rep(c(0, 1, 2), times = 10)? Посчитайте в уме, не выполняя код.\n\n\n\n\n\n\n\n\n Факторы внешне похожи на строки, но в отличие от них хранят информацию об уровнях категориальных переменных. Уровень может обозначаться как числом (например, 1 и 0), так и строкой.\n\nt &lt;- factor(c(\"A\", \"B\", \"C\"), levels = c(\"A\", \"B\", \"C\"))\nt\n\n[1] A B C\nLevels: A B C\n\n\nВажно: вектор может хранить данные только одного типа. При попытке объединить в единый вектор данные разных типов они будут принудительно приведены к одному типу:\n\nx &lt;- c(TRUE, 1, 3, FALSE)\nx # логические значения приведены к числовым\n\n[1] 1 1 3 0\n\ny &lt;- c(1, \"a\", 2, \"лукоморье\") \ny # числа превратились в строки\n\n[1] \"1\"         \"a\"         \"2\"         \"лукоморье\"\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 4 Vectors. Это позволит больше узнать про логические и символьные векторы.\n\n\nНесколько вопросов для самопроверки.\n\n\n\n\n\n\nВопрос\n\n\n\nКакие значение вернет команда (3 &gt; 5) & (4 == 4)?\n\n\n\n\nTRUE\nFALSE\nNA\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие значения вернет команда (TRUE == TRUE) | (TRUE == FALSE)?\n\n\n\n\nTRUE\nFALSE\nNA\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКоманда paste(LETTERS, 1:4, sep = \"-\") вернет…\n\n\n\n\nчисловой вектор длиной 26\n\n\nсимвольный вектор длиной 26\n\n\nчисловой вектор длиной 4\n\n\nсимвольный вектор длиной 4\n\n\nошибку\n\n\n\n\n\n Логические векторы можно получить в результате применения логических операторов (== “равно”, != “не равно”, &lt;= “меньше или равно”) к данным других типов:\n\nx &lt;- c(1:10) # числа от 1 до 10\ny &lt;- x &gt; 5\ny # значения TRUE соответствуют единице, поэтому их можно складывать\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum(y)\n\n[1] 5\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗдесь можно запустить swirl() и пройти урок 8 Logic. Это не обязательно, но очень полезно, если хотите разобраться в операторах!\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nПопробуйте посчитать в уме: какое из выражений ниже вернет значение TRUE?\n\n\n\n\n7 == 9\n\n\n!(57 != 8)\n\n\n9 &gt;= 10\n\n\n-6 &gt; -7\n\n\n\n\n\nФункции all() и any() также возвращают логические значения:\n\nx &lt;- 10:20 \nany(x == 15)\n\n[1] TRUE\n\nall(x &gt; 9)\n\n[1] TRUE\n\n\nОтсутствие данных любого типа в R передается двумя способами. NULL означает, что значение не существует. Например, если мы создадим пустой вектор, то при попытке распечатать его получим NULL. А вот длина пустого вектора равна нулю!\n\ny &lt;- c() \ny \n\nNULL\n\nlength(y) \n\n[1] 0\n\n\nNA (not available) указывает на то, что значение существует, но оно неизвестно. Любые операции с NA приводят к появлению новых NA! Сравните:\n\nx &lt;- c(1, NA, 2)\nmean(x)\n\n[1] NA\n\ny &lt;- c(1, NULL, 2)\nmean(y)\n\n[1] 1.5\n\n\nКак проверить, есть ли в данных NA или NULL? Знак == здесь не подойдет.\n\nx &lt;- NA\nx == NA\n\n[1] NA\n\ny &lt;- NULL\ny == NULL\n\nlogical(0)\n\n\nДля этого есть специальные функции.\n\nis.na(x)\n\n[1] TRUE\n\nis.null(y)\n\n[1] TRUE\n\n\n\nWhen some people first get to R, they spend a lot of time trying to get rid of NAs. People probably did the same sort of thing when zero was invented. NA is a wonderful thing to have available to you. It is seldom pleasant when your data have missing values, but life if much better with NA than without.\nBurns (2012)\n\nКак избавиться от NA? В некоторых случаях достаточно аргумента функции.\n\nmean(c(1, NA, 2), na.rm=T) \n\n[1] 1.5\n\n\nЧуть более сложные способы вы узнаете из урока swirl ниже.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 5 Missing Values.\n\n\nГотово? Тогда попробуйте ответить на вопрос ниже, не выполняя вычислений в R.\n\n\n\n\n\n\nВопрос\n\n\n\nДан вектор x &lt;- c(44, NA, 5, NA). Сколько NA вернет команда x == NA?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 6 Subsetting Vectors.\n\n\nПроверьте, все ли вы поняли из этого урока.\n\n\n\n\n\n\nВопрос\n\n\n\nЕсли вектор x содержит числовые значения и некоторое количество NA, то что вернет команда x[is.na(x)]?\n\n\n\n\nвектор длиной 0\n\n\nвектор всех NA\n\n\nлогический вектор\n\n\nвектор без NA\n\n\nошибку\n\n\n\n\n\nЧто надо изменить в этом коде, чтобы получить все, кроме NA?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "start.html#списки",
    "href": "start.html#списки",
    "title": "1  Начало работы",
    "section": "1.9 Списки",
    "text": "1.9 Списки\nВ отличие от атомарных векторов, списки, или рекурсивные векторы, могут хранить данные разных типов.\n\nlist = list(a = c(\"a\", \"b\", \"c\"), b = c(1, 2, 3), c = c(T, F, T))\nlist\n\n$a\n[1] \"a\" \"b\" \"c\"\n\n$b\n[1] 1 2 3\n\n$c\n[1]  TRUE FALSE  TRUE\n\n\nМожно получить доступ как к элементам списка целиком, так и к их содержимому.\n\nlist$a # обращение к поименованным элементам \n\n[1] \"a\" \"b\" \"c\"\n\nlist[2] # одинарные квадратные скобки извлекают элемент списка целиком\n\n$b\n[1] 1 2 3\n\nclass(list[2])\n\n[1] \"list\"\n\nlist[[2]] #  элементы второго элемента \n\n[1] 1 2 3\n\nclass(list[[2]])\n\n[1] \"numeric\"\n\nlist$c[1]# первый элемент второго элемента\n\n[1] TRUE\n\n\nОбратите внимание, что list[2] и list[[2]] возвращают объекты разных классов. Нам это еще понадобится при работе с XML.\n\n\n\nИндексирование списка в R. Источник 🧂\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите библиотеку rcorpora и загрузите список с названиями хлеба и сладкой выпечки.\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nУзнайте длину my_list и введите ее в поле ниже.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДостаньте из my_list элемент pastries и узнайте его длину.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nА теперь извлеките пятый элемент из pastries и введите ниже его название.\n\n\n\n\n\n\n\n\nСо списками покончено. Теперь можно пойти выпить кофе с my_list$pastries[13]. Дальше будет сложнее, но интереснее.\n\n\n\n\nBurns, Patrick. 2012. The R inferno. Lulu.com.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Начало работы</span>"
    ]
  },
  {
    "objectID": "tabular.html",
    "href": "tabular.html",
    "title": "2  Таблицы",
    "section": "",
    "text": "2.1 Матрицы\nМатрица – это вектор, который имеет два дополнительных атрибута: количество строк и количество столбцов. Из этого следует, что матрица, как и вектор, может хранить данные одного типа. Проверим.\nM = matrix(c(1, 2, 3, 4), nrow = 2)\nM # все ок\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nM = matrix(c(1, 2, 3, \"a\"), nrow = 2)\nM # все превратилось в строку! \n\n     [,1] [,2]\n[1,] \"1\"  \"3\" \n[2,] \"2\"  \"a\"\nВ матрице есть ряды и столбцы. Их количество определяет размер (порядок) матрицы. Выше мы создали матрицу 2 x 2. Элементы матрицы, как и элементы вектора, можно извлекать по индексу. Сначала указывается номер ряда (строки), потом номер столбца.\nM = matrix(c(1, 2, 3, 4), nrow = 2)\nM\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\nM[1, ] # первая строка полностью\n\n[1] 1 3\n\nM[,2] # второй столбец полностью\n\n[1] 3 4\n\nM[1,1] # одно значение\n\n[1] 1\nОбратите внимание, как меняется размерность при индексировании.\nM = matrix(c(1, 2, 3, 4), nrow = 2)\ndim(M) # функция для извлечения измерений\n\n[1] 2 2\n\ndim(M[1, ]) \n\nNULL\nПопытка узнать измерения вектора возвращает NULL, потому что, с точки зрения R, векторы не являются матрицами из одного столбца или одной строки и потому не имеют измерений.\nВ этом уроке мы не будем много работать с матрицами, но полезно помнить, что они существуют: матрицы и алгебраические операции с ними задействованы при латентно-семантическом анализе и построении эмбеддингов (см. ниже).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#таблицы-датафреймы",
    "href": "tabular.html#таблицы-датафреймы",
    "title": "2  Таблицы",
    "section": "2.2 Таблицы (датафреймы)",
    "text": "2.2 Таблицы (датафреймы)\nЕсли матрица – это двумерный аналог вектора, то таблица (кадр данных, data frame) – это двумерный аналог списка. Как и список, датафрейм может хранить данные разного типа.\n\n# создание датафрейма\ndf &lt;- data.frame(names = c(\"John\", \"Mary\"), age = c(18, 25), sport = c(\"basketball\", \"tennis\"))\ndf\n\n\n  \n\n\n\nИзвлечение данных тоже напоминает работу со списком.\n\ndf$names # забирает весь столбец\n\n[1] \"John\" \"Mary\"\n\ndf[,\"names\"] # то же самое, другой способ\n\n[1] \"John\" \"Mary\"\n\ndf[1, ] # забирает ряд",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#импорт-табличных-данных",
    "href": "tabular.html#импорт-табличных-данных",
    "title": "2  Таблицы",
    "section": "2.3 Импорт табличных данных",
    "text": "2.3 Импорт табличных данных\nВ этом уроке мы будем работать с датасетом из Репозитория открытых данных по русской литературе и фольклору под названием “Программы по литературе для средней школы с 1919 по 1991 гг.” Этот датасет был использован при подготовке интерактивной карты российского школьного литературного канона (1852-2023). Карта была представлена в 2023 г. Лабораторией проектирования содержания образования ВШЭ. Подробнее о проекте можно посмотреть материал “Системного блока”.\nОсновная функция для скачивания файлов из Сети – download.file(), которой необходимо задать в качестве аргументов url, название сохраняемого файла, иногда также метод.\n\nurl &lt;- \"https://dataverse.pushdom.ru/api/access/datafile/4229\"\n\n# скачивание в папку files в родительской директории\ndownload.file(url, destfile = \"../files/curricula.tsv\") \n\nОсновные функции для чтения табличных данных в базовом R - это read.table() и read.csv(). Файл, который мы скачали, имеет расширение .tsv (tab separated values). Чтобы его прочитать, используем read.table(), указав тип разделителя:\n\ncurricula_df &lt;- read.table(\"../files/curricula.tsv\", sep = \"\\t\", header = TRUE)\n\ncurricula_df\n\n\n  \n\n\n\nФункция read.csv() отличается лишь тем, что автоматически выставляет значения аргументов sep = \",\", header = TRUE.\nФункция class() позволяет убедиться, что перед нами датафрейм.\n\nclass(curricula_df)\n\n[1] \"data.frame\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#работа-с-датафреймами",
    "href": "tabular.html#работа-с-датафреймами",
    "title": "2  Таблицы",
    "section": "2.4 Работа с датафреймами",
    "text": "2.4 Работа с датафреймами\n\n# узнать имена столбцов\ncolnames(curricula_df) \n\n[1] \"author\"     \"title\"      \"comment\"    \"curriculum\" \"id\"        \n[6] \"year\"       \"grade\"      \"priority\"  \n\n\n\n# извлечь ряд(ы) по значению\ncurricula_df[curricula_df$year == \"1919\", ]\n\n\n  \n\n\n\n\n# извлечь столбец \ncurricula_df$year |&gt; head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\ncurricula_df[ , \"year\"] |&gt; head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\ncurricula_df[ , 6] |&gt;  head()\n\n[1] \"1919\" \"1919\" \"1919\" \"1919\" \"1919\" \"1919\"\n\n\n\n# узнать тип данных в столбцах\nstr(curricula_df) \n\n'data.frame':   10306 obs. of  8 variables:\n $ author    : chr  \"Андреев Л.Н.\" \"Андреев Л.Н.\" \"Андреев Л.Н.\" \"Бальмонт К.Д.\" ...\n $ title     : chr  \"Жили-были\" \"Иуда\" \"Рассказ о семи повешенных\" \"\" ...\n $ comment   : chr  \"\" \"\" \"\" \"\" ...\n $ curriculum: chr  \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" \"19 ИРЛ 2 ст\" ...\n $ id        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ year      : chr  \"1919\" \"1919\" \"1919\" \"1919\" ...\n $ grade     : int  9 9 9 9 9 8 8 8 8 8 ...\n $ priority  : chr  \"\" \"\" \"*\" \"*\" ...\n\n\n\n# преобразовать тип данных в столбцах\ncurricula_df$year &lt;- as.numeric(curricula_df$year)\n\n\n# вывести сводку\nsummary(curricula_df)\n\n    author             title             comment           curriculum       \n Length:10306       Length:10306       Length:10306       Length:10306      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n       id             year          grade          priority        \n Min.   : 1.00   Min.   :1919   Min.   : 5.000   Length:10306      \n 1st Qu.:13.00   1st Qu.:1946   1st Qu.: 8.000   Class :character  \n Median :31.00   Median :1966   Median :10.000   Mode  :character  \n Mean   :28.01   Mean   :1963   Mean   : 9.195                     \n 3rd Qu.:42.00   3rd Qu.:1981   3rd Qu.:10.000                     \n Max.   :50.00   Max.   :1991   Max.   :11.000                     \n                 NA's   :12                                        \n\n\nНебольшое упражнение на кодинг позволит закрепить навыки работы с матрицами и датафреймами.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl() и пройдите урок 7 Matrices and Data Frames.\n\n\nВсе ли вы запомнили?\n\n\n\n\n\n\nВопрос\n\n\n\nДля чего нужна функция cbind()?\n\n\n\n\nдля добавления рядов\n\n\nдля добавления столбцов\n\n\nдля извлечения имен столбцов\n\n\nдля извлечения имен рядов\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nФункция colnames() позволяет как назначать новые имена таблице, так и извлекать существующие.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Испанские писатели”.\n\n\n\n# устанавливаем и загружаем нужный пакет\ninstall.packages(\"languageR\")\nlibrary(languageR)\n\n# загружаем датасет\nmeta &lt;- spanishMeta\n\n# допишите ваш код ниже\n# посчитайте средний год публикации романов Камило Хосе Селы\n\n\n# вычислите суммарное число слов в романах Эдуардо Мендосы\n\n\n# извлеките ряды с текстами, опубликованными до 1980 г.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#tibble",
    "href": "tabular.html#tibble",
    "title": "2  Таблицы",
    "section": "2.5 Tibble",
    "text": "2.5 Tibble\nСуществуют два основных “диалекта” R, один из которых опирается главным образом на функции и структуры данных базового R, а другой пользуется синтаксисом tidyverse. Tidyverse – это семейство пакетов (метапакет), разработанных Хадли Уикхемом и др., которое включает в себя в том числе пакеты dplyr, ggplot2 и многие другие.\n\n# загрузить все семейство\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nОсновная структура данных в tidyverse – это tibble, современный вариант датафрейма. Тиббл, как говорят его разработчики, это ленивые и недовольные датафреймы: они делают меньше и жалуются больше. Это позволяет решать проблемы на более ранних этапах, что, как правило, приводит к созданию более чистого и выразительного кода.\nОсновные отличия от обычного датафрейма:\n\nусовершенствованный метод print(), не нужно постоянно вызывать head();\nнет имен рядов;\nдопускает синтаксически “неправильные” имена столбцов;\nпри индексировании не превращается в вектор.\n\nПреобразуем наш датафрейм в тиббл для удобства работы с ним.\n\ncurricula_tbl &lt;- as_tibble(curricula_df)\n\nСравним поведение датафрейма и тиббла.\n\n# индексирование \ncurricula_df[, 1] |&gt; class()\n\n[1] \"character\"\n\ncurricula_tbl[,1]  |&gt; class()\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nПора тренироваться.\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Getting and Cleaning Data\"). Загрузите библиотеку library(swirl), запустите swirl(), выберите этот курс и пройдите из него урок 1 Manipulating Data with dplyr. При попытке загрузить урок 1 вы можете получить сообщение об ошибке. В таком случае установите версию курса из github, как указано здесь, или загрузите файл вручную, как указано здесь.\n\n\nВремя вопросов! Обычный датафрейм или тиббл?\n\n\n\n\n\n\nВопрос\n\n\n\nПо умолчанию распечатывает только первые 10 рядов в консоль.\n\n\n\n\nдатафрейм\n\n\nтиббл\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nМолчаливо исправляет некорректные названия столбцов.\n\n\n\n\nдатафрейм\n\n\nтиббл\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nНе имеет названий рядов.\n\n\n\n\nдатафрейм\n\n\nтиббл\n\n\n\n\n\nКстати, обратили внимание, как работает оператор &lt;= с символьным вектором?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#dplyr",
    "href": "tabular.html#dplyr",
    "title": "2  Таблицы",
    "section": "2.6 Dplyr",
    "text": "2.6 Dplyr\nВ уроке swirl выше вы уже немного познакомились с “грамматикой манипуляции данных”, лежащей в основе dplyr. Здесь об этом будет сказано подробнее. Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными:\n\nmutate() добавляет новые переменные, которые являются функциями существующих переменных;\nselect() выбирает переменные (столбцы) на основе их имен;\nfilter() выбирает наблюдения (ряды) на основе их значений;\nsummarise() обобщает значения;\narrange() изменяет порядок следования строк.\n\nВсе эти глаголы естественным образом сочетаются с функцией group_by(), которая позволяет выполнять любые операции “по группам”, и с оператором pipe |&gt; из пакета magrittr.\nВ итоге получается более лаконичный и читаемый код. Узнаем, за какие года у нас есть программы по литературе.\n\ncurricula_tbl |&gt; \n  count(curriculum, year) \n\n\n  \n\n\n\nОтберем две программы для 8 класса и выясним, какие авторы в них представлены лучше всего.\n\ncurricula_tbl |&gt; \n  filter(year %in% c(1919, 1922), grade == 8) |&gt; \n  count(author, year) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nТеперь упражнения в swirl. Вам придется редактировать код, который предложит программа, так что сгруппируйтесь.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(), выберите курс Getting and Cleaning Data и пройдите из него урок 2 Grouping and Chaining with dplyr.\n\n\nПравда или ложь?\n\n\n\n\n\n\nВопрос\n\n\n\nФункция n_distinct() возвращает все уникальные значения.\n\n\n\n\nПравда\n\n\nЛожь",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#опрятные-данные",
    "href": "tabular.html#опрятные-данные",
    "title": "2  Таблицы",
    "section": "2.7 Опрятные данные",
    "text": "2.7 Опрятные данные\n\nTidy datasets are all alike, but every messy dataset is messy in its own way.\n— Hadley Wickham\n\nTidyverse – это не только особый синтаксис, но и отдельная идеология “опрятных данных”. “Сырые” данные, с которыми мы работаем, редко бывают опрятны, и перед анализом их следует “почистить” и преобразовать.\nОсновные принципы опрятных данных:\n\nотдельный столбец для каждой переменной;\nотдельный ряд для каждого наблюдения;\nу каждого значения отдельная ячейка;\nодин датасет – одна таблица.\n\n\n\n\nПринципы опрятных данных. Источник.\n\n\n\nПосмотрите на учебные тибблы из пакета tidyr и подумайте, какое из этих правил нарушено в каждом случае.\n\ndata(\"table2\")\ntable2\n\n\n  \n\n\ndata(\"table3\")\ntable3\n\n\n  \n\n\ndata(\"table4a\")\ntable4a\n\n\n  \n\n\ndata(\"table4b\")\ntable4b\n\n\n  \n\n\n\nВажные функции для преобразования данных из пакета tidyr:\n\nseparate() делит один столбец на новые;\nunite() объединяет столбцы;\npivot_longer() удлиняет таблицу;\npivot_wider() расширяет таблицу;\ndrop_na() и replace_na() указывают, что делать с NA и др.\n\nКроме того, в dplyr есть полезное семейство функций _join, позволяющих объединять данные в различных таблицах. Дальше мы потренируемся с ними работать, но сначала пройдем урок swirl. Это достаточно сложный урок (снова понадобится редактировать скрипт), но он нам дальше здорово поможет.\n\n\n\n\n\n\nЗадание\n\n\n\nЗапустите swirl(), выберите курс Getting and Cleaning Data и пройдите из него урок 3 Tidying Data with tidyr.\n\n\nПравда или ложь?\n\n\n\n\n\n\nВопрос\n\n\n\nФункция separate() обязательно требует указать разделитель.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nПринципы опрятных данных требуют, чтобы одному наблюдению соответствовал один столбец.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nФункция contains() используется вместе с filter() для выбора рядов.\n\n\n\n\nПравда\nЛожь\n\n\n\n\n\n Отличная работа! Прежде чем двигаться дальше, приведите в порядок table2, 3, 4a-4b, используя dplyr и tidyr.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Библиотека Gutenberg”\n\n\n\ndevtools::install_github(\"ropensci/gutenbergr\")\nlibrary(gutenbergr)\nlibrary(dplyr)\nlibrary(tidyr)\n\nworks &lt;- gutenberg_works()\n\n# Отберите ряды, в которых gutenberg_author_id равен 65;\n# после этого выберите два столбца: author, title\nmy_data &lt;- works |&gt; \n  # ваш код здесь\n  \n# Загрузите данные об авторах и выберите столбцы: author, deathdate\nauthors &lt;- gutenberg_authors |&gt; \n  # ваш код здесь\n\n# Соедините my_data с данными о смерти автора из authors, \n# так чтобы к my_data добавился новый столбец. \n# После этого используйте функцию separate, \n# чтобы разделить столбец с именем и фамилией на два новых: author, name. \n# Удалите столбец с именем автора, оставив только фамилию.\n# Добавьте новый столбец century, \n# используя функцию mutate и данные из столбца deathdate. \n# Используйте оператор pipe, не сохраняйте промежуточные результаты!\n\nmy_data |&gt; \n  # ваш код здесь",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "tabular.html#обобщение-данных",
    "href": "tabular.html#обобщение-данных",
    "title": "2  Таблицы",
    "section": "2.8 Обобщение данных",
    "text": "2.8 Обобщение данных\nТеперь вернемся к датасету curricula и попробуем частично воспроизвести результаты, полученные авторами проекта “Список чтения”, упомянутого выше.\nУ каких авторов больше всего произведений (во всех программах)?\n\ncurricula_tbl |&gt; \n  group_by(author, title) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nКакие произведения упоминаются в программах чаще всего?\n\ncurricula_tbl |&gt; \n  group_by(author, title) |&gt; \n  count() |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nНа принятые в каких годах программы приходится больше всего произведений? (Объяснение здесь.)\n\ncurricula_tbl |&gt; \n  group_by(year) |&gt; \n  distinct(author, title) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nВ заключение попробуйте сформулировать новые вопросы и ответить на них при помощи этого датасета.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Таблицы</span>"
    ]
  },
  {
    "objectID": "plot.html",
    "href": "plot.html",
    "title": "3  Визуализации",
    "section": "",
    "text": "3.1 Графические системы\nВ R есть несколько графических систем: базовый R, lattice и ggplot2. В этом курсе мы будем работать лишь с ggplot2 как с наиболее современной. Если вам интересны первые две, то вы можете обратиться к версии курса 2023/2024 г. и к интерактивным урокам swirl.\nНастоящая графическая сила R – это пакет ggplot2. В его основе лежит идея “грамматики графических элементов” Лиланда Уилкинсона (Мастицкий 2017) (отсюда “gg” в названии). С одной стороны, вы можете постепенно достраивать график, добавляя элемент за элементом (как в базовом R); с другой – множество параметров подбираются автоматически, как в Lattice.\nО различных видах графиков можно почитать по ссылке. В этом уроке мы научимся строить диаграмму рассеяния (scatter plot), столбиковую диаграмму (bar chart) и линейную диаграмму (line chart). Вот к чему мы стремимся.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#графические-системы",
    "href": "plot.html#графические-системы",
    "title": "3  Визуализации",
    "section": "",
    "text": "Задание\n\n\n\nЗапустите swirl(); курс R Programming у вас уже установлен. Из него сделайте урок 15 Base Graphics. Также установите курс swirl::install_course(\"Exploratory Data Analysis\"). Из него можно пройти любые уроки: это необязательно, но поможет разобраться в теме.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#датасет-метаданные-романов-xix-xx-вв.",
    "href": "plot.html#датасет-метаданные-романов-xix-xx-вв.",
    "title": "3  Визуализации",
    "section": "3.2 Датасет: метаданные романов XIX-XX вв.",
    "text": "3.2 Датасет: метаданные романов XIX-XX вв.\nЗнакомиться с ggplot2 мы будем на примере датасета из коллекции “NovelTM Datasets for English-Language Fiction, 1700-2009”, подготовленного Тедом Андервудом, Патриком Кимутисом и Джессикой Уайт. Они собрали метаданные о 210,266 томах художественной прозы в HathiTrust Digital Library и составили из них несколько датасетов.\nМы возьмем небольшой датасет, который содержит провернные вручную метаданные, а также сведения о категории художественной прозы для 2,730 произведений, созданных в период 1799-2009 г. (равные выборки для каждого года). Об особенностях сбора и подготовки данных можно прочитать по ссылке. Нужный нам файл (в формате tsv) скопирован в репозиторий курса.\n\nurl &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/manual_title_subset.tsv\"\ndownload.file(url, destfile = \"../files/manual_title_subset.tsv\")\n\nПрежде всего избавимся от лишних столбцов и посмотрим на данные.\n\nlibrary(tidyverse)\nnoveltm &lt;- read_tsv(\"../files/manual_title_subset.tsv\")\n\nnoveltm &lt;- noveltm |&gt; \n  select(author, inferreddate, latestcomp, gender, nationality, shorttitle, category)\n\nnoveltm\n\n\n  \n\n\n\nМы попробуем проверить наблюдение, сделанное Франко Моретти в статье “Корпорация стиля: размышления о 7 тысячах заглавий (британские романы 1740-1850)” (2009 г., рус. перевод в книге “Дальнее чтение”, 2016 г.). Моретти заметил, что на протяжении XVIII-XIX вв. названия становятся короче, причем уменьшается не только среднее, но и стандартное отклонение (т.е. разброс значений). В публикации он предлагает несколько возможных объяснений для этого тренда. В датасете NovelTM есть не только романы (и не только британские), но тем более интересно будет сравнить результат.\nВ наших данных сведения о публикации хранятся в столбце inferreddate, а названия – в столбце shorttitle. Количество слов в названии придется посчитать: для этого можно посчитать количество пробелов и добавить единицу.\n\nnoveltm &lt;- noveltm |&gt; \n  mutate(n_words = str_count(shorttitle, \" \"))\n\nnoveltm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#диаграмма-рассеяния",
    "href": "plot.html#диаграмма-рассеяния",
    "title": "3  Визуализации",
    "section": "3.3 Диаграмма рассеяния",
    "text": "3.3 Диаграмма рассеяния\nФункция ggplot() имеет два основных аргумента: data и mapping. Аргумент mapping задает эстетические атрибуты геометрических объектов. Обычно используется в виде mapping = aes(x, y), где aes() означает aesthetics.\nПод “эстетикой” подразумеваются графические атрибуты, такие как размер, форма или цвет. Вы не увидите их на графике, пока не добавите какие-нибудь “геомы” – геометрические объекты (точки, линии, столбики и т.п.). Эти объекты могут слоями накладываться друг на друга (Wickham и Grolemund 2016).\nДиаграмма рассеяния, которая подходит для отражения связи между двумя переменными, делается при помощи geom_point(). Попробуем настройки по умолчанию.\n\nnoveltm |&gt; \n  ggplot(aes(inferreddate, n_words)) + \n  geom_point()\n\n\n\n\n\n\n\n\nУпс. Точек очень много, и они накладываются друг на друга, так как число слов – дискретная величина. Поступим так же, как Моретти, который отразил на графике среднее для каждого года. Для этого нам надо снова поколдовать над данными.\n\nnoveltm_summary &lt;- noveltm |&gt;\n  group_by(inferreddate) |&gt;\n  summarise(n = n(),\n            mean_w = mean(n_words, na.rm = TRUE)) |&gt; \n  filter(n &gt; 1)\n\nnoveltm_summary\n\n\n  \n\n\n\nСнова построим диаграмму рассеяния. Добавим линию тренда, изменим внешний вид точек и тему оформления, а также уберем подпись оси X.\n\nnoveltm_summary |&gt; \n  ggplot(aes(inferreddate, mean_w)) +\n  geom_point(color = \"steelblue\", alpha = 0.7, size = 2) +\n  geom_smooth(color = \"tomato\") + \n  theme_bw() +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nНисходящая тенденция, о которой писал Моретти, хорошо прослеживается. Но, возможно, она характерна не для всех стран?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#сравнение-двух-групп",
    "href": "plot.html#сравнение-двух-групп",
    "title": "3  Визуализации",
    "section": "3.4 Сравнение двух групп",
    "text": "3.4 Сравнение двух групп\nВ столбце nationality хранятся данные о происхождении писателя.\n\nnoveltm |&gt; \n  group_by(nationality) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nОтберем только английских и американских авторов и сравним тенденции в этих двух группах.\n\nnoveltm_nation &lt;- noveltm |&gt; \n  filter(nationality %in% c(\"uk\", \"us\")) |&gt; \n  group_by(nationality, inferreddate) |&gt; \n  summarise(n = n(),\n            mean_w = mean(n_words, na.rm = TRUE)) |&gt; \n  filter(n &gt; 1) |&gt; \n  select(-n) |&gt; \n  arrange(-mean_w)\n\nnoveltm_nation\n\n\n  \n\n\n\nКатегориальную переменную (национальность) в нашем случае проще всего закодировать цветом. Также добавим заголовок и подзаголовок.\n\nnoveltm_nation |&gt; \n  ggplot(aes(inferreddate, mean_w, color = nationality)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth() +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL\n  )\n\n\n\n\n\n\n\n\nДля разведывательного анализа данных вполне достаточно настроек по умолчанию, но для публикации вы, вероятно, захотите вручную поправить шрифтовое и цветовое оформление.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#цветовые-шкалы",
    "href": "plot.html#цветовые-шкалы",
    "title": "3  Визуализации",
    "section": "3.5 Цветовые шкалы",
    "text": "3.5 Цветовые шкалы\nGgplot2 дает возможность легко поменять цветовую палитру и шрифтовое оформление, а также добавить фон.\nФункции scale_color_brewer() и scale_fill_brewer() позволяют использовать специально подобранные палитры хорошо сочетаемых цветов. Посмотреть эти палитры можно на сайте https://colorbrewer2.org.\nОбщее правило для выбора таково.\n\nЕсли дана качественная переменная с упорядоченными уровнями (например, “холодный”, “теплый”, “горячий”) или количественная переменная, и необходимо подчеркнуть разницу между высокими и низкими значениями, то для визуализации подойдет последовательная шкала.\nЕсли дана количественная переменная с осмысленным средним значением, например нулем, 50%, медианой, целевым показателем и т.п., то выбираем расходящуюся шкалу.\nЕсли дана качественная переменная, уровни которой невозможно упорядочить (названия городов, имена авторов и т.п.), ищем качественную шкалу.\n\n\n\n\nИсточник.\n\n\nВот основные (но не единственные!) цветовые шкалы в R. Также цвета можно задавать и вручную – по названию или коду.\n\n\nnoveltm_nation |&gt; \n  ggplot(aes(inferreddate, mean_w, color = nationality)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL) +\n  scale_color_brewer(palette = \"Dark2\")\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#шрифты",
    "href": "plot.html#шрифты",
    "title": "3  Визуализации",
    "section": "3.6 Шрифты",
    "text": "3.6 Шрифты\nПакет ggplot2 и расширения для него дают возможность использовать пользовательские шрифты.\n\nlibrary(showtext)\nfont_add_google(\"Special Elite\", family = \"special\")\nshowtext_auto()\n\nnoveltm_nation |&gt; \n  ggplot(aes(inferreddate, mean_w, color = nationality)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL) +\n  scale_color_brewer(palette = \"Dark2\") + \n  theme(\n    axis.title = element_text(family = \"special\"),\n    title = element_text(family = \"special\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#изображения",
    "href": "plot.html#изображения",
    "title": "3  Визуализации",
    "section": "3.7 Изображения",
    "text": "3.7 Изображения\nИзображения можно добавлять и в качестве фона, и вместо отдельных геомов, например точек. Поправим цвета, чтобы они лучше сочетались с цветом изображения.\n\nlibrary(ggimage)\nurl &lt;- \"./images/book.jpg\"\n\nfont_add_google(\"Special Elite\", family = \"special\")\nshowtext_auto()\n\n\ng &lt;- noveltm_nation |&gt; \n  ggplot(aes(inferreddate, mean_w, color = nationality)) +\n  geom_point(alpha = 0.7, size = 1.5) +\n  geom_smooth(se = FALSE) +\n  theme_bw() +\n  labs(\n    title = \"Title Length in UK and US\",\n    subtitle = \"NovelTM Data 1800-2009 \\n \",\n    x = NULL,\n    y = NULL) +\n  scale_color_manual(\"country\", values = c(\"#A03B37\", \"#50684E\")) + \n  theme(\n    axis.title = element_text(family = \"special\", color = \"#8B807C\"),\n    title = element_text(family = \"special\", color = \"#52211E\"),\n    axis.text = element_text(color = \"#52211E\"),\n    axis.ticks = element_blank(),\n    plot.margin = unit(c(0.4, 3, 0.4, 0.4), \"inches\"), # t, r, b, l\n    panel.border = element_rect(color = \"#8B807C\"),\n    legend.position = c(0.8, 0.8)\n  )\n\nggbackground(g, url)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#столбиковая-диаграмма",
    "href": "plot.html#столбиковая-диаграмма",
    "title": "3  Визуализации",
    "section": "3.8 Столбиковая диаграмма",
    "text": "3.8 Столбиковая диаграмма\nДля визуализации распределений качественных переменных подходит стобиковая диаграмма, которая наглядно показывает число наблюдений в каждой группе. В датасете NovelTM представлены следующие категории литературы.\n\nnoveltm |&gt; \n  ggplot(aes(category, fill = category)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nНас будет интересовать категория longfiction, т.к. именно сюда попадает популярный в XIX в. жанр романа. Известно, что примерно до 1840 г. почти половина романистов были женщинами, но к началу XX в. их доля снизилась (Underwood 2019, 133). Отчасти это объясняется тем, что после середины XIX в. профессия писателя становится более престижной, а его социальный статус повышается, что приводит к “джентрификации” романа. Посмотрим, что на этот счет могут сказать данные NovelTM. Переменная gender хранит данные о гендере автора.\n\nnoveltm |&gt; \n  ggplot(aes(gender, fill = gender)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nОтберем лишь одну категорию и два гендера.\n\nnoveltm_lf &lt;- noveltm |&gt; \n  select(inferreddate, gender, category) |&gt; \n  filter(gender != \"u\", category == \"longfiction\") |&gt; \n  select(-category)\n\nnoveltm_lf\n\n\n  \n\n\n\nМожно предположить, что соотношение мужчин и женщин в разные десятилетия менялось. Чтобы это выяснить, нам надо преобразовать данные, указав для каждого года соответствующую декаду, и посчитать число мужчин и женщин в каждой декаде.\n\nnoveltm_decade &lt;- noveltm_lf |&gt; \n  mutate(decade = (inferreddate %/% 10) * 10) \n\nnoveltm_decade\n\n\n  \n\n\n\nЭтого уже достаточно для визуализации, но она будет не очень наглядная.\n\nnoveltm_decade |&gt; \n  ggplot(aes(decade, fill = gender)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nУзнаем, сколько всего наблюдений в каждом десятилетии.\n\ntotal &lt;- noveltm_decade |&gt; \n  group_by(decade) |&gt; \n  summarise(total = n()) \n\ntotal\n\n\n  \n\n\n\n\nsummary &lt;- noveltm_decade |&gt; \n  group_by(decade, gender) |&gt; \n  summarise(counts = n()) |&gt; \n  filter(counts &gt; 1)\n\nsummary\n\n\n  \n\n\n\nТеперь объединим две таблицы и посчитаем долю мужчин и женщин.\n\nnoveltm_share &lt;- summary |&gt; \n  left_join(total) |&gt; \n  mutate(share = counts / total) |&gt; \n  select(-counts, -total)\n\nJoining with `by = join_by(decade)`\n\nnoveltm_share\n\n\n  \n\n\n\n\nnoveltm_share |&gt; \n  ggplot(aes(decade, share, fill = gender)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nПопробуем развернуть диаграмму - так ее будет легче читать.\n\nnoveltm_share |&gt; \n  ggplot(aes(decade, share, fill = gender)) +\n  geom_bar(stat = \"identity\") + \n  coord_flip() + \n  xlab(NULL) + \n  ylab(NULL)\n\n\n\n\n\n\n\n\nПоскольку нас интересует доля женщин, логично поменять группы местами. Также поменяем порядок, в котором идут года (от меньшего к большему).\n\nnoveltm_share |&gt; \n  ggplot(aes(as.factor(decade), share, fill = gender)) +\n  geom_bar(stat = \"identity\", position = position_fill(reverse = TRUE)) + \n  scale_x_discrete(limits = rev) +\n  coord_flip() + \n  ylab(NULL) + \n  xlab(NULL) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nУбавим цвет в мужской части диаграммы и добавим заголовки.\n\nnoveltm_share |&gt; \n  ggplot(aes(as.factor(decade), share, fill = gender)) +\n  geom_bar(stat = \"identity\", \n           position = position_fill(reverse = TRUE),\n           color = \"grey\",\n           show.legend = FALSE) + \n  scale_x_discrete(limits = rev) +\n  scale_fill_manual(values = c(\"lightcoral\", \"white\")) +\n  coord_flip() + \n  ylab(NULL) + \n  xlab(NULL) + \n  theme_minimal() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  theme(text=element_text(size=12, family=\"serif\")) \n\n\n\n\n\n\n\n\nСтоит подвинуть заголовок, убрать просветы между столбцами и добавить подписи.\n\nlabel_data &lt;- noveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  mutate(share = round(share, 2))\n\nnoveltm_share |&gt; \n  ggplot(aes(as.factor(decade), share, fill = gender)) +\n  geom_bar(stat = \"identity\", \n           position = position_fill(reverse = TRUE),\n           color = \"darkred\",\n           show.legend = FALSE,\n           width = 1) + \n  scale_x_discrete(limits = rev) +\n  scale_fill_manual(values = c(\"#f5b2b2\", \"white\")) +\n  coord_flip() + \n  ylab(NULL) + \n  xlab(NULL) + \n  theme_minimal() +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Women Share per Decade\",\n    subtitle = \"NovelTM Data 1800-2009\"\n  ) + \n  geom_text(data = label_data, aes(label = share), \n            hjust = 1.2, \n            color = \"darkred\",\n            family = \"serif\") +\n  theme(text=element_text(size=12, family=\"serif\", color = \"darkred\"),\n        plot.title.position = \"plot\",\n        axis.text = element_text(color = \"darkred\"),\n        )",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#линейная-диаграмма",
    "href": "plot.html#линейная-диаграмма",
    "title": "3  Визуализации",
    "section": "3.9 Линейная диаграмма",
    "text": "3.9 Линейная диаграмма\nДанные о доли женщин-писателей можно представить и в виде линии: в нашем случае это не лишено смысла, поскольку ось x – это временная шкала.\n\nnoveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = gender)) +\n  geom_line(show.legend = FALSE)\n\n\n\n\n\n\n\n\nПо умолчанию ось y усекается, и создается впечатление, что доля женщин ок. 1900 падает чуть ли не до нуля. Поправим вручную границы оси.\n\nnoveltm_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = gender)) +\n  geom_line(show.legend = FALSE) +\n  expand_limits(y = 0)\n\n\n\n\n\n\n\n\nГрафик, кажется, подтверждает, что доля женщин в литературе снижалась примерно до середины XX в. Однако при разделении данных на группы можно заметить другую тенденцию.\n\nnoveltm_nation &lt;- noveltm |&gt; \n  filter(category == \"longfiction\") |&gt; \n  select(inferreddate, gender, nationality) |&gt; \n  mutate(nationality = case_when(!nationality %in% c(\"uk\", \"us\") ~ \"other\",\n                                 .default = nationality)) |&gt; \n  filter(gender != \"u\") |&gt; \n  mutate(decade = (inferreddate %/% 10) * 10)\n\nnoveltm_nation\n\n\n  \n\n\ntotal_nation &lt;- noveltm_nation |&gt; \n  group_by(decade) |&gt; \n  summarise(total = n()) |&gt; \n  filter(total &gt; 1)\n\nsummary_nation &lt;- noveltm_nation |&gt; \n  group_by(decade, nationality, gender) |&gt; \n  summarise(counts = n()) |&gt; \n  filter(counts &gt; 1)\n\nsummary_nation\n\n\n  \n\n\nnoveltm_nation_share &lt;- summary_nation |&gt; \n  left_join(total) |&gt; \n  mutate(share = counts / total) |&gt; \n  select(-counts, -total)\n\nnoveltm_nation_share\n\n\n  \n\n\n\n\nnoveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line() \n\n\n\n\n\n\n\n\nДобавим название и немного поменяем оформление.\n\nnoveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_minimal() + \n  labs(\n    title = \"Female Writers' Share\",\n    subtitle = \"NovelTM Data 1800-2009 \\n \",\n    x = NULL,\n    y = NULL) +\n  theme(text=element_text(size=14, family=\"serif\")) + \n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\nМожно добавить темную рамку и переместить легенду.\n\ng &lt;- noveltm_nation_share |&gt; \n  filter(gender == \"f\") |&gt; \n  ggplot(aes(decade, share, color = nationality)) +\n  geom_line(linewidth = 2, alpha = 0.7) + \n  theme_light() + \n  labs(\n    title = \"Female Writers' Share\",\n    subtitle = \"NovelTM Data 1800-2009\",\n    x = NULL,\n    y = NULL) +\n  theme(text=element_text(size=14, family=\"serif\"),\n        axis.text = element_text(color = \"white\"),\n        legend.position = c(0.5, 0.83), \n        legend.direction = \"horizontal\",\n        legend.title = element_blank(),\n        legend.text = element_text(color = \"#440151FF\"),\n        legend.background = element_blank(),\n        plot.title = element_text(hjust=0.5, color = \"white\"),\n        plot.subtitle = element_text(hjust=0.5, color = \"white\"),\n        plot.background = element_rect(fill = \"#440151FF\"),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.grid.major.y = element_line(linewidth = 0.5)) + \n  scale_color_viridis_d()\n\ng",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "plot.html#экспорт-графиков-из-среды-r",
    "href": "plot.html#экспорт-графиков-из-среды-r",
    "title": "3  Визуализации",
    "section": "3.10 Экспорт графиков из среды R",
    "text": "3.10 Экспорт графиков из среды R\nСпособы:\n\nреализованные в R драйверы стандартных графических устройств;\nфункция ggsave()\nменю программы RStudio.\n\n\n# код сохранит pdf в рабочую директорию \npdf(file = \"plot.pdf\")\n \ng \n\ndev.off()\n\nЕще один способ сохранить последний график из пакета ggplot2.\n\nggsave(\n  filename = \"plot.png\",\n  plot = last_plot(),\n  device = \"png\",\n  scale = 1,\n  width = NA,\n  height = 500,\n  units = \"px\",\n  dpi = 300\n)\n\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Старофрацузская литература”\n\n\n\n# загружаем нужные пакеты\nlibrary(languageR)\nlibrary(ggplot2)\n\n# загружаем датасет\nmeta &lt;- oldFrenchMeta\n\n# допишите ваш код ниже\n# постройте столбиковую диаграмму, \n# показывающую распределение произведений по темам; цветом закодируйте жанр; \n# уберите названия осей; \n# поверните координатную ось; \n# поменяйте тему оформления на черно-белую, \n# а шрифт -- на Palatino; \n# добавьте заголовок \"Plot by [Your Name]\"\n\n\n\n#  экспортируйте график в формате jpg \n# с раширением 300 dpi; \n# в названии файла должна быть \n# ваша фамилия и номер группы\n\n\n\n\n\nUnderwood, Ted. 2019. Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nМастицкий, Сергей. 2017. Визуализация данных с помощью ggplot2. ДМК.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Визуализации</span>"
    ]
  },
  {
    "objectID": "iterate.html",
    "href": "iterate.html",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "4.1 Циклы и их аналоги\nХорошая новость: многие функции в R уже векторизованы, и если необходимо применить функцию к каждому элементу вектора, в большинстве случаев достаточно просто вызвать функцию. Это называется векторизация.\nНапример, у нас есть символьный вектор, и мы хотим узнать количество символов в каждом слове.\nhomer &lt;- c(\"в\", \"мысли\", \"ему\", \"то\", \"вложила\", \"богиня\", \"державная\", \"гера\")\nДля каждого компонента вектора необходимо выполнить одну итерацию цикла, в нашем случае – применить функцию nchar(). В некоторых языках программирования это делается как-то так:\nfor(i in homer) print(nchar(i))\n\n[1] 1\n[1] 5\n[1] 3\n[1] 2\n[1] 7\n[1] 6\n[1] 9\n[1] 4\nМы написали цикл for, который считает количество букв для каждого слова в векторе. Как видно, все сработало. Но в R это избыточно, потому что nchar() уже векторизована:\nnchar(homer)\n\n[1] 1 5 3 2 7 6 9 4\nЭто относится не только ко многим встроенным функциям R, но и к даже к операторам. x + 4 в действительности представляет собой +(x, 4):\nx &lt;- c(1.2, 2.51, 3.8)\n\n`+`(x, 4) \n\n[1] 5.20 6.51 7.80\nКлючевую роль здесь играет переработка данных, о которой мы говорили в первом уроке: короткий вектор повторяется до тех пор, пока его длина не сравняется с длиной более длинного вектора. Как-то так:\n\\[ \\left(\n    \\begin{array}{c}\n      1.2 \\\\\n      2.51 \\\\\n      3.8\n    \\end{array}\n  \\right) + \\left(\n    \\begin{array}{c}\n      4 \\\\\n      4 \\\\\n      4\n    \\end{array}\n  \\right) \\]\nЛишний цикл может замедлить вычисления. Проверим. Дан вектор x &lt;- c(3, 5, 7, 13). Необходимо возвести в квадрат каждое число, а из результата вычесть 100. Выполним двумя способами.\nlibrary(tictoc)\nx &lt;- c(2, 3, 5, 7, 11, 13)\n\n# способ первый\ntic()\nfor (i in x) print(i^2 - 100)\n\n[1] -96\n[1] -91\n[1] -75\n[1] -51\n[1] 21\n[1] 69\n\ntoc()\n\n0.002 sec elapsed\n\n# способ второй \ntic()\nx^2 - 100\n\n[1] -96 -91 -75 -51  21  69\n\ntoc()\n\n0.001 sec elapsed\nДля работы со списками циклы тоже чаще всего избыточны. Снова воспользуемся списком печенек из коллекции rcorpora.\nlibrary(rcorpora)\nmy_list &lt;-  corpora(\"foods/breads_and_pastries\")\npaste(\"Длина списка:\", length(my_list))\n\n[1] \"Длина списка: 3\"\n\nnames(my_list)\n\n[1] \"description\" \"breads\"      \"pastries\"\nКак узнать длину каждого вложенного в список вектора? Попробуем цикл (снова заметим время):\ntic()\nfor (i in 1:length(my_list)) print(length(my_list[[i]]))\n\n[1] 1\n[1] 35\n[1] 20\n\ntoc()\n\n0.001 sec elapsed\nНо в базовом R для таких случаев существуют функционалы lapply() и sapply(). Они принимают на входе список и функцию и применяют функцию к каждому элементу списка. Получается быстрее:\ntic()\nlapply(my_list, length)\n\n$description\n[1] 1\n\n$breads\n[1] 35\n\n$pastries\n[1] 20\n\ntoc()\n\n0 sec elapsed\nФункция sapply() упростит результат до вектора (s означает simplify):\ntic()\nsapply(my_list, length)\n\ndescription      breads    pastries \n          1          35          20 \n\ntoc()\n\n0.001 sec elapsed\nЕсли в виде списка хранится корпус, то так можно сделать, например, случайную выборку. Заметьте, как переданы аргументы функции sample().\nlapply(my_list[2:3], sample, 5, replace = TRUE)\n\n$breads\n[1] \"bagel\"        \"pumpernickel\" \"pumpernickel\" \"sourdough\"    \"lavash\"      \n\n$pastries\n[1] \"doughnut\"     \"stollen\"      \"kouign-amann\" \"morning bun\"  \"stollen\"\nМожет быть, с датафреймами будут полезны циклы? Например, так.\ndf &lt;- data.frame(author=c(\"Joe\",\"Jane\"), year=c(1801,1901), reprints=c(TRUE,FALSE))\ndf\ntic()\nfor (i in seq_along(df)) {\n print(class(df[,i]))\n}\n\n[1] \"character\"\n[1] \"numeric\"\n[1] \"logical\"\n\ntoc()\n\n0.002 sec elapsed\nНо и здесь можно ускориться. Второй аргумент apply означает, что мы работаем со столбцами (1 - строки).\ntic()\napply(df, 2, class)\n\n     author        year    reprints \n\"character\" \"character\" \"character\" \n\ntoc()\n\n0.001 sec elapsed\nЕсть еще vapply(), tapply() и mapply(), но и про них мы не будем много говорить, потому что все их с успехом заменяет семейство map_() из пакета purrr в tidyverse.\nТем не менее, перед освоением семейства map_() стоит потренироваться работать с обычными циклами, особенно если вам не приходилось иметь с ними дела (например, на Python). Несмотря на все недостатки, цикл for интуитивно понятен и часто проще начинать именно с него. Поэтому, прежде чем двинуться дальше, сделаем несколько упражнений из (Wickham и Grolemund 2016, 316).\n# стоит заглянуть в документацию к функции\nout &lt;- \"\"\nfor (x in letters) {\n  out &lt;- stringr::str_c(out, x)\n}\n\n# ответ - всего пять символов...\nx &lt;- sample(100)\nsd &lt;- 0\nfor (i in seq_along(x)) {\n  sd &lt;- sd + (x[i] - mean(x)) ^ 2\n}\nsd &lt;- sqrt(sd / (length(x) - 1))\n\n# надо понять, что тут происходит \nx &lt;- runif(100)\nout &lt;- vector(\"numeric\", length(x))\nout[1] &lt;- x[1]\nfor (i in 2:length(x)) {\n  out[i] &lt;- out[i - 1] + x[i]\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#циклы-и-их-аналоги",
    "href": "iterate.html#циклы-и-их-аналоги",
    "title": "4  Циклы, условия, функции",
    "section": "",
    "text": "На заметку\n\n\n\nВ циклах часто используется буква i. Но никакой особой магии в ней нет, имя переменной можно изменить.\n\n\n\n\n\n\n\n\n\n\n\nОдин из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми.\n— Нормат Мэтлофф\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПройдите урок 10 lapply and sapply и урок 11 vapply and tapply из курса R Programming в swirl.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nПосчитайте среднее для всех столбцов в mtcars.\nОпределите тип данных во всех столбцах nycflights13::flights.\nПосчитайте число уникальных значений в каждом столбце iris.\nСгенерируйте 10 случайных чисел из нормального распределения - это делает функция rnorm() - со средним -10, 0, 10.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПопробуйте избавиться от цикла 😜.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#условия",
    "href": "iterate.html#условия",
    "title": "4  Циклы, условия, функции",
    "section": "4.2 Условия",
    "text": "4.2 Условия\nИногда необходимо ограничить выполнение функции неким условием. Короткие условия можно писать в одну строку без фигурных скобок.\n\ny &lt;-  \"Эйяфьятлайокудль\"\n\nif(nchar(y) &gt; 10) print(\"много букв\")\n\n[1] \"много букв\"\n\n\nБолее сложные и множественные условия требуют фигурных скобок. Можно сравнить это с условным периодом: протасис (всегда либо TRUE, либо FALSE) в круглых скобках, аподосис в фигурных.\n\nif (nchar(y) &gt; 10) {\n  print(\"много букв\")\n} else if (nchar(y) &lt; 5) {\n  print(\"мало букв\")\n} else {\n  print(\"норм букв\")\n}\n\n[1] \"много букв\"\n\n\nТакже в R можно использовать специальную функцию:\n\nifelse(nchar(y) &gt; 10, \"много букв\", \"мало букв\")\n\n[1] \"много букв\"\n\n\nПрописывая условие, не забывайте, что применение булева оператора к вектору возвращает логический вектор:\n\nx &lt;- c(1:10)\nx &gt;= 5\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nТакое условие вернет ошибку.\n\nif (x &gt;= 5) print(\"все сломалось\")\n\nError in if (x &gt;= 5) print(\"все сломалось\"): the condition has length &gt; 1\n\n\nМожно скорректировать код так:\n\nif (any(x &gt;= 5)) print(\"все сработало\")\n\n[1] \"все сработало\"\n\n\nПо той же причине внутри условия не надо использовать логические операторы | (“или”) или & (“и”), потому что они векторизованы:\n\nx &lt; 3 | x &gt; 7\n\n [1]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#функции",
    "href": "iterate.html#функции",
    "title": "4  Циклы, условия, функции",
    "section": "4.3 Функции",
    "text": "4.3 Функции\nФункция и код – не одно и то же. Чтобы стать функцией, кусок кода должен получить имя. Но зачем давать имя коду, который и так работает?\nВот три причины, которые приводит Хадли Уикхем:\n\nу функции есть выразительное имя, которое облегчает понимание кода;\nпри изменении требований необходимо обновлять код только в одном месте, а не во многих;\nменьше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом)\n\n\nWriting good functions is a lifetime journey.\n— Hadley Wickham\n\nМашине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена!\nДалее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть).\nНаписание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию.\nНапример, нам нужна функция, которая ищет совпадения в двух векторах и возвращает совпавшие элементы. Сначала решим задачу для двух векторов.\n\nline1 &lt;- c(\"гнев\", \"богиня\", \"воспой\")\nline2 &lt;- c(\"в\", \"мысли\", \"ему\", \"то\", \"вложила\", \"богиня\", \"державная\", \"гера\")\nidx &lt;- which(line2 %in% line1) # 2\nline2[idx]\n\n[1] \"богиня\"\n\n\nТеперь заменяем фактические переменные на формальные.\n\ncommon_words &lt;- function(x, y){\n  idx &lt;- which(x %in% y)\n  x[idx]\n}\n\nИ применяем к новым данным.\n\nline3 &lt;- c(\"лишь\", \"явилась\", \"заря\", \"розоперстая\", \"вестница\", \"утра\")\nline4 &lt;- c(\"вестница\", \"утра\", \"заря\", \"на\", \"великий\", \"олимп\", \"восходила\")\ncommon_words(line4, line3)\n\n[1] \"вестница\" \"утра\"     \"заря\"    \n\n\n\n\n\n\n\n\nЗадание\n\n\n\nЗагрузите библиотеку swirl, выберите курс R Programming и пройдите из него урок 9 Functions.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nДля просмотра исходного кода любой функции необходимо…\n\n\n\n\n\n\nединственный способ — найти код функции в репозитории на GitHub\n\n\nиспользовать специальную функцию для просмотра кода\n\n\nвызвать help к функции\n\n\nнабрать имя функции без аргументов и без скобок\n\n\n\n\n\n\n\nНапишем функцию, которая будет центрировать данные, то есть вычитать среднее из каждого значения (забудем на время, что это уже делает базовая scale()):\n\ncenter &lt;- function(x){ \n  n = x - mean(x)\n  return(n) \n}\n\nx &lt;- c(5, 10, 15)\ncenter(x) \n\n[1] -5  0  5\n\n\nВнутри нашей функции есть переменная n, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции.\nФункция может принимать произвольное число аргументов. Доработаем наш код:\n\ncenter &lt;- function(x, na.rm = F){\n  if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие\n  x - mean(x) # на этот раз без return()\n}\n\nx &lt;- c(5, 10, NA)\ncenter(x)\n\n[1] NA NA NA\n\n\nЧто произошло? Почему следующий код выдает другой результат?\n\ncenter(x, na.rm = T)\n\n[1] -2.5  2.5\n\n\nВычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет.\n\ncenter &lt;- function(x, na.rm = F, what_is_your_name){\n  if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие\n  x - mean(x) # на этот раз без return()\n}\n\ncenter(x, na.rm = T)\n\n[1] -2.5  2.5\n\ncenter(x, na.rm = T, what_is_your_name = \"Locusclassicus\")\n\n[1] -2.5  2.5\n\n\nЧасто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении.\n\ncenter &lt;- function(x){\n  if (length(x) == 1) {stop(\"Отстань, старушка, я в печали.\")}\n  x - mean(x) # на этот раз без return()\n}\n\nx &lt;- 10\ncenter(x) # вернет ошибку\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию awesome_plot, которая будет принимать в качестве аргументов два вектора, трансформировать их в тиббл и строить диаграмму рассеяния при помощи ggplot(). Задайте цвет и прозрачность точек, а в подзаголовке выведите коэффициент корреляции.\n\n\n\n\n\n\n\n\nЗадание\n\n\n\n\nНапишите код, который распечатает стихи детской песни “Alice the Camel”.\nПревратите детскую потешку “Ted in the Bed” в функцию. Обобщите до любого числа спящих.\nЗапишите в виде функции текст песни “99 Bottles of Beer on the Wall”. Обобщите до любого числа любых напитков на любой поверхности.\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНапишите функцию, которая будет говорить “доброе утро”, “добрый день” или “добрый вечер” в зависимости от времени суток. Используйте lubridate::now() в качестве значения аргумента по умолчанию.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#purrr",
    "href": "iterate.html#purrr",
    "title": "4  Циклы, условия, функции",
    "section": "4.4 Purrr",
    "text": "4.4 Purrr\nПо-настоящему мощный инструмент для итераций – это пакет purrr из семейства tidyverse. Разработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham и Grolemund 2016).\n\nYou should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work.\n— Hadley Wickham & Garrett Grolemund\n\nВ семействе функций map_ из этого пакета всего 23 вариации. Вот основные из них:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nВсе они принимают на входе данные и функцию (или формулу), которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_int() – целочисленный вектор, и т.д.\nВоспользуемся возможностями purrr, чтобы исследовать датасет starwars из пакета dplyr. Для начала узнаем число отсутствующих значений в каждом столбце. Косая черта (\\) указывает на то, что мы используем анонимную функцию\n\nlibrary(tidyverse)\nstarwars &lt;- starwars\nmap_int(starwars, \\(x) sum(is.na(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year \n         0          6         28          5          0          0         44 \n       sex     gender  homeworld    species      films   vehicles  starships \n         4          4         10          4          0          0          0 \n\n\nОбратите внимание, что map_int, как и map_dbl возвращает именованный вектор. Чтобы избавиться от имен, можно использовать unname():\n\nmap_chr(starwars, class) |&gt; \n  unname()\n\n [1] \"character\" \"integer\"   \"numeric\"   \"character\" \"character\" \"character\"\n [7] \"numeric\"   \"character\" \"character\" \"character\" \"character\" \"list\"     \n[13] \"list\"      \"list\"     \n\n\n\nИспользуйте map_dbl и n_distinct, чтобы узнать число уникальных наблюдений в каждом столбце.\n\nЕсли функция принимает дополнительные аргументы, их можно задать после названия функции. В таком случае для каждого вызова функции будет использовано это значение аргумента. В примере ниже это аргумент na.rm.\n\nstarwars |&gt; \n  select(mass, height) |&gt; \n  map(mean, na.rm = TRUE)\n\n$mass\n[1] 97.31186\n\n$height\n[1] 174.6049\n\n\nПри вызове map_df есть дополнительная возможность сохранить названия столбцов, используя аргумент .id:\n\nstarwars |&gt; \n  map_df(~data.frame(unique_values = n_distinct(.x),\n                     col_class = class(.x)),\n         .id = \"variable\"\n         )\n\n\n  \n\n\n\nФункции map можно передать пользовательскую функцию. Для примера создадим функцию describe_vec(), которая возвращает строку с длиной и классом вектора, и применим ее к хлебо-булочному списку из примеров выше.\n\n# пользовательская функция\ndescribe_vec &lt;- function(vec){\n  l = paste(\"Длина вектора: \", length(vec))\n  c = paste(\"Класс вектора: \", class(vec))\n  result = paste(l, c, sep = \" | \")\n  return(result)\n}\n\nmap_chr(my_list, describe_vec) |&gt; \n  unname()\n\n[1] \"Длина вектора:  1 | Класс вектора:  character\" \n[2] \"Длина вектора:  35 | Класс вектора:  character\"\n[3] \"Длина вектора:  20 | Класс вектора:  character\"\n\n\nКроме того, мы можем передать map анонимную функцию (вместо function можно поставить \\):\n\nmap_chr(my_list, \n        function(x) paste(\"Длина вектора: \", length(x))\n        )\n\n         description               breads             pastries \n \"Длина вектора:  1\" \"Длина вектора:  35\" \"Длина вектора:  20\" \n\n\nЕсли необходимо несколько раз вызывать одну и ту же функцию с двумя аргументами, используется функция map2(). Аргументы, которые меняются при каждом вызове, пишутся до функции; аргументы, которые остаются неизменны, – после.\n\nmean = list(5, 10, -3)\nsd = list(1, 5, 50)\nmap2(mean, sd, rnorm, n = 5)\n\n[[1]]\n[1] 6.325714 6.269545 4.160860 6.706881 3.648079\n\n[[2]]\n[1]  9.8766360 16.8188806  5.1923310  0.8869719 12.0884858\n\n[[3]]\n[1] -73.08652 -42.88582 -11.73483 -26.45696 -53.57738\n\n\n\n\n\n\n_Как работает map2()_\n\n\n\nЭто можно обобщить следующим образом (источник):\n\nМожно было бы предположить, что должны быть и map3(), map4() и т.д., но во всех случаеях, когда у функции больше двух аргументов, используется pmap().\n\n\n\n\n\n\nЗадание\n\n\n\nУстановите курс swirl::install_course(\"Advanced R Programming\") и пройдите из него урок 3 Functional Programming with purrr.\n\n\nНесколько вопросов для самопроверки.\n\n\n\n\n\n\nВопрос\n\n\n\nФункции-предикаты (predicate functions) возвращают TRUE или FALSE. Выберите из списка все функции-предикаты.\n\n\n\n\nevery()\n\n\nsome()\n\n\nnone()\n\n\nhas_element()\n\n\nis.factor()\n\n\nkeep()\n\n\ndiscard()\n\n\nis.numeric()\n\n\ndetect()\n\n\n\n\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nКакие из функций ниже принимают в качестве аргумента функции-предикаты?\n\n\n\n\nevery()\n\n\nsome()\n\n\nnone()\n\n\nhas_element()\n\n\nis.factor()\n\n\nkeep()\n\n\ndiscard()\n\n\nis.numeric()\n\n\ndetect()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "iterate.html#гарри-поттер-цикл-vs.-map_",
    "href": "iterate.html#гарри-поттер-цикл-vs.-map_",
    "title": "4  Циклы, условия, функции",
    "section": "4.5 Гарри Поттер: цикл vs. map_()",
    "text": "4.5 Гарри Поттер: цикл vs. map_()\nКак вы уже поняли, одни и те же задачи можно решать при помощи циклов и при помощи map_. Мы потренируемся на датасете, который в 2023 г. был доступен на сайте Британской библиотеки (https://www.bl.uk/), но потом оттуда исчез (но у нас сохранилась копия).\nДатасет представляет собой набор файлов .csv, содержащих метаданные о ресурсах, связанных с Гарри Поттером, из коллекций Британской библиотеки. Первоначально он был выпущен к 20-летию публикации книги «Гарри Поттер и философский камень» 26 июня 2017 года и с тех пор ежегодно обновлялся. Всего в датасете пять файлов, каждый из которых содержит разное представление данных.\nСкачаем архив.\n\nmy_url &lt;- \"https://github.com/locusclassicus/text_analysis_2024/raw/main/files/HP.zip\"\ndownload.file(url = my_url, destfile = \"../files/HP.zip\")\n\nПосле этого переходим в директорию с архивом и распаковываем его.\n\nunzip(\"../files/HP.zip\")\n\nСохраним список всех файлов с расширением .csv, используя подходящую функцию из base R.\n\nmy_files &lt;- list.files(\"../files/HP\", pattern = \".csv\", full.names = TRUE)\nmy_files\n\n[1] \"../files/HP/classification.csv\" \"../files/HP/names.csv\"         \n[3] \"../files/HP/records.csv\"        \"../files/HP/titles.csv\"        \n[5] \"../files/HP/topics.csv\"        \n\n\n\n4.5.1 Цикл\nТеперь напишем цикл, который\n\nпрочитает все файлы из my_files, используя для этого функцию read_csv() из пакета readr;\nдля каждого датасета выяснит количество рядов без NA в столбце BNB Number;\nразделит число таких рядов на общее число рядов; (@)) вернет таблицу c четырьми столбцами:\n\n\nназвание файла (id),\nчисло рядов (total),\nчисло рядов без NA (complete),\nдоля полных рядов (ratio).\n\nСначала создаем таблицу, в которую будем складывать результат.\n\nmy_files_short &lt;- list.files(\"../files/HP\", pattern = \".csv\")\n\nmy_df &lt;- data.frame(id = my_files_short, \n                    total = rep(0, length(my_files)),\n                    complete = rep(0, length(my_files)),\n                    ratio = rep(0, length(my_files)))\n\nmy_df\n\n\n  \n\n\n\nТеперь тело цикла:\n\nfor (i in 1:length(my_files)) {\n\n  # читаем очередной файл из my_files\n  current_file &lt;- my_files[i]\n  current_df &lt;- readr::read_csv(current_file, show_col_types = FALSE) \n\n  # выявляем общее число рядов и число рядов без NA в BNB number\n  # из-за пробела в названии столбца BNB number нужно использовать\n  # с бэктиками ``, а не с \"такими\" или 'такими' кавычками \n  current_total &lt;- nrow(current_df)\n  current_complete &lt;- sum(!is.na(current_df$`BNB number`))\n    \n\n  # помещаем значения в нужное место в заранее созданном my_df вместо нулей\n  my_df$total[i] &lt;- current_total  \n  my_df$complete[i] &lt;- current_complete\n  my_df$ratio[i] &lt;- current_complete / current_total\n}\n\nСмотрим на результат.\n\nmy_df\n\n\n  \n\n\n\n\n\n4.5.2 map_()\nТеперь исследуем датасет при помощи функционалов. Прочитаем все файлы одним вызовом функции.\n\n# чтение файлов \nHP &lt;- map(my_files, read_csv, col_types = cols())\n\nОбъект HP – это список. В нем пять элементов, так как на входе у нас было пять файлов. Для удобства назначим имена элементам списка.\n\nnames(HP) &lt;- my_files_short\n\n\nНачнем с простого: при помощи map можно извлечь столбцы (по имени) или ряды (по условию) из всех пяти таблиц. Прежде чем выполнить код ниже, подумайте, как будет выглядеть результат.\n\n# извлечь столбцы\nmap(HP, select, `BNB number`)\n\n# извлечь ряды\nmap(HP, filter, !(is.na(`BNB number`)))\n\n\n\n\n\n\n\nЗадание\n\n\n\nИзвлеките все уникальные названия (столбец Title) из всех пяти таблиц в HP. Используйте функцию distinct.\n\n\nЧто, если мы не знаем заранее, какие столбцы есть во всех пяти таблицах, и хотим это выяснить? Для этого подойдет функция reduce() из того же purrr. Она принимает на входе вектор (или список) и функцию и применяет функцию последовательно к каждой паре значений.\n\n\n\nИсточник.\n\n\n\nВоспользуемся этим, чтобы найти общие для всех таблиц имена столбцов.\n\nmap(HP, colnames) |&gt; \n  reduce(intersect)\n\n [1] \"Dewey classification\"       \"BL record ID\"              \n [3] \"Type of resource\"           \"Content type\"              \n [5] \"Material type\"              \"BNB number\"                \n [7] \"ISBN\"                       \"ISSN\"                      \n [9] \"Name\"                       \"Dates associated with name\"\n[11] \"Type of name\"               \"Role\"                      \n[13] \"Title\"                      \"Series title\"              \n[15] \"Number within series\"       \"Country of publication\"    \n[17] \"Place of publication\"       \"Publisher\"                 \n[19] \"Date of publication\"        \"Edition\"                   \n[21] \"Physical description\"       \"BL shelfmark\"              \n[23] \"Genre\"                      \"Languages\"                 \n[25] \"Notes\"                     \n\n\nЕще одна неочевидная возможность функции reduce - объединение нескольких таблиц в одну одним вызовом. Например, так:\n\nHP_joined &lt;- HP |&gt; \n  reduce(left_join)\n\nHP_joined\n\n\n  \n\n\n\n\n\n4.5.3 EDA\nТеперь можно почистить данные и построить несколько разведывательных графиков.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\ndata_sum &lt;- HP_joined |&gt; \n  separate(`Date of publication`, into = c(\"year\", NA)) |&gt; \n  separate(`Country of publication`, into = c(\"country\", NA), sep = \";\") |&gt;\n  mutate(country = str_squish(country)) |&gt; \n  mutate(country = \n           case_when(country == \"England\" ~ \"United Kingdom\",\n                     country == \"Scotland\" ~ \"United Kingdom\",\n                     TRUE ~ country)) |&gt; \n  filter(!is.na(year)) |&gt; \n  filter(!is.na(country)) |&gt; \n  group_by(year, country) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n  \n\ndata_sum\n\n\n  \n\n\n\n\ndata_sum |&gt; \n  ggplot(aes(year, n, fill = country)) + \n  geom_col() + \n  xlab(NULL) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nВ качестве небольшого бонуса к этому уроку построим облако слов. Вектор слов возьмем из столбца Topic.\n\ndata_topics &lt;- HP_joined |&gt; \n  filter(!is.na(Topics)) |&gt; \n  separate(Topics, into = c(\"topic\", NA)) |&gt; \n  mutate(topic = tolower(topic)) |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  filter(!topic %in% c(\"harry\", \"rowling\", \"potter\", \"children\", \"literary\"))\n\n\npal &lt;- c(\"#f1c40f\", \"#34495e\", \n         \"#8e44ad\", \"#3498db\",\n         \"#2ecc71\")\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\npar(mar = c(1, 1, 1, 1))\nwordcloud(data_topics$topic, \n          data_topics$n,\n          min.freq = 3,\n          #max.words = 50, \n          scale = c(3, 0.8),\n          colors = pal, \n          random.color = T, \n          rot.per = .2,\n          vfont=c(\"script\",\"plain\")\n          )\n\n\n\n\n\n\n\n\nИнтерактивное облако слов можно построить с использованием пакета wordcloud2. Сделаем облако в форме шляпы волшебника!\n\n# devtools::install_github(\"lchiffon/wordcloud2\")\nlibrary(wordcloud2)\n\n\nwordcloud2(data_topics, \n           figPath = \"./book/images/Wizard-Hat.png\",\n           size = 1.5,\n           backgroundColor=\"black\",\n           color=\"random-light\", \n           fontWeight = \"normal\",\n)\n\n\nТеперь попробуйте сами.\n\n\n\n\n\n\nЗадание\n\n\n\nПрактическое задание “Алиса в стране чудес”\n\n\n\n# постройте облако слов для \"Алисы в стране чудес\"\n\nlibrary(languageR)\nlibrary(dplyr)\nlibrary(tidytext)\n\n# вектор с \"Алисой\"\nalice &lt;- tolower(alice)\n\n# частотности для слов\nfreq &lt;- as_tibble(table(alice)) |&gt; \n  rename(word = alice)\n\n# удалить стоп-слова\nfreq_tidy &lt;- freq |&gt; \n  anti_join(stop_words) \n# возможно, вы захотите произвести и другие преобразования\n\n# облако можно строить в любой библиотеке\n\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Циклы, условия, функции</span>"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "5  Импорт",
    "section": "",
    "text": "5.1 JSON\nФормат JSON (JavaScript Object Notation) предназначен для представления структурированных данных. JSON имеет шесть основных типов данных. Четыре из них - скаляры:\nСтроки, числа и булевы значения в JSON очень похожи на символьные, числовые и логические векторы в R. Основное отличие заключается в том, что скаляры JSON могут представлять только одно значение. Для представления нескольких значений необходимо использовать один из двух оставшихся типов: массивы и объекты.\nИ массивы, и объекты похожи на списки в R, разница заключается в том, именованы они или нет. Массив подобен безымянному списку и записывается через []. Например, [1, 2, 3] - это массив, содержащий 3 числа, а [null, 1, \"string\", false] - массив, содержащий ноль, число, строку и булево значение.\nОбъект подобен именованному списку и записывается через {}. Имена (ключи в терминологии JSON) являются строками, поэтому должны быть заключены в кавычки. Например, {“x”: 1, “y”: 2} - это объект, который сопоставляет x с 1, а y – с 2.\nЗагрузим небольшой файл TBBT.json, хранящий данные о сериале “Теория большого взрыва” (источник). Скачать лучше из репозитория курса ссылка.\nlibrary(jsonlite)\n\npath &lt;- \"../files/TBBT.json\"\ntbbt &lt;- fromJSON(txt =  path,\n                 simplifyVector = T)\nФункция fromJSON() вернула нам список, в предпросмотре он выглядит так.\nВыборочно преобразуем список в тиббл:\nlibrary(tidyverse)\n\ncast_tbl &lt;- tbbt$casting |&gt; \n  transpose() |&gt; \n  map(as.character) |&gt; \n  as_tibble()\n\ncast_tbl\nПроделаем то же самое для списка эпизодов, но немного другим способом.\nepisodes_tbl &lt;- tibble(\n  episode_id = map_chr(tbbt$episode_list, pluck, \"episode_id\"),\n  title = map_chr(tbbt$episode_list, pluck, \"title\"))\n\nepisodes_tbl",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#json",
    "href": "import.html#json",
    "title": "5  Импорт",
    "section": "",
    "text": "cамый простой тип - null (нуль), который играет ту же роль, что и NA в R. Он представляет собой отсутствие данных;\ncтрока (string) похожа на строку в R, но в ней всегда должны использоваться двойные кавычки;\nчисло аналогично числам в R, при этом поддерживается целочисленная (например, 123), десятичная (например, 123.45) или научная (например, 1,23e3) нотация. JSON не поддерживает Inf, -Inf или NaN;\nлогическое значение аналогично TRUE и FALSE в R, но использует строчные буквы true и false.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСамостоятельно создайте тиббл, в котором будет храниться количество серий для каждого сезона.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#xml",
    "href": "import.html#xml",
    "title": "5  Импорт",
    "section": "5.2 XML",
    "text": "5.2 XML\nXML (от англ. eXtensible Markup Language) — расширяемый язык разметки. Слово “расширяемый” означает, что список тегов не зафиксирован раз и навсегда: пользователи могут вводить свои собственные теги и создавать так называемые настраиваемые языки разметки (Холзнер 2004, 29). Один из таких настраиваемых языков – это TEI (Text Encoding Initiative), о котором будет сказано дальше.\nНазначение языков разметки заключается в описании структурированных документов. Структура документа представляется в виде набора вложенных в друг друга элементов (дерева XML). У элементов есть открывающие и закрывающие теги.\nВсе составляющие части документа обобщаются в пролог и корневой элемент. Корневой элемент — обязательная часть документа, в которую вложены все остальные элементы. Пролог может включать объявления, инструкции обработки, комментарии.\nВ правильно сформированном XML открывающий и закрывающий тег вложенного элемента всегда находятся внутри одного родительского элемента.\nСоздадим простой XML из строки. Сначала идет инструкция по обработке XML (со знаком вопроса), за ней следует объявление типа документа (с восклицательным знаком) и открывающий тег корневого элемента. В этот корневой элемент вложены все остальные элементы.\n\nstring_xml &lt;- '&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!DOCTYPE recipe&gt;\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n   &lt;title&gt;\n      Простой хлеб\n   &lt;/title&gt;\n   &lt;composition&gt;\n      &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n      &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n      &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n   &lt;/composition&gt;\n   &lt;instructions&gt;\n     &lt;step&gt;\n        Смешать все ингредиенты и тщательно замесить. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Закрыть тканью и оставить на один час в тёплом помещении. \n     &lt;/step&gt;\n     &lt;step&gt;\n        Замесить ещё раз, положить на противень и поставить в духовку.\n     &lt;/step&gt;\n   &lt;/instructions&gt;\n&lt;/recipe&gt;'\n\nДля работы с xml понадобится установить одноименную библиотеку. Функция xmlTreeParse() создаст R-структуру, представляющую дерево XML.\n\nlibrary(XML)\ndoc &lt;- xmlTreeParse(string_xml)\nclass(doc)\n\n[1] \"XMLDocument\"         \"XMLAbstractDocument\"\n\n\nФункция xmlRoot позволяет извлечь корневой элемент вместе со всеми детьми.\n\nrootnode &lt;- xmlRoot(doc)\nrootnode\n\n&lt;recipe name=\"хлеб\" preptime=\"5min\" cooktime=\"180min\"&gt;\n &lt;title&gt;Простой хлеб&lt;/title&gt;\n &lt;composition&gt;\n  &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n  &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n  &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n &lt;/composition&gt;\n &lt;instructions&gt;\n  &lt;step&gt;Смешать все ингредиенты и тщательно замесить.&lt;/step&gt;\n  &lt;step&gt;Закрыть тканью и оставить на один час в тёплом помещении.&lt;/step&gt;\n  &lt;step&gt;Замесить ещё раз, положить на противень и поставить в духовку.&lt;/step&gt;\n &lt;/instructions&gt;\n&lt;/recipe&gt;\n\n\nЕсли документ большой, бывает удобнее не распечатывать все дерево, а вывести имена дочерних элементов.\n\nnames(xmlChildren(rootnode))\n\n[1] \"title\"        \"composition\"  \"instructions\"\n\n\nРазмер узла – это число вложенных в него “детей”. Его можно узнать, применив к узлу функцию xmlSize() – или подсчитав число “детей”.\n\nxmlSize(rootnode) == length(xmlChildren(rootnode))\n\n[1] TRUE\n\n\nРаботать с xml можно как с обычным списком, то есть индексировать узлы по имени или по номеру элемента при помощи квадратных скобок. Так мы достаем узел по имени:\n\nrootnode[[\"composition\"]]\n\n&lt;composition&gt;\n &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n&lt;/composition&gt;\n\n\nА так – по индексу:\n\nrootnode[[2]]\n\n&lt;composition&gt;\n &lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n &lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n &lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n&lt;/composition&gt;\n\n\nКак и с обычными списками, мы можем использовать последовательности квадратных скобок:\n\ningr_node &lt;- rootnode[[2]][[\"ingredient\"]]\ningr_node\n\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n\nНо обычно нам нужен не элемент как таковой, а его содержание (значение). Чтобы добраться до него, используем функцию xmlValue():\n\nxmlValue(ingr_node)\n\n[1] \"Мука\"\n\n\nМожно уточнить атрибуты узла при помощи xmlAttrs():\n\nxmlAttrs(ingr_node)\n\n  amount     unit \n     \"3\" \"стакан\" \n\n\nЧтобы извлечь значение атрибута, используем функцию xmlGetAttr(). Первым аргументом функции передаем xml-узел, вторым – имя атрибута.\n\nxmlGetAttr(ingr_node, \"unit\")\n\n[1] \"стакан\"\n\n\nКак насчет того, чтобы применить функцию к набору узлов – например, ко всем инредиентам? Вспоминаем функции для работы со списками – sapply() из базового R или map() из пакета purrr:\n\ningr_nodes &lt;- xmlChildren(rootnode[[2]])\n\nsapply(ingr_nodes, xmlValue)\n\n   ingredient    ingredient    ingredient \n       \"Мука\"      \"Дрожжи\" \"Тёплая вода\" \n\n\n\nsapply(ingr_nodes, xmlGetAttr, \"unit\")\n\ningredient ingredient ingredient \n  \"стакан\"    \"грамм\"   \"стакан\" \n\n\nДобраться до узлов определенного уровня можно также при помощи синтаксиса XPath. XPath – это язык запросов к элементам XML-документа. С его помощью можно описать “путь” до нужного узла: абсолютный (начиная с корневого элемента) или относительный. В пакете XML синтаксис XPath поддерживает функция getNodeSet().\n\n# абсолютный путь\ningr_nodes &lt;- getNodeSet(rootnode, \"/recipe//composition//ingredient\")\n\ningr_nodes\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n\n[[3]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n# относительный путь\ningr_nodes &lt;- getNodeSet(rootnode, \"//composition//ingredient\")\n\ningr_nodes\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"0.25\" unit=\"грамм\"&gt;Дрожжи&lt;/ingredient&gt;\n\n[[3]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nВ большинстве случаев функция getNodeSet() требует задать пространство имен (namespace), но в нашем случае оно не определено, поэтому пока передаем только дерево и путь до узла. С пространством имен встретимся чуть позже!\n\n\n\nСинтаксис XPath позволяет отбирать узлы с определенными атрибутами. Допустим, нам нужны только те узлы, где значение атрибута unit = “стакан”:\n\ngetNodeSet(rootnode, \"//composition//ingredient[@unit='стакан']\")\n\n[[1]]\n&lt;ingredient amount=\"3\" unit=\"стакан\"&gt;Мука&lt;/ingredient&gt;\n\n[[2]]\n&lt;ingredient amount=\"1.5\" unit=\"стакан\"&gt;Тёплая вода&lt;/ingredient&gt;\n\n\nПри работе с xml в большинстве случаев наша задача – извлечь значения определеннных узлов или их атрибутов и сохранить их в прямоугольном формате.\nВ нашем простом примере это можно сделать несколькими способами. Первый: просто связать воедино несколько векторов.\n\ntitle &lt;- xmlValue(rootnode[[\"title\"]])\ningredients &lt;- map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlValue)\nunit &lt;- map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlGetAttr, \"unit\")\namount &lt;- map_chr(xmlChildren(rootnode[[\"composition\"]]), xmlGetAttr, \"amount\")\n\n\ntibble(title, ingredients, unit, amount)\n\n\n  \n\n\n\nВ некоторых случаях бывает удобно также воспользоваться функциями из пакета xml2 в сочетании с функциями семейства unnest_ из tidyr.\n\nlibrary(xml2)\n\ndoc &lt;- as_list(read_xml(string_xml))\n\n# попробуем достать атрибуты\ndoc |&gt; \n  as_tibble() |&gt; \n  unnest_longer(recipe) |&gt; \n  filter(recipe_id == \"ingredient\") |&gt; \n  mutate(unit = map_chr(recipe, attr, \"unit\")) |&gt; \n  mutate(amount = map_chr(recipe, attr, \"amount\")) |&gt; \n  select(-recipe_id) |&gt; \n  unnest_longer(recipe)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#tei",
    "href": "import.html#tei",
    "title": "5  Импорт",
    "section": "5.3 TEI",
    "text": "5.3 TEI\nБольшая часть размеченных литературных корпусов хранится именно в формате XML. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п.\nДобавлять и удалять разметку может любой пользователь в редакторе XML кода или даже в простом текстовом редакторе. При этом в качестве универсального языка разметки в гуманитарных дисциплинах используется язык TEI (Скоринкин 2016). Корневой элемент в документах TEI называется TEI, внутри него располагается элемент teiHeader с метаинформацией о документе и элемент text. Последний содержит текст документа с элементами, определяющими его структурное членение.\n&lt;TEI&gt;\n  &lt;teiHeader&gt;&lt;/teiHeader&gt;\n  &lt;text&gt;&lt;/text&gt;\n&lt;/TEI&gt;\nПример оформления документа можно посмотреть по ссылке.\nУ teiHeader есть четыре главных дочерних элемента:\n\nfileDesc (описание документа c библиографической информацией)\nencodingDesc (описание способа кодирование первоисточника)\nprofileDesc (“досье” на текст, например отправитель и получатель для писем, жанр, используемые языки, обстоятельства создания, место написания и т.п.)\nrevisionDesc (история изменений документа)\n\nВ самом тексте язык TEI дает возможность представлять разные варианты (авторские, редакторские, корректорские и др.) Основным средством параллельного представления является элемент choice. Например, в тексте Лукреция вы можете увидеть такое:\nsic calor atque &lt;choice&gt;&lt;reg&gt;aer&lt;/reg&gt;&lt;orig&gt;aër&lt;/orig&gt;&lt;/choice&gt; et venti caeca potestas\nЗдесь reg указывает на нормализованное написание, а orig – на оригинальное.\nВ качестве примера загрузим датасет “Пушкинского дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”.\n\nfilename = \"../files/War_and_Peace.xml\"\ndoc &lt;- xmlTreeParse(filename, useInternalNodes = T)\nrootnode &lt;- xmlRoot(doc)\n\nТеперь можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена:\n\nnames(xmlChildren(rootnode)) \n\n[1] \"teiHeader\" \"text\"     \n\n\nОчевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее:\n\nnames(xmlChildren(rootnode[[\"text\"]])) \n\n[1] \"div\" \"div\" \"div\" \"div\" \"div\"\n\n\nИтак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это четыре тома и эпилог.\n\n# это список\ndivs &lt;-  rootnode[[\"text\"]][\"div\"]\n\nsapply(divs, xmlGetAttr, \"type\")\n\n       div        div        div        div        div \n  \"volume\"   \"volume\"   \"volume\"   \"volume\" \"epilogue\" \n\n\nКак мы уже знаем, добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – при помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на два уровня ниже: там тоже будет тег div, но с другим атрибутом. Как легко убедиться, теперь это главы, всего их 358.\n\ndivs &lt;- getNodeSet(doc, \"/tei:TEI//tei:text//tei:div//tei:div//tei:div\",\n                     namespaces = c(tei = \"http://www.tei-c.org/ns/1.0\")) \n\nlength(divs)\n\n[1] 358\n\nunique(sapply(divs, xmlGetAttr, \"type\"))\n\n[1] \"chapter\"\n\n\nОбратите внимание, что в данном случае надо прямо прописать пространство имен (namespaces). Это можно посмотреть в самом xml, а можно воспользоваться специальной функцией:\n\nxmlNamespace(rootnode)\n\n[1] \"http://www.tei-c.org/ns/1.0\"\nattr(,\"class\")\n[1] \"XMLNamespace\"\n\n\nЗабрать конкретную главу можно путем индексации, но лучше – по значению соответствующего атрибута.\n\nidx &lt;- which(map(divs, xmlGetAttr, \"xml:id\") == \"chapter1part1Volume1\")\nch1 &lt;- divs[[idx]]\n\nЧтобы извлечь текст, понадобится функция xmlValue.\n\nchapter_1 &lt;- xmlValue(ch1)\n\nРаспечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на параграфы и выведем первый и последний:\n\nlibrary(stringr)\nchapter_lines &lt;- str_split(chapter_1, pattern = \"\\n\")\n\nchapter_lines[[1]][[5]]\n\n[1] \"        — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j'y crois) — je ne vous connais plus, vous n'êtes plus mon ami, vous n'êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.\"\n\nchapter_lines[[1]][[838]]\n\n[1] \"       Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.\"\n\n\nПервая и последняя реплика по-французски: все правильно!\n\n\n\n\n\n\nЗадание\n\n\n\nСкачайте по ссылке “Горе от ума” Грибоедова и преобразуйте xml в прямоугольный формат таким образом, чтобы для каждой реплики был указан акт, сцена и действующее лицо.\n\n\nПодбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan и Lang 2014) и (Холзнер 2004).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "import.html#бонус-gutenbergr",
    "href": "import.html#бонус-gutenbergr",
    "title": "5  Импорт",
    "section": "5.4 Бонус: GutenbergR",
    "text": "5.4 Бонус: GutenbergR\nПакет GutenbergR поможет достать тексты из библиотеки Gutenberg, но будьте осторожны: распознаны они не всегда хорошо и порой содержат много разного шума, например примечания редактора, номера страниц и т.п. В билингвах источник и перевод могут идти вперемешку. И если в XML подобные элементы будут окружены соответствующими тегами, которые позволят их легко отбросить при анализе, то Gutenberg дает вам сырой текст. Часто его надо хорошенько чистить при помощи регулярных выражений или даже вручную.\nРаботать с метаданными GutenbergR вы уже научились, теперь можете пользоваться пакетом и для скачивания текстов. Сначала узнаем id нужных текстов^ [https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html]\n\nlibrary(gutenbergr)\n\ncaesar &lt;- gutenberg_works(author == \"Caesar, Julius\", languages = \"la\") \n\ncaesar \n\n\n  \n\n\n\nЧтобы извлечь отдельный текст (тексты):\n\nde_bello_gallico &lt;- gutenberg_download(218, meta_fields = \"title\", mirror = \"ftp://mirrors.xmission.com/gutenberg/\")\nde_bello_gallico\n\n\n  \n\n\n\n\n\n\n\n\n\nНа заметку\n\n\n\nСуществует несколько зеркал библиотеки Gutenberg, и, если при выполнении функции gutenberg_download() возникает ошибка “could not download a book at http://aleph.gutenberg.org/”, то следует использовать аргумент mirror. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL\n\n\n\n\n\n\nNolan, D., и D. T. Lang. 2014. XML and Web Technologies for Data Science with R. Springer.\n\n\nСкоринкин, Даниил. 2016. «Электронное представление текста с помощью стандарта разметки TEI», 90–108.\n\n\nХолзнер, Стивен. 2004. Энциклопедия XML. Питер.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Импорт</span>"
    ]
  },
  {
    "objectID": "share.html",
    "href": "share.html",
    "title": "6  Публикация",
    "section": "",
    "text": "6.1 О воспроизводимости\nПолученный в результате количественных исследований результат должен быть проверяем и воспроизводим. Это значит, что в большинстве случаев недостаточно просто рассказать, что вы проделали. Теоретически читатель должен иметь возможность проделать тот же путь, что и автор: вопроизвести его результаты, но в обратном направлении.\nДля этого должны выполняться три основных требования:\nУже на этапе планирования исследования очень важно продумать, как вы будете его документировать. Важно помнить, что код пишется не только для машин, но и для людей, поэтому стоит документировать не только то, что вы делали, но и почему. R дает для этого множество возможностей, главная из которых – это Markdown.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#о-воспроизводимости",
    "href": "share.html#о-воспроизводимости",
    "title": "6  Публикация",
    "section": "",
    "text": "На заметку\n\n\n\nВоспроизводимость (reproducibility) – это не то же, что повторяемость (replicability). Ученый, который повторяет исследование, проводит его заново на новых данных. Воспроизведение – гораздо более скромная задача, не требующая таких ресурсов, как повторение (Winter 2020, 47).\n\n\n\n\nдоступность данных и метаданных;\nдоступность компьютерного кода;\nдоступность программного обеспечения.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#markdown",
    "href": "share.html#markdown",
    "title": "6  Публикация",
    "section": "6.2 Markdown",
    "text": "6.2 Markdown\nMarkdown – это облегчённый язык разметки. Он позволяет создавать документы разного формата – не только HTML (веб-страницы), но и PDF и Word. Markdown дает возможность создания полностью воспроизводимых документов, сочетающих код и поясняющий текст. Этот язык используется для создания сайтов, статей, книг, презентаций, отчетов, дашбордов и т.п. Этот курс написан с использованием Markdown.\nЧтобы начать работать с документами .rmd, нужен пакет rmarkdown; в RStudio он уже предустановлен. Создание нового документа .rmd происходит из меню.\nПо умолчанию документ .rmd снабжен шапкой yaml. Она не обязательна. Здесь содержатся данные об авторе, времени создания, формате, сведения о файле с библиографией и т.п.\n---\ntitle: \"Demo\"\nauthor: \"My name\"\ndate: \"2024-08-29\"\noutput: html_document\n---\nТакже в документе .rmd скорее всего будет простой текст и блоки кода. Чтобы “сшить” html (pdf, doc), достаточно нажать кнопку knit либо запустить в консоли код: rmarkdown::render(\"Demo.Rmd\"). После этого в рабочей директории появится новый файл (html, pdf, или doc), которым можно поделиться с коллегами, грантодателями или друзьями.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#quarto",
    "href": "share.html#quarto",
    "title": "6  Публикация",
    "section": "6.3 Quarto",
    "text": "6.3 Quarto\nРаботать с маркдауном мы будем, используя издательскую систему Quarto с открытым исходным кодом. Она позволяет создавать и публиковать статьи, презентации, информационные панели, веб-сайты, блоги и книги в HTML, PDF, MS Word, ePub и других форматах. В общем, обычный Markdown тоже позволяет все это делать, но чуть сложнее. Quarto объединяет различные пакеты из экосистемы R Markdown воедино и значительно упрощает работу с ними.\n\n\n\n\n\n\nЗадание\n\n\n\nСоздайте новый .qmd документ. Потренируйтесь запускать код и сшивать документ в .html, .pdf, .docx.\n\n\nДля .pdf может понадобиться установка LaTeX.\n\n# install.packages(\"tinytex\")\ntinytex::install_tinytex()\n# to uninstall TinyTeX, run\n# tinytex::uninstall_tinytex()\n\nМожно указать сразу несколько форматов для файла, как показано здесь, и “сшить” их одновременно:\n\nquarto::quarto_render(\n  \"untitled.qmd\", \n  output_format = c(\"pdf\", \"html\", \"docx\")\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#шапка-yaml",
    "href": "share.html#шапка-yaml",
    "title": "6  Публикация",
    "section": "6.4 Шапка YAML",
    "text": "6.4 Шапка YAML\nОсновные параметры документа хранятся в YAML-шапке. К ним относятся format, title, subtitle, date, date-format, author, abstract, lang, toc, number-sections и другие.\nПопробуйте изменить шапку своего .qmd-документа и заново его сшить. Сравните с предыдущей версией.\n---\ntitle: \"Заголовок\"\nsubtitle: \"Подзаголовок\"\nformat: html\nauthor: locusclassicus\ndate: today\ndate-format: D.MM.YYYY\nabstract: Значенье бублика нам непонятно.\nlang: ru\ntoc: true\nnumber-sections: true\n---\n\nПоле execute позволяет задать параметры всех фрагментов кода в документе, например:\n---\nexecute:\n  echo: false\n  fig-width: 9\n---\n  \nНо для отдельных кусков кода эти настройки можно поменять:\n```\n#| echo: true\n\nsqrt(16)\n```\nПараметр df-print позволяет выбрать один из возможных способов отображения датафреймов:\n\ndefault — стандартный, как в консоли;\ntibble — стандартный, как в консоли, но в формате tibble;\nkable — минималистичный вариант, подходит для всех видов документов;\npaged — интерактивная таблица, подходит для html страниц.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#синтаксис-markdown",
    "href": "share.html#синтаксис-markdown",
    "title": "6  Публикация",
    "section": "6.5 Синтаксис Markdown",
    "text": "6.5 Синтаксис Markdown\n\n6.5.1 Заголовки\nЗаголовки разного уровня задаются при помощи решетки:\n# Заголовок первого уровня\n## Заголовок второго уровня\n### Заголовок третьего уровня\n#### Заголовок четвёртого уровня\nПример заголовка третьего уровня:\n\n\n6.5.2 Форматирование\n*курсив*  \n_курсив_\n\n**полужирный**  \n__полужирный__\n\n***полужирный курсив***  \n___полужирный курсив___\n\n~~зачеркнутый~~\n\n&lt;mark&gt;выделение&lt;/mark&gt;\nПример:\nкурсив\nполужирный\nуж и не знаю как выделить\nзачеркнутый\nвыделение\n\n\n6.5.3 Списки\nНумерованный список\n1. Пункт первый\n2. Пункт второй\n3. Пункт третий\nПример:\n\nПункт первый\nПункт второй\nПункт третий\n\nМаркированный список\n- Пункт первый\n- Пункт второй\n- Пункт третий\nПример:\n\nПункт первый\nПункт второй\nПункт третий\n\nТакже Markdown позволяет делать вложенные списки:\n1. Пункт первый\n    - Подпункт первый\n    - Подпункт второй\n2. Пункт второй\nПример:\n\nПункт первый\n\nПодпункт первый\nПодпункт второй\n\nПункт второй\n\nСамое удобное, что элементы списка не обязательно нумеровать:\n(@) Пункт первый.\n(@) Пункт не знаю какой.\n\nПункт первый.\nПункт не знаю какой.\n\n\n\n6.5.4 Ссылки\n[Текст ссылки](http://antibarbari.ru/)\nПример:\nТекст ссылки\n\n\n6.5.5 Изображения\n![Текст описания](http://antibarbari.ru/wp-content/uploads/2023/03/corderius-656x300.png)\nПример:\n\n\n\nМоя картинка\n\n\nДва нюанса:\n\nможно давать ссылки на локальные файлы (то есть такие файлы, которые хранятся на компьютере), но имейте в виду, что такой код не будет работать у другого пользователя;\nизображения можно вставлять, пользуясь непосредственно разметкой html.\n\n&lt;img src=\"images/my_image.jpg\" width=40%&gt;\n\n\n6.5.6 Блоки кода\nМожно вставлять непосредственно в текст; для этого код выделяют одинарным обратным апострофом (грависом). Но чаще код дают отдельным блоком. Эти блоки можно именовать; тогда в случае ошибки будет сразу понятно, где она случилась.\n```{}\nsome code here\n```\nВ фигурных скобках надо указать язык, например {r}, только в этом случае код будет подсвечиваться и выполняться.\nТам же в фигурных скобках можно задать следующие параметры:\n\neval = FALSE код будет показан, но не будет выполняться;\ninclude = FALSE код будет выполнен, но ни код, ни результат не будут показаны;\necho = FALSE код будет выполнен, но не показан, результаты при этом видны;\nmessage = FALSE или warning = FALSE прячет сообщения или предупреждения;\nresults = 'hide' не распечатывает результат, а fig.show = 'hide' прячет графики;\nerror = TRUE “сшивание” продолжается, даже если этот блок вернул ошибку.\n\n\n\n6.5.7 Цитаты\n&gt; Omnia praeclara rara.\nПример:\n\nOmnia praeclara rara.\n\nЦитата с подписью может быть оформлена так:\n&gt; Omnia praeclara rara.\n&gt;\n&gt; --- Cicero\nПример:\n\nOmnia praeclara rara.\n— Cicero\n\n\n\n6.5.8 Разделители\nЧтобы создать горизонтальную линию, можно использовать ---, *** или ___.\nПример:\n\n\n\n6.5.9 Таблицы\nТаблицы можно задать вручную при помощи дефисов - и вертикальных линий |; идеальная точность при этом не нужна. Перед таблицей обязательно оставляйте пустую строку, иначе волшебство не сработает.\n\n| Фрукты   | Калории  |\n| -----  | ---- |\n| Яблоко   | 52  |\n| Апельсин | 47  |\nПример:\n\n\n\nФрукты\nКалории\n\n\n\n\nЯблоко\n52\n\n\nАпельсин\n47\n\n\n\nПо умолчанию Markdown распечатывает таблицы так, как они бы выглядели в консоли.\n\ndata(\"iris\")\nhead(iris)\n\n\n  \n\n\n\nДля дополнительного форматирования можно использовать функцию knitr::kable():\n\nknitr::kable(iris[1:6, ], caption = \"Таблица knitr\")\n\n\nТаблица knitr\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\nИнтерактивную таблицу можно создать так:\n\nDT::datatable(iris[1:6,])\n\n\n\n\n\n\n\n6.5.10 Чек-листы\n- [x] Таблицы\n- [ ] Графики\nПример:\n\nТаблицы\nГрафики\n\n\n\n6.5.11 Внутренние ссылки\nУдобны для навигации по документу. К названию любого раздела можно добавить {#id}.\n[Вернуться к чек-листам](#id)\nПример:\nВернуться к чек-листам\n\n\n6.5.12 Графики\nMarkdown позволяет встраивать любые графики.\n\nlibrary(ggplot2)\nggplot(aes(x = Sepal.Length, y = Petal.Length, col = Species), data = iris) +\n  geom_point(show.legend = F)\n\n\n\n\n\n\n\n\nДля интерактивных графиков понадобится пакет plotly:\n\nlibrary(plotly)\nplot_ly(data=iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species)\n\n\n\n\n\nПодробное руководство по созданию интерактивных графиков можно найти на сайте https://plotly.com/r/.\n\n\n6.5.13 Математические формулы\nПишутся с использованием синтаксиса LaTeX, о котором можно прочитать подробнее здесь.\nФормулы заключаются в одинарный $, если пишутся в строку, и в двойной $$, если отдельным блоком.\n\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\nВот так это выглядит в тексте: \\(\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\).\nА вот так – блоком:\n\\[\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\]\n\n\n6.5.14 Смайлы\nУдобнее вставлять через визуальный редактор (“шестеренка” &gt; Use Visual Editor), но можно и без него:\n\n# devtools::install_github(\"hadley/emo\")\nlibrary(emo)\nemo::ji(\"apple\")\n\n🍎 \n\n\nКод можно записать в строку, тогда смайл появится в тексте: 💀.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#библиография",
    "href": "share.html#библиография",
    "title": "6  Публикация",
    "section": "6.6 Библиография",
    "text": "6.6 Библиография\nMarkdown позволяет добавлять библиографию в формате BibTeX. BibTeX — программное обеспечение для создания форматированных списков библиографии; обычно используется совместно с LaTeX’ом. Многие сайты, например GoogleScholar, позволяют экспортировать библиографические записи в формате BibTeX. При необходимости запись можно исправить вручную.\nКаждая запись имеет следующую форму.\n@book{winter2020,\n  author = {Bodo Winter},\n  title = \"{Statistics for Linguists: An Introduction Using R}\",\n  year = {2020},\n  publisher = {Routledge}\n}\nЗдесь book — тип записи («книга»), winter2020 — метка-идентификатор записи, дальше список полей со значениями.\nОдна запись описывает ровно одну публикацию статью, книгу, диссертацию, и т. д. Подробнее о типах записей можно посмотреть вот здесь.\nПодобные записи хранятся в текстовом файле с расширением .bib. Чтобы привязать библиографию, нужно указать имя файла в шапке yaml.\n---\nbibliography: bibliography.bib\n---\nДальше, чтобы добавить ссылку, достаточно ввести ключ публикации после @ (в квадратных скобках, чтобы публикация отражалась в круглых): [@wickham2016].\nПример:\n(Wickham и Grolemund 2016).\nМожно интегрировать BibTex с Zotero или другим менеджером библиографии. Для этого придется установить специальное расширение.\nЧтобы изменить стиль цитирования, необходимо добавить в шапку yaml название csl-файла (CSL - Citation Style Language), например:\n---\noutput: html_document\nbibliography: references.bib\ncsl: archiv-fur-geschichte-der-philosophie.csl\n---\nНайти необходимый csl-файл можно, например, в репозитории стилей Zotero.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "share.html#публикация-html",
    "href": "share.html#публикация-html",
    "title": "6  Публикация",
    "section": "6.7 Публикация html",
    "text": "6.7 Публикация html\nДля публикации на RPubs понадобится установить пакеты packrat, rsconnect.\n\n\n\n\nWickham, Hadley, и Garrett Grolemund. 2016. R for Data Science. O’Reilly. https://r4ds.had.co.nz/index.html.\n\n\nWinter, Bodo. 2020. Statistics for Linguists: An Introduction Using R. Routledge.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Публикация</span>"
    ]
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "7  Регулярные выражения",
    "section": "",
    "text": "7.1 Regex в базовом R\nВ базовом R за работу со строками отвечают, среди прочего, такие функции, как grep() и grepl(). При этом grepl() возвращает TRUE, если шаблон найден в соответствующей символьной строке, а grep() возвращает вектор индексов символьных строк, содержащих паттерн.\nОбеим функциям необходим аргумент pattern и аргумент x, где pattern - регулярное выражение, по которому производится поиск, а аргумент x - вектор символов, по которым следует искать совпадения.\nФункция gsub() позволяет производить замену и требует также аргумента replacement.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#литералы-и-классы",
    "href": "regex.html#литералы-и-классы",
    "title": "7  Регулярные выражения",
    "section": "7.2 Литералы и классы",
    "text": "7.2 Литералы и классы\nБуквальные символы – это то, что вы ожидаете увидеть (или не увидеть – для управляющих и пробельных символов); можно сказать, что это символы, которые ничего не “имеют в виду”. Их можно объединять в классы при помощи квадратных скобок, например, так: [abc].\n\nvec &lt;- c(\"a\", \"d\", \"c\")\ngrepl(\"[abc]\", vec)\n\n[1]  TRUE FALSE  TRUE\n\ngrep(\"[abc]\", vec)\n\n[1] 1 3\n\n\nДля некоторых классов есть специальные обозначения.\n\n\n\n\n\n\n\n\nКласс\nЭквивалент\nЗначение\n\n\n\n\n[:upper:]\n[A-Z]\nСимволы верхнего регистра\n\n\n[:lower:]\n[a-z]\nСимволы нижнего регистра\n\n\n[:alpha:]\n[[:upper:][:lower:]]\nБуквы\n\n\n[:digit:]\n[0-9], т. е. \\d\nЦифры\n\n\n[:alnum:]\n[[:alpha:][:digit:]]\nБуквы и цифры\n\n\n[:word:]\n[[:alnum:]_], т. е. \\w\nСимволы, образующие «слово»\n\n\n[:punct:]\n[-!“#$%&’()*+,./:;&lt;=&gt;?@[\\]_`{|}~]\nЗнаки пунктуации\n\n\n[:blank:]\n[\\s\\t]\nПробел и табуляция\n\n\n[:space:]\n[[:blank:]\\v\\r\\n\\f], т. е. \\s\nПробельные символы\n\n\n[:cntrl:]\n\nУправляющие символы (перевод строки, табуляция и т.п.)\n\n\n[:graph:]\n\nПечатные символы\n\n\n[:print:]\n\nПечатные символы с пробелом\n\n\n\nЭти классы тоже можно задавать в качестве паттерна.\n\nvec &lt;- c(\"жираф\", \"верблюд1\", \"0зебра\")\ngsub( \"[[:digit:]]\",  \"\", vec)\n\n[1] \"жираф\"   \"верблюд\" \"зебра\"  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nВ пакете stringr есть небольшой датасет words. Найдите все слова с последовательностью символов wh. Сколько слов содержат два гласных после w?\n\n\n\nВ качестве классов можно рассматривать и следующие обозначения:\n\n\n\n\n\n\n\n\nПредставление\nЭквивалент\nЗначение\n\n\n\n\n\\d\n[0-9]\nЦифра\n\n\n\\D\n[^\\\\d]\nЛюбой символ, кроме цифры\n\n\n\\w\n[A-Za-zА-Яа-я0-9_]\nСимволы, образующие «слово» (буквы, цифры и символ подчёркивания)\n\n\n\\W\n[^\\\\w]\nСимволы, не образующие «слово»\n\n\n\\s\n[ \\t\\v\\r\\n\\f]\nПробельный символ\n\n\n\\S\n[^\\\\s]\nНепробельный символ\n\n\n\n\ngsub( \"\\\\d\",  \"\", vec) # вторая косая черта \"экранирует\" первую\n\n[1] \"жираф\"   \"верблюд\" \"зебра\"  \n\n\nВнутри квадратных скобор знак ^ означает отрицание:\n\ngsub( \"[^[:digit:]]\",  \"\", vec) \n\n[1] \"\"  \"1\" \"0\"\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, в которых за w следует согласный. Замените всю пунктуацию в строке “tomorrow?and-tomorrow_and!tomorrow” на пробелы.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#якоря",
    "href": "regex.html#якоря",
    "title": "7  Регулярные выражения",
    "section": "7.3 Якоря",
    "text": "7.3 Якоря\nЯкоря позволяют искать последовательности символов в начале или в конце строки. Знак ^ (вне квадратных скобок!) означает начало строки, а знак $ – конец. Мнемоническое правило: First you get the power (^) and then you get the money ($).\n\nvec &lt;- c(\"The spring is a lovely time\", \n         \"Fall is a time of peace\")\ngrepl(\"time$\", vec)\n\n[1]  TRUE FALSE\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, которые заканчиваются на x. Найдите все слова, которые начинаются на b или на g.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#метасимволы",
    "href": "regex.html#метасимволы",
    "title": "7  Регулярные выражения",
    "section": "7.4 Метасимволы",
    "text": "7.4 Метасимволы\nВсе метасимволы представлены в таблице ниже.\n\n\n\nОписание\nСимвол\n\n\n\n\nоткрывающая квадратная скобка\n[\n\n\nзакрывающая квадратная скобка\n]\n\n\nобратная косая черта\n\\\n\n\nкарет\n^\n\n\nзнак доллара\n$\n\n\nточка\n.\n\n\nвертикальная черта\n|\n\n\nзнак вопроса\n?\n\n\nастериск\n*\n\n\nплюс\n+\n\n\nоткрывающая фигурная скобка\n{\n\n\nзакрывающая фигурная скобка\n}\n\n\nоткрывающая круглая скобка\n(\n\n\nзакрывающая круглая скобка\n)\n\n\n\nКвадратные скобки используются для создания классов, карет и знак доллара – это якоря, но карет внутри квадратных скобок может также быть отрицанием. Точка – это любой знак.\n\nvec &lt;- c(\"жираф\", \"верблюд1\", \"0зебра\")\ngrep(\".б\", vec) \n\n[1] 2 3\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите все слова в words, в которых есть любые два символа между b и k.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#экранирование",
    "href": "regex.html#экранирование",
    "title": "7  Регулярные выражения",
    "section": "7.5 Экранирование",
    "text": "7.5 Экранирование\nЕсли необходимо найти буквальную точку, буквальный знак вопроса и т.п., то используется экранирование: перед знаком ставится косая черта. Но так как сама косая черта – это метасимвол, но нужно две косые черты, первая из которых экранирует вторую.\n\nvec &lt;- c(\"жираф?\", \"верблюд.\", \"зебра\")\ngrep(\"\\\\?\", vec) \n\n[1] 1\n\ngrepl(\"\\\\.\", vec)\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nУзнайте, все ли предложения в sentences (входит в stringr) кончаются на точку.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#квантификация",
    "href": "regex.html#квантификация",
    "title": "7  Регулярные выражения",
    "section": "7.6 Квантификация",
    "text": "7.6 Квантификация\nКвантификатор после символа, символьного класса или группы определяет, сколько раз предшествующее выражение может встречаться. Квантификатор может относиться более чем к одному символу в регулярном выражении, только если это символьный класс или группа.\n\n\n\nПредставление\nЧисло повторений\nЭквивалент\n\n\n\n\n?\nНоль или одно\n{0,1}\n\n\n*\nНоль или более\n{0,}\n\n\n+\nОдно или более\n{1,}\n\n\n\nПример:\n\nvec &lt;- c(\"color\", \"colour\", \"colouur\")\ngrepl(\"ou?r\", vec) # ноль или одно \n\n[1]  TRUE  TRUE FALSE\n\ngrepl(\"ou+r\", vec) # одно или больше\n\n[1] FALSE  TRUE  TRUE\n\ngrepl(\"ou*r\", vec) # ноль или больше\n\n[1] TRUE TRUE TRUE\n\n\nТочное число повторений (интервал) можно задать в фигурных скобках:\n\n\n\nПредставление\nЧисло повторений\n\n\n\n\n{n}\nРовно n раз\n\n\n{m,n}\nОт m до n включительно\n\n\n{m,}\nНе менее m\n\n\n{,n}\nНе более n\n\n\n\n\nvec &lt;- c(\"color\", \"colour\", \"colouur\", \"colouuuur\")\ngrepl(\"ou{1}r\", vec)\n\n[1] FALSE  TRUE FALSE FALSE\n\ngrepl(\"ou{1,2}r\", vec)\n\n[1] FALSE  TRUE  TRUE FALSE\n\ngrepl(\"ou{,2}r\", vec) # это включает и ноль!\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nЧасто используется последовательность .* для обозначения любого количества любых символов между двумя частями регулярного выражения.\n\n\n\n\n\n\nЗадание\n\n\n\nУзнайте, в каких предложениях в sentences за пробелом следует ровно три согласных.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#жадная-и-ленивая-квантификация",
    "href": "regex.html#жадная-и-ленивая-квантификация",
    "title": "7  Регулярные выражения",
    "section": "7.7 Жадная и ленивая квантификация",
    "text": "7.7 Жадная и ленивая квантификация\nВ регулярных выражениях квантификаторам соответствует максимально длинная строка из возможных (квантификаторы являются жадными, англ. greedy). Это может оказаться значительной проблемой. Например, часто ожидают, что выражение &lt;.*&gt; найдёт в тексте теги HTML. Однако если в тексте есть более одного HTML-тега, то этому выражению соответствует целиком строка, содержащая множество тегов.\n\nvec &lt;- c(\"&lt;p&gt;&lt;b&gt;Википедия&lt;/b&gt; — свободная энциклопедия, в которой &lt;i&gt;каждый&lt;/i&gt; может изменить или дополнить любую статью.&lt;/p&gt;\")\ngsub(\"&lt;.*&gt;\", \"\", vec) # все исчезло!\n\n[1] \"\"\n\n\nЧтобы этого избежать, надо поставить после квантификатора знак вопроса. Это сделает его ленивым.\n\n\n\nregex\nзначение\n\n\n\n\n??\n0 или 1, лучше 0\n\n\n*?\n0 или больше, как можно меньше\n\n\n+?\n1 или больше, как можно меньше\n\n\n{n,m}?\nот n до m, как можно меньше\n\n\n\nПример:\n\ngsub(\"&lt;.*?&gt;\", \"\", vec) # все получилось!\n\n[1] \"Википедия — свободная энциклопедия, в которой каждый может изменить или дополнить любую статью.\"\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nДана строка “tomorrow (and) tomorrow (and) tomorrow”. Необходимо удалить первые скобки с их содержанием.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#regex-в-stringr-основы",
    "href": "regex.html#regex-в-stringr-основы",
    "title": "7  Регулярные выражения",
    "section": "7.8 Regex в stringr: основы",
    "text": "7.8 Regex в stringr: основы\nПакет stringr не является частью tidyverse, хотя и разделяет его принципы1. Его надо загружать отдельно:\n\nlibrary(stringr)\n\nЭто очень удобный инструмент для работы со строками. Вот так можно узнать длину строки или объединить ее с другими строками:\n\nvec &lt;- c(\"жираф\", \"верблюд\")\nstr_length(vec)\n\n[1] 5 7\n\nstr_c(\"красивый_\", vec)\n\n[1] \"красивый_жираф\"   \"красивый_верблюд\"\n\n\nЭлементы вектора можно объединить в одну строку:\n\nstr_c(vec, collapse = \", \") # теперь у них общие кавычки\n\n[1] \"жираф, верблюд\"\n\n\nС помощью str_sub() и str_sub_all() можно выбрать часть строки2.\n\nvec &lt;- c(\"жираф\", \"верблюд\")\nstr_sub(vec, 1, 3)\n\n[1] \"жир\" \"вер\"\n\nstr_sub(vec, 1, -2)\n\n[1] \"жира\"   \"верблю\"\n\n\nФункции ниже меняют начертание с прописного на строчное или наоборот:\n\nVEC &lt;- str_to_upper(vec)\nVEC\n\n[1] \"ЖИРАФ\"   \"ВЕРБЛЮД\"\n\nstr_to_lower(VEC)\n\n[1] \"жираф\"   \"верблюд\"\n\nstr_to_title(vec)\n\n[1] \"Жираф\"   \"Верблюд\"\n\n\nОдна из полезнейших функций в этом пакете – str_view(); она помогает увидеть, что поймало регулярное выражение – до того, как вы внесете какие-то изменения в строку.\n\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\n\n[2] │ &lt;a.c&gt;\n\n\nНапример, с помощью этой функции можно убедиться, что вертикальная черта выступает как логический оператор “или”:\n\nstr_view(c(\"grey\", \"gray\"), \"gr(e|a)y\")\n\n[1] │ &lt;grey&gt;\n[2] │ &lt;gray&gt;\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nСоздайте тиббл с двумя столбцами: letters и numbers (1:26). Преобразуйте, чтобы в третьем столбце появился результат соединения первых двух через подчеркивание, например a_1. Отфильтруйте, чтобы остались только ряды, где есть цифра 3 или буква x.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_detect-и-str_count",
    "href": "regex.html#str_detect-и-str_count",
    "title": "7  Регулярные выражения",
    "section": "7.9 str_detect() и str_count()",
    "text": "7.9 str_detect() и str_count()\nАналогом grepl() в stringr является функция str_detect()\n\nlibrary(rcorpora)\ndata(\"fruit\")\nhead(fruit)\n\n[1] \"apple\"       \"apricot\"     \"avocado\"     \"banana\"      \"bell pepper\"\n[6] \"bilberry\"   \n\nstr_detect(head(fruit), \"[aeiou]$\")\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n\n# какая доля слов заканчивается на гласный?\nmean(str_detect(fruit, \"[aeiou]$\"))\n\n[1] 0.35\n\n# сколько всего слов заканчивается на гласный?\nsum(str_detect(fruit, \"[aeiou]$\"))\n\n[1] 28\n\n\nОтрицание можно задать двумя способами:\n\ndata(\"words\")\n\nno_vowels1 &lt;- !str_detect(words, \"[aeiou]\") # слова без гласных\n\nno_vowels2 &lt;- str_detect(words, \"^[^aeiou]+$\") # слова без гласных\n\nsum(no_vowels1 != no_vowels2)\n\n[1] 0\n\n\nЛогический вектор можно использовать для индексирования:\n\nwords[!str_detect(words, \"[aeiou]\")]\n\n[1] \"by\"  \"dry\" \"fly\" \"mrs\" \"try\" \"why\"\n\n\nЭту функцию можно применять вместе с функцией filter() из пакета dplyr:\n\nlibrary(dplyr)\ngods &lt;- corpora(which = \"mythology/greek_gods\")\n\ndf &lt;- tibble(god = as.character(gods$greek_gods), \n             i = seq_along(god)\n             )\n\ndf |&gt; \n  filter(str_detect(god, \"s$\"))\n\n\n  \n\n\n\nВариацией этой функции является str_count():\n\nstr_count(as.character(gods$greek_gods), \"[Aa]\")\n\n [1] 1 1 1 1 2 0 0 1 1 1 0 1 0 0 1 2 1 0 0 0 0 1 2 1 0 2 3 2 1 0 0\n\n\nЭту функцию удобно использовать вместе с mutate() из dplyr:\n\ndf |&gt; \n  mutate(\n    vowels = str_count(god, \"[AEIOYaeiou]\"),\n    consonants = str_count(god, \"[^AEIOYaeiou]\")\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nПреобразуйте sentences из пакета stringr в тиббл; в новом столбце сохраните количество пробелов в каждом предложении.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_extract-str_subset-и-str_match",
    "href": "regex.html#str_extract-str_subset-и-str_match",
    "title": "7  Регулярные выражения",
    "section": "7.10 str_extract(), str_subset() и str_match()",
    "text": "7.10 str_extract(), str_subset() и str_match()\nФункция str_extract() извлекает совпадения3.\nСначала зададим паттерн для поиска.\n\ncolours &lt;- c(\" red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\")\ncolour_match &lt;- str_c(colours, collapse = \"|\")\ncolour_match\n\n[1] \" red|orange|yellow|green|blue|purple\"\n\n\nИ применим к предложениями. Используем str_extract_all(), т.к. str_extract() возвращает только первое вхождение.\n\nhas_colour &lt;- str_subset(sentences, colour_match)\nmatches &lt;- str_extract_all(has_colour, colour_match)\nhead(unlist(matches))\n\n[1] \"blue\"   \"blue\"   \"blue\"   \"yellow\" \"green\"  \" red\"  \n\n\nКруглые скобки используются для группировки. Например, мы можем задать шаблон для поиска существительного или прилагательного с артиклем.\n\nnoun &lt;- \"(a|the) ([^ ]+)\" # как минимум один непробельный символ после пробела\n\nhas_noun &lt;- sentences |&gt;\n  str_subset(noun) |&gt;\n  head(10)\nhas_noun\n\n [1] \"The birch canoe slid on the smooth planks.\"       \n [2] \"Glue the sheet to the dark blue background.\"      \n [3] \"It's easy to tell the depth of a well.\"           \n [4] \"These days a chicken leg is a rare dish.\"         \n [5] \"The box was thrown beside the parked truck.\"      \n [6] \"The boy was there when the sun rose.\"             \n [7] \"The source of the huge river is the clear spring.\"\n [8] \"Kick the ball straight and follow through.\"       \n [9] \"Help the woman get back to her feet.\"             \n[10] \"A pot of tea helps to pass the evening.\"          \n\n\nДальше можно воспользоваться уже известной функцией str_extract() или применить str_match. Результат будет немного отличаться: вторая функция вернет матрицу, в которой хранится не только сочетание слов, но и каждый компонент отдельно.\n\nhas_noun |&gt; \n  str_extract(noun)\n\n [1] \"the smooth\" \"the sheet\"  \"the depth\"  \"a chicken\"  \"the parked\"\n [6] \"the sun\"    \"the huge\"   \"the ball\"   \"the woman\"  \"a helps\"   \n\nhas_noun |&gt; \n  str_match(noun)\n\n      [,1]         [,2]  [,3]     \n [1,] \"the smooth\" \"the\" \"smooth\" \n [2,] \"the sheet\"  \"the\" \"sheet\"  \n [3,] \"the depth\"  \"the\" \"depth\"  \n [4,] \"a chicken\"  \"a\"   \"chicken\"\n [5,] \"the parked\" \"the\" \"parked\" \n [6,] \"the sun\"    \"the\" \"sun\"    \n [7,] \"the huge\"   \"the\" \"huge\"   \n [8,] \"the ball\"   \"the\" \"ball\"   \n [9,] \"the woman\"  \"the\" \"woman\"  \n[10,] \"a helps\"    \"a\"   \"helps\"  \n\n\nФункция tidyr::extract работает похожим образом, но требует дать имена для каждого элемента группы. Этим удобно пользоваться, если ваши данные хранятся в виде тиббла.\n\ntibble(sentence = sentences) |&gt; \n  tidyr::extract(\n    sentence, c(\"article\", \"noun\"), \"(a|the) ([^ ]+)\", \n    remove = FALSE\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nНайдите в sentences все предложения, где есть to, и выберите следующее за этим слово. Переведите в нижний регистр. Узнайте, сколько всего уникальных сочетаний.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_replace",
    "href": "regex.html#str_replace",
    "title": "7  Регулярные выражения",
    "section": "7.11 str_replace",
    "text": "7.11 str_replace\nФункции str_replace() и str_replace_all() позволяют заменять совпадения на новые символы.\n\nx &lt;- c(\"apple\", \"pear\", \"banana\")\nstr_replace(x, \"[aeiou]\", \"-\")\n\n[1] \"-pple\"  \"p-ar\"   \"b-nana\"\n\nstr_replace_all(x, \"[aeiou]\", \"-\")\n\n[1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\n\n\nЭтим можно воспользоваться, если вы хотите, например, удалить из текста все греческие символы. Для стандартного греческого алфавита хватит [Α-Ωα-ω], но для древнегреческого этого, например, не хватит. Попробуем на отрывке из письма Цицерона Аттику, которое содержит греческий текст.\n\ncicero &lt;- \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀπεραντολογίας ἀηδοῦς! \"\n\nstr_replace_all(cicero, \"[Α-Ωα-ω]\", \"\")\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀί ἀῦ! \"\n\n\nὢ ἀί ἀῦ! Не все у нас получилось гладко. Попробуем иначе:\n\nstr_replace_all(cicero, \"[\\u0370-\\u03FF]\", \"\")\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀ ἀῦ! \"\n\n\nУдалилась (буквально была заменена на пустое место) та диакритика, которая есть в новогреческом (ί). Но остались еще буквы со сложной диакритикой, которой современные греки не пользуются.\n\nno_greek &lt;- str_replace_all(cicero, \"[[\\u0370-\\u03FF][\\U1F00-\\U1FFF]]\", \"\")\nno_greek\n\n[1] \"nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius.   ! \"\n\n\n! Мы молодцы. Избавились от этого непонятного греческого.\nНа самом деле, конечно, str_replace хорош тем, что он позволяет производить осмысленные замены. Например, мы можем в оставшемся латинском текст заменить гласные с макроном (черточка, означающая долготу) на обычные гласные.\n\nstr_replace_all(no_greek, c(\"ā\" = \"a\", \"ū\" = \"u\", \"ī\" = \"i\", \"ō\" = \"o\"))\n\n[1] \"nihil hac solitudine iucundius, nisi paulum interpellasset Amyntae filius.   ! \"\n\n\nКрасота. О более сложных заменах с перемещением групп можно посмотреть видео здесь и здесь. Это помогает даже в таком скорбном деле, как переоформление библиографии.\n\n\n\n\n\n\nЗадание\n\n\n\nДана библиографическая запись:\nAst, Friedrich. 1816. Platon’s Leben und Schriften. Leipzig, Weidmann.\nИспользуя регулярные выражения, замените полное имя на инициал. Запятую перед инициалом удалите. Уберите название издательства. Год поставьте в круглые скобки.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#str_split",
    "href": "regex.html#str_split",
    "title": "7  Регулярные выражения",
    "section": "7.12 str_split",
    "text": "7.12 str_split\nФункция str_split() помогает разбить текст на предложения, слова или просто на бессмысленные наборы символов. Это важный этап подготовки текста для анализа, и проводится он нередко именно с применением регулярных выражений.\n\nsentences |&gt;\n  head(2) |&gt; \n  str_split(\" \")\n\n[[1]]\n[1] \"The\"     \"birch\"   \"canoe\"   \"slid\"    \"on\"      \"the\"     \"smooth\" \n[8] \"planks.\"\n\n[[2]]\n[1] \"Glue\"        \"the\"         \"sheet\"       \"to\"          \"the\"        \n[6] \"dark\"        \"blue\"        \"background.\"\n\n\nНо можно обойтись и без регулярных выражений.\n\nx &lt;- \"This is a sentence.  This is another sentence.\"\nstr_view_all(x, boundary(\"word\"))\n\n[1] │ &lt;This&gt; &lt;is&gt; &lt;a&gt; &lt;sentence&gt;.  &lt;This&gt; &lt;is&gt; &lt;another&gt; &lt;sentence&gt;.\n\nstr_view_all(x, boundary(\"sentence\"))\n\n[1] │ &lt;This is a sentence.  &gt;&lt;This is another sentence.&gt;\n\n\nОчень удобно, но убедитесь, что в вашем языке границы слов и предложения выглядят как у людей. С древнегреческим эта штука не справится (как делить на предложения греческие и латинские тексты, я рассказывала здесь):\n\napology &lt;- c(\"νῦν δ' ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ' ὅς. “Τίς,” ἦν δ' ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;\")\n\nstr_view_all(apology, boundary(\"sentence\"))\n\n[1] │ &lt;νῦν δ' ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ' ὅς. &gt;&lt;“Τίς,” ἦν δ' ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&gt;\n\n\nПолный крах 💩",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "regex.html#footnotes",
    "href": "regex.html#footnotes",
    "title": "7  Регулярные выражения",
    "section": "",
    "text": "https://r4ds.had.co.nz/strings.html↩︎\nhttps://stringr.tidyverse.org/reference/str_sub.html↩︎\nhttps://r4ds.had.co.nz/strings.html#extract-matches↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Регулярные выражения</span>"
    ]
  },
  {
    "objectID": "scrape.html",
    "href": "scrape.html",
    "title": "8  Веб-скрапинг",
    "section": "",
    "text": "8.1 Структура html\nДокументы html (HyperText Markup Language) имеют ирархическую структуру, состоящую из элементов. В каждом элементе есть открывающий тег (&lt;tag&gt;), опциональные атрибуты (id='first') и закрывающий тег (&lt;/tag&gt;). Все, что находится между открывающим и закрывающим тегом, называется содержанием элемента.\nВажнейшие теги, о которых стоит знать:\nЧтобы увидеть структуру веб-страницы, надо нажать правую кнопку мыши и выбрать View Source (это работает и для тех html, которые хранятся у вас на компьютере).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#структура-html",
    "href": "scrape.html#структура-html",
    "title": "8  Веб-скрапинг",
    "section": "",
    "text": "&lt;html&gt; (есть всегда), с двумя детьми (дочерними элементами): &lt;head&gt; и &lt;body&gt;;\nэлементы, отвечающие за структуру: &lt;h1&gt; (заголовок), &lt;section&gt;, &lt;p&gt; (параграф), &lt;ol&gt; (упорядоченный список);\nэлементы, отвечающие за оформление: &lt;b&gt; (bold), &lt;i&gt; (italics), &lt;a&gt; (ссылка).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#каскадные-таблицы-стилей",
    "href": "scrape.html#каскадные-таблицы-стилей",
    "title": "8  Веб-скрапинг",
    "section": "8.2 Каскадные таблицы стилей",
    "text": "8.2 Каскадные таблицы стилей\nУ тегов могут быть именованные атрибуты; важнейшие из них – это id и class, которые в сочетании с CSS контролируют внешний вид страницы.\n\n\n\n\n\n\nНа заметку\n\n\n\nCSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML).\n\n\nПример css-правила (такие инфобоксы использованы в предыдущей версии курса):\n\n.infobox {\n  padding: 1em 1em 1em 4em;\n  background: aliceblue 5px center/3em no-repeat;\n  color: black;\n}\n\n\nПроще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — селектор и блок объявлений. Селектор, расположенный в левой части правила до знака {, определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;».\nСелекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так:\n\np выберет все элементы &lt;p&gt;\n.title выберет элементы с классом “title”\n#title выберет все элементы с атрибутом id=‘title’\n\nВажно: если изменится структура страницы, откуда вы скрапили информацию, то и код придется переписывать.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#чтение-html",
    "href": "scrape.html#чтение-html",
    "title": "8  Веб-скрапинг",
    "section": "8.3 Чтение html",
    "text": "8.3 Чтение html\nЧтобы прочесть файл html, используем одноименную функцию.\n\nlibrary(rvest)\nantibarbari_files &lt;- list.files(\"../files/antibarbari_2024-08-18\", pattern = \"html\", full.names = TRUE)\n\nИспользуем пакет purrr, чтобы прочитать сразу три файла из архива.\n\nlibrary(tidyverse)\nantibarbari_archive &lt;- map(antibarbari_files, read_html)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#парсинг-html-отдельные-элементы",
    "href": "scrape.html#парсинг-html-отдельные-элементы",
    "title": "8  Веб-скрапинг",
    "section": "8.4 Парсинг html: отдельные элементы",
    "text": "8.4 Парсинг html: отдельные элементы\nНа следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельный html; rvest позволяет это сделать (но внутри двойных кавычек должны быть только одинарные):\n\nexample_html &lt;-  minimal_html(\"\n&lt;div class='message default clearfix' id='message83'&gt;\n      &lt;div class='pull_left userpic_wrap'&gt;\n       &lt;div class='userpic userpic2' style='width: 42px; height: 42px'&gt;\n        &lt;div class='initials' style='line-height: 42px'&gt;\nA\n        &lt;/div&gt;\n       &lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div class='body'&gt;\n       &lt;div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'&gt;\n11:18\n       &lt;/div&gt;\n       &lt;div class='from_name'&gt;\nAntibarbari HSE \n       &lt;/div&gt;\n       &lt;div class='text'&gt;\nЭтот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. &lt;br&gt;&lt;br&gt;«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).&lt;br&gt;&lt;a href='http://antibarbari.ru/2022/05/19/digest_1/'&gt;http://antibarbari.ru/2022/05/19/digest_1/&lt;/a&gt;\n       &lt;/div&gt;\n       &lt;div class='signature details'&gt;\nOlga Alieva\n       &lt;/div&gt;\n      &lt;/div&gt;\n     &lt;/div&gt;\n\")\n\nИз всего этого мне может быть интересно id сообщения (\\&lt;div class='message default clearfix' id='message83'\\&gt;), текст сообщения (\\&lt;div class='text'\\&gt;), дата публикации (\\&lt;div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'\\&gt;), а также, если указан, автор сообщения (\\&lt;div class='signature details'\\&gt;). Извлекаем текст (для этого рекомендуется использовать функцию html_text2()):\n\nexample_html |&gt;\n  html_element(\".text\") |&gt; \n  html_text2()\n\n[1] \"Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора.\\n\\n«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).\\nhttp://antibarbari.ru/2022/05/19/digest_1/\"\n\n\nВ классе signature details есть пробел, достаточно на его месте поставить точку:\n\nexample_html |&gt;\n  html_element(\".signature.details\") |&gt; \n  html_text2()\n\n[1] \"Olga Alieva\"\n\n\nОсталось добыть дату и message id:\n\nexample_html |&gt; \n  html_element(\".pull_right.date.details\") |&gt; \n  html_attr(\"title\")\n\n[1] \"19.05.2022 11:18:07 UTC+03:00\"\n\n\n\nexample_html |&gt;\n  html_element(\"div\") |&gt; \n  html_attr(\"id\")\n\n[1] \"message83\"\n\n\nТеперь мы можем сохранить все нужные нам данные в таблицу.\n\ntibble(id = example_html |&gt; \n         html_element(\"div\") |&gt; \n         html_attr(\"id\"),\n       date = example_html |&gt; \n         html_element(\".pull_right.date.details\") |&gt; \n         html_attr(\"title\"),\n       signature = example_html |&gt;\n         html_element(\".signature.details\") |&gt; \n         html_text2(),\n       text = example_html |&gt; \n         html_element(\".text\") |&gt;\n         html_text2()\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#парсинг-html-вложенные-элементы",
    "href": "scrape.html#парсинг-html-вложенные-элементы",
    "title": "8  Веб-скрапинг",
    "section": "8.5 Парсинг html: вложенные элементы",
    "text": "8.5 Парсинг html: вложенные элементы\nДо сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, и нам надо извлечь только такие div, которым соответствует определенный класс. Также не будем забывать, что архив выгрузился в виде трех html-файлов, так что понадобится наше знание итераций в purrr. Пока пробуем на одном из них:\n\narchive_1 &lt;- antibarbari_archive[[1]]\n\narchive_1 |&gt;\n  html_elements(\"div.message.default\") |&gt; \n  head()\n\n{xml_nodeset (6)}\n[1] &lt;div class=\"message default clearfix\" id=\"message3\"&gt;\\n\\n      &lt;div class= ...\n[2] &lt;div class=\"message default clearfix\" id=\"message5\"&gt;\\n\\n      &lt;div class= ...\n[3] &lt;div class=\"message default clearfix\" id=\"message6\"&gt;\\n\\n      &lt;div class= ...\n[4] &lt;div class=\"message default clearfix\" id=\"message7\"&gt;\\n\\n      &lt;div class= ...\n[5] &lt;div class=\"message default clearfix\" id=\"message8\"&gt;\\n\\n      &lt;div class= ...\n[6] &lt;div class=\"message default clearfix\" id=\"message9\"&gt;\\n\\n      &lt;div class= ...\n\n\nУже из этого набора узлов можем доставать все остальное.\n\narchive_1_tbl &lt;- tibble(id = archive_1 |&gt; \n         html_elements(\"div.message.default\") |&gt; \n         html_attr(\"id\"),\n       date = archive_1 |&gt; \n         html_elements(\"div.message.default\") |&gt; \n         html_element(\".pull_right.date.details\") |&gt; \n         html_attr(\"title\"),\n       signature = archive_1 |&gt;\n         html_elements(\"div.message.default\") |&gt; \n         html_element(\".signature.details\") |&gt; \n         html_text2(),\n       text = archive_1 |&gt; \n         html_elements(\"div.message.default\") |&gt; \n         html_element(\".text\") |&gt;\n         html_text2()\n)\n\narchive_1_tbl\n\n\n  \n\n\n\nОбратите внимание, что мы сначала извлекаем нужные элементы при помощи html_elements(), а потом применяем к каждому из них html_element(). Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений, т.к. функция html_element(), если она не может найти, например, подпись, возвращает NA.\nКак вы уже поняли, теперь нам надо проделать то же самое для двух других файлов из архива антиварваров, а значит пришло время превратить наш код в функцию.\n\nscrape_antibarbari &lt;- function(html_file){\n  messages_tbl &lt;- tibble(\n    id = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_attr(\"id\"),\n    date = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".pull_right.date.details\") |&gt;\n      html_attr(\"title\"),\n    signature = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".signature.details\") |&gt;\n      html_text2(),\n    text = html_file |&gt;\n      html_elements(\"div.message.default\") |&gt;\n      html_element(\".text\") |&gt;\n      html_text2()\n  )\n  messages_tbl\n}\n\n\nmessages_tbl &lt;- map_df(antibarbari_archive, scrape_antibarbari)\n\nВот что у нас получилось.\n\nmessages_tbl",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#разведывательный-анализ",
    "href": "scrape.html#разведывательный-анализ",
    "title": "8  Веб-скрапинг",
    "section": "8.6 Разведывательный анализ",
    "text": "8.6 Разведывательный анализ\nСоздатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. Кроме того, в некоторых постах только фото, для них в столбце text – NA, их можно сразу отсеять.\n\nmessages_tbl &lt;- messages_tbl |&gt;\n  filter(!is.na(text))\n\nmessages_tbl\n\n\n  \n\n\n\nТакже преобразуем столбец, в котором хранится дата и время. Разделим его на два и выясним, в какое время и день недели чаще всего публикуются сообщения.\n\n\n\n\n\n\nЗадание\n\n\n\nИз курса Getting and Cleaning Data в swirl будет полезно пройти урок Dates and Times with lubridate.\n\n\n\nmessages_tbl2 &lt;- messages_tbl |&gt; \n  separate(date, into = c(\"date\", \"time\", NA), sep = \" \") |&gt; \n  mutate(date = dmy(date), \n         time = hms(time)) |&gt; \n  mutate(year = year(date), \n        month = month(date, label = TRUE),\n        wday = wday(date, label = TRUE),\n        hour = hour(time),\n        length = str_count(text, \" \") + 1) |&gt; \n  mutate(wday = factor(wday, levels = c(\"Sun\", \"Sat\", \"Fri\", \"Thu\", \"Wed\", \"Tue\", \"Mon\")))\n\n\nmessages_tbl2\n\n\n  \n\n\n\n\nsummary1 &lt;- messages_tbl2 |&gt; \n  group_by(year, month) |&gt; \n  summarise(n = n()) \n\nsummary1\n\n\n  \n\n\nsummary2 &lt;- messages_tbl2 |&gt; \n  group_by(year, hour) |&gt; \n  summarise(n = n()) |&gt; \n  mutate(hour = case_when(hour == 0 ~ 24,\n                          .default = hour))\n\nsummary2\n\n\n  \n\n\nsummary3 &lt;- messages_tbl2 |&gt; \n   group_by(wday) |&gt; \n   summarise(n = n())\n\nsummary3\n\n\n  \n\n\n\n\nlibrary(gridExtra)\nlibrary(grid)\n\np1 &lt;- summary1 |&gt; \n  ggplot(aes(month, n, color = as.factor(year), group = year)) +\n  geom_line(show.legend = FALSE, linewidth = 1.2, alpha = 0.8) +\n  labs(title = \"Число постов в месяц\") +\n  theme(legend.title = element_blank(), \n        legend.position = c(0.8, 0.3),\n        title = element_text(face=\"italic\")) +\n  labs(x = NULL, y = NULL) +\n  scale_color_viridis_d()\n\n\np2 &lt;- summary2 |&gt; \n  ggplot(aes(hour, n, color = as.factor(year), group = year)) + \n  geom_line(linewidth = 1.2, alpha = 0.8) +\n  scale_x_continuous(breaks = seq(1,24,1)) +\n  labs(x = NULL, y = NULL, title = \"Время публикации поста\") + \n  theme(legend.title = element_blank(), \n        legend.position = \"left\",\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        title = element_text(face=\"italic\")\n        ) +\n  coord_polar(start = 0) +\n  scale_color_viridis_d()\n\n\np3 &lt;- summary3 |&gt; \n  ggplot(aes(wday, n, fill = wday)) + \n  geom_bar(stat = \"identity\", \n           show.legend = FALSE) + \n  coord_flip() + \n  labs(x = NULL, y = NULL, title  = \"Публикации по дням недели\") +\n  theme(title = element_text(face=\"italic\"))\n\n\np4 &lt;- messages_tbl2 |&gt; \n  ggplot(aes(as.factor(year), length, fill = as.factor(year))) +\n  geom_boxplot(show.legend = FALSE) +\n  labs(title = \"Длина поста по годам\") + \n  labs(x = NULL, y = NULL) + \n  scale_fill_viridis_d() + \n  theme(title = element_text(face=\"italic\"))\n\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2,\n             top =  textGrob(\"Телеграм-канал Antibarbari HSE\",\n                    gp=gpar(fontsize=16)),\n             bottom = textGrob(\"@Rantiquity\",\n                    gp = gpar(fontface = 3, fontsize = 9), hjust = 1, x = 1))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#html-таблицы",
    "href": "scrape.html#html-таблицы",
    "title": "8  Веб-скрапинг",
    "section": "8.7 Html таблицы",
    "text": "8.7 Html таблицы\nЕсли вам повезет, то ваши данные уже будут храниться в HTML-таблице, и их можно будет просто считать из этой таблицы. Распознать таблицу в браузере обычно несложно: она имеет прямоугольную структуру из строк и столбцов, и ее можно скопировать и вставить в такой инструмент, как Excel.\nТаблицы HTML строятся из четырех основных элементов: &lt;table&gt;, &lt;tr&gt; (строка таблицы), &lt;th&gt; (заголовок таблицы) и &lt;td&gt; (данные таблицы). Мы достанем программу курса “Количественные методы в гуманитарных науках: критическое введение” (2023/2024).\n\nhtml &lt;- read_html(\"http://criticaldh.ru/program/\")\nmy_table &lt;- html |&gt;  \n  html_table() |&gt; \n  pluck(1)\n\nmy_table\n\n\n  \n\n\n\n\n\n\n\n\n\nЗадание\n\n\n\nС сайта Новой философской энциклопедии извлеките список слов на букву П. Используйте map_df() для объединения таблиц.\n\n\n\n\n\n\n\n\nВопрос\n\n\n\nСколько всего слов на букву П в НФЭ?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "scrape.html#selector-gadget",
    "href": "scrape.html#selector-gadget",
    "title": "8  Веб-скрапинг",
    "section": "8.8 Selector Gadget",
    "text": "8.8 Selector Gadget\nМногие тексты доступны на сайте &lt;wikisource.org&gt;. Попробуем извлечь латинский текст “Записок о Галльской войне” Цезаря: он пригодится нам в следующем уроке.\n\nurl &lt;- \"https://la.wikisource.org/wiki/Commentarii_de_bello_Gallico\"\nhtml = read_html(url)\n\nДля того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот тут можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни.\n\ntoc &lt;- html |&gt; \n  html_elements(\"td , #toc a\")\n\ntoc\n\n{xml_nodeset (11)}\n [1] &lt;td class=\"fr-text\" style=\"vertical-align: middle;\"&gt;Accuracy&lt;/td&gt;\\n\n [2] &lt;td class=\"fr-value40\" style=\"vertical-align: middle;\"&gt;Spot checked&lt;/td&gt;\n [3] &lt;td align=\"center\" style=\"background: #efefef\"&gt;\\n&lt;a href=\"/wiki/Commenta ...\n [4] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_I\" title=\"Commentarii  ...\n [5] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_II\" title=\"Commentarii ...\n [6] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_III\" title=\"Commentari ...\n [7] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_IV\" title=\"Commentarii ...\n [8] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_V\" title=\"Commentarii  ...\n [9] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VI\" title=\"Commentarii ...\n[10] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VII\" title=\"Commentari ...\n[11] &lt;a href=\"/wiki/Commentarii_de_bello_Gallico/Liber_VIII\" title=\"Commentar ...\n\n\nИзвлекаем путь и имя файла для web-страниц.\n\nlibri &lt;- tibble(\n  title = toc |&gt;\n    html_attr(\"title\"),\n  href = toc |&gt; \n    html_attr(\"href\")\n) |&gt; \n  filter(!is.na(title))\n\nlibri\n\n\n  \n\n\n\nТеперь добавляем протокол доступа и доменное имя для каждой страницы.\n\nlibri &lt;- libri |&gt; \n  mutate(link = paste0(\"https://la.wikisource.org\", href)) |&gt; \n  select(-href)\n\nlibri\n\n\n  \n\n\n\nДальше необходимо достать текст для каждой книги. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила.\n\nurls &lt;- libri |&gt; \n  pull(link)\n\ntext &lt;- read_html(urls[1]) |&gt; \n  html_elements(\".mw-heading3+ p\") |&gt; \n  html_text2() \n\ntext[1]\n\n[1] \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt. Gallos ab Aquitanis Garumna flumen, a Belgis Matrona et Sequana dividit. Horum omnium fortissimi sunt Belgae, propterea quod a cultu atque humanitate provinciae longissime absunt, minimeque ad eos mercatores saepe commeant atque ea quae ad effeminandos animos pertinent important, proximique sunt Germanis, qui trans Rhenum incolunt, quibuscum continenter bellum gerunt. Qua de causa Helvetii quoque reliquos Gallos virtute praecedunt, quod fere cotidianis proeliis cum Germanis contendunt, cum aut suis finibus eos prohibent aut ipsi in eorum finibus bellum gerunt. Eorum una pars, quam Gallos obtinere dictum est, initium capit a flumine Rhodano, continetur Garumna flumine, Oceano, finibus Belgarum, attingit etiam ab Sequanis et Helvetiis flumen Rhenum, vergit ad septentriones. Belgae ab extremis Galliae finibus oriuntur, pertinent ad inferiorem partem fluminis Rheni, spectant in septentrionem et orientem solem. Aquitania a Garumna flumine ad Pyrenaeos montes et eam partem Oceani quae est ad Hispaniam pertinet; spectat inter occasum solis et septentriones.\"\n\n\nУбедившись, что параграфы извлечены верно, обобщаем: пишем функцию для извлечения текстов и применяем ее ко всем книгам.\n\nget_text &lt;- function(url) {\n  read_html(url) |&gt; \n  html_elements(\".mw-heading3+ p\") |&gt; \n  html_text2() |&gt; \n  paste(collapse= \" \")\n}\n\nЭто займет некоторое время.\n\nlibri_text &lt;- map(urls, get_text)\n\nСоединим две таблицы.\n\nlibri_text &lt;- libri_text |&gt;\n  flatten_chr() |&gt; \n  as_tibble()\n\ncaesar &lt;- libri |&gt; \n  bind_cols(libri_text) |&gt; \n  mutate(title = str_remove(title, \"Commentarii de bello Gallico/\"))\n\ncaesar\n\n\n  \n\n\n\nСохраним подготовленный датасет для дальнейшего анализа.\n\nsave(caesar, file = \"../data/caesar.Rdata\")\n\n\nПоздравляем, на этом закончился первый большой раздел нашего курса “Основы работы в R” 🎃. За восемь уроков вы познакомились с основными структурами данных в R, научились собирать и трансформировать данные, строить графики, писать функции и циклы, а также готовить html-отчеты о своих исследованиях. Впереди нас ждут методы анализа текстовых данных.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Веб-скрапинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html",
    "href": "tokenize.html",
    "title": "9  Синтаксический парсинг",
    "section": "",
    "text": "9.1 Токенизация\nТокенизация — процесс разделения текста на составляющие (их называют «токенами»). Токенами могут быть слова, символьные или словесные энграмы (n-grams), то есть сочетания символов или слов, даже предложения или параграфы.\nТокенизировать можно в базовом R с использованием регулярных выражений, и Jockers (2014) прекрасно показывает, как это можно делать. Но мы воспользуемся двумя пакетами, которые предназначены специально для работы с текстовыми данными и разделяют идеологию tidyverse: tidytext (Silge и Robinson 2017) и tokenizers (Hvitfeldt и Silge 2022).\nlibrary(tidyverse) \nlibrary(tidytext)\nlibrary(tokenizers)\nДля анализа воспользуемся датасетом c латинским текстом “Записок о Галльской войне”, который мы подготовили в предыдущем уроке. Его можно забрать отсюда.\nload(\"../data/caesar.RData\")\ncaesar &lt;- caesar |&gt; \n  rename(text = value) |&gt; \n  select(-link)\n\ncaesar\nФункция unnest_tokens() из пакета tidytext принимает на входе тиббл, название столбца, в котором хранится текст для токенизации, а также название нового столбца, куда будут “сложены” отдельные токены (зачастую это слова, но не обязательно).\nАргумент token принимает следующие значения:\nИспользуя уже знакомую функцию map, можно запустить unnest_tokens() с разными аргументами:\ntest &lt;- tibble(text = \"Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur. Hi omnes lingua, institutis, legibus inter se differunt.\")\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token,\n  test, \"word\", \"text\", \"words\", \n  test, \"sentence\", \"text\", \"sentences\",\n  test, \"char\", \"text\", \"characters\", \n)\n\nparams\nparams |&gt; \n  pmap(unnest_tokens) \n\n[[1]]\n# A tibble: 29 × 1\n   word    \n   &lt;chr&gt;   \n 1 gallia  \n 2 est     \n 3 omnis   \n 4 divisa  \n 5 in      \n 6 partes  \n 7 tres    \n 8 quarum  \n 9 unam    \n10 incolunt\n# ℹ 19 more rows\n\n[[2]]\n# A tibble: 2 × 1\n  sentence                                                                      \n  &lt;chr&gt;                                                                         \n1 gallia est omnis divisa in partes tres, quarum unam incolunt belgae, aliam aq…\n2 hi omnes lingua, institutis, legibus inter se differunt.                      \n\n[[3]]\n# A tibble: 166 × 1\n   char \n   &lt;chr&gt;\n 1 g    \n 2 a    \n 3 l    \n 4 l    \n 5 i    \n 6 a    \n 7 e    \n 8 s    \n 9 t    \n10 o    \n# ℹ 156 more rows\nСледующие значения аргумента token требуют также аргумента n:\nparams &lt;- tribble(\n  ~tbl, ~output, ~input, ~token, ~n,\n  test, \"ngram\", \"text\", \"ngrams\", 3,\n  test, \"shingles\", \"text\", \"character_shingles\", 3\n)\n\nparams  |&gt; \n  pmap(unnest_tokens)  |&gt; \n  head()\n\n[[1]]\n# A tibble: 27 × 1\n   ngram                \n   &lt;chr&gt;                \n 1 gallia est omnis     \n 2 est omnis divisa     \n 3 omnis divisa in      \n 4 divisa in partes     \n 5 in partes tres       \n 6 partes tres quarum   \n 7 tres quarum unam     \n 8 quarum unam incolunt \n 9 unam incolunt belgae \n10 incolunt belgae aliam\n# ℹ 17 more rows\n\n[[2]]\n# A tibble: 164 × 1\n   shingles\n   &lt;chr&gt;   \n 1 gal     \n 2 all     \n 3 lli     \n 4 lia     \n 5 iae     \n 6 aes     \n 7 est     \n 8 sto     \n 9 tom     \n10 omn     \n# ℹ 154 more rows\nДальше мы будем работать со словами, поэтому сохраним токенизированный текст “Записок” в виде “опрятного” датасета (одно наблюдение - один ряд).\ncaesar_tokens &lt;- caesar |&gt; \n  unnest_tokens(\"word\", \"text\")\n\ncaesar_tokens\nПри работе с данными в текстовом формате unnest_tokens() опирается на пакет tokenizers, из которого в нашем случае подтягивает функцию tokenize_words. У этой функции есть несколько полезных аргументов: strip_non_alphanum (удаляет пробельные символы и пунктуацию), strip_punct (удаляет пунктуацию), strip_numeric (удаляет числа).\nЭти аргументы мы тоже можем задать через unnest_tokens(), поскольку у функции есть аргумент ... (загляните в документацию, чтобы убедиться).\ncaesar |&gt; \n  unnest_tokens(\"word\", \"text\", strip_punct = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#токенизация",
    "href": "tokenize.html#токенизация",
    "title": "9  Синтаксический парсинг",
    "section": "",
    "text": "unnest_tokens(\n  tbl,\n  output,\n  input,\n  token = \"words\",\n  format = c(\"text\", \"man\", \"latex\", \"html\", \"xml\"),\n  to_lower = TRUE,\n  drop = TRUE,\n  collapse = NULL,\n  ...\n)\n\n\n“words” (default),\n“characters”,\n“character_shingles”,\n“ngrams”,\n“skip_ngrams”,\n“sentences”,\n“lines”,\n“paragraphs”,\n“regex”,\n“ptb” (Penn Treebank).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#лемматизация-и-частеречная-разметка",
    "href": "tokenize.html#лемматизация-и-частеречная-разметка",
    "title": "9  Синтаксический парсинг",
    "section": "9.2 Лемматизация и частеречная разметка",
    "text": "9.2 Лемматизация и частеречная разметка\nЛемматизация – приведение слов к начальной форме (лемме). Как правило, она сопровождается частеречной разметкой слов (POS-tagging). В R это умеет делать, например, пакет udpipe (Universal Dependencies Pipeline). Он позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных данных.\nПрежде всего нужно выбрать и загрузить модель (список); в нашем случае это модель Perseus, но можно попробовать и другие доступные на сайте https://universaldependencies.org/.\n\nlibrary(udpipe)\n\n#  скачиваем модель в рабочую директорию\nudpipe_download_model(language = \"latin-perseus\")\n\n# загружаем модель\nlatin_perseus &lt;- udpipe_load_model(file = \"latin-perseus-ud-2.5-191206.udpipe\")\n\n# аннотируем\ncaesar_annotate &lt;- udpipe_annotate(latin_perseus, caesar$text)\n\nРезультат возвращается в формате CONLL-U; это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. Вот пример разбора предложения:\n\nCтроки слов содержат следующие поля:\n\nID: индекс слова, целое число, начиная с 1 для каждого нового предложения; может быть диапазоном токенов с несколькими словами.\nFORM: словоформа или знак препинания.\nLEMMA: Лемма или основа словоформы.\nUPOSTAG: универсальный тег части речи.\nXPOSTAG: тег части речи для конкретного языка.\nFEATS: список морфологических характеристик.\nHEAD: заголовок текущего токена, который является либо значением ID, либо нулем (0).\nDEPREL: Universal Stanford dependency relation к (root iff HEAD = 0) или определенному зависящему от языка подтипу.\nDEPS: Список вторичных зависимостей.\nMISC: любая другая аннотация.\n\nДля работы данные удобнее трансформировать в прямоугольный формат.\n\ncaesar_pos &lt;- as_tibble(caesar_annotate) |&gt; \n  select(-paragraph_id)\n\ncaesar_pos",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#обучение-модели",
    "href": "tokenize.html#обучение-модели",
    "title": "9  Синтаксический парсинг",
    "section": "9.3 Обучение модели",
    "text": "9.3 Обучение модели\nМожно заметить, что модель Perseus 2.5 справилась не безупречно: все бельги оказались женского рода, а кельты и вовсе признаны глаголом. Есть ошибки в падежах и числах: например, “provinciae” в четвертом предложении, конечно, не именительный, а родительный падеж. Множество топонимов не опознано в качестве имен собственных.\nЗдесь есть два пути. Первый: пробовать другие модели, доступные в пакете udpipe. Например, для латыни это PROIEl, обученная не только на классических авторах, но и на Вульгате, или ITTB, обученная на сочинениях Фомы.\nВторой путь - обучить модель самостоятельно. Например, для трибанка Perseus доступны более свежие версии (2.13 на момент написания этой главы) на GitHub. Вот некоторые изменения:\n\nпоявилась метка dep_rel для ablativus absolutus (advcl:abs);\nисправлены аннотации для супина (VerbForm=Conv, Aspect=Prosp), а также герундия и герундива (VerbForm=Part, Aspect=Prosp);\nдобавлен тип для местоимения (PronType) и вид для глагола (Aspect) и др.\n\nИнструкцию по обучению модели можно найти здесь. По сути трибанк представляет собой коллекцию проверенных вручную CONLL-U файлов, которые передаются нейросети. Следуя этой инструкции и используя трибанк Perseus 2.13, мы обучили новую модель (это заняло около 8 часов на персональном компьютере), которую можно загрузить и использовать для аннотации.\nНадо иметь в виду, что само по себе обновление трибанка еще не гарантирует того, что модель будет лучше справляться с парсингом: многое зависит от параметров обучения. В нашем случае, впрочем, некоторые улучшения есть: например, “provinciae” корректно опознано как родительный падеж. Но есть и потери: “fortissimi” в том же предложении выше - это nominativus pluralis, который ошибочно опознан как генитив единственного числа.\n\nlatin_perseus_new &lt;- udpipe_load_model(\"../latin_model/la_perseus-2.13-20231115.udpipe\")\n\ncaesar_annotate2 &lt;- udpipe_annotate(latin_perseus_new, caesar$text[1])\n\ncaesar_pos2 &lt;- as_tibble(caesar_annotate2) |&gt; \n  select(-paragraph_id)\n\n\ncaesar_pos2\n\n\n  \n\n\n\nПока для наших задач достигнутой точности хватит, но можно попробовать построить нейросеть с более сложной архитектурой. Например, в 2024 г. такая архитектура была предложена и для латинского языка.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-upos",
    "href": "tokenize.html#поле-upos",
    "title": "9  Синтаксический парсинг",
    "section": "9.4 Поле UPOS",
    "text": "9.4 Поле UPOS\nМорфологическая аннотация, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, местоимения.\n\ncaesar_pos2 |&gt; \n  filter(upos == \"PRON\") |&gt; \n  select(token, lemma, upos, xpos)\n\n\n  \n\n\n\nПосчитать части речи можно так:\n\nupos_counts &lt;- caesar_pos2 |&gt; \n  group_by(upos) |&gt; \n  count() |&gt; \n  arrange(-n)\n\nupos_counts\n\n\n  \n\n\n\nСтолбиковая диаграмма позволяет наглядно представить результаты подсчетов:\n\nupos_counts |&gt; \n  ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) +\n  geom_bar(stat = \"identity\", show.legend = F) +\n  coord_flip() +\n  labs(x = NULL) +\n  theme_bw() \n\n\n\n\n\n\n\n\nОтберем наиболее частотные имена и имена собственные.\n\nnouns &lt;- caesar_pos2  |&gt; \n  filter(upos %in% c(\"NOUN\", \"PROPN\")) |&gt; \n  count(lemma) |&gt; \n  arrange(-n) \n\nnouns\n\n\n  \n\n\n\n\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(RColorBrewer)\n\npal &lt;- RColorBrewer::brewer.pal(8, \"Dark2\")\n\nwordcloud(nouns$lemma, nouns$n, colors = pal, max.words = 130)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-feats",
    "href": "tokenize.html#поле-feats",
    "title": "9  Синтаксический парсинг",
    "section": "9.5 Поле FEATS",
    "text": "9.5 Поле FEATS\nДопустим, нам нужны не все местоимения, а лишь определенные их формы: например, относительные.\n\nrel_pron &lt;- caesar_pos2  |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) \n\nrel_pron\n\n\n  \n\n\n\nПосмотрим на некоторые местоимения в контексте. Для этого добавим html-теги:\nhighlight_string &lt;- function(idx) str_replace_all(\n  rel_pron$sentence[idx], \n  str_glue(\"(?&lt;= ){rel_pron$token[idx]}(?=\\\\W)\"),\n  str_glue(\"&lt;mark&gt;{rel_pron$token[idx]}&lt;/mark&gt;\"))\n\nhighlight_string(1)\n[1] “Gallia est omnis divisa in partes tres, quarum unam incolunt Belgae, aliam Aquitani, tertiam qui ipsorum lingua Celtae, nostra Galli appellantur.”\nhighlight_string(13)\n[1] “In eo itinere persuadet Castico, Catamantaloedis filio, Sequano, cuius pater regnum in Sequanis multos annos obtinuerat et a senatu populi Romani amicus appellatus erat, ut regnum in civitate sua occuparet, quod pater ante habuerit;”",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-xpos",
    "href": "tokenize.html#поле-xpos",
    "title": "9  Синтаксический парсинг",
    "section": "9.6 Поле XPOS",
    "text": "9.6 Поле XPOS\nЧтение xpos требует сноровки: например причастие sublata там описывается так: v-srppfb-, где\n\nv = verbum;\n- на месте лица;\ns = singularis;\nr = perfectum (не p, потому что p = praesens);\np = participium;\np = passivum;\nf = femininum;\nb = ablativus (не a, потому что a = accusativus).\n\nСравним с описанием личной формы глагола differunt v3ppia---:\n\nv = verbum;\n3 = 3. persona;\np = pluralis;\np = praesens;\ni = indicativus;\na = activum;\n-- на месте рода и падежа, т.к. форма неличная.\n\nПоследнее “место” (Degree) у глаголов всегда свободно; в первой книге там стоит s (superlativus) лишь у florentissimis, что явно ошибка, потому что это не глагол.\n\n\n\n\n\n\nНа заметку\n\n\n\nСпецификацию всех xpos-тегов для латинского языка можно найти по ссылке.\n\n\nДля удобства разобьем xpos на 9 столбцов.\n\ncaesar_pos2_sep &lt;- caesar_pos2 |&gt; \n  separate(xpos, into = c(\"POS\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"persona\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"numerus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"tempus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"modus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"vox\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"genus\", \"xpos\"), sep = 1) |&gt; \n  separate(xpos, into = c(\"casus\", \"gradus\"), sep = 1)\n\ncaesar_pos2_sep\n\n\n  \n\n\n\nЭти столбцы тоже можно использовать для поиска конкретных признаков. Посмотрим, например, в каком числе и падеже чаще всего стоит относительное местоимения.\n\npron_rel_sum &lt;- caesar_pos2_sep  |&gt; \n  filter(upos == \"PRON\") |&gt; \n  filter(str_detect(feats, \"PronType=Rel\")) |&gt; \n  group_by(numerus, casus) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\npron_rel_sum\n\n\n  \n\n\n\nДля удобства преобразуем сокращения.\n\npron_rel_sum &lt;- pron_rel_sum |&gt; \n  filter(casus != \"-\") |&gt; \n  mutate(casus = case_when(casus == \"n\" ~ \"nom\",\n                           casus == \"g\" ~ \"gen\",\n                           casus == \"d\" ~ \"dat\",\n                           casus == \"a\" ~ \"acc\",\n                           casus == \"b\" ~ \"abl\")) |&gt; \n  mutate(numerus = case_when(numerus == \"s\" ~ \"sing\",\n                              numerus == \"p\" ~ \"plur\"))\n\npron_rel_sum\n\n\n  \n\n\n\nФункция facet_wrap позволяет разбить график на две части на основании значения переменной numerus.\n\npron_rel_sum |&gt; \n  ggplot(aes(casus, n, fill = casus)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  coord_flip() +\n  theme_light() +\n  facet_wrap(~numerus) +\n  labs(x = NULL, y = NULL, title = \"Относительные местоимения в BG 1-7\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#поле-dep_rel",
    "href": "tokenize.html#поле-dep_rel",
    "title": "9  Синтаксический парсинг",
    "section": "9.7 Поле DEP_REL",
    "text": "9.7 Поле DEP_REL\nАналогичным образом можно отбирать синтаксические признаки и их комбинации, а также визуализировать деревья зависимостей для отдельных предложений.\nДерево зависимостей – это направленный граф, который имеет единственную корневую вершину (сказуемое главного предложения) без входящих дуг (рёбер), при этом все остальные вершины имеют ровно одну входящую дугу. Иными словами, каждое слово зависит от другого, но только от одного. Это выглядит примерно так:\n\nlibrary(textplot)\n\nsent &lt;- caesar_pos |&gt; \n  filter(doc_id == \"doc1\", sentence_id == 10) \n\nsent |&gt; \n  distinct(sentence) |&gt; \n  pull(sentence) \n\n[1] \"Apud Helvetios longe nobilissimus fuit et ditissimus Orgetorix.\"\n\ntextplot_dependencyparser(sent, size = 3)\n\n\n\n\n\n\n\n\nМожно поспорить с тем, что nobilissiumus и ditissimus - это глаголы, хотя модель Perseus 2.5 верно опознала их в качестве именной части сказуемого при подлежащем “Оргеториг”. Информация, которая на графе представлена стрелками, хранится в таблице в полях token_id и head_token_id и dep_rel. Корневой токен всегда имеет значение 0, то есть ни от чего не зависит.\n\nsent |&gt; \n  select(token_id, token, head_token_id, dep_rel)\n\n\n  \n\n\n\n\nПравила синтаксической разметки для латинского языка доступны по ссылке, а расшифровку сокращений (для всех языков) надо смотреть здесь.\n\nВообще латынь (как и древнегреческий ) – не очень ресурсный язык; для многих языков доступны хорошие предобученные модели.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "tokenize.html#совместная-встречаемость-слов",
    "href": "tokenize.html#совместная-встречаемость-слов",
    "title": "9  Синтаксический парсинг",
    "section": "9.8 Совместная встречаемость слов",
    "text": "9.8 Совместная встречаемость слов\nФункция cooccurence() из пакета udpipe позволяет выяснить, сколько раз некий термин встречается совместно с другим термином, например:\n\nслова встречаются в одном и том же документе/предложении/параграфе;\nслова следуют за другим словом;\nслова находятся по соседству с другим словом на расстоянии n слов.\n\nКод ниже позволяет выяснить, какие существительные встречаются в одном предложении:\n\ncaesar_subset &lt;-  subset(caesar_pos2, upos == \"NOUN\")\ncooc &lt;- cooccurrence(caesar_subset, term = \"lemma\", group = c(\"doc_id\", \"sentence_id\")) |&gt;   as_tibble() |&gt; \n  filter(cooc &gt; 25)\n\ncooc\n\n\n  \n\n\n\nЭтот результат легко визуализировать, используя пакет ggraph (подробнее о нем мы будем говорить в следующих уроках):\n\nlibrary(igraph)\nlibrary(ggraph)\n\nwordnetwork &lt;- graph_from_data_frame(cooc)\nggraph(wordnetwork, layout = \"fr\") +\n  geom_edge_link(aes(width = cooc), alpha = 0.8, edge_colour = \"grey90\", show.legend=FALSE) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  theme_void() +\n  labs(title = \"Совместная встречаемость существительных\", subtitle = \"De Bello Gallico 1-7\")\n\n\n\n\n\n\n\n\nЧтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но с другими аргументами:\n\ncooc2 &lt;- cooccurrence(caesar_subset$lemma, relevant = caesar_subset$upos %in% c(\"NOUN\", \"ADJ\"), skipgram = 1) |&gt; \n  as_tibble() |&gt; \n  filter(cooc &gt; 10)\n\ncooc2\n\n\n  \n\n\n\n\nwordnetwork &lt;- graph_from_data_frame(cooc2)\n\nggraph(wordnetwork, layout = \"fr\") +\n  geom_edge_link(aes(width = cooc), edge_colour = \"grey90\", edge_alpha=0.8, show.legend = F) +\n  geom_node_label(aes(label = name), col = \"#1f78b4\", size = 4) +\n  labs(title = \"Слова, стоящие рядом в тексте\", subtitle = \"De Bello Gallico 1-7\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nHvitfeldt, Emil, и Julia Silge. 2022. Supervised Machine Learning for Text Analysis in R. Taylor; Francis.\n\n\nJockers, Matthew L. 2014. Text Analysis with R for Students of Literature. Springer.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Синтаксический парсинг</span>"
    ]
  },
  {
    "objectID": "count.html",
    "href": "count.html",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "",
    "text": "10.1 Подготовка данных\nЗа основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты:\nИсточники для этого урока доступны в библиотеке Gutengerg; чтобы их извлечь, следует выяснить gutenberg_id. Пример ниже; таким же образом можно найти id для трактатов Локка и Беркли.\n# install.packages(\"gutenbergr\")\nlibrary(gutenbergr)\nlibrary(tidyverse)\nlibrary(stringr)\n\ngutenberg_works(str_detect(author, \"Hume\"), languages = \"en\")\nКогда id найдены, gutenbergr позволяет загрузить сочинения; на этом этапе часто возникают ошибки – в таком случае надо воспользоваться одним из зеркал. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL.\nmy_corpus &lt;- gutenberg_download(meta_fields = c(\"author\", \"title\"), c(9662, 4723, 10615), mirror = \"https://www.gutenberg.org/dirs/\")\n\nmy_corpus\nВ этом тиббле хранятся все три текста, которые нам нужны. Уточнить уникальные называния можно при помощи функции distinct() из tidyverse.\nmy_corpus |&gt; \n  distinct(author)\nПрежде чем приступать к анализу, придется немного прибраться. Для этого используем инструменты tidyverse, о которых шла речь в главе про опрятные данные.\nmy_corpus &lt;- my_corpus |&gt; \n  select(-gutenberg_id) |&gt; \n  select(-title) |&gt; \n  relocate(text, .after = author) |&gt; \n  mutate(author = str_remove(author, \",.+$\")) |&gt; \n  filter(text != \"\")\n\nmy_corpus\nВ случае с Юмом отрезаем предисловия, оглавление и индексы, а также номера разделов (везде прописными). Многие слова, которые в оригинале были выделены курсивом, окружены знаками подчеркивания (_), их тоже удаляем.\nHume &lt;- my_corpus |&gt; \n  filter(author == \"Hume\")|&gt; \n  filter(!row_number() %in% c(1:25),\n         !row_number() %in% c(4814:nrow(my_corpus))) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"_\", \" \")) |&gt; \n  filter(!str_detect(text, \"SECTION .{1,4}\"))\nВ случае с Беркли отрезаем метаданные и посвящение в самом начале, а также удаляем нумерацию параграфов. Кроме того, текст содержит примечания следующего вида: Note: Vide Hobbes’ Tripos, ch. v. sect. 6., от них тоже следует избавиться.\nBerkeley &lt;- my_corpus |&gt; \n  filter(author == \"Berkeley\") |&gt; \n  filter(!row_number() %in% c(1:38)) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]+?\\\\.\", \" \"))  |&gt; \n  mutate(text = str_replace_all(text, \"\\\\[.+?\\\\]\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]+\", \" \"))\nЧто касается Локка, то здесь удаляем метаданные и оглавление в самом начале, а также посвящение и подчеркивания вокруг слов. “Письмо к читателю” уже содержит некоторые философские положения, и его можно оставить.\nLocke &lt;- my_corpus  |&gt;  \n  filter(author == \"Locke\") |&gt; \n  filter(!row_number() %in% c(1:135)) |&gt; \n  mutate(text = str_replace_all(text, \"_\", \" \")) |&gt; \n  mutate(text = str_replace_all(text, \"[[:digit:]]\", \" \"))\nСоединив обратно все три текста, замечаем некоторые орфографические нерегулярности; исправляем.\ncorpus_clean &lt;- bind_rows(Hume, Berkeley, Locke) |&gt; \n  mutate(text = str_replace_all(text, c(\"[Mm]an’s\" = \"man's\", \"[mM]en’s\" = \"men's\", \"[hH]ath\" = \"has\")))\n\ncorpus_clean\nПосле этого делим корпус на слова.\nlibrary(tidytext)\n\ncorpus_words &lt;- corpus_clean |&gt; \n  unnest_tokens(word, text)\n\ncorpus_words\nПодготовленный таким образом корпус можно скачать по ссылке.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#подготовка-данных",
    "href": "count.html#подготовка-данных",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "",
    "text": "“Опыт о человеческом разумении” Джона Локка (1690), первые две книги;\n“Трактат о принципах человеческого знания” Джорджа Беркли (1710);\n“Исследование о человеческом разумении” Дэвида Юма (1748).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#cтоп-слова",
    "href": "count.html#cтоп-слова",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.2 Cтоп-слова",
    "text": "10.2 Cтоп-слова\nБольшая часть слов, которые мы сейчас видим в корпусе, нам пока не интересна – это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. Функция anti_join() позволяет от них избавиться; в случае с английским языком список стоп-слов уже доступен в пакете tidytext; в других случаях их следует загружать отдельно. Для многих языков стоп-слова доступны в пакете stopwords.\nФункция anti_join() работает так:\n\n\nother &lt;- c(\"section\", \"chapter\", 0:40, \"edit\", 1710, \"v.g\", \"v.g.a\")\n\ncorpus_words_tidy &lt;- corpus_words  |&gt;  \n  anti_join(stop_words) |&gt; \n  filter(!word %in% other)\n\ncorpus_words_tidy\n\n\n  \n\n\n\nУборка закончена, мы готовы к подсчетам.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#абсолютная-частотность",
    "href": "count.html#абсолютная-частотность",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.3 Абсолютная частотность",
    "text": "10.3 Абсолютная частотность\nДля начала посмотрим на самые частотные слова во всем корпусе.\n\nlibrary(ggplot2)\n\ncorpus_words_tidy |&gt; \n  count(word, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder(word, n), n, fill = word)) +\n  geom_col(show.legend = F) + \n  coord_flip() \n\n\n\n\n\n\n\n\nЭтот график уже дает общее представление о тематике нашего корпуса: это теория познания, в центре которой для всех трех авторов стоит понятие “idea”.\nОднако можно заподозрить, что высокие показатели для слов “simple”, “distinct” и “powers” – это заслуга прежде всего Локка, который вводит понятия “простой идеи” и “отчетливой идеи”, а также говорит о “силах” вещей, благодаря которым они воздействуют как друг на друга, так и на разум. Силы для Локка – это причины идей, и как таковые они часто упоминаются в его тексте. Понятие врожденности (“innate”) также занимает в первую очередь его: вся первая книга “Опыта” – это опровержение теории врожденных идей. Беркли о врожденности не говорит вообще, а Юм – очень кратко.\nКроме того, хотя мы взяли только две книги из “Опыта” Локка – это самый длинный текст в нашем корпусе, что создает значительный перекос:\n\ncorpus_words_tidy |&gt; \n  group_by(author) |&gt; \n  summarise(sum = n())\n\n\n  \n\n\n\nПосмотрим статистику по отдельным авторам.\n\ncorpus_words_tidy  |&gt; \n  group_by(author) |&gt; \n  count(word, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder_within(word, n, author), n, fill = word)) +\n  geom_col(show.legend = F) + \n  facet_wrap(~author, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nНаиболее частотные слова (при условии удаления стоп-слов) дают вполне адекватное представление о тематике каждого из трех трактатов.\nСогласно Локку, объектом мышления является идея (желательно отчетливая, но тут уж как получится). Все идеи приобретены умом из опыта, направленного на либо на внешние предметы (ощущения, или чувства), либо на внутренние действия разума (рефлексия, или внутреннее чувство). Никаких врожденных идей у человека нет, изначально его душа похожа на чистый лист (антинативизм). Идеи могут быть простыми и сложными; они делятся на модусы, субстанции и отношения. К числу простых модусов относятся пространство, в котором находятся тела, а также продолжительность; измеренная продолжительность представляет собой время.\nБеркли спорит с мнением, согласно котором ум способен образовывать абстрактные идеи. В том числе, утверждает он, невозможна абстрактная идея движения, отличная от движущегося тела. Он пытается устранить заблуждение Локка, согласно которому слова являются знаками абстрактных общих идей. В мыслящей душе (которую он также называет умом и духом) существуют не абстрактные идеи, а ощущения, и существование немыслящих вещей безотносительно к их воспринимаемости совершенно невозможно. Нет иной субстанции, кроме духа; немыслящие вещи ее совершенно лишены. По этой причине нельзя допустить, что существует невоспринимающая протяженная субстанция, то есть материя. Идеи ощущений возникают в нас согласно с некоторыми правилами, которые мы называем законами природы. Действительные вещи – это комбинации ощущений, запечатлеваемые в нас могущественным духом.\nСогласно Юму, все объекты, доступные человеческому разуму, могут быть разделены на два вида, а именно: на отношения между идеями и факты. К суждениям об отношениях можно прийти благодаря одной только мыслительной деятельности, в то время как все заключения о фактах основаны на отношениях причины и действия. В свою очередь знание о причинности возникает всецело из опыта: только привычка заставляет нас ожидать наступления одного события при наступлении другого. Прояснение этого позволяет добиться большей ясности и точности в философии.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#стемминг",
    "href": "count.html#стемминг",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.4 Стемминг",
    "text": "10.4 Стемминг\nПоскольку мы не лемматизировали текст, то единственное и множественное число слова idea рассматриваются как разные токены. Один из способов справиться с этим - стемминг.\nСтемминг (англ. stemming “поиск происхождения”) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. Стемминг применяется в поисковых системах для расширения поискового запроса пользователя, является частью процесса нормализации текста. Один из наиболее популярных алгоритмов стемминга был написан Мартином Портером и опубликован в 1980 году.\nВ R стеммер Портера доступен в пакете snowball. К сожалению, он поддерживает не все языки, но русский, французский, немецкий и др. там есть. Не для всех языков, впрочем, и не для всех задач стемминг – это хорошая идея. Но попробуем применить его к нашему корпусу.\n\nlibrary(SnowballC)\n\ncorpus_stems &lt;- corpus_words_tidy |&gt; \n  mutate(stem = wordStem(word)) \n\ncorpus_stems |&gt; \n  count(stem, sort = TRUE)  |&gt; \n  slice_head(n = 15) |&gt; \n  ggplot(aes(reorder(stem, n), n, fill = stem)) +\n  geom_col(show.legend = F) + \n  coord_flip() \n\n\n\n\n\n\n\n\nВсе слова немного покромсаны, но вполне узнаваемы. При этом общее количество уникальных токенов стало значительно меньше:\n\n# до стемминга\nn_distinct(corpus_words_tidy$word)\n\n[1] 8132\n\n# после стемминга\nn_distinct(corpus_stems$stem)\n\n[1] 5229\n\n\nСтемминг применяется в некоторых алгоритмах машинного обучения, но сегодня - все реже, потому что современные компьютеры прекрасно справляются с лемматизацией.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#относительная-частотность",
    "href": "count.html#относительная-частотность",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.5 Относительная частотность",
    "text": "10.5 Относительная частотность\nАбсолютная частотность – плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте.\nCначала считаем частотность для всех токенов по авторам.\n\nauthor_word_counts &lt;- corpus_words  |&gt; \n  count(author, word, sort = T) |&gt; \n  filter(!word %in% other) |&gt; \n  ungroup()\n\nauthor_word_counts\n\n\n  \n\n\n\nЗатем - число токенов в каждой книге.\n\ntotal_counts &lt;- author_word_counts |&gt; \n  group_by(author) |&gt; \n  summarise(total = sum(n))\n\ntotal_counts\n\n\n  \n\n\n\nСоединяем два тиббла:\n\nauthor_word_counts &lt;- author_word_counts |&gt; \n  left_join(total_counts)\n\nauthor_word_counts\n\n\n  \n\n\n\nСчитаем относительную частотность:\n\nauthor_word_tf &lt;- author_word_counts |&gt; \n  mutate(tf = round((n / total), 5))\n\nauthor_word_tf\n\n\n  \n\n\n\nНаиболее частотные слова – это служебные части речи. На графике видно, что подавляющее большинство слов встречается очень редко, а слов с высокой частотностью - мало.\n\nauthor_word_tf |&gt; \n  ggplot(aes(tf, fill = author)) +\n  geom_histogram(show.legend = FALSE) +\n  facet_wrap(~author, scales = \"free_y\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#закон-ципфа",
    "href": "count.html#закон-ципфа",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.6 Закон Ципфа",
    "text": "10.6 Закон Ципфа\nПодобная картина характерна для естественных языков. Распределения слов в них подчиняются закону Ципфа. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) и утверждает следующее: если все слова языка или длинного текста упорядочить по убыванию частоты использования, частота (tf) n-го слова в списке окажется обратно пропорциональной его рангу (r) в степени α. Это значит (в самом общем случае), что если ранг увеличится в n раз, то частотность во столько же раз должна упасть: второе слово в корпусе встречается примерно в два раза реже, чем первое (Savoy 2020, 24).\n\\[tf_{r_i} = \\frac{c}{r^α_i}\\]\nЗдесь c - это константа, которая оценивается для каждого случая отдельно, как и параметр α. Иначе говоря:\n\\[ tf_{r_i} \\times r^α_i = c \\] Посмотрим на ранги и частотность первых 50 слов.\n\nauthor_word_tf_rank &lt;- author_word_tf |&gt; \n  group_by(author) |&gt; \n  mutate(rank = row_number()) \n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  coord_cartesian(xlim = c(NA, 50)) +\n  scale_x_continuous(breaks = seq(0,50,5))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nВспомнив, что логарифм дроби равен разности логарифмов числителя и знаменателя, запишем:\n\\[log(tf_{r_i}) = c - α \\times log(r_i) \\] Таким образом, мы получаем близкую к линейность зависимость, где константа c определяет точку пересечения оси y, a коэффициентα - угол наклона прямой. Графически это выглядит так:\n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЧтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом поговорим подробнее в следующих уроках):\n\nlm_zipf &lt;- lm(data = author_word_tf_rank, \n              formula = log10(tf) ~ log10(rank))\n\ncoefficients(lm_zipf)\n\n(Intercept) log10(rank) \n -0.2501025  -1.2728170 \n\n\nМы получили коэффициент наклона α чуть больше -1 (на практике точно -1 встречается редко). Добавим линию регрессии на график:\n\nauthor_word_tf_rank |&gt; \n  ggplot(aes(rank, tf, color = author)) +\n  geom_line(size = 1.1, alpha = 0.7) +\n  geom_abline(intercept = coefficients(lm_zipf)[1],\n              slope = coefficients(lm_zipf)[2], \n              linetype = 2, \n              color = \"grey50\") +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nЗдесь видно, что отклонения наиболее заметны в “хвостах” графика. Это характерно для многих корпусов: как очень редких, так и самых частотных слов не так много, как предсказывает закон Ципфа. Кроме того, внизу кривая почти всегда приобретает ступенчатый вид, потому что слова встречаются в корпусе дискретное число раз: ранг у них разный, а частотности одинаковые.\n\n\n\n\n\n\nВопрос\n\n\n\nДальше всего вправо уходит кривая Локка. Почему?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#меры-лексического-разнообразия",
    "href": "count.html#меры-лексического-разнообразия",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.7 Меры лексического разнообразия",
    "text": "10.7 Меры лексического разнообразия\nКоэффициент наклона α для кривой Ципфа колеблется в достаточно узком диапазоне между 0.7 и 2 и, как полагают, может быть связан с “когнитивным усилием” говорящего: например, для устной речи α чуть больше, для письменной - ниже. Однако рассматривать этот наклон как индивидуальную характеристику стиля не стоит: как и другие меры лексического разнообразия, он сильно коррелирует с длиной текста.\nДело в том, что редкие слова (события) встречаются очень часто; это явление известно под названием Large Number of Rare Events (LNRE). И чем длиннее текст, тем больше в нем будет редких слов, но скорость их прибавления постепенно уменьшается: чем дальше, тем сложнее встретить слово, которого еще не было.\nЧтобы в этом убедиться, взглянем на наиболее известную мера лексического разнообразия под названием type-token ratio (TTR).\n\\[ TTR(T) = \\frac{Voc(T)}{n} \\] Здесь n - общее число токенов, а Voc (т.е. словарь) - число уникальных токенов (типов).\nВ пакете languageR, написанном лингвистом Гаральдом Баайеном, есть функция, позволяющая быстро производить такие вычисления. Она требует на входе вектор, а не тиббл, поэтому для эксперимента извлечем один из текстов.\n\nlocke_words &lt;- corpus_words %&gt;% \n  filter(author == \"Locke\") %&gt;% \n  pull(word)\n\nlength(locke_words)\n\n[1] 148171\n\n\nФункция считает различные меры лексического разнообразия, из которых нас сейчас будет интересовать наклон Ципфа и TTR.\n\nlibrary(languageR)\nlocke.growth = growth.fnc(text = locke_words, size = 1000, nchunks = 40)\n\n........................................\n\ngrowth_df &lt;- locke.growth@data$data \ngrowth_df\n\n\n  \n\n\n\nБыстро визуализировать результат можно при помощи plot(locke.growth), но мы воспользуемся ggplot2.\n\nlibrary(gridExtra)\n\np1 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, Types)) + \n  geom_point(color = \"steelblue\")\n\np2 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, Zipf)) + \n  geom_point(color = \"#B44682\") +\n  ylab(\"Zipf Slope\")\n\np3 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, TypeTokenRatio)) + \n  geom_point(color = \"#81B446\")\n\np4 &lt;- growth_df |&gt; \n  ggplot(aes(Tokens, HapaxLegomena / Tokens)) + \n  geom_point(color = \"#B47846\") +\n  ylab(\"Growth Rate\")\n\ngrid.arrange(p1, p2, p3, p4, nrow=2)\n\n\n\n\n\n\n\n\nПодробнее о различных мерах лексического разнообразия см.: (Baayen 2008, 222–36) и (Savoy 2020).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#tf-idf",
    "href": "count.html#tf-idf",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.8 TF-IDF",
    "text": "10.8 TF-IDF\nНаиболее частотные слова (с низким рангом) наименее подвержены влиянию тематики, поэтому их используют для стилометрического анализа. Если отобрать наиболее частотные после удаления стоп-слов, то мы получим достаточно адекватное отражение тематики документов. Если же мы необходимо найти наиболее характерные для документов токены, то применяется другая мера, которая называется tf-idf (term frequency - inverse document frequency).\n\nЛогарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для документа. При этом относительная частотность тоже учитывается! Например, Беркли один раз упоминает “сахарные бобы”, а Локк – “миндаль”, но из-за редкой частотности tf-idf для подобных слов будет низкой.\nФункция bind_tf_idf() принимает на входе тиббл с абсолютной частотностью для каждого слова.\n\nauthor_word_tfidf &lt;- author_word_tf |&gt; \n  bind_tf_idf(word, author, n)\n\nauthor_word_tfidf\n\n\n  \n\n\n\nПосмотрим на слова с высокой tf-idf:\n\nauthor_word_tfidf |&gt; \n  select(-total) |&gt; \n  arrange(desc(tf_idf))\n\n\n  \n\n\n\nСнова визуализируем.\n\nauthor_word_tfidf |&gt; \n  arrange(-tf_idf) |&gt; \n  group_by(author) |&gt; \n  top_n(15) |&gt; \n  ungroup() |&gt; \n  ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) +\n  geom_col(show.legend = F) +\n  labs(x = NULL, y = \"tf-idf\") +\n  facet_wrap(~author, scales = \"free\") +\n  scale_x_reordered() +\n  coord_flip()\n\n\n\n\n\n\n\n\nНа такой диаграмме авторы совсем не похожи друг на друга, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "count.html#сравнение-при-помощи-диаграммы-рассеяния",
    "href": "count.html#сравнение-при-помощи-диаграммы-рассеяния",
    "title": "10  Распределения слов и анализ частотностей",
    "section": "10.9 Сравнение при помощи диаграммы рассеяния",
    "text": "10.9 Сравнение при помощи диаграммы рассеяния\nСтолбиковая диаграмма – не единственный способ сравнить частотности слов. Еще один наглядный метод – это диаграмма рассеяния с относительными частотностями. Сначала “расширим” наш тиббл.\n\nspread_freq &lt;- author_word_tf  |&gt; \n  anti_join(stop_words) |&gt; \n  filter(!word %in% other) |&gt; \n  filter(tf &gt; 0.0001) |&gt; \n  select(-n, -total) |&gt; \n  pivot_wider(names_from = author, values_from = tf, values_fill = 0) \n\nspread_freq\n\n\n  \n\n\n\nТеперь “удлиним”.\n\nlong_freq &lt;- spread_freq |&gt; \n  pivot_longer(c(\"Hume\", \"Locke\"), names_to = \"author\", values_to = \"tf\")\n\nlong_freq\n\n\n  \n\n\n\nМожно визуализировать.\n\nlibrary(scales)\n\nlong_freq |&gt; \n  ggplot(aes(x = tf, y = Berkeley)) +\n  geom_abline(color = \"grey40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, \n              height = 0.3, color = \"darkblue\") +\n  geom_text(aes(label = word), check_overlap = TRUE, \n            vjust = 1.5, color = \"grey30\") +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  facet_wrap(~author, ncol = 2) +\n  theme(legend.position = \"none\") +\n  theme_minimal() +\n  labs(y = \"Berkeley\", x = NULL)\n\n\n\n\n\n\n\n\nСлова, расположенные ближе к линии, примерно одинаково представлены в обоих текстах (например, “ум” и “душа” у Беркли и Локка); слова, которые находятся дальше от линии, более свойственны одному из двух авторов: например, у Беркли чаще встречается “абстрактный” по сравнению с первой книгой Локка, а у Локка чаще используется слово “простой”.\n\n\n\n\nBaayen, R. H. 2008. Analyzing Linguistic Data: A Practical Introduction to Statistics using R. Cambridge University Press.\n\n\nSavoy, Jacques. 2020. Machine Learning Methods for Stylometry. Springer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Распределения слов и анализ частотностей</span>"
    ]
  },
  {
    "objectID": "sentiment.html",
    "href": "sentiment.html",
    "title": "11  Эмоциональная тональность",
    "section": "",
    "text": "11.1 Анализ тональности\nАнализ тональности текста (англ. Sentiment analysis) — задача компьютерной лингвистики, заключающаяся в определении эмоциональной окраски (тональности) текста и, в частности, в выявлении эмоциональной оценки авторов по отношению к объектам, описываемым в тексте.\nАнализ тональности – это частный случай бинарной (позитивная / негативная) или многоклассовой (радость / гнев / обида и т.п.) классификации, хотя иногда бывает также необходимо оценить эмоциональную окрашенность текста по заданной шкале.\nО том, как в решении подобных задач могут быть полезны методы машинного обучения, мы поговорим в следующих уроках, а здесь речь пойдет о достаточно простом и в то же време эффективном подходе, основанном на тональных словарях (англ. affective lexicons). Тональный словарь представляет из себя список слов со значением тональности для каждого слова.\nСравнивая текст (или отрывок текста) со словарем, мы можем вычислить тональность для всего текста (или отрывка). Словари эмоциональной тональности размечаются вручную, полуавтоматически или автоматически на основании уже существующих тезаурусов, при этом используются различные шкалы:\nВ некоторых случаях дополнительно вводятся различия между оценочной лексикой (“неряшливый”) и негативным фактом (“кража”) и т.п.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#анализ-тональности",
    "href": "sentiment.html#анализ-тональности",
    "title": "11  Эмоциональная тональность",
    "section": "",
    "text": "бинарную: negative / positive (-1 / 1)\nтринарную: бинарная + 0 (neutral)\nранжированную: например, от -5 до 5",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#лексиконы-для-русского-языка",
    "href": "sentiment.html#лексиконы-для-русского-языка",
    "title": "11  Эмоциональная тональность",
    "section": "11.2 Лексиконы для русского языка",
    "text": "11.2 Лексиконы для русского языка\nПакет с лексиконами устанавливается напрямую из GitHub.\n\nremotes::install_github(\"dmafanasyev/rulexicon\")\n\nНачало работы.\n\nlibrary(rulexicon)\nlibrary(tidyverse)\nlibrary(tidytext)\n\nРусский язык входит в языков, для которых Й. Чен и С. Скиена собрали оценочную лексику (Chen и Skiena 2014). Их лексикон построен на основе графа знаний, связывающего слова на разных языках (на основе Wiktionary, Google Translate, транслитерационных ссылок и WordNet). Слова оцениваются по бинарной шкале ( -1 / 1).\n\nset.seed(0211)\nchen_skiena &lt;- hash_sentiment_chen_skiena\nsample_n(chen_skiena, 10)\n\n\n  \n\n\n\nДля русского языка в свободном доступе находится “РуСентиЛекс” («Создание лексикона оценочных слов русского языка РуСентилекс» 2016). Он содержит около 15000 уникальных слов или фраз, среди которых оценочные слова, а также слова и выражения, не передающие оценочное отношения автора, но имеющие положительную или отрицательную ассоциацию (коннотацию). Возможные значения переменной sentiment: neutral, positive, negative, positive/negative.\n\nset.seed(1102)\nrusenti2017 &lt;- hash_rusentilex_2017\nsample_n(rusenti2017, 10) |&gt; \n  select(-source, -token)\n\n\n  \n\n\n\nПри работе с этим лексиконом следует учитывать, что для отдельных слов он содержит несколько вхождений, как положительных, так и отрицательных, например:\n\nrusenti2017 %&gt;% \n  filter(token == \"нежный\") |&gt; \n  select(-source, -token)\n\n\n  \n\n\n\nСловарь AFINN содержит 7268 оценочных слов. Их тональность оценивается по шкале от -5 (крайне негативная) до 5 (в высшей степени положительная). Например, слово “адский” имеет оценку -5, а слово “ангельский” – +5.\n\nset.seed(0211)\nafinn &lt;- hash_sentiment_afinn_ru\nsample_n(afinn, 10)\n\n\n  \n\n\n\nNRC** для русского языка – это переведенная версия списка положительных и отрицательных слов Mohammad & Turney (2010). Таблица содержит 5179 слов с не нейтральными оценками. Бинарная шкала: -1 / 1.\n\nset.seed(1102)\nnrc &lt;- hash_sentiment_nrc_emolex_ru\nsample_n(nrc, 10)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#анализ-тональности-опрятный-подход",
    "href": "sentiment.html#анализ-тональности-опрятный-подход",
    "title": "11  Эмоциональная тональность",
    "section": "11.3 Анализ тональности: опрятный подход",
    "text": "11.3 Анализ тональности: опрятный подход\nСогласно Silge и Robinson (2017), анализе эмоциональной тональности в духе tidy data предполагает следующий алгоритм работы:\n\nПрежде всего текст делится на токены (или лемматизируется), затем каждому токену присваивается некое значение тональности, после чего эти значения суммируются и визуализируются.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#подготовка-текста",
    "href": "sentiment.html#подготовка-текста",
    "title": "11  Эмоциональная тональность",
    "section": "11.4 Подготовка текста",
    "text": "11.4 Подготовка текста\nПрежде всего текст необходимо токенизировать, лемматизировать и привести в опрятный формат. Можно загрузить уже подготовленные данные по ссылке.\n\nload(\"../data/liza_tbl.Rdata\")\n\nРазделим весь текст “Лизы” на отрывки по 100 слов: это позволит понять, как меняется эмоциональная тональность произведения по мере развития сюжета.\n\nliza_tbl &lt;- as_tibble(liza_tbl) |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  select(lemma) |&gt;  \n  rename(token = lemma)  |&gt;  \n  mutate(chunk = round(((row_number() + 50) / 100), 0))\n\nliza_tbl\n\n\n  \n\n\n\nВ тексте чуть более 5000 слов, у нас получился 51 отрывок.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#модификация-лексикона",
    "href": "sentiment.html#модификация-лексикона",
    "title": "11  Эмоциональная тональность",
    "section": "11.5 Модификация лексикона",
    "text": "11.5 Модификация лексикона\nСовременные лексиконы могут не очень подходят для анализа классической литературы. Например, в лексиконе AFINN, доступном в пакете rulexicon, слово “старый” имеет отрицательную оценку, как и слово “чувствительный”.\nКод ниже показывает, как можно удалить слово или поменять его знак в R. Разумеется, все то же самое можно сделать вручную, сохранив лексикон локально в виде файла.\n\nlex &lt;- hash_sentiment_afinn_ru |&gt; \n  filter(token != \"старый\")\n\nlex &lt;- lex  |&gt; \n  mutate_at(vars(score), ~ case_when(token == \"чувствительный\" ~  1.7,\n                 TRUE ~ .))\n\nlex |&gt; \n  filter(str_detect(token, \"чувств\"))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#соединение-лексикона-с-документом",
    "href": "sentiment.html#соединение-лексикона-с-документом",
    "title": "11  Эмоциональная тональность",
    "section": "11.6 Соединение лексикона с документом",
    "text": "11.6 Соединение лексикона с документом\nСтоп-слова, то есть слова, не несущие никакой смысловой нагрузки, нам не нужны, но удалять их отдельно нет смысла: мы соединим, при помощи функции inner_join(), документ с одним из лексиконов, в котором не будет стоп-слов. Функция inner_join() работает так:\n\n\nliza_sent &lt;- liza_tbl |&gt; \n  inner_join(lex)\n\nliza_sent\n\n\n  \n\n\n\nЗдесь “горе” – ошибка лемматизации (“стоя на сей горе…”).\nСложив положительно и отрицательно окрашенную лексику для каждого отрывка, получаем значение, позволяющее судить о доминирующей тональности:\n\nliza_chunk_sent &lt;- liza_sent |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  arrange(sum)\n\nliza_chunk_sent\n\n\n  \n\n\n\nДовольно неожиданно, что самый негативный отрывок находится не в конце повести, ближе к трагической развязке, а почти в начале (отрывок 5, ср. отрывки 3 и 4 рядом). Представим эмоционально окрашенную лексику отрывков 3-5 в виде сравнительного облака слов. Палитру берем отсюда.\n\nlibrary(reshape2)\nlibrary(wordcloud)\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\n# добавляем новый столбец для удобства визуализации\nliza_sent_class &lt;- liza_sent |&gt; \n  mutate(tone = case_when( score &gt;= 0 ~ \"pos\",\n                           score &lt; 0 ~ \"neg\"))\nset.seed(0211)\nliza_sent_class |&gt; \n  filter(chunk  %in%  c(3, 4, 5)) |&gt; \n  count(token, tone, sort = T) |&gt; \n  acast(token ~ tone, value.var = \"n\", fill = 0) |&gt; \n  comparison.cloud(colors = c(pal[1], pal[5]),\n                   max.words = 99)\n\n\n\n\n\n\n\n\nЗдесь видно, что негативная тональность в этой части не связана с судьбой героев: об этом говорят такие слова, как “лютый”, “враг”, “свирепый”. Рассказчик, глядя на заброшенный Симонов монастырь, вспоминает о “печальной истории” Москвы. Если верить нашей модели, самый мрачный фрагмент повести посвящен не судьбе бедной девушки, а “глухому стону времен”:\n\nИногда на вратах храма рассматриваю изображение чудес, в сем монастыре случившихся, там рыбы падают с неба для насыщения жителей монастыря, осажденного многочисленными врагами; тут образ богоматери обращает неприятелей в бегство. Все сие обновляет в моей памяти историю нашего отечества — печальную историю тех времен, когда свирепые татары и литовцы огнем и мечом опустошали окрестности российской столицы и когда несчастная Москва, как беззащитная вдовица, от одного бога ожидала помощи в лютых своих бедствиях.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#тональность-на-оси-времени",
    "href": "sentiment.html#тональность-на-оси-времени",
    "title": "11  Эмоциональная тональность",
    "section": "11.7 Тональность на оси времени",
    "text": "11.7 Тональность на оси времени\nТаблица, которую мы подготовили, позволяет наглядно показать, как меняется тональность во времени – разумеется, речь идет о повествовательном времени, которое измеряется не в минутах, а в словах.\nОбозначим как положительный или отрицательный каждый из отрывков, как мы это делали для слов.\n\nliza_chunk_sent &lt;- liza_chunk_sent |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_chunk_sent\n\n\n  \n\n\n\nПалитра у нас уже сохранена.\n\nlibrary(showtext)\nfont_add(family = \"vibes\", \"GreatVibes-Regular.ttf\")\nshowtext_auto()\n\nlibrary(paletteer)\npal &lt;- paletteer_d(\"rcartocolor::ArmyRose\")\n\np1 &lt;- liza_chunk_sent |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (без учета отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\np1\n\n\n\n\n\n\n\n\nВ целом график получился осмысленным. Мы уже сказали выше про отрывки 3-4. Дальше немного скорби в отрывке 8 посвящено покойному отцу Лизы. В 11-м отрывке отразилась тревога матери за судьбу дочери: “коварно”, “обидеть”, “дурной” вносят вклад в настроение этого фрагмента. Это достаточно характерно для сентиментальной прозы с ее противопоставлением пороков городской жизни и пасторальных добродетелей.\n\nУ меня всегда сердце бывает не на своем месте, когда ты ходишь в город; я всегда ставлю свечу перед образ и молю господа бога, чтобы он сохранил тебя от всякой беды и напасти.\n\nЕще два минимума: отрывки 31 и 34. В первом из них Лиза встревожена вестью о возможном замужестве с сыном крестьянина. Отрывок 34 – это падение Лизы:\n\nГрозно шумела буря, дождь лился из черных облаков — казалось, что натура сетовала о потерянной Лизиной невинности.\n\nНа графике видно, что это место гораздо более эмоционально, чем эпизод самоубийства Лизы: именно после знаменитых карамзинских многоточий и тире события устремляются к трагическому финалу. О самой смерти девушки Карамзин говорит, конечно, с грустью, но без надрыва: “Тут она бросилась в воду”.\nОтрывки 38, 39, 42 – Эраст отправляется на войну. Все, как положено, плачут, что зафиксировал и наш график.\nНаконец, в отрывках 49-51 доминирует тема смерти, причем часть этих слов относится не к самой девушке, а к ее матери.\n\nliza_sent_class %&gt;% \n  filter(chunk %in% c(49:51)) %&gt;% \n  filter(tone == \"neg\") %&gt;% \n  count(token, sort = T) %&gt;% \n  with(wordcloud(token, n, max.words = 100, colors = pal[2]))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#отрицания",
    "href": "sentiment.html#отрицания",
    "title": "11  Эмоциональная тональность",
    "section": "11.8 Отрицания",
    "text": "11.8 Отрицания\nВ отрывке 15 несколько негативных слов имеют перед собой отрицания (“не подозревая”, “никакого худого намерения” и т.п.), поэтому к числу отрицательно окрашенных он отнесен ошибочно. К сожалению, это недостаток подхода, основанного на словарях, не принимающего в учет синтаксические связи в предложении.\nОдно из самых простых решений заключается в том, что бы соединить отрицание и следующее за ним слово (или добавить отрицание ко всем словам до следующего знака препинания).\n\nneg_sent &lt;- \"Старушка с охотою приняла сие предложение, не подозревая в нем никакого худого намерения.\"\n\nstr_replace_all(neg_sent, \"( не | никакого )(\\\\w+)\", \" NEG_\\\\2\")\n\n[1] \"Старушка с охотою приняла сие предложение, NEG_подозревая в нем NEG_худого намерения.\"\n\n\nЧтобы систематически применить этот подход ко всему документу (или коллекции документов), необходим список отрицаний для выбранного языка. Список ниже не претендует на полноту, но иллюстрирует общий принцип.\n\nnegations &lt;- c(\"никто\", \"никого\", \"никем\", \"ничто\", \"ничем\", \"ничего\", \"ни\", \"никакой\", \"никакого\", \"никаких\", \"никаким\", \"никак\", \"ничей\", \"ничьих\", \"нисколько\", \"никогда\", \"нигде\", \"никуда\", \"некого\", \"нельзя\", \"нечего\", \"незачем\", \"нет\", \"едва\", \"не\", \"ничуть\")\n\nregex &lt;- str_c(negations, collapse = \" | \")\nregex &lt;- paste0(\"( \", regex, \"  )(\\\\w+)\")\nregex\n\n[1] \"( никто | никого | никем | ничто | ничем | ничего | ни | никакой | никакого | никаких | никаким | никак | ничей | ничьих | нисколько | никогда | нигде | никуда | некого | нельзя | нечего | незачем | нет | едва | не | ничуть  )(\\\\w+)\"\n\n\n\nload(\"../data/liza_tbl.Rdata\") \ntext &lt;- liza_tbl |&gt; \n  filter(upos != \"PUNCT\") |&gt; \n  pull(lemma) |&gt; \n  str_c(collapse = \" \")\n\nЗаменяем отрицания и считаем статистику по отрывкам.\n\ntext &lt;-  str_replace_all(text, regex, \" NEG_\\\\2\")\n\n\nliza_NEG &lt;- tibble(text = text) |&gt; \n  unnest_tokens(token, text) |&gt; \n  mutate(chunk = round(((row_number() + 50) / 100), 0)) |&gt; \n  inner_join(lex) \n\nJoining with `by = join_by(token)`\n\nliza_NEG_chunk &lt;- liza_NEG |&gt; \n  group_by(chunk) |&gt; \n  summarise(sum = sum(score)) |&gt; \n  mutate(tone = case_when( sum &gt;= 0 ~ \"pos\",\n                           sum &lt; 0 ~ \"neg\"))\n\nliza_NEG_chunk \n\n\n  \n\n\n\nОсталось заново построить график. Для сравнения оставим рядом старую версию.\n\nlibrary(gridExtra)\n\np2 &lt;- liza_NEG_chunk |&gt; \n  ggplot(aes(chunk, sum, fill = tone)) +\n  geom_col(show.legend = F) + \n  scale_x_continuous(breaks = seq(0, 51, 5)) + \n  labs(title = \"Эмоциональная тональность (с учетом отрицаний)\",\n       x = \"повествовательное время\",\n       y = NULL) +\n  theme_light() + \n  theme(axis.title = element_text(family = \"vibes\", size = 12, color = \"grey40\"), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_text(family = \"vibes\", size = 12, color = \"grey40\")) + \n  scale_fill_manual(values = c(pal[1], pal[5]))\n\ngrid.arrange(p1, p2, nrow = 2)\n\n\n\n\n\n\n\n\nИз-за изменения числа токенов отрывки сдвинулись, но незначительно. Бывший отрывок 15, как мы и ожидали, перешел в число положительно окрашенных (несмотря на ошибочную оценку слова “левый”).\n\n\nБыло:\n\n\n\n  \n\n\n\n\nСтало:\n\n\n\n  \n\n\n\n\n\nПомимо этого, повысилось абсолютное значение негативной тональности в последних отрывках, хотя на это повлияли не столько отрицания, сколько изменение числа слов и перераспределение их по отрывкам.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#пакет-ggpage",
    "href": "sentiment.html#пакет-ggpage",
    "title": "11  Эмоциональная тональность",
    "section": "11.9 Пакет ggpage",
    "text": "11.9 Пакет ggpage\n\nlibrary(ggpage)\n\npage_data &lt;- liza_tbl |&gt; \n  select(lemma) |&gt; \n  rename(text = lemma)  # required by ggpage_build()\n\npage_data |&gt; \n  ggpage_build(lpp = 22, character_height = 3) |&gt; \n  rename(token = word) |&gt; # required by join\n  left_join(lex) |&gt; \n  rename(text = token) |&gt; \n  mutate(neg = case_when(score &lt; 0 ~ TRUE,\n                         .default = FALSE)) |&gt; \n  ggpage_plot(aes(fill = neg), page.number = \"top-left\") +\n  labs(title = \"Негативная лексика в «Бедной Лизе»\", x = NULL, y = NULL) +\n  scale_fill_manual(values = c(pal[5], pal[1]),\n                    labels = c(\"другая\", \"негативная\"),\n                    name = NULL) + \n  theme(axis.title = element_blank(), \n        title = element_text(family = \"vibes\", size = 16, color = \"grey30\"),\n        axis.text = element_blank(),\n        text = element_text(family = \"vibes\", size = 12, color = \"grey30\"),\n        )",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "sentiment.html#p.s.-для-других-языков",
    "href": "sentiment.html#p.s.-для-других-языков",
    "title": "11  Эмоциональная тональность",
    "section": "11.10 P.S.: Для других языков",
    "text": "11.10 P.S.: Для других языков\nДля языков, которые используют латиницу, в R есть отличный пакет под названием syuzhet, разработанный Мэтью Джокерсом. Название пакета, как говорит его разработчик, подсмотрено у русских формалистов Виктора Шкловского и Владимира Проппа. Возможности и ограничения этого пакета обсуждались в специальной литературе.\n\n\n\n\nChen, Y., и S. Skiena. 2014. «Building Sentiment Lexicons for All Major Languages». Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, 383–89.\n\n\nSilge, Julia, и David Robinson. 2017. Text Mining with R. O’Reilly. http://www.tidytextmining.com.\n\n\n«Создание лексикона оценочных слов русского языка РуСентилекс». 2016. Труды конференции OSTIS-2016, 377–82.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Эмоциональная тональность</span>"
    ]
  },
  {
    "objectID": "lsa.html",
    "href": "lsa.html",
    "title": "12  Векторные представления слов",
    "section": "",
    "text": "12.1 Векторы\nВекторные представления слов - это совокупность подходов к моделированию языка, которые позволяют осуществлять семантический анализ слов и составленных из них документов. Например, находить синонимы и квазисинонимы, а также анализировать значения слов в диахронной перспективе.\nВ математике вектор – это объект, у которого есть длина и направление, заданные координатами вектора. Мы можем изобразить вектор в двумерном или трехмерном пространстве, где таких координат две или три (по числу измерений), но это не значит, что не может быть 100- или даже 1000-мерного вектора: математически это вполне возможно. Обычно, когда говорят о векторах слов, имеют в виду именно многомерное пространство.\nЧто в таком случае соответствует измерениям и координатам? Тут есть несколько подходов.\nМы можем, например, создать матрицу термин-документ, где каждое слово “описывается” вектором его встречаемости в различных документах (разделах, параграфах…). Слова считаются похожими, если “похожи” их векторы (о том, как сравивать векторы, мы скажем чуть дальше). Аналогично можно сравнивать и сами документы.\nВторой подход - зафиксировать совместную встречаемость (или другую меру ассоциации) между словами. В таком случае мы строим матрицу термин-термин. За контекст в таком случае часто принимается произвольное контекстное окно, а не целый документ. Небольшое контекстное окно (на уровне реплики) скорее сохранит больше синтаксической информации. Более широкое окно позволяет скорее судить о семантике: в таком случае мы скорее заинтересованы в словах, которые имеют похожих соседей.\nИ матрица термин-документ, и матрица термин-термин на реальных данных будут длинными и сильно разреженными (sparse), т.е. большая часть значений в них будет равна 0. С точки зрения вычислений это не представляет большой трудности, но может служить источником “шума”, поэтому в обработке естественного языка вместо них часто используют так называемые плотные (dense) векторы. Для этого к исходной матрице применяются различные методы снижения размерности.\nВ этом уроке мы рассмотрим алгоритм LSA, который использует матрицу термин-документ и снижение размерности при помощи SVD, а в следующий раз поговорим о других подходах, в том числе с использованием (поверхностных) нейросетей.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#векторы",
    "href": "lsa.html#векторы",
    "title": "12  Векторные представления слов",
    "section": "",
    "text": "As You Like It\nTwelfth Night\nJulius Caesar\nHenry V\n\n\n\n\nbattle\n1\n1\n8\n15\n\n\nsoldier\n2\n2\n12\n36\n\n\nfool\n37\n58\n1\n5\n\n\nclown\n6\n117\n0\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#латентно-семантический-анализ",
    "href": "lsa.html#латентно-семантический-анализ",
    "title": "12  Векторные представления слов",
    "section": "12.2 Латентно-семантический анализ",
    "text": "12.2 Латентно-семантический анализ\nLSA (Latent Semantic Analysis), или LSI (Latent Semantic Indexing) – это метод семантического анализа текста, который позволяет сопоставить слова и документы с некоторыми темами (топиками). Слово “latent” (англ. “скрытый”) в названии указывает на то, сами темы заранее не известны, и задача алгоритма как раз и заключается в том, чтобы их выявить.\nСоздатели метода LSA опираются на основополагающий принцип дистрибутивной семантики, согласно которому смысл слова определяется его контекстами, а смысл предложений и целых документов представляет собой своего рода сумму (или среднее) отдельных слов.\nНа входе алгоритм LSA требует матрицу термин-документ. Она может хранить сведения о встречаемости слов в документах, хотя нередко используется уже рассмотренная мера tf-idf. Это связано с тем, что не все слова (даже после удаления стоп-слов) служат хорошими показателями темы: слово “дорожное”, например, служит лучшим показателем темы, чем слово “происшествие”, которое можно встретить и в других контекстах. Tf-idf понижает веса для слов, которые присутствуют во многих документах коллекции.\nДальше мы рассмотрим общий принцип действия алгоритма на очень простом примере, после чего попытаемся его применить к реальным данным.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#lsa-на-простом-примере",
    "href": "lsa.html#lsa-на-простом-примере",
    "title": "12  Векторные представления слов",
    "section": "12.3 LSA на простом примере",
    "text": "12.3 LSA на простом примере\nДан “корпус” из пяти документов.\n\n\n\ndoc\ntext\n\n\n\n\nd1\nRomeo and Juliet.\n\n\nd2\nJuliet: O happy dagger!\n\n\nd3\nRomeo died by dagger.\n\n\nd4\n“Live free or die”, that’s the New-Hampshire’s motto.\n\n\nd5\nDid you know, New Hampshire is in New-England.\n\n\n\nПосле удаления стоп-слов термдокументная матрица выглядит так.\n\n\n\n  \n\n\n\nПо этой матрице пока нельзя сделать вывод о том, с какими темами связаны, с одной стороны, слова, а с другой - документы. Ее необходимо “переупорядочить” так, чтобы сгруппировать слова и документы по темам и избавиться от малоинформативных тем. Примерно так.\n\n\n\nИсточник.\n\n\nДля этого используется алгебраическая процедура под названием сингулярное разложение матрицы (SVD).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#сингулярное-разложение-матрицы",
    "href": "lsa.html#сингулярное-разложение-матрицы",
    "title": "12  Векторные представления слов",
    "section": "12.4 Сингулярное разложение матрицы",
    "text": "12.4 Сингулярное разложение матрицы\nПри сингулярном разложении исходная матрица \\(A_r\\) проецируется в пространство меньшей размерности, так что получается новая матрица \\(A_k\\), которая представляет собой малоранговую аппроксимацию исходной матрицы.\nДля получения новой матрицы применяется следующая процедура. Сначала для матрицы \\(A_r\\) строится ее сингулярное разложение (Singular Value Decomposition) по формуле: \\(A = UΣV^t\\) . Иными словами, одна матрица представляется в виде произведения трех других, из которых средняя - диагональная.\n\n\n\nИсточник: Яндекс Практикум\n\n\nЗдесь U — матрица левых сингулярных векторов матрицы A; Σ — диагональная матрица сингулярных чисел матрицы A; V — матрица правых сингулярных векторов матрицы A. Мы пока не будем пытаться понять, что такое сингулярные векторы с математической точки зрения; достаточно думать о них как о топиках-измерениях, которые задают пространство для наших документов.\nСтроки матрицы U соответствуют словам, при этом каждая строка состоит из элементов разных сингулярных векторов (на иллюстрации они показаны разными оттенками). Аналогично в V^t столбцы соответствуют отдельным документам. Следовательно, кажда строка матрицы U показывает, как связаны слова с топиками, а столбцы V^T – как связаны топики и документы.\nНекоторые векторы соответствуют небольшим сингулярным значениям (они хранятся в диагональной матрице) и потому хранят мало информации, поэтому на следующем этапе их отсекают. Для этого наименьшие значения в диагональной матрице заменяются нулями. Такое SVD называется усеченным. Сколько топиков оставить при усечении, решает человек.\nСобственно эмбеддингами, или векторными представлениями слова, называют произведения каждой из строк матрицы U на Σ, а эмбеддингами документа – произведение столбцов V^t на Σ. Таким образом мы как бы “вкладываем” (англ. embed) слова и документы в единое семантическое пространство, число измерений которого будет равно числу сингулярных векторов.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-в-базовом-r",
    "href": "lsa.html#svd-в-базовом-r",
    "title": "12  Векторные представления слов",
    "section": "12.5 SVD в базовом R",
    "text": "12.5 SVD в базовом R\nПрименим SVD к игрушечной термдокументной матрице, которую мы создали выше. В R для этого есть специальная функция (и не одна).\n\nmy_svd = svd(df)\n\nmy_svd\n\n$d\n[1] 2.2852979 2.0102582 1.3606993 1.1181404 0.7965768\n\n$u\n           [,1]       [,2]        [,3]        [,4]        [,5]\n[1,] -0.3961528  0.2800574  0.57117132  0.44968498  0.10183880\n[2,] -0.3142681  0.4495321 -0.41059055  0.51301824 -0.20390607\n[3,] -0.1782395  0.2689915 -0.49732052 -0.25699778 -0.04305233\n[4,] -0.4383638  0.3685083 -0.01287918 -0.57732882  0.21964021\n[5,] -0.2638806 -0.3459214 -0.14578908  0.04748488 -0.41748402\n[6,] -0.5240048 -0.2464047  0.33865227 -0.27284616 -0.15479149\n[7,] -0.2638806 -0.3459214 -0.14578908  0.04748488 -0.41748402\n[8,] -0.3263732 -0.4596688 -0.31700297  0.23724380  0.72485145\n\n$v\n           [,1]       [,2]       [,3]        [,4]        [,5]\n[1,] -0.3108657  0.3629332  0.1180134  0.86098600 -0.12813236\n[2,] -0.4073304  0.5407425 -0.6767037 -0.28735960 -0.03429449\n[3,] -0.5944614  0.2000544  0.6591790 -0.35817507  0.20925479\n[4,] -0.6030458 -0.6953914 -0.1983751  0.05309476 -0.33255810\n[5,] -0.1428143 -0.2286616 -0.2329706  0.21217712  0.90995798\n\n\nСингулярные значения меньше двух отсекаем, остается два значения. Это позволит нам визуализировать результат; в реальном исследовании используется больше измерений (от 50 до 1000 в зависимости от корпуса).\n\nmy_svd$d[3:5] &lt;- 0\n\ns &lt;- diag(my_svd$d) \n\ns\n\n         [,1]     [,2] [,3] [,4] [,5]\n[1,] 2.285298 0.000000    0    0    0\n[2,] 0.000000 2.010258    0    0    0\n[3,] 0.000000 0.000000    0    0    0\n[4,] 0.000000 0.000000    0    0    0\n[5,] 0.000000 0.000000    0    0    0\n\n\nМатрицу правых сингулярных векторов транспонируем.\n\nvt &lt;- t(my_svd$v)\n\nvt\n\n           [,1]        [,2]       [,3]        [,4]       [,5]\n[1,] -0.3108657 -0.40733041 -0.5944614 -0.60304575 -0.1428143\n[2,]  0.3629332  0.54074246  0.2000544 -0.69539140 -0.2286616\n[3,]  0.1180134 -0.67670369  0.6591790 -0.19837510 -0.2329706\n[4,]  0.8609860 -0.28735960 -0.3581751  0.05309476  0.2121771\n[5,] -0.1281324 -0.03429449  0.2092548 -0.33255810  0.9099580\n\n\nТеперь перемножим матрицы, чтобы получить эмбеддинги.\n\n# эмбеддинги слов\nu &lt;- my_svd$u\nword_emb &lt;- u %*% s |&gt; \n  round(3)\n\nrownames(word_emb) &lt;- rownames(df)\n\nword_emb\n\n                [,1]   [,2] [,3] [,4] [,5]\nromeo         -0.905  0.563    0    0    0\njuliet        -0.718  0.904    0    0    0\nhappy         -0.407  0.541    0    0    0\ndagger        -1.002  0.741    0    0    0\nlive          -0.603 -0.695    0    0    0\ndie           -1.198 -0.495    0    0    0\nfree          -0.603 -0.695    0    0    0\nnew-hampshire -0.746 -0.924    0    0    0\n\n\n\n# эмбеддинги документов\ndoc_emb &lt;- s %*% vt |&gt; \n  round(3)\n\ncolnames(doc_emb) &lt;- colnames(df)\n\ndoc_emb \n\n        d1     d2     d3     d4     d5\n[1,] -0.71 -0.931 -1.359 -1.378 -0.326\n[2,]  0.73  1.087  0.402 -1.398 -0.460\n[3,]  0.00  0.000  0.000  0.000  0.000\n[4,]  0.00  0.000  0.000  0.000  0.000\n[5,]  0.00  0.000  0.000  0.000  0.000\n\n\nДобавим условный поисковый запрос: dies, dagger. Очевидно, ближе всего к документы d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 – ни одного. Координаты поискового запроса (который рассматриваем как псевдодокумент) считаем как среднее арифметическое координат:\n\nq = c(\"die\", \"dagger\")\nq_doc &lt;-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2\nq_doc\n\n[1] -1.100  0.123  0.000  0.000  0.000\n\n\nОбъединив все в единый датафрейм, можем визуализировать.\n\nlibrary(tidyverse)\n\nplot_tbl &lt;- rbind(word_emb, t(doc_emb), q_doc) |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"item\") |&gt; \n  rename(dim1 = V1, dim2 = V2) |&gt; \n  mutate(type = c(rep(\"word\", 8), rep(\"doc\", 6))) |&gt; \n  select(!starts_with(\"V\"))\n\nplot_tbl\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nИтак, “поисковый запрос” оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#межвекторное-расстояние",
    "href": "lsa.html#межвекторное-расстояние",
    "title": "12  Векторные представления слов",
    "section": "12.6 Межвекторное расстояние",
    "text": "12.6 Межвекторное расстояние\nМы исследовали наш небольшой корпус визуально, но там, где число измерений больше двух, это просто невозможно. На практике расстояние или сходство между векторами слов (или документов) вычисляется алгебраически. Наиболее известны манхэттенское и евклидово расстояние, а также косинусное сходство. Для анализа текстовых данных как правило применяется косинусное сходство.\n\nВсе их можно посчитать в R для заданной пары векторов.\n\ndoc_mx &lt;- plot_tbl |&gt; \n  filter(row_number() &gt; 8 ) |&gt; \n  column_to_rownames(\"item\") |&gt; \n  select(dim1, dim2) |&gt; \n  as.matrix()\n\ndoc_mx\n\n        dim1   dim2\nd1    -0.710  0.730\nd2    -0.931  1.087\nd3    -1.359  0.402\nd4    -1.378 -1.398\nd5    -0.326 -0.460\nq_doc -1.100  0.123\n\n\n\ndist_mx &lt;- doc_mx |&gt; \n  philentropy::distance(method = \"cosine\", use.row.names = T) \n\ndist_mx\n\n               d1         d2        d3          d4         d5     q_doc\nd1     1.00000000  0.9979996 0.8719223 -0.02109092 -0.1817325 0.7725616\nd2     0.99799957  1.0000000 0.8392224 -0.08425530 -0.2435368 0.7308749\nd3     0.87192229  0.8392224 1.0000000  0.47114573  0.3230341 0.9845083\nd4    -0.02109092 -0.0842553 0.4711457  1.00000000  0.9869622 0.6185045\nd5    -0.18173249 -0.2435368 0.3230341  0.98696218  1.0000000 0.4839672\nq_doc  0.77256165  0.7308749 0.9845083  0.61850449  0.4839672 1.0000000\n\n\nЧтобы получить расстояние (а не сходство), вычитаем результат из единицы.\n\nround(1 - dist_mx, 3)\n\n         d1    d2    d3    d4    d5 q_doc\nd1    0.000 0.002 0.128 1.021 1.182 0.227\nd2    0.002 0.000 0.161 1.084 1.244 0.269\nd3    0.128 0.161 0.000 0.529 0.677 0.015\nd4    1.021 1.084 0.529 0.000 0.013 0.381\nd5    1.182 1.244 0.677 0.013 0.000 0.516\nq_doc 0.227 0.269 0.015 0.381 0.516 0.000\n\n\nАналогично вычисляются расстояния между словами. При желании все косинусы можно пересчитать в градусы, чтобы узнать точный угол между векторами.\n\nacos(dist_mx[3,1])  # acos для d3 и d1 (cos = 0.872)\n\n[1] 0.5116817\n\n180 * acos(dist_mx[3,1]) / pi # переводим из радиан в градусы\n\n[1] 29.3172",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#подготовка-данных",
    "href": "lsa.html#подготовка-данных",
    "title": "12  Векторные представления слов",
    "section": "12.7 Подготовка данных",
    "text": "12.7 Подготовка данных\nТеперь, когда мы поняли общий принцип работы алгоритма LSA, оценим его возможности на датасете с подборкой новостей на русском языке. Источник; копия в репозитории курса.\nДатасет содержит более 800 тыс. новостей на русском языке с",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-опрятный-подход",
    "href": "lsa.html#svd-опрятный-подход",
    "title": "12  Векторные представления слов",
    "section": "12.8 SVD: опрятный подход",
    "text": "12.8 SVD: опрятный подход",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "index.html#дополнительные-материалы",
    "href": "index.html#дополнительные-материалы",
    "title": "Компьютерный анализ текста",
    "section": "Дополнительные материалы",
    "text": "Дополнительные материалы\nЭтот курс опирается на четыре книги, к которым можно обращаться за дополнительной информацией. Все они находятся в открытом доступе.\n\n\n\n\n\n\n\n\n\n1.\n\n\n\n\n\n\n\n2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.\n\n\n\n\n\n\n\n4.",
    "crumbs": [
      "Введение"
    ]
  },
  {
    "objectID": "lsa.html#подгтовка-данных",
    "href": "lsa.html#подгтовка-данных",
    "title": "12  Векторные представления слов",
    "section": "12.7 Подгтовка данных",
    "text": "12.7 Подгтовка данных\nТеперь, когда мы поняли общий принцип работы алгоритма LSA, оценим его возможности на датасете с подборкой новостей на русском языке. Файл в формате .Rdata можно скачать в формате .Rdata по ссылке.\n\nload(\"../data/news.Rdata\")\n\nstr(news_2019)\n\ntibble [3,407 × 2] (S3: tbl_df/tbl/data.frame)\n $ text : chr [1:3407] \"Россиянам дали советы при выборе чая. Рекомендации в честь Международного дня чая, который отмечается 15 декабр\"| __truncated__ \"Спикер Госдумы Вячеслав Володин назвал угрозой суверенитету России заявление председателя Коммунистической парт\"| __truncated__ \"Украинская ЛГБТ-активистка Виктория Гуйвик обвинила известного ню-фотографа Марата Сафина в изнасиловании. Пост\"| __truncated__ \"В Москве полицейские застрелили мужчину при попытке его задержать. Об этом «Ленте.ру» сообщили в пресс-службе м\"| __truncated__ ...\n $ topic: chr [1:3407] \"Россия\" \"Россия\" \"Культура\" \"Силовые структуры\" ...\n\n\nИсходный датасет содержит почти полмиллиона новостей на русском языке с 2019 по 2023 г.; для ускорения вычислений мы взяли данные только за один год. Добавим id для документов.\n\nnews_2019 &lt;- news_2019 |&gt; \n  mutate(id = paste0(\"doc\", row_number()))\n\nОсновные темы статей выглядят так:\n\nnews_2019 |&gt; \n  group_by(topic) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n) \n\n\n  \n\n\n\nСоставим список стоп-слов.\n\nlibrary(stopwords)\nstopwords_ru &lt;- c(\n  stopwords(\"ru\", source = \"snowball\"),\n  stopwords(\"ru\", source = \"marimo\"),\n  stopwords(\"ru\", source = \"nltk\"), \n  stopwords(\"ru\", source  = \"stopwords-iso\")\n  )\n\nstopwords_ru &lt;- sort(unique(stopwords_ru))\nlength(stopwords_ru)\n\n[1] 715\n\n\nРазделим статьи на слова и удалим стоп-слова; это может занять несколько минут.\n\nlibrary(tidytext)\nnews_tokens &lt;- news_2019 |&gt; \n  unnest_tokens(token, text) |&gt; \n  filter(!token %in% stopwords_ru)\n\nДаже после удаления стоп-слов в нашем датасете осталось примерно 10 млн токенов; однако значительная их часть встречается лишь несколько раз и для тематического моделирования бесполезна. Поэтому можно от них избавиться.\n\nnews_tokens_pruned &lt;- news_tokens |&gt; \n  add_count(token) |&gt; \n  filter(n &gt; 10) |&gt; \n  select(-n)\n\nТакже избавимся от цифр, хотя стоит иметь в виду, что их пристутствие в тексте может быть индикатором темы: в некоторых случах лучше не удалять цифры, а, например, заменять их на некую последовательность символов вроде digit и т.п. Токены на латинице тоже удаляем.\n\nnews_tokens_pruned &lt;- news_tokens_pruned |&gt; \n  filter(str_detect(token, \"[\\u0400-\\u04FF]\")) |&gt; \n  filter(!str_detect(token, \"\\\\d\"))\n\n\n\nWarning in rm(news_tokens): object 'news_tokens' not found\n\n\n\nnews_tokens_pruned\n\n\n  \n\n\n\nПосмотрим на статистику по словам.\n\nnews_tokens_pruned |&gt; \n  group_by(token) |&gt; \n  summarise(n = n()) |&gt; \n  arrange(-n)\n\n\n  \n\n\n\nЭтап подготовки данных – самый трудоемкий и не самый творческий, но не стоит им пренебрегать, потому что от этой работы напрямую зависит качество модели.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#tf-idf-опрятный-подход",
    "href": "lsa.html#tf-idf-опрятный-подход",
    "title": "12  Векторные представления слов",
    "section": "12.8 TF-IDF: опрятный подход",
    "text": "12.8 TF-IDF: опрятный подход\nВместо показателей абсолютной встречаемости при анализе больших текстовых данных применяется tf-idf. Эта статистическая мера не используется, если дана матрица термин-термин, но она хорошо работает с матрицами термин-документ, позволяя повысить веса для тех слов, которые служат хорошими дискриминаторами. Например, “заявил” и “отметил”, хотя это не стоп-слова, могут встречаться в разных темах.\n\nnews_counts &lt;- news_tokens_pruned |&gt;\n  count(token, id)\n\nnews_counts\n\n\nnews_counts |&gt; \n  arrange(id)\n\n\n  \n\n\n\nДобавляем tf_idf.\n\nnews_tf_idf &lt;- news_counts |&gt; \n  bind_tf_idf(token, id, n) |&gt; \n  arrange(tf_idf) |&gt; \n  select(-n, -tf, -idf)\n\nnews_tf_idf",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#documenttermmatrix",
    "href": "lsa.html#documenttermmatrix",
    "title": "12  Векторные представления слов",
    "section": "12.9 DocumentTermMatrix",
    "text": "12.9 DocumentTermMatrix\nПосмотрим на размер получившейся таблицы.\n\nobject.size(news_tf_idf)\n\n5369712 bytes\n\nformat(object.size(news_tf_idf), units = \"auto\")\n\n[1] \"5.1 Mb\"\n\n\nЧтобы вычислить SVD, такую таблицу необходимо преобразовать в матрицу термин-документ. Оценим ее размер:\n\n# число уникальных токенов\nm &lt;- unique(news_tf_idf$token) |&gt; \n  length()\nm\n\n[1] 6299\n\n# число уникальных документов\nn &lt;- unique(news_tf_idf$id) |&gt; \n  length()  \nn\n\n[1] 3407\n\n# число элементов в матрице \nm * n\n\n[1] 21460693\n\n\nПоэтому мы используем специальный формат для хранения разреженных матриц.\n\ndtm &lt;- news_tf_idf |&gt; \n  cast_sparse(token, id, tf_idf)\n\n\n# первые 10 рядов и 5 столбцов\ndtm[1:10, 1:5]\n\n10 x 5 sparse Matrix of class \"dgCMatrix\"\n               doc608     doc1670     doc2170     doc2184     doc2219\nранее     0.003530193 .           .           0.005002585 .          \nроссии    0.010127611 0.003675658 0.004689633 0.004783897 0.005471238\nсловам    .           .           0.011776384 .           0.006869557\nрублей    0.006686328 .           0.027865190 .           .          \nрассказал 0.007250151 .           .           0.010274083 .          \nданным    0.007406320 .           .           .           0.012003345\nиздание   0.007759457 .           .           .           0.012575671\nходе      0.008675929 .           .           .           .          \nзаявил    0.016729327 .           .           .           .          \nчастности 0.008860985 .           0.012309349 .           .          \n\n\nСнова уточним размер матрицы.\n\nformat(object.size(dtm), units = \"auto\")\n\n[1] \"3 Mb\"",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#svd-с-пакетом-irlba",
    "href": "lsa.html#svd-с-пакетом-irlba",
    "title": "12  Векторные представления слов",
    "section": "12.10 SVD с пакетом irlba",
    "text": "12.10 SVD с пакетом irlba\nМетод для эффективного вычисления усеченного SVD на больших матрицах реализован в пакете irlba. Возможно, придется подождать ⏳.\n\nlibrary(irlba)\nlsa_space&lt;- irlba::svdr(dtm, 20)\n\nФункция вернет список из трех элементов:\n\nd: k аппроксимированных сингулярных значений;\nu: k аппроксимированных левых сингулярных векторов;\nv: k аппроксимированных правых сингулярных векторов.\n\nПолученную LSA-модель можно использовать для поиска наиболее близких слов и документов или для изучения тематики корпуса – в последнем случае нас может интересовать, какие топики доминируют в тех или иных документах и какими словами они в первую очередь представлены.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#эмбеддинги-слов",
    "href": "lsa.html#эмбеддинги-слов",
    "title": "12  Векторные представления слов",
    "section": "12.11 Эмбеддинги слов",
    "text": "12.11 Эмбеддинги слов\nВернем имена рядов матрице левых сингулярных векторов и добавим имена столбцов.\n\nrownames(lsa_space$u) &lt;- rownames(dtm)\ncolnames(lsa_space$u) &lt;- paste0(\"dim\", 1:20)\n\nТеперь посмотрим на эмбеддинги слов.\n\nword_emb &lt;- lsa_space$u |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"word\") |&gt; \n  as_tibble()\n\nword_emb\n\n\n  \n\n\n\nПреобразуем наши данные в длинный формат.\n\nword_emb_long &lt;- word_emb |&gt; \n  pivot_longer(-word, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\nword_emb_long",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#визуализация-топиков",
    "href": "lsa.html#визуализация-топиков",
    "title": "12  Векторные представления слов",
    "section": "12.12 Визуализация топиков",
    "text": "12.12 Визуализация топиков\nВизуализируем несколько топиков, чтобы понять, насколько они осмыслены.\n\nword_emb_long |&gt; \n  filter(dimension &lt; 10) |&gt; \n  group_by(dimension) |&gt; \n  top_n(10, abs(value)) |&gt; \n  ungroup() |&gt; \n  mutate(word = reorder_within(word, value, dimension)) |&gt; \n  ggplot(aes(word, value, fill = dimension)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~dimension, scales = \"free_y\", ncol = 3) +\n  scale_x_reordered() +\n  coord_flip() +\n  labs(\n    x = NULL, \n    y = \"Value\",\n    title = \"Первые 9 главных компонент за 2019 г.\",\n    subtitle = \"Топ-10 слов\"\n  ) +\n  scale_fill_viridis_c()",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#ближайшие-соседи",
    "href": "lsa.html#ближайшие-соседи",
    "title": "12  Векторные представления слов",
    "section": "12.13 Ближайшие соседи",
    "text": "12.13 Ближайшие соседи\nЭмбеддинги можно использовать для поиска ближайших соседей.\n\nlibrary(widyr)\nnearest_neighbors &lt;- function(df, feat, doc=F) {\n  inner_f &lt;- function() {\n    widely(\n        ~ {\n          y &lt;- .[rep(feat, nrow(.)), ]\n          res &lt;- rowSums(. * y) / \n            (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[feat, ] ^ 2)))\n          \n          matrix(res, ncol = 1, dimnames = list(x = names(res)))\n        },\n        sort = TRUE\n    )}\n  if (doc) {\n    df |&gt; inner_f()(doc, dimension, value) }\n  else {\n    df |&gt; inner_f()(word, dimension, value)\n  }\n}\n\n\nnearest_neighbors(word_emb_long, \"посол\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  },
  {
    "objectID": "lsa.html#похожие-документы",
    "href": "lsa.html#похожие-документы",
    "title": "12  Векторные представления слов",
    "section": "12.14 Похожие документы",
    "text": "12.14 Похожие документы\nИнформация о документах хранится в матрице правых сингулярных векторов.\n\nrownames(lsa_space$v) &lt;- colnames(dtm)\ncolnames(lsa_space$v) &lt;- paste0(\"dim\", 1:20)\n\nПосмотрим на эмбеддинги документов.\n\ndoc_emb &lt;- lsa_space$v |&gt; \n  as.data.frame() |&gt; \n  rownames_to_column(\"doc\") |&gt; \n  as_tibble()\n\ndoc_emb\n\n\n  \n\n\n\nПреобразуем в длинный формат.\n\ndoc_emb_long &lt;- doc_emb |&gt; \n  pivot_longer(-doc, names_to = \"dimension\", values_to = \"value\") |&gt;\n  mutate(dimension = as.numeric(str_remove(dimension, \"dim\")))\n  \n\ndoc_emb_long\n\n\n  \n\n\n\nИ найдем соседей для произвольного документа.\n\nnearest_neighbors(doc_emb_long, \"doc14\", doc = TRUE)\n\n\n  \n\n\n\nВыведем документ 14 вместе с его соседями 1893 и 2043.\n\nnews_2019 |&gt; \n  filter(id %in% c(\"doc14\", \"doc1893\", \"doc2043\")) |&gt; \n  mutate(text = str_trunc(text, 70)) \n\n\n  \n\n\n\nПоздравляем, вы построили свою первую рекомендательную систему 🍸.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Векторные представления слов</span>"
    ]
  }
]